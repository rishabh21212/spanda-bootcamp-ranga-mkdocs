
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../14_Federated_Learning_Implementation_-_Client_Side/">
      
      
        <link rel="next" href="../16_Introducing_Existing_Federated_Learning_Frameworks/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.9">
    
    
      
        <title>15 Federated Learning Running the System & Analyzing Results - Spanda DL Bootcamp</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.f2e4d321.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#section-17" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Spanda DL Bootcamp" class="md-header__button md-logo" aria-label="Spanda DL Bootcamp" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Spanda DL Bootcamp
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              15 Federated Learning Running the System &amp; Analyzing Results
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-hidden="true"  type="radio" name="__palette" id="__palette_2">
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  The Complete Generative AI Bootcamp

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../01_Course_Vision_and_Running_a_Local_LLM/" class="md-tabs__link">
          
  
  Lecturewise mkdocs

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Spanda DL Bootcamp" class="md-nav__button md-logo" aria-label="Spanda DL Bootcamp" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Spanda DL Bootcamp
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The Complete Generative AI Bootcamp
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Lecturewise mkdocs
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Lecturewise mkdocs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01_Course_Vision_and_Running_a_Local_LLM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    01 Course Vision and Running a Local LLM
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02_Software_Engineering_for_AI/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    02 Software Engineering for AI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03_Ops_-_DevOps%2C_MLOps%2C_AIOps/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    03 Ops   DevOps, MLOps, AIOps
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04_Data_Collection%2C_Data_Labeling%2C_Data_Management%2C_Analytics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    04 Data Collection, Data Labeling, Data Management, Analytics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05_Testing_and_Deployment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    05 Testing and Deployment
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06.5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    06.5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06_LLM_Serving_and_Reinforcement_Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    06 LLM Serving and Reinforcement Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07_Distributed_Systems_Patterns/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    07 Distributed Systems Patterns
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08_Workflow_Patterns/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    08 Workflow Patterns
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09_Scheduling_%26amp%3B_Metadata_Patterns/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    09 Scheduling &amp; Metadata Patterns
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10_Data_Lakes_%26amp%3B_Intro_to_Federated_Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10 Data Lakes &amp; Intro to Federated Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_Federated_Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11 Federated Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12_Federated_Learning_System_Details/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12 Federated Learning System Details
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13_Federated_Learning_Implementation_-_Server_Side/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13 Federated Learning Implementation   Server Side
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14_Federated_Learning_Implementation_-_Client_Side/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14 Federated Learning Implementation   Client Side
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    15 Federated Learning Running the System &amp; Analyzing Results
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    15 Federated Learning Running the System &amp; Analyzing Results
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#section-17" class="md-nav__link">
    <span class="md-ellipsis">
      Section 17
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../16_Introducing_Existing_Federated_Learning_Frameworks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16 Introducing Existing Federated Learning Frameworks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../17_Federated_Learning_Use_Cases_and_Case_Studies/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17 Federated Learning Use Cases and Case Studies
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18_Federated_Learning_Future_Directions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18 Federated Learning Future Directions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19_LLMs_and_NLP_Introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    19 LLMs and NLP Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../20_Retrieval_Augmented_Generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    20 Retrieval Augmented Generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../21_RAG_Pipeline_Implementation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    21 RAG Pipeline Implementation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../22_From_Simple_to_Advanced_RAG/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    22 From Simple to Advanced RAG
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../23_RAG_Observability/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    23 RAG Observability
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  <h1>15 Federated Learning Running the System &amp; Analyzing Results</h1>

<h2 id="section-17"><strong><span style="text-decoration:underline;">Section 17</span></strong></h2>
<p><strong>Running the Federated Learning System and Analyzing the Results</strong></p>
<ul>
<li>
<p>Here, you will run the <strong>federated learning</strong> (<strong>FL</strong>) system that
    has been discussed in previous sections and analyze the system
    behaviors and the outcomes of the aggregated models.</p>
</li>
<li>
<p>We will start by explaining the configuration of the FL system
    components in order to run the systems properly.</p>
</li>
<li>
<p>Basically, after installing the simple FL system provided by our
    GitHub sample, you first need to pick up the server machines or
    instances to run the database and aggregator modules.</p>
</li>
<li>
<p>Then, you can run agents to connect to the aggregator that is
    already running. The IP address of the aggregator needs to be
    correctly set up in each agent-side configuration.</p>
</li>
<li>
<p>Also, there is a simulation mode so that you can run all the
    components on the same machine or laptop to just test the
    functionality of the FL system. After successfully running all the
    modules of the FL system, you will be able to see the data folder
    and a database created under the path that you set up in the
    database server as well as on the agent side.</p>
</li>
<li>
<p>You will be able to check both the local and global models, trained
    and aggregated, so that you can download the recent or
    best-performing models from the data folders.</p>
</li>
<li>
<p>In addition, you can also see examples of running the FL system on a
    minimal engine and image classification. By reviewing the outcomes
    of the generated models and the performance data, you can understand
    the aggregation algorithms as well as the actual interaction of the
    models between an aggregator and agents.</p>
</li>
<li>
<p>Here, we will cover the following main topics:</p>
</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   Configuring and running the FL system</p>
<ul>
<li>
<p>Understanding what happens when the minimal example runs</p>
</li>
<li>
<p>Running image classification and analyzing the results</p>
</li>
</ul>
<p>Technical requirements</p>
<ul>
<li>All the code files introduced Here can be found on GitHub
    (https://github.com/keshavaspanda/simple-fl).</li>
</ul>
<p>Configuring and running the FL system</p>
<ul>
<li>Configuring the FL system and installing its environment are simple
    enough to do. Follow the instructions in the next subsections.</li>
</ul>
<p>Installing the FL environment</p>
<ul>
<li>First, to run the FL system discussed in the previous section, clone
    the following repository to the machines that you want to run FL on
    using the following command:</li>
</ul>
<p><strong>git clone https://github.com/keshavaspanda/simple-fl</strong></p>
<p>CopyExplain</p>
<ul>
<li>
<p>Once done with the cloning process, change the directory to
    the simple-fl folder in the command line. The simulation run can be
    carried out using just one machine or using multiple systems. In
    order to run the FL process on one or multiple machines that include
    the FL server (aggregator), FL client (agent), and database server,
    you should create a conda virtual environment and activate it.</p>
</li>
<li>
<p>To create a conda environment in macOS, you will need to type the
    following command:</p>
</li>
</ul>
<p>conda env create -n federatedenv -f ./setups/federatedenv.yaml</p>
<p>CopyExplain</p>
<ul>
<li>If you're using a Linux machine, you can create
    the conda environment by using the following command:</li>
</ul>
<p>conda env create -n federatedenv -f ./setups/federatedenv_linux.yaml</p>
<p>CopyExplain</p>
<ul>
<li>
<p>Then, activate the conda environment federatedenv when you run the
    code. For your information,
    the federatedenv.yaml and federatedenv_linux.yaml files can be found
    in the setups folder of the simple-fl GitHub repository and include
    the libraries that are used in the code examples throughout this
    book.</p>
</li>
<li>
<p>As noted in the README file of the GitHub repo, there are mainly
    three components to run: the database server, aggregator, and
    agent(s). If you want to conduct a simulation within one machine,
    you can just install a conda environment (federatedenv) on that
    machine.</p>
</li>
<li>
<p>If you want to create a distributed environment, you need to install
    the conda environment on all the machines you want to use, such as
    the database server on a cloud instance, the aggregator server on a
    cloud instance, and the local client machine.</p>
</li>
<li>
<p>Now that the installation process for the entire FL process is
    ready, let's move on to configuring the FL system with configuration
    files.</p>
</li>
</ul>
<p>Configuring the FL system with JSON files for each component</p>
<ul>
<li>First, edit the configuration JSON files in the setups folder of the
    provided GitHub repository. These JSON files are read by a database
    server, aggregator, and agents to configure their initial setups.
    Again, the configuration details are explained as follows.</li>
</ul>
<p>config_db.json</p>
<ul>
<li>The config_db.json file deals with configuring a database server.
    Use the following information to properly operate the server:</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   db_ip: The database server's IP address (for example, localhost). If
    you want to run the database server on a cloud instance, such as
    an <strong>Amazon Web Services</strong> (<strong>AWS</strong>) EC2 instance, you can specify
    the private IP address of the instance.</p>
<ul>
<li>
<p>db_socket: The socket number used between the database and
    aggregator (for example, 9017).</p>
</li>
<li>
<p>db_name: The name of the SQLite database (for example, sample_data).</p>
</li>
<li>
<p>db_data_path: The path to the SQLite database (for example, ./db).</p>
</li>
<li>
<p>db_model_path: The path to the directory to save all <strong>Machine
    Learning</strong> (<strong>ML</strong>) models (for example, ./db/models).</p>
</li>
</ul>
<p>config_aggregator.json</p>
<ul>
<li>The config_aggregator.json file deals with configuring an aggregator
    in the FL server. Use the following information to properly operate
    the aggregator:</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   aggr_ip: The aggregator's IP address (for example, localhost). If
    you want to run the aggregator server on a cloud instance, such as
    an AWS EC2 instance, you can specify the private IP address of the
    instance.</p>
<ul>
<li>
<p>db_ip: The database server's IP address (for example, localhost). If
    you want to connect to the database server hosted on a different
    cloud instance, you can specify the public IP address of the
    database instance. If you host the database server on the same cloud
    instance as the aggregator's instance, you can specify the same
    private IP address of the instance.</p>
</li>
<li>
<p>reg_socket: The socket number used by agents to connect to an
    aggregator for the first time (for example, 8765).</p>
</li>
<li>
<p>recv_socket: The socket number used to upload local models or poll
    to an aggregator from an agent. Agents will learn this socket
    information by communicating with an aggregator (for example, 7890).</p>
</li>
<li>
<p>exch_socket: The socket number used to send global models back to an
    agent from an aggregator when a push method is used. Agents will
    learn this socket information by communicating with an aggregator
    (for example, 4321).</p>
</li>
<li>
<p>db_socket: The socket number used between the database and an
    aggregator (for example, 9017).</p>
</li>
<li>
<p>round_interval: The period of time after which an agent checks
    whether there are enough models to start an aggregation step (unit:
    seconds; for example, 5).</p>
</li>
<li>
<p>aggregation_threshold: The percentage of collected local models
    required to start an aggregation step (for example, 0.85).</p>
</li>
<li>
<p>polling: The flag to specify whether to use a polling method or not.
    If the flag is 1, use the polling method; if the flag is 0, use a
    push method. This value needs to be the same between the aggregator
    and agent.</p>
</li>
</ul>
<p>config_agent.json</p>
<ul>
<li>The config_agent.json file deals with configuring an agent in the FL
    client. Use the following information to properly operate the agent:</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   aggr_ip: The aggregator server's IP address (for
    example, localhost). If you want to connect to the aggregator server
    hosted on a cloud instance, such as an AWS EC2 instance, you can
    specify the public IP address of the aggregator instance.</p>
<ul>
<li>
<p>reg_socket: The socket number used by agents to join an aggregator
    for the first time (for example, 8765).</p>
</li>
<li>
<p>model_path: The path to a local director in the agent machine to
    save local and global models and some state information (for
    example, ./data/agents).</p>
</li>
<li>
<p>local_model_file_name: The filename to save local models in the
    agent machine (for example, lms.binaryfile).</p>
</li>
<li>
<p>global_model_file_name: The filename to save local models in the
    agent machine (for example, gms.binaryfile).</p>
</li>
<li>
<p>state_file_name: The filename to store the agent state in the agent
    machine (for example, state).</p>
</li>
<li>
<p>init_weights_flag: 1 if the weights are initialized with certain
    values, 0 otherwise, where weights are initialized with zeros.</p>
</li>
<li>
<p>polling: The flag to specify whether to use a polling method or not.
    If the flag is 1, use the polling method; if the flag is 0, use a
    push method. This value needs to be the same between the aggregator
    and agent.</p>
</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   Now, the FL systems can be configured using the configuration files
    explained in this section. Next, you will run the database and
    aggregator on the FL server side.</p>
<p>Running the database and aggregator on the FL server</p>
<ul>
<li>In this section, you will configure the database and aggregator on
    the FL server side. Then, you will edit the configuration files in
    the setups folder of the simple-fl GitHub repo. After that, you will
    run pseudo_db first, and then server_th, as follows:</li>
</ul>
<blockquote>
<p>python -m fl_main.pseudodb.pseudo_db</p>
<p>python -m fl_main.aggregator.server_th</p>
</blockquote>
<p>CopyExplain</p>
<p>Important note</p>
<ul>
<li>
<p>If the database server and aggregator server are running on
    different machines, you will need to specify the IP address of the
    database server or instance of the aggregator. The IP address of the
    database server can be modified in the config_aggregator.json file
    in the setups folder.</p>
</li>
<li>
<p>Also, if both the database and aggregator instances are running in
    public cloud environments, the IP address of the configuration files
    of those servers needs to be the private IP address.</p>
</li>
<li>
<p>Agents need to connect to the aggregator using the public IP address
    and the connecting socket (port number) needs to be open to accept
    inbound messages.</p>
</li>
<li>
<p>After you start the database and aggregator servers, you will see a
    message such as the following in the console:</p>
</li>
</ul>
<blockquote>
<p># Database-side Console Example</p>
<p>INFO:root:--- Pseudo DB Started ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>On the aggregator side of the console, you will see something like
    the following:</li>
</ul>
<blockquote>
<p># Aggregator-side Console Example</p>
<p>INFO:root:--- Aggregator Started ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>Behind this aggregator server, the model synthesis module is running
    every 5 seconds, where it starts checking whether the number of
    collected local models is more than the number that the aggregation
    threshold defines.</p>
</li>
<li>
<p>We have now run the database and aggregator modules and are ready to
    run a minimal example with the FL client.</p>
</li>
</ul>
<p>Running a minimal example with the FL client</p>
<ul>
<li>
<p>In the previous section, we talked about the integration of local ML
    engines into the FL system. Here, using a minimal sample that does
    not have actual training data, we will try to run the FL systems
    that have been discussed. This minimal example can be used as a
    template when implementing any locally distributed ML engine.</p>
</li>
<li>
<p>Before running the minimal example, you should check whether the
    database and aggregator servers are running already. Then, run the
    following command:</p>
</li>
</ul>
<p>python -m examples.minimal.minimal_MLEngine</p>
<p>CopyExplain</p>
<ul>
<li>
<p>In this case, only one agent with a minimal ML engine is connected.
    Thus, the aggregation happens every time this default agent uploads
    the local model.</p>
</li>
<li>
<p>Note that if the aggregator server is running on a different
    machine, you will need to specify the public IP address of the
    aggregator server or instance. The IP address of the aggregator can
    be modified in the config_agent.json file in the setups folder. We
    also recommend setting the polling flag to 1 when running the
    aggregator and database in a cloud instance.</p>
</li>
</ul>
<p><em>Figure 6.1</em> shows an example of the console screen when running a
database server:</p>
<p><img alt="Figure 6.1 -- Example of a database-side console
" src=".\images\/media/image39.jpg" />{width="6.268055555555556in"
height="4.277777777777778in"}</p>
<p>Figure 6.1 -- Example of a database-side console</p>
<p><em>Figure 6.2</em> shows an example of the console screen when running an
aggregator:</p>
<p><img alt="Figure 6.2 -- Example of an aggregator-side console
" src=".\images\/media/image29.jpg" />{width="6.268055555555556in"
height="4.104861111111111in"}</p>
<p>Figure 6.2 -- Example of an aggregator-side console</p>
<p><em>Figure 6.3</em> shows an example of the console screen when running an
agent:</p>
<p><img alt="Figure 6.3 -- Example of an agent-side console
" src=".\images\/media/image34.jpg" />{width="6.268055555555556in"
height="4.260416666666667in"}</p>
<p>Figure 6.3 -- Example of an agent-side console</p>
<ul>
<li>
<p>Now we know how to run all the FL components: a database,
    aggregator, and agent.</p>
</li>
<li>
<p>In the next section, we will examine how outputs are generated by
    running the FL system.</p>
</li>
</ul>
<p>Data and database folders</p>
<ul>
<li>
<p>After running the FL system, you will notice that the database
    folder and data folder are created under the locations that you
    specified in the config files of the database and agent.</p>
</li>
<li>
<p>For example, the db folder is created under db_data_path, written in
    the config_db.json file. In the database folder, you will find the
    SQLite database, such as model_data12345.db, where the metadata of
    local and cluster global models is stored, as well as
    a models folder that contains all the actual local models uploaded
    by the agents and global models created by the aggregator.</p>
</li>
<li>
<p><em>Figure 6.4</em> shows the SQLite database and ML model files in a
    binary file format stored in the db folder created by running the
    minimal example code:</p>
</li>
</ul>
<p><img alt="Figure 6.4 -- The SQLite database and ML model files in a binary file
format stored in the db folder
" src=".\images\/media/image75.jpg" />{width="6.268055555555556in"
height="2.6215277777777777in"}</p>
<p>Figure 6.4 -- The SQLite database and ML model files in a binary file
format stored in the db folder</p>
<ul>
<li>The data folder is created under an agent device at the location of
    the model_path, a string value defined in config_agent.json. In the
    example run of the minimal example, the following files are created
    under the data/agents/default-agent folder:</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   lms.binaryfile: A binary file containing a local model created by
    the agent</p>
<ul>
<li>
<p>gms.binaryfile: A binary file containing a global model created by
    the aggregator sent back to the agent</p>
</li>
<li>
<p>state: A file that has an integer value that indicates the state of
    the client itself</p>
</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   <em>Figure 6.5</em> shows the structure of the agent-side data, which
    includes global and local ML models represented with a binary file
    format, as well as the file reflecting the FL client state:</p>
<p><img alt="Figure 6.5 -- Data of the agents including global and local ML models
with a binary file format as well as the client state
" src=".\images\/media/image50.jpg" />{width="6.268055555555556in"
height="1.9270833333333333in"}</p>
<p>Figure 6.5 -- Data of the agents including global and local ML models
with a binary file format as well as the client state</p>
<ul>
<li>Now we understand where the key data, such as global and local
    models, is stored. Next, we will take a closer look at the database
    using SQLite.</li>
</ul>
<p>Databases with SQLite</p>
<ul>
<li>The database created in the db folder can be viewed using any tool
    to show the SQLite database that can open files with
    the ***.db format. The database tables are defined in the
    following sections.</li>
</ul>
<p>Local models in a database</p>
<ul>
<li><em>Figure 6.6</em> shows sample database entries related to uploaded local
    models where each entry lists the local model ID, the time that the
    model was generated, the ID of the agent that uploaded the local
    model, round information, performance metrics, and the number of
    data samples:</li>
</ul>
<p><img alt="Figure 6.6 -- Sample database entries related to uploaded local models
" src=".\images\/media/image31.jpg" />{width="6.268055555555556in"
height="1.5618055555555554in"}</p>
<p>Figure 6.6 -- Sample database entries related to uploaded local models</p>
<p>Cluster models in a database</p>
<ul>
<li><em>Figure 6.7</em> shows sample database entries related to uploaded
    cluster models where each entry lists the cluster model ID, the time
    that the model was created, the ID of the aggregator that created
    this cluster model, round information, and the number of data
    samples:</li>
</ul>
<p><img alt="Figure 6.7 -- Sample database entries related to uploaded cluster
models " src=".\images\/media/image11.jpg" />{width="6.268055555555556in"
height="1.7402777777777778in"}</p>
<p>Figure 6.7 -- Sample database entries related to uploaded cluster models</p>
<ul>
<li>Now we have learned how to configure and run the FL system with a
    minimal example and how to examine the results. In the next section,
    you will learn about the behavior of the FL system and what happens
    when the minimal example is run.</li>
</ul>
<p>Understanding what happens when the minimal example runs</p>
<ul>
<li>Understanding the behavior of the entire FL system step by step will
    help you design applications with FL enabled and further enhance the
    FL system itself. Let us first look into what happens when we run
    just one agent by printing some procedures of the agent and
    aggregator modules.</li>
</ul>
<p>Running just one minimal agent</p>
<ul>
<li>Let's run the minimal agent after running the database and
    aggregator servers and see what happens. When the agent is started
    with the minimal ML engine, you will see the following messages in
    the agent console:</li>
</ul>
<blockquote>
<p># Agent-side Console Example</p>
<p>INFO:root:--- This is a minimal example ---</p>
<p>INFO:root:--- Agent initialized ---</p>
<p>INFO:root:--- Your IP is xxx.xxx.1.101 ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>When the agent initializes the model to be used for FL, it shows
    this message, and if you look at the state file, it has entered
    the sending state, which will trigger sending models to the
    aggregator when the FL client is started:</li>
</ul>
<blockquote>
<p># Agent-side Console Example</p>
<p>INFO:root:--- Model template generated ---</p>
<p>INFO:root:--- Local (Initial/Trained) Models saved ---</p>
<p>INFO:root:--- Client State is now sending ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>Then, after the client is started with the start_fl_client function,
    the participation message is sent to the aggregator. Here is the
    participation message sent to the aggregator:</li>
</ul>
<blockquote>
<p>[</p>
<p>\&lt;AgentMsgType.participate: 0&gt;, # Agent Message Type</p>
<p>\'A89fd1c2d9*****\', # Agent ID</p>
<p>\'047b18ddac*****\',    # Model ID</p>
<p>{</p>
<p>\'model1\': array([[1, 2, 3], [4, 5, 6]]),</p>
<p>\'model2\': array([[1, 2], [3, 4]])</p>
<p>}, # ML Models</p>
<p>True,    # Init weights flag</p>
<p>False, # Simulation flag</p>
<p>0, # Exch Port</p>
<p>1645141807.846751, # Generated Time of the models</p>
<p>{\'accuracy\': 0.0, \'num_samples\': 1}, # Meta information</p>
<p>\'xxx.xxx.1.101\' # Agent\'s IP Address</p>
<p>]</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>The participation message to the aggregator includes the message
    type, agent ID, model ID, ML model with NumPy, initialization
    weights flag, simulation flag, exchange port number, time the models
    were generated, and meta information such as performance metrics and
    the agent's IP address.</p>
</li>
<li>
<p>The agent receives the welcome message from an aggregator confirming
    the connection of this agent, which also includes the following
    information:</p>
</li>
</ul>
<blockquote>
<p># Agent-side Console Example</p>
<p>INFO:root:--- Init Response: [</p>
<p>\&lt;AggMsgType.welcome: 0&gt;, # Message Type</p>
<p>\'4e2da*****\', # Aggregator ID</p>
<p>\'23487*****\', # Model ID</p>
<p>{\'model1\': array([[1, 2, 3], [4, 5, 6]]),</p>
<p>\'model2\': array([[1, 2], [3, 4]])}, # Global Models</p>
<p>1, # FL Round</p>
<p>\'A89fd1c2d9*****\', # Agent ID</p>
<p>\'7890\', # exch_socket number</p>
<p>\'4321\' # recv_socket number</p>
<p>] ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>On the aggregator side, after this agent sends a participation
    message to the aggregator, the aggregator confirms the participation
    and pushes this initial model to the database:</li>
</ul>
<blockquote>
<p># Aggregator-side Console Example</p>
<p>INFO:root:--- Participate Message Received ---</p>
<p>INFO:root:--- Model Formats initialized, model names: [\'model1\',
\'model2\'] ---</p>
<p>INFO:root:--- Models pushed to DB: Response [\'confirmation\']
---</p>
<p>INFO:root:---  Global Models Sent to A89fd1c2d9***** ---</p>
<p>INFO:root:--- Aggregation Threshold (Number of agents needed for
aggregation): 1 ---</p>
<p>INFO:root:--- Number of collected local models: 0 ---</p>
<p>INFO:root:--- Waiting for more local models to be collected ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>In the database server-side console, you can also check that the
    local model is sent from the aggregator and the model is saved in
    the database:</li>
</ul>
<blockquote>
<p># DB-side Console Example</p>
<p>INFO:root:Request Arrived</p>
<p>INFO:root:--- Model pushed: ModelType.local ---</p>
<p>INFO:root:--- Local Models are saved ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>After the aggregator sends the global model back to the agent, the
    agent receives and saves it and changes the client state
    from waiting_gm to gm_ready, indicating the global model is ready
    for retraining locally:</li>
</ul>
<blockquote>
<p># Agent-side Console Example</p>
<p>INFO:root:--- Global Model Received ---</p>
<p>INFO:root:--- Global Models Saved ---</p>
<p>INFO:root:--- Client State is now gm_ready ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>Here is the message sent to the agent from an aggregator, including
    the global model. The contents of the message include the message
    type, aggregator ID, cluster model ID, FL round, and ML models with
    NumPy:</li>
</ul>
<blockquote>
<p>[</p>
<p>\&lt;AggMsgType.sending_gm_models: 1&gt;, # Message Type</p>
<p>\'8c6c946472*****\', # Aggregator ID</p>
<p>\'ab633380f6*****\', # Global Model ID</p>
<p>1, # FL Round Info</p>
<p>{    </p>
<p>\'model1\': array([[1., 2., 3.],[4., 5., 6.]]),</p>
<p>\'model2\': array([[1., 2.],[3., 4.]])</p>
<p>} # ML models</p>
<p>]</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>Then, the agent reads the global models to proceed with using them
    for local training and changes the client state to training:</li>
</ul>
<blockquote>
<p># Agent-side Console Example</p>
<p>INFO:root:--- Global Models read by Agent ---</p>
<p>INFO:root:--- Client State is now training ---</p>
<p>INFO:root:--- Training ---</p>
<p>INFO:root:--- Training is happening ---</p>
<p>INFO:root:--- Training is happening ---</p>
<p>INFO:root:--- Training Done ---</p>
<p>INFO:root:--- Local (Initial/Trained) Models saved ---</p>
<p>INFO:root:--- Client State is now sending ---</p>
<p>INFO:root:--- Local Models Sent ---</p>
<p>INFO:root:--- Client State is now waiting_gm ---</p>
<p>INFO:root:--- Polling to see if there is any update (shown only when
polling) ---</p>
<p>INFO:root:--- Global Model Received ---</p>
<p>INFO:root:--- The global models saved ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>After the preceding local training process, the agent proceeds
    with sending the trained local models to the aggregator and changes
    the client state to waiting_gm, which means it waits for the global
    model with the polling mechanism.</p>
</li>
<li>
<p>Here is the message sent to the aggregator as a trained local model
    message. The contents of the message include message type, agent ID,
    model ID, ML models, generated time of the models, and metadata such
    as performance data:</p>
</li>
</ul>
<blockquote>
<p>[</p>
<p>\&lt;AgentMsgType.update: 1&gt;, # Agent\'s Message Type</p>
<p>\'a1031a737f*****\', # Agent ID</p>
<p>\'e89ccc5dc9*****\', # Model ID</p>
<p>{</p>
<p>\'model1\': array([[1, 2, 3],[4, 5, 6]]),</p>
<p>\'model2\': array([[1, 2],[3, 4]])</p>
<p>}, # ML Models</p>
<p>1645142806.761495, # Generated Time of the models</p>
<p>{\'accuracy\': 0.5, \'num_samples\': 1} # Meta information</p>
<p>]</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>Then, in the aggregator, after the local model is pushed to the
    database, it shows the change in the buffer, that the number of
    collected local models is up to 1 from 0, thus indicating that
    enough local models are collected to start the aggregation:</li>
</ul>
<blockquote>
<p># Aggregator-side Console Example</p>
<p>INFO:root:--- Models pushed to DB: Response [\'confirmation\']
---</p>
<p>INFO:root:--- Local Model Received ---</p>
<p>INFO:root:--- Aggregation Threshold (Number of agents needed for
aggregation): 1 ---</p>
<p>INFO:root:--- Number of collected local models: 1 ---</p>
<p>INFO:root:--- Enough local models are collected. Aggregation will
start. ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>Then, aggregation for round 1 happens and the cluster global models
    are formed, pushed to the database, and sent to the agent once the
    polling message arrives from the agent. The aggregator can also push
    the message back to the agent via a push method:</li>
</ul>
<blockquote>
<p># Aggregator-side Console Example</p>
<p>INFO:root:Round 1</p>
<p>INFO:root:Current agents: [{\'agent_name\': \'default_agent\',
\'agent_id\': \'A89fd1c2d9*****\', \'agent_ip\':
\'xxx.xxx.1.101\', \'socket\': 7890}]</p>
<p>INFO:root:--- Cluster models are formed ---</p>
<p>INFO:root:--- Models pushed to DB: Response [\'confirmation\']
---</p>
<p>INFO:root:--- Global Models Sent to A89fd1c2d9***** ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>On the database server side, the cluster global model is received
    and pushed to the database:</li>
</ul>
<blockquote>
<p># DB-side Console Example</p>
<p>INFO:root:Request Arrived</p>
<p>INFO:root:--- Model pushed: ModelType.cluster ---</p>
<p>INFO:root:--- Cluster Models are saved ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>This process in this section is repeated after cluster models are
    generated and saved for the upcoming FL round and the round of FL
    proceeds with this interaction mechanism.</p>
</li>
<li>
<p>If you look at both the local and cluster global models, they are as
    follows:</p>
</li>
</ul>
<blockquote>
<p>{</p>
<p>\'model1\': array([[1, 2, 3],[4, 5, 6]]),</p>
<p>\'model2\': array([[1, 2],[3, 4]])</p>
<p>}</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>This means only one fixed model is used all the time even if
    aggregation happens, so the global model is exactly the same as the
    initial one as the dummy training process is used here.</p>
</li>
<li>
<p>We will now look into the results when running two minimal agents in
    the next section.</p>
</li>
</ul>
<p>Running two minimal agents</p>
<ul>
<li>
<p>With the database and aggregator servers running, you can run many
    agents using the minimal_MLEngine.py file in
    the simple-fl/examples/minimal folder.</p>
</li>
<li>
<p>You should run the two individual agents from different local
    machines by specifying the IP address of the aggregator to connect
    those agents with the minimal ML example.</p>
</li>
<li>
<p>You can also run multiple agents from the same machine for
    simulation purposes by specifying the different port numbers for the
    individual agents.</p>
</li>
<li>
<p>In the code provided in the simple-fl repository on GitHub, you can
    run the multiple agents by using the following command:</p>
</li>
</ul>
<blockquote>
<p>python -m examples.minimal.minimal_MLEngine [simulation_flag]
[gm_recv_port] [agent_name]</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>To conduct the simulation, simulation_flag should be set
    to 1. gm_recv_port is the port number to receive the global models
    from the aggregator. The agent will be notified of the port number
    by the aggregator through the response of a participation message.
    Also, agent_name is the name of the local agent and the directory
    name storing the state and model files. This needs to be unique for
    every agent.</p>
</li>
<li>
<p>For instance, you can run the first and second agents with the
    following commands:</p>
</li>
</ul>
<blockquote>
<p># First agent</p>
<p>python -m examples.minimal.minimal_MLEngine 1 50001 a1</p>
<p># Second agent</p>
<p>python -m examples.minimal.minimal_MLEngine 1 50002 a2</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>You can edit the configuration JSON files in the setups folder if
    needed. In this case, agg_threshold is set to 1.</p>
</li>
<li>
<p>When you run the simulation in the database server running a minimal
    example with multiple agents, the console screen will look similar
    to that in <em>Figure 6.1</em>.</p>
</li>
<li>
<p><em>Figure 6.8</em> shows the console screen of a simulation in the
    aggregator server running a minimal example using dummy ML models:</p>
</li>
</ul>
<p><img alt="Figure 6.8 -- Example of an aggregator-side console running a minimal
example connecting two agents
" src=".\images\/media/image15.jpg" />{width="6.268055555555556in"
height="4.104861111111111in"}</p>
<p>Figure 6.8 -- Example of an aggregator-side console running a minimal
example connecting two agents</p>
<ul>
<li><em>Figure 6.9</em> shows the console screen of a simulation in one of the
    agents running a minimal example using dummy ML models:</li>
</ul>
<p><img alt="Figure 6.9 -- Example of agent 1's console running a minimal example
using dummy ML models
" src=".\images\/media/image16.jpg" />{width="6.268055555555556in"
height="4.509027777777778in"}</p>
<p>Figure 6.9 -- Example of agent 1's console running a minimal example
using dummy ML models</p>
<ul>
<li><em>Figure 6.10</em> shows the console screen of a simulation in another
    agent running a minimal example using dummy ML models:</li>
</ul>
<p><img alt="Figure 6.10 -- Example of agent 2's console running a minimal example
using dummy ML models
" src=".\images\/media/image25.jpg" />{width="6.268055555555556in"
height="4.522222222222222in"}</p>
<p>Figure 6.10 -- Example of agent 2's console running a minimal example
using dummy ML models</p>
<ul>
<li>Now we know how to run the minimal example with two agents. In order
    to further look into the FL procedure using this example, we will
    answer the following questions:</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   Has aggregation been done correctly for the simple cases?</p>
<ul>
<li>
<p>Has the FedAvg algorithm been applied correctly?</p>
</li>
<li>
<p>Does aggregation threshold work with connected agents?</p>
</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   After running and connecting the two agents, the aggregator will
    wait to receive two models from the two connected agents, as
    follows:</p>
<blockquote>
<p># Aggregator-side Console Example</p>
<p>INFO:root:--- Aggregation Threshold (Number of agents needed for
aggregation): 2 ---</p>
<p>INFO:root:--- Number of collected local models: 0 ---</p>
<p>INFO:root:--- Waiting for more local models to be collected ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>In this case, the aggregation threshold is set to 1.0 in
    the config_aggregator.json file in the setups folder, so the
    aggregator needs to collect all the models from connected agents,
    meaning it needs to receive local ML models from all the agents that
    are connected to the aggregator.</p>
</li>
<li>
<p>Then, it receives one model from one of the agents and the number of
    collected local models is increased to 1. However, as the aggregator
    is still missing one local model, it does not start aggregation yet:</p>
</li>
</ul>
<blockquote>
<p># Aggregator-side Console Example</p>
<p>INFO:root:--- Local Model Received ---</p>
<p>INFO:root:--- Aggregation Threshold (Number of agents needed for
aggregation): 2 ---</p>
<p>INFO:root:--- Number of collected local models: 1 ---</p>
<p>INFO:root:--- Waiting for more local models to be collected ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>On the agent side, after the local models are sent to the
    aggregator, it will wait until the cluster global model to be
    created in the aggregator and sent back to the agent. In this way,
    you can synchronize the FL process at the agent side and automate
    the local training procedure when the global model is sent back to
    the agent and ready for retraining.</p>
</li>
<li>
<p>After the aggregator receives another local model, enough models are
    collected to start the aggregation process:</p>
</li>
</ul>
<p># Aggregator-side Console Example</p>
<blockquote>
<p>INFO:root:--- Local Model Received ---</p>
<p>INFO:root:--- Aggregation Threshold (Number of agents needed for
aggregation): 2 ---</p>
<p>INFO:root:--- Number of collected local models: 2 ---</p>
<p>INFO:root:--- Enough local models are collected. Aggregation will
start. ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>It will finally start the aggregation for the first round, as
    follows:</li>
</ul>
<blockquote>
<p># Aggregator-side Console Example</p>
<p>INFO:root:Round 1</p>
<p>INFO:root:Current agents: [{\'agent_name\': \'a1\', \'agent_id\':
\'1f503*****\', \'agent_ip\': \'xxx.xxx.1.101\', \'socket\':
50001}, {\'agent_name\': \'a2\', \'agent_id\': \'70de8*****\',
\'agent_ip\': \'xxx.xxx.1.101\', \'socket\': 50002}]</p>
<p>INFO:root:--- Cluster models are formed ---</p>
<p>INFO:root:--- Models pushed to DB: Response [\'confirmation\']
---</p>
<p>INFO:root:--- Global Models Sent to 1f503***** ---</p>
<p>INFO:root:--- Global Models Sent to 70de8***** ---</p>
</blockquote>
<p>CopyExplain</p>
<p>Here, let's look at the agent-side ML models that are locally trained:</p>
<blockquote>
<p># Agent 1\'s Console Example</p>
<p>INFO:root:--- Training ---</p>
<p>INFO:root:--- Training is happening ---</p>
<p>INFO:root:--- Training Done ---</p>
<p>Trained models: {\'model1\': array([[1, 2, 3],</p>
<p>[4, 5, 6]]), \'model2\': array([[1, 2],</p>
<p>[3, 4]])}</p>
<p>INFO:root:--- Local (Initial/Trained) Models saved ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>Also, let's look at another agent's ML models that are locally
    trained:</li>
</ul>
<blockquote>
<p># Agent 2\'s Console Example</p>
<p>INFO:root:--- Training ---</p>
<p>INFO:root:--- Training is happening ---</p>
<p>INFO:root:--- Training Done ---</p>
<p>Trained models: {\'model1\': array([[3, 4, 5],</p>
<p>[6, 7, 8]]), \'model2\': array([[3, 4],</p>
<p>[5, 6]])}</p>
<p>INFO:root:--- Local (Initial/Trained) Models saved ---</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>As in the models sent to the aggregator from agents 1 and 2,
    if FedAvg is correctly applied, the global model should be the
    averaged value of these two models. In this case, the number of data
    samples is the same for both agents 1 and 2, so the global model
    should just be an average of the two models.</p>
</li>
<li>
<p>So, let's look at the global models that are generated in the
    aggregator:</p>
</li>
</ul>
<blockquote>
<p># Agent 1 and 2\'s Console Example</p>
<p>Global Models: {\'model1\': array([[2., 3., 4.],</p>
<p>[5., 6., 7.]]), \'model2\': array([[2., 3.],</p>
<p>[4., 5.]])}</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>The received model is the average of the two local models and thus
    averaging has been correctly conducted.</p>
</li>
<li>
<p>The database and data folders are created in
    the model_path specified in the agent configuration file. You can
    look at the database values with an SQLite viewer application and
    look for some models based on the model ID.</p>
</li>
<li>
<p>Now that we understand what's happening with minimal example runs,
    in the next section, we will run a real ML application using an
    image classification model using a <strong>Convolutional Neural
    Network</strong> (<strong>CNN</strong>).</p>
</li>
</ul>
<p>Running image classification and analyzing the results</p>
<ul>
<li>
<p>This example demonstrates the use of this FL framework for
    image classification tasks. We will use a famous image dataset,
    CIFAR-10
    (URL: <a href="https://www.cs.toronto.edu/~kriz/cifar.html">[https://www.cs.toronto.edu/\~kriz/cifar.html]{.underline}</a>),
    to show how an ML model grows through the FL process over time.</p>
</li>
<li>
<p>However, this example is only given for the purposes of using the FL
    system we have discussed so far and is not focused on maximizing the
    performance of the image classification task.</p>
</li>
</ul>
<p>Preparing the CIFAR-10 dataset</p>
<ul>
<li>The following is the information required related to the dataset
    size, the training and test data, the number of classes, and the
    image size:</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   Dataset size: 60,000 images</p>
<ul>
<li>
<p>Training data: 50,000 images</p>
</li>
<li>
<p>Test data: 10,000 images</p>
</li>
<li>
<p>Number of classes: 10
    (airplane, automobile, bird, cat, deer, dog, frog, horse, ship,
    and truck)</p>
</li>
<li>
<p>Each class has 6,000 images</p>
</li>
<li>
<p>Image size: 32x32 pixels, in color</p>
</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   <em>Figure 6.11</em> shows a collection of sample pictures of 10 different
    classes in the dataset with 10 random images for each:</p>
<p><img alt="" src=".\images\/media/image26.jpg" />{width="6.268055555555556in"
height="4.816666666666666in"}</p>
<p>Figure 6.11 -- The classes in the dataset as well as 10 random images
for each category (the images are adapted from
https://www.cs.toronto.edu/\~kriz/cifar.html)</p>
<ul>
<li>Now that the dataset is prepared, we will look into a CNN model used
    for the FL process.</li>
</ul>
<p>The ML model used for FL with image classification</p>
<ul>
<li>Here is the description of the ML model architecture of the CNN
    model used in this image classification example. To learn more about
    what the CNN is, you can find many useful study resources, such
    as <a href="https://cs231n.github.io/convolutional-networks/">[https://cs231n.github.io/convolutional-networks/]{.underline}</a>:</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   Conv2D</p>
<ul>
<li>
<p>MaxPool2D (maximum pooling)</p>
</li>
<li>
<p>Conv2D</p>
</li>
<li>
<p>3 fully-connected layers</p>
</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   The script to define the CNN model is already designed and can be
    found in cnn.py in examples/image_classification in
    the simple-fl repository on GitHub.</p>
<ul>
<li>Next, we will run the image classification application with the FL
    system.</li>
</ul>
<p>How to run the image classification example with CNN</p>
<ul>
<li>As mentioned in the installation steps at the beginning of This
    section, we first install the necessary libraries with federatedenv,
    and then install torch and torchvision after that:</li>
</ul>
<blockquote>
<p>pip install torch</p>
<p>pip install torchvision</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>You can configure many settings through the JSON config files in
    the setups folder of the simple-fl repo of GitHub. For more details,
    you can read the general description of the config files in
    our setups documentation
    (<a href="https://github.com/tie-set/simple-fl/tree/master/setups">[https://github.comkeshavaspandat/simple-fl/tree/master/setups]{.underline}</a>).</p>
</li>
<li>
<p>First, you can run two agents. You can increase the number of agents
    running on the same device by specifying the appropriate port
    numbers.</p>
</li>
<li>
<p>As you already know, the first thing you can do is run the database
    and aggregator:</p>
</li>
</ul>
<blockquote>
<p># FL server side</p>
<p>python -m fl_main.pseudodb.pseudo_db</p>
<p>python -m fl_main.aggregator.server_th</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>Then, start the first and second agents to run the image
    classification example:</li>
</ul>
<blockquote>
<p># First agent</p>
<p>python -m examples.image_classification.classification</p>
<p>_engine 1 50001 a1</p>
<p># Second agent</p>
<p>python -m examples.image_classification.classification</p>
<p>_engine 1 50002 a2</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>To simulate the actual FL scenarios, the amount of training data
    accessible from each agent can be limited to a specific number. This
    should be specified with the num_training_data variable
    in classification_engine.py. By default, it uses 8,000 images (2,000
    batches) for each round.</p>
</li>
<li>
<p>Now that we can run the two agents to test the FL process using CNN
    models, let us look further into the results by running the image
    classification example.</p>
</li>
</ul>
<p>Evaluation of running the image classification with CNN</p>
<ul>
<li>
<p>The performance data (the accuracy of each local model cluster
    model) is stored in our database. You can access the
    corresponding .db file to see the performance history.</p>
</li>
<li>
<p>The DataManager instance (defined in ic_training.py) has a function
    to return one batch of images and their labels (get_random_images).
    You can use this function to show the actual labels and the
    predicted labels by the trained CNN on specific images.</p>
</li>
<li>
<p><em>Figure 6.12</em> shows a plot of the learning performance from our
    experimental runs on our side; the results may look different when
    you run it with your own settings:</p>
</li>
</ul>
<p><img alt="Figure 6.12 -- Plot of the learning performance from the experimental
runs for FL using CNN for image classification
" src=".\images\/media/image27.jpg" />{width="6.268055555555556in"
height="3.204861111111111in"}</p>
<p>Figure 6.12 -- Plot of the learning performance from the experimental
runs for FL using CNN for image classification</p>
<ul>
<li>Again, as we only use two agents here, the results just look
    slightly different. However, with the proper hyperparameter
    settings, data amount, and the number of agents, you will be able to
    carry out an FL evaluation that produces meaningful results, which
    we would like you to explore on your own, as the focus here is just
    how to connect the actual ML models to this FL environment.</li>
</ul>
<p>Running five agents</p>
<ul>
<li>
<p>You can easily run five agents for the image classification
    application by just specifying different port numbers and agent
    names in the terminal.</p>
</li>
<li>
<p>The results look similar to what we discussed in the previous
    section except the real ML models are connected (in this case, the
    ML model being aggregated is CNN).</p>
</li>
<li>
<p>After running the five agents, the data and database folders look
    like in <em>Figure 6.13</em>:</p>
</li>
</ul>
<p><img alt="Figure 6.13 -- Results to be stored in each folder with the agent's
unique name " src=".\images\/media/image18.jpg" />{width="6.268055555555556in"
height="3.6479166666666667in"}</p>
<p>Figure 6.13 -- Results to be stored in each folder with the agent's
unique name</p>
<ul>
<li><em>Figure 6.14</em> shows the uploaded local models in the database with
    information about the local model ID, the time the models were
    generated, the ID of the agent that uploaded the local model,
    performance metrics, and round information:</li>
</ul>
<p><img alt="Figure 6.14 -- Information about the local models in the database 
" src=".\images\/media/image17.jpg" />{width="6.268055555555556in"
height="4.133333333333334in"}</p>
<p>Figure 6.14 -- Information about the local models in the database </p>
<ul>
<li>
<p>If you look at the database in <em>Figure 6.14</em>, there are five models
    collected by the five agents with local performance data.</p>
</li>
<li>
<p>For each round, those five local models are aggregated to produce a
    cluster global model, as in the cluster_models table in the
    database, as shown in <em>Figure 6.15</em>.</p>
</li>
<li>
<p>The database storing cluster models has information about the
    cluster model ID, the time the models were generated, the ID of the
    aggregator that created the cluster model, and round information:</p>
</li>
</ul>
<p><img alt="Figure 6.15 -- Information about the cluster models in the database
" src=".\images\/media/image22.jpg" />{width="6.268055555555556in"
height="1.6520833333333333in"}</p>
<p>Figure 6.15 -- Information about the cluster models in the database</p>
<ul>
<li>In this way, you can connect as many agents as possible. It is up to
    you to optimize the settings of the local ML algorithms to obtain
    the best-performing federated models out of the FL system.</li>
</ul>
<p>Summary</p>
<ul>
<li>
<p>Here, we discussed the execution of FL systems in detail and how the
    system will behave according to the interactions between the
    aggregator and agents.</p>
</li>
<li>
<p>The step-by-step explanation of the FL system behavior based on the
    outcomes of the console examples guides you to understand the
    aggregation process of the FedAvg algorithm.</p>
</li>
<li>
<p>Furthermore, the image classification example showed how CNN models
    are connected to the FL system and how the FL process increases the
    accuracy through aggregation, although this was not optimized to
    maximize the training results but simplified to validate the
    integration using CNN.</p>
</li>
<li>
<p>With what you have learned Here, you will be able to design your own
    FL applications integrating the principles and framework introduced
    in this book, and furthermore, will be able to assess the FL
    behavior on your own to see whether the whole flow of the FL process
    and model aggregation is happening correctly and consistently.</p>
</li>
</ul>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2024 <a href="https://spanda.io"  target="_blank" rel="noopener">Spanda.io</a>

    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://ranga-rangarajan.github.io/spanda-bootcamp/" target="_blank" rel="noopener" title="ranga-rangarajan.github.io" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.8fd75fb4.min.js"></script>
      
    
  </body>
</html>