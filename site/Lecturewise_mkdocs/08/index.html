
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../07/">
      
      
        <link rel="next" href="../09/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.7">
    
    
      
        <title>08 - Spanda DL Bootcamp</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.f2e4d321.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#challenge-find-a-better-way-to-handle-model-serving-requests" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Spanda DL Bootcamp" class="md-header__button md-logo" aria-label="Spanda DL Bootcamp" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Spanda DL Bootcamp
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              08
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-hidden="true"  type="radio" name="__palette" id="__palette_2">
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  The Complete Generative AI Bootcamp

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../01/" class="md-tabs__link">
          
  
  Lecturewise mkdocs

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Spanda DL Bootcamp" class="md-nav__button md-logo" aria-label="Spanda DL Bootcamp" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Spanda DL Bootcamp
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The Complete Generative AI Bootcamp
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Lecturewise mkdocs
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Lecturewise mkdocs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    01
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    02
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    03
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    04
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    05
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06.5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    06.5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    06
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    07
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    08
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    08
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#challenge-find-a-better-way-to-handle-model-serving-requests" class="md-nav__link">
    <span class="md-ellipsis">
      Challenge: Find a better way to handle model serving requests
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#solution" class="md-nav__link">
    <span class="md-ellipsis">
      Solution:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Solution:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#points-to-note" class="md-nav__link">
    <span class="md-ellipsis">
      Points to note:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quiz" class="md-nav__link">
    <span class="md-ellipsis">
      Quiz:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#challenge-design-the-model-serving-system-to-handle-large-requests" class="md-nav__link">
    <span class="md-ellipsis">
      Challenge: Design the model serving system to handle large requests
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#solution-1" class="md-nav__link">
    <span class="md-ellipsis">
      Solution:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Solution:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#section-unnumbered" class="md-nav__link">
    <span class="md-ellipsis">
      {#section .unnumbered}
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unable-to-handle-all-requests-within-this-time-window" class="md-nav__link">
    <span class="md-ellipsis">
      unable to handle all requests within this time window
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-challenge-respond-to-model-serving-requests-based-on-events" class="md-nav__link">
    <span class="md-ellipsis">
      The challenge: Respond to model serving requests based on events
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-solution" class="md-nav__link">
    <span class="md-ellipsis">
      The solution
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The solution">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#points-to-note-1" class="md-nav__link">
    <span class="md-ellipsis">
      Points to Note:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quiz-1" class="md-nav__link">
    <span class="md-ellipsis">
      Quiz
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#solution-2" class="md-nav__link">
    <span class="md-ellipsis">
      Solution
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#points-to-note-2" class="md-nav__link">
    <span class="md-ellipsis">
      Points to note:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Points to note:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#section-2-unnumbered" class="md-nav__link">
    <span class="md-ellipsis">
      {#section-2 .unnumbered}
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#questions" class="md-nav__link">
    <span class="md-ellipsis">
      Questions:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#section-3-unnumbered" class="md-nav__link">
    <span class="md-ellipsis">
      {#section-3 .unnumbered}
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#section-4-unnumbered" class="md-nav__link">
    <span class="md-ellipsis">
      {#section-4 .unnumbered}
    </span>
  </a>
  
    <nav class="md-nav" aria-label="{#section-4 .unnumbered}">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#section-5-unnumbered" class="md-nav__link">
    <span class="md-ellipsis">
      {#section-5 .unnumbered}
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-solution-1" class="md-nav__link">
    <span class="md-ellipsis">
      The solution
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#points-to-note-3" class="md-nav__link">
    <span class="md-ellipsis">
      Points to Note:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quiz-2" class="md-nav__link">
    <span class="md-ellipsis">
      Quiz:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#section-8-unnumbered" class="md-nav__link">
    <span class="md-ellipsis">
      {#section-8 .unnumbered}
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#background" class="md-nav__link">
    <span class="md-ellipsis">
      Background
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Background">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#section-9-unnumbered" class="md-nav__link">
    <span class="md-ellipsis">
      {#section-9 .unnumbered}
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#system-components" class="md-nav__link">
    <span class="md-ellipsis">
      System components
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-ingestion" class="md-nav__link">
    <span class="md-ellipsis">
      Data ingestion
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#section-10-unnumbered" class="md-nav__link">
    <span class="md-ellipsis">
      {#section-10 .unnumbered}
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-challenge-reduce-sequential-training-inefficiency" class="md-nav__link">
    <span class="md-ellipsis">
      The Challenge: Reduce Sequential Training Inefficiency
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Challenge: Reduce Sequential Training Inefficiency">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-ingestion-1" class="md-nav__link">
    <span class="md-ellipsis">
      7.2 Data ingestion
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-solution-approach-2" class="md-nav__link">
    <span class="md-ellipsis">
      The Solution Approach
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quiz-5" class="md-nav__link">
    <span class="md-ellipsis">
      Quiz
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-training" class="md-nav__link">
    <span class="md-ellipsis">
      Model training
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#section-11-unnumbered" class="md-nav__link">
    <span class="md-ellipsis">
      {#section-11 .unnumbered}
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-challenge" class="md-nav__link">
    <span class="md-ellipsis">
      The Challenge:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-solution-2" class="md-nav__link">
    <span class="md-ellipsis">
      The solution
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The solution">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#model-serving" class="md-nav__link">
    <span class="md-ellipsis">
      7.4 Model serving
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-serving-1" class="md-nav__link">
    <span class="md-ellipsis">
      Model serving
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model serving">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-challenge-1" class="md-nav__link">
    <span class="md-ellipsis">
      The Challenge
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quiz-6" class="md-nav__link">
    <span class="md-ellipsis">
      Quiz:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#end-to-end-workflow" class="md-nav__link">
    <span class="md-ellipsis">
      End-to-end workflow
    </span>
  </a>
  
    <nav class="md-nav" aria-label="End-to-end workflow">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-challenges" class="md-nav__link">
    <span class="md-ellipsis">
      The Challenges
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-solutions" class="md-nav__link">
    <span class="md-ellipsis">
      The solutions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quiz-7" class="md-nav__link">
    <span class="md-ellipsis">
      Quiz:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary-2" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    09
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../17/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM's & NLP Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Retrieval Augmented Generation (RAG)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../21/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RAG Pipeline Implementation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    From Simple to Advanced RAG
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../23/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Observability Tools for RAG
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  <h1>08</h1>

<p>[<strong>Part
2</strong>____________________________________________________________________]{.underline}</p>
<p><strong>We will cover:</strong></p>
<ul>
<li>
<p>Using model serving to generate predictions or make inferences on
    new data with previously trained machine learning models</p>
</li>
<li>
<p>Handling model serving requests and achieving horizontal scaling
    with replicated model serving services</p>
</li>
<li>
<p>Processing large model serving requests using the sharded services
    services patterns</p>
</li>
<li>
<p>Assess model serving systems and event driven design</p>
</li>
</ul>
<p><strong>Outcome:</strong></p>
<ul>
<li>
<p>Learn three key patterns used to achieve scaling and performance
    outcomes in distributed ML systems</p>
</li>
<li>
<p>Understand the tradeoffs involved</p>
</li>
</ul>
<p><strong>Model Serving</strong></p>
<ul>
<li>
<p><em>Model serving</em> is the process of <em>loading a previously trained
    machine learning model to generate predictions or make inferences on
    new input data</em>.</p>
</li>
<li>
<p>It is done after successfully training a machine learning model.
    (Fig 4.2)</p>
</li>
</ul>
<blockquote>
<p><strong>Model serving is the next step after we have successfully trained a
machine learning model. We use the trained</strong></p>
</blockquote>
<p>Figure 4.1 A diagram showing where model serving fits in the machine
learning pipeline</p>
<ul>
<li>
<p>In traditional machine learning applications, model serving is
    usually a single program running on a local desktop or machine and
    generates predictions on new datasets that are not used for model
    training.</p>
</li>
<li>
<p>Both the dataset and the machine learning model used should be small
    enough to fit on a single machine for traditional model serving, and
    they are stored in the local disk of a single machine.</p>
</li>
<li>
<p>In contrast, distributed model serving usually happens in a cluster
    of machines. Both the dataset and the trained machine learning model
    used for model serving can be very large and must be stored in a
    remote distributed database or partitioned on disks of multiple
    machines.</p>
</li>
<li>
<p>The differences between traditional model serving and distributed
    model serving systems is summarized in table 4.1.</p>
</li>
</ul>
<p>Table 4.1 Comparison between traditional model serving and distributed
model serving systems</p>
<p>+-------------------+------------------------+------------------------+
|                   | Traditional model      | Distributed model      |
|                   | serving                | serving                |
+===================+========================+========================+
| &gt; Computational   | &gt; Personal laptop or   | &gt; Cluster of machines  |
| &gt; resources       | &gt; single remote server |                        |
+-------------------+------------------------+------------------------+
| &gt; Dataset         | Local disk on a single | &gt; Remote distributed   |
| &gt; location        | laptop or machine      | &gt; database or          |
|                   |                        | &gt; partitioned on disks |
|                   |                        | &gt; of multiple          |
|                   |                        | &gt;                      |
|                   |                        | &gt; machines             |
+-------------------+------------------------+------------------------+
| &gt; Size of model   | &gt; Small enough to fit  | &gt; Medium to large      |
| &gt; and dataset     | &gt; on a single machine  |                        |
+-------------------+------------------------+------------------------+</p>
<p><strong>Pattern 1: Replicated services pattern</strong></p>
<ul>
<li>
<p>Challenge: <strong><em>Building a machine learning model to</em> *tag the main
    themes of new videos that the model hasn't seen before</strong>*</p>
</li>
<li>
<p>We have used the YouTube8M dataset
    (<a href="http://research.google.com/youtube8m/">http://research.google.com/youtube8m/</a>)</p>
</li>
<li>
<p>This consists of millions of YouTube video IDs, with high-quality
    machine-generated annotations from a diverse vocabulary of 3,800+
    visual entities viz, Car, Music, etc. (fig. 4.2) for training. Hence
    we have a model</p>
</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   We now need to build a <strong><em>model serving system</em></strong> that allows users
    to upload <strong><em>new</em></strong> videos.</p>
<ul>
<li>
<p>The system would load the <strong><em>previously trained machine learning
    model</em></strong> to tag entities/themes that appear in the uploaded videos.</p>
</li>
<li>
<p>The model serving system is stateless, so users' requests won't
    affect the model serving results.</p>
</li>
<li>
<p>The system takes videos uploaded by users and sends requests to the
    model server.</p>
</li>
<li>
<p>The model server then retrieves the previously trained
    entity-tagging machine learning model from the model storage to
    process the videos and eventually generate possible entities that
    appear in the videos. (Fig. 4.3)</p>
</li>
</ul>
<blockquote>
<p><img alt="" src="../media/media/image21.jpg" />{width="5.666666666666667in"
height="2.9133333333333336in"}</p>
</blockquote>
<p>Figure 4.2 A screenshot of what the videos in the YouTube-8M dataset
looks like. (Source: Sudheendra</p>
<blockquote>
<p>Vijayanarasimhan et al. Licensed under Nonexclusive License 1.0)</p>
</blockquote>
<p><em>Users upload videos and then submit requests to the model serving
system to tag the entities within the video</em></p>
<blockquote>
<p>Dog,</p>
<p>Swimmer</p>
<p>Horse</p>
<p>...</p>
</blockquote>
<p>Figure 4.3 The single-node model serving system</p>
<ul>
<li>
<p>This first version of the model server runs on a single machine and
    responds to model serving requests from users on a first-come,
    first-served basis (figure 4.4) and will work if only very few users
    are testing the system.</p>
</li>
<li>
<p>As the number of users or model serving requests increases, users
    will experience huge delays while waiting for the system to finish
    processing any previous requests. In the real world, this bad user
    experience would immediately lose our users' interest in engaging
    with this system.</p>
</li>
</ul>
<p>Figure 4.4 The model server only runs on a single machine and responds
to model serving requests from users on a first-come, first-served
basis.</p>
<h4 class="unnumbered" id="challenge-find-a-better-way-to-handle-model-serving-requests"><strong>Challenge:</strong> Find a better way to handle model serving requests</h4>
<ul>
<li>
<p>This system can only effectively serve a limited number of model
    serving requests on a first-come, first-served basis.</p>
</li>
<li>
<p>As the number of requests grows in the real world, the user
    experience worsens when users must wait a long time to receive the
    model serving result.</p>
</li>
<li>
<p>All requests are waiting to be processed by the model serving
    system, but the computational resources are <em>bound to this single
    node</em>.</p>
</li>
<li>
<p>Is there a better way to handle model serving requests than
    sequentially?</p>
</li>
</ul>
<h4 class="unnumbered" id="solution"><strong>Solution:</strong></h4>
<ul>
<li>
<p>The existing model server is stateless</p>
</li>
<li>
<p>The model serving results for each request aren't affected by other
    requests, and the machine learning model can only process a single
    request.</p>
</li>
<li>
<p>The model server doesn't require a saved state to operate correctly.</p>
</li>
<li>
<p>Since the model server is stateless, we can add more server
    instances to help handle additional user requests without the
    requests interfering with each other (figure 4.5).</p>
</li>
<li>
<p>These additional model server instances are exact copies of the
    original</p>
</li>
</ul>
<blockquote>
<p>Figure 4.5 Additional server instances help handle additional user
requests without the requests interfering with each other.</p>
</blockquote>
<ul>
<li>
<p>The server addresses are different, and each handles different model
    serving requests</p>
</li>
<li>
<p>In other words, they are <em>replicated services</em> for model serving or,
    in short, <em>model server replicas</em>.</p>
</li>
<li>
<p>Adding additional resources into our system with more machines is
    called <em>horizontal scaling</em>.</p>
</li>
<li>
<p>Horizontal scaling systems handle more and more users or traffic by
    adding more replicas.</p>
</li>
<li>
<p>The opposite of horizontal scaling is <em>vertical scaling</em>, which is
    usually implemented by adding computational resources to existing
    machines.</p>
</li>
<li>
<p>The system now has multiple model server replicas to process the
    model serving requests asynchronously.</p>
</li>
<li>
<p>Each model server replica takes a single request, retrieves the
    previously trained entity-tagging machine learning model from model
    storage, and then processes the videos in the request to tag
    possible entities in the videos.</p>
</li>
<li>
<p>As a result, we've successfully scaled up our model server by adding
    model server replicas to the existing model serving system (4.6) .</p>
</li>
<li>
<p>The model server replicas are capable of handling many requests at a
    time since each replica can process individual model serving
    requests independently.</p>
</li>
</ul>
<blockquote>
<p><em>Users upload videos and then submit requests to the model serving
system to tag the entities within the videos.</em></p>
<p>Swimmer</p>
<p>Horse</p>
<p>...</p>
</blockquote>
<p>Figure 4.6 The system architecture after we've scaled up our model
server by adding model server replicas to the system</p>
<ul>
<li>
<p>Here multiple model serving requests from users are sent to the
    model server replicas at the same time.</p>
</li>
<li>
<p>Q: How are they being distributed and processed? Which request is
    being processed by which model server replica?</p>
<ul>
<li>We haven't defined a clear mapping relationship between the
    requests and the model server replicas.</li>
</ul>
</li>
<li>
<p>We need a <em>load balancer</em>, which handles the distribution of model
    serving requests among the replicas.</p>
</li>
<li>
<p>It takes multiple model serving requests from our users and then
    distributes the requests evenly to each of the model server
    replicas, which then are responsible for processing individual
    requests, including model retrieval and inference on the new data in
    the request. (Figure 4.7).</p>
</li>
<li>
<p>The load balancer can use different algorithms to decide which
    request goes to which model server replica.</p>
</li>
<li>
<p>They include round robin, the least connection method, hashing, etc.</p>
</li>
<li>
<p>The replicated services pattern provides a great way to scale our
    model serving system horizontally.</p>
</li>
<li>
<p>It can also be generalized for any systems that serve a large amount
    of traffic. Whenever a single instance cannot handle the traffic,
    introducing this pattern ensures that all traffic can be handled
    equivalently and efficiently.</p>
</li>
</ul>
<blockquote>
<p>Figure 4.7 Loader balancer distributes the requests evenly across
model server replicas</p>
</blockquote>
<h5 class="unnumbered" id="points-to-note">Points to note:</h5>
<ul>
<li>
<p>With load-balanced model server replicas in place, we should be able
    to support the growing number of user requests, and the entire model
    serving system achieves horizontal scaling.</p>
</li>
<li>
<p>The overall model serving system also becomes <em>highly available</em>
    (<a href="https://mng.bz/EQBd">https:// mng.bz/EQBd</a>).</p>
</li>
<li>
<p>High availability is a characteristic of a system that maintains an
    agreed-on level of operational performance, usually uptime, for a
    longer-than-normal period expressed as a percentage of uptime in a
    given year.</p>
</li>
<li>
<p>For example, some organizations may require services to reach a
    highly available service-level agreement, which means the service is
    up and running 99.9% of the time (known as three-nines
    availability). In other words, the service can only get 1.4 minutes
    of downtime per day (24 hours × 60 minutes × 0.1%).</p>
</li>
<li>
<p>With the help of replicated model services, if any of the model
    server replicas crashes or gets preempted on a spot instance, the
    remaining model server replicas are still available and ready to
    process any incoming model serving requests from users, which
    provides a good user experience and makes the system reliable.</p>
</li>
</ul>
<p><strong>Readiness Probes:</strong></p>
<ul>
<li>
<p>Since our model server replicas will need to retrieve previously
    trained machine learning models from a remote model storage, they
    need to be <em>ready</em> in addition to being <em>alive</em>.</p>
</li>
<li>
<p>It's important to build and deploy <em>readiness probes</em> to inform the
    load balancer that the replicas are all successfully established
    connections to the remote model storage and are ready to serve model
    serving requests from users.</p>
</li>
<li>
<p>A readiness probe helps the system determine whether a particular
    replica is ready to serve.</p>
</li>
<li>
<p>With readiness probes, users do not experience unexpected behaviors
    when the system is not ready due to internal system problems.</p>
</li>
</ul>
<p><strong>Issues:</strong></p>
<ul>
<li>
<p>Not only the number of serving requests increases but also the size
    of each request, which can get extremely large if the data or the
    payload is large.</p>
</li>
<li>
<p>In that case, replicated services may not be able to handle the
    large requests.</p>
</li>
</ul>
<h5 class="unnumbered" id="quiz">Quiz:</h5>
<ol>
<li>
<p>Are replicated model servers stateless or stateful?</p>
</li>
<li>
<p>What happens when we don't have a load balancer as part of the model
    serving system?</p>
</li>
<li>
<p>Can we achieve three-nines service-level agreements with only one
    model server instance?</p>
</li>
</ol>
<p><strong>Pattern 2: Sharded services pattern</strong></p>
<ul>
<li>
<p>The replicated services pattern efficiently resolves our horizontal
    scalability problem so that our model serving system can support a
    growing number of user requests. We achieve the additional benefit
    of high availability with the help of model server replicas and a
    load balancer.</p>
</li>
<li>
<p>Each model server replica has a limited and pre-allocated amount of
    computational resources.</p>
</li>
<li>
<p>More important, the amount of computational resources for each
    replica must be identical for the load balancer to distribute
    requests correctly and evenly.</p>
</li>
<li>
<p>Consider this scenario: A user wants to upload a high-resolution
    YouTube video that needs to be tagged with an entity using the model
    server application.</p>
</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   The high-resolution video, though too large, may be uploaded
    successfully to the model server replica if it has sufficient disk
    storage.</p>
<ul>
<li>
<p>Processing the request in any of the individual model server
    replicas themselves failed since processing this single large
    request would require a larger memory allocated in the model server
    replica.</p>
</li>
<li>
<p>This need for a large amount of memory is often due to the
    complexity of the trained machine learning model, as it may contain
    a lot of expensive matrix computations or mathematical operations.
    Eventually, we notify the user of this failure after they have
    waited a long time, which results in a bad user experience. A
    diagram for this situation is shown in figure 4.8.</p>
</li>
</ul>
<blockquote>
<p><em>User uploads a high-resolution video to the model serving system.</em></p>
<p><em>This fails as the model server replica that's processing this large
request does not have enough computational resources.</em></p>
<p>Figure 4.8 Model server fails to process the large data in the request
as the model server replica responsible for processing this request
does not have sufficient memory</p>
</blockquote>
<h3 class="unnumbered" id="challenge-design-the-model-serving-system-to-handle-large-requests"><strong>Challenge:</strong> Design the model serving system to handle large requests</h3>
<ul>
<li>How do we design the model serving system to handle large requests
    of high resolution videos successfully?</li>
</ul>
<h3 class="unnumbered" id="solution-1"><strong>Solution:</strong></h3>
<ul>
<li>
<p>Q: Can we scale vertically by increasing each replica's
    computational resources so it can handle large requests like
    high-resolution videos? Since we are vertically scaling all the
    replicas by the same amount, we will not affect our load balancer's
    work.</p>
</li>
<li>
<p>A: Unfortunately, we cannot simply scale the model server replicas
    vertically since we don't know how many such large requests there
    are. A couple of users could have high-resolution videos needing to
    be processed, and the remaining vast majority of the users only
    upload videos from their smartphones with much smaller resolutions.</p>
</li>
<li>
<p>As a result, most of the added computational resources on the model
    server replicas are idling, which results in very low resource
    utilization.</p>
</li>
<li>
<p>Note: We will examine the resource utilization perspective
    subsequently, but for now, we know that this approach is not
    practical due to usage economics (Architecture is money).</p>
</li>
<li>
<p>The parameter server pattern discussed earlier allows us to
    partition a very large model.</p>
</li>
<li>
<p>Figure 4.9 shows distributed model training with multiple parameter
    servers; the large model has been partitioned, and each partition is
    located on different parameter servers.</p>
</li>
<li>
<p>Each worker node takes a subset of the dataset, performs
    calculations required in each neural network layer, and then sends
    the calculated gradients to update one model partition stored in one
    of the parameter servers.</p>
</li>
</ul>
<blockquote>
<p>Figure 4.9 Distributed model training with multiple parameter servers
where the large model has been sharded and each partition is located
on different parameter servers</p>
</blockquote>
<h5 id="section-unnumbered">{#section .unnumbered}</h5>
<ul>
<li>
<p>To deal with the problem of large model serving requests, the same
    idea can be applied here</p>
<ul>
<li>
<p>Divide the original high-resolution video into multiple separate
    videos,</p>
</li>
<li>
<p>Each video is processed by multiple <em>model server shards</em>
    independently.</p>
</li>
<li>
<p>The model server shards are partitions from a single model
    server instance, and each is responsible for processing a subset
    of a large request.</p>
</li>
</ul>
</li>
</ul>
<p><em>The high-resolution video is divided into two separate videos and sent
to each of the model server shards.</em></p>
<blockquote>
<p>Figure 4.10 Sharded services pattern: High-resolution video gets
divided into two separate videos. Each video represents a subset of
the original large request and is processed by different model server
shard independently.</p>
</blockquote>
<ul>
<li>
<p>After the model server shards receive the sub-requests where each
    contains part of the original large model serving request, each
    model server shard then retrieves the previously trained
    entity-tagging machine learning model from model storage and then
    processes the videos in the request to tag possible entities that
    appear in the videos, similar to the previous model serving system.</p>
</li>
<li>
<p>Once all the sub-requests have been processed by each of the model
    server shards, we merge the model inference result from two
    sub-requests to obtain a result for the original large model serving
    request with the high-resolution video.</p>
</li>
</ul>
<p><strong>Sharding Function:</strong></p>
<ul>
<li>
<p>Q: How do we distribute the two sub-requests to different model
    server shards?</p>
</li>
<li>
<p>A: We use a <em>sharding function</em>, which is very similar to a hashing
    function, to determine which shard in the list of model server
    shards should be responsible for processing each sub-request.</p>
</li>
<li>
<p>Usually, the sharding function is defined using a hashing function
    and the modulo (%) operator.</p>
<ul>
<li>
<p><em>For example, hash(request) % 10 would return 10 shards even
    when the outputs of the hash function are significantly larger
    than the number of shards in a sharded service.</em></p>
</li>
<li>
<p><strong><em>Characteristics of hashing functions for sharding</em></strong></p>
<ul>
<li>
<p><em>The hashing function that defines the sharding function
    transforms an arbitrary object into an integer representing
    a particular shard index. It has two important
    characteristics:</em></p>
</li>
<li>
<p><em>The output from hashing is always the same for a given
    input.</em></p>
</li>
<li>
<p><em>The distribution of outputs is always uniform within the
    output space.</em></p>
</li>
<li>
<p><em>These characteristics are important and can ensure that a
    particular request will always be processed by the same
    shard server and that the requests are evenly distributed
    among the shards.</em></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   When a distributed system has limited computational resources for a
    single machine, we can apply this pattern to offload the
    computational burden to multiple machines.</p>
<p><strong>Points to Consider:</strong></p>
<ul>
<li>
<p>Unlike the replicated services pattern which is useful when building
    stateless services, the sharded services pattern is generally used
    for building stateful services.</p>
</li>
<li>
<p>In our case, we need to maintain the state or the results from
    serving the sub-requests from the original large request using
    sharded services and then merge the results into the final response
    so it includes all entities from the original high-resolution video.</p>
</li>
<li>
<p>In some cases, this approach may not work well because it depends on
    how we divide the original large request into smaller requests.</p>
</li>
<li>
<p>For example, if the original video has been divided into more than
    two sub-requests, some may not be meaningful since they don't
    contain any complete entities that are recognizable by the machine
    learning model we've trained.</p>
</li>
<li>
<p>For situations like that, we need additional handling and cleaning
    of the merged result to remove meaningless entities that are not
    useful to our application.</p>
</li>
</ul>
<p><strong>Quiz:</strong></p>
<ol>
<li>
<p>Would vertical scaling be helpful when handling large requests?</p>
</li>
<li>
<p>Are the model server shards stateful or stateless?</p>
</li>
<li></li>
</ol>
<p><strong>Pattern 3: The event-driven processing pattern</strong></p>
<ul>
<li>
<p>For cases in which we <strong><em>do not know how much model serving traffic
    the system will be receiving</em></strong>, <strong><em>or if the demand traffic pattern
    is lumpy or shows up in spikes</em></strong>, it's hard to allocate and use
    resources efficiently. (e.g. hotel room price prediction)</p>
</li>
<li>
<p>The problem with this traffic pattern is that it could cause very
    low resource utilization rates with static resource allocation
    schemes.</p>
</li>
<li>
<p>In our current architecture, the underlying computational resources
    allocated to the model remain unchanged at all times. This strategy
    is not an optimal one.</p>
</li>
</ul>
<blockquote>
<p><em>Users enter date range and location and then submit requests to the
serving system.</em></p>
</blockquote>
<p>Figure 4.11 A diagram of the model serving system to predict hotel
prices</p>
<blockquote>
<p>Figure 4.12 The traffic changes of the model serving system over time
with an equal amount of computational resources allocated all the
time.</p>
</blockquote>
<ul>
<li>Since we know, more or less, when those holiday periods are, why
    don't we plan accordingly? Unfortunately, some events make it hard
    to predict surges in traffic.</li>
</ul>
<p>Thanksgiving Christmas time</p>
<h4 class="unnumbered" id="unable-to-handle-all-requests-within-this-time-window"><strong>unable to handle all requests within this time window</strong></h4>
<p>Figure 4.13 The traffic of our model serving system over time with an
optimal amount of computational resources allocated for different time
windows. In addition, an unexpected event happened before Christmas that
suddenly added traffic during that particular time window (solid line).</p>
<h3 class="unnumbered" id="the-challenge-respond-to-model-serving-requests-based-on-events"><strong>The challenge: Respond to model serving requests based on events</strong></h3>
<h3 class="unnumbered" id="the-solution"><strong>The solution</strong></h3>
<ul>
<li>
<p>The solution is in maintaining a pool of computational resources
    (e.g., CPUs, memory, disk, etc.) allocated not only to this
    particular model serving system but also to model serving of other
    applications or other components of the distributed machine learning
    pipeline.</p>
</li>
<li>
<p>This shared resource pool gives us enough resources to handle peak
    traffic for the model serving system by pre-allocating resources
    required during historical peak traffic and autoscaling when the
    limit is reached.</p>
</li>
<li>
<p>Therefore, we only use resources when needed and only the specific
    amount of resources required for each particular model serving
    request.</p>
</li>
<li>
<p>Figure 4.15 shows the traffic of our model serving system over time
    with an unexpected bump. The unexpected bump is due to a new very
    large international conference that happens before Christmas.</p>
</li>
</ul>
<blockquote>
<p>Figure 4.14 An architecture diagram in which a shared resource pool is
being used by different components--- for example, data ingestion,
model training, model selection, and model deployment---and two
different model serving systems at the same time. The arrows with
solid lines indicate resources, and the arrows with dashed lines
indicate requests.</p>
</blockquote>
<p>Thanksgiving Christmas days</p>
<blockquote>
<p>Figure 4.15 The traffic of our model serving system over time. An
unexpected bump happened before Christmas that suddenly added traffic.
The jump in requests is handled successfully by the model serving
system by borrowing the necessary amount of resources from the shared
resource pool. The resource utilization rate remains high during this
unexpected event.</p>
</blockquote>
<ul>
<li>
<p>This event suddenly adds traffic, but the model serving system
    successfully handles the surge in traffic by borrowing a necessary
    amount of resources from the shared resource pool.</p>
</li>
<li>
<p>With the help of the shared resource pool, the resource utilization
    rate remains high during this unexpected event.</p>
</li>
<li>
<p>The shared resource pool monitors the current amount of available
    resources and autoscales when needed.</p>
</li>
<li>
<p>This approach, in which the system listens to the user requests and
    only responds and utilizes the computational resources when the user
    request is being made, is called <em>event-driven processing</em>.</p>
</li>
</ul>
<p><strong>Event-driven processing vs. long-running serving systems</strong></p>
<ul>
<li>
<p>Event-driven processing is different from the model serving systems
    that we've looked at in previous sections , where the servers that
    handle user requests are always up and running.</p>
</li>
<li>
<p>Those long-running serving systems work well for many applications
    that are under heavy load, keep a large amount of data in memory, or
    require some sort of background processing.</p>
</li>
<li>
<p>However, for applications that handle very few requests during non
    peak periods or respond to specific events, such as our hotel price
    prediction system, the event-driven processing pattern is more
    suitable.</p>
</li>
<li>
<p>This event-driven processing pattern has flourished in recent years
    as cloud providers have developed <strong><em>function-as-a-service</em></strong>
    products.</p>
</li>
<li>
<p>Each model serving request made from our hotel price prediction
    system represents an <em>event</em>.</p>
</li>
<li>
<p>Our serving system listens for this type of event, utilizes
    necessary resources from the shared resource pool, and retrieves and
    loads the trained machine learning model from the distributed
    database to estimate the hotel prices for the specified
    time/location query (Figure 4.16)</p>
</li>
<li>
<p>Using this event-driven processing pattern for our serving system,
    we can make sure that our system is using only the resources
    necessary to process every request without concerning ourselves with
    resource utilization and idling.</p>
</li>
<li>
<p>As a result, the system has sufficient resources to deal with peak
    traffic and return the predicted prices without users experiencing
    noticeable delays or lags when using the system.</p>
</li>
</ul>
<p><strong>Denial of Service and Rate Limiting</strong></p>
<ul>
<li>
<p>Even though we now have a shared pool of sufficient computational
    resources where we can borrow computational resources from the
    shared resource pool to handle user requests on demand, we should
    also build a mechanism in our model serving system to defend
    <em>denial-of-service attacks</em>.</p>
</li>
<li>
<p>Denial-of-service attacks interrupt an authorized user's access to a
    computer network, typically caused with malicious intent and often
    seen in model serving systems.</p>
</li>
<li>
<p>These attacks can cause unexpected use of computational resources
    from the shared resource pool, which may eventually lead to resource
    scarcity for other services that rely on the shared resource pool.</p>
</li>
<li>
<p>Denial-of-service attacks may happen in various cases malicious and
    non-malicious.</p>
</li>
</ul>
<p>Figure 4.16 A Event-driven model serving system to predict hotel prices</p>
<ul>
<li>
<p>To deal with these situations, which often happen in real-world
    applications, it makes sense to introduce a defense mechanism for
    denial-of-service attacks</p>
</li>
<li>
<p>One approach to avoid these attacks is via <em>rate limiting</em>, which
    adds the model serving requests to a queue and limits the rate the
    system is processing the requests in the queue (Fig. 4.17).</p>
</li>
<li>
<p>Figure 4.17 is a flowchart showing four model serving requests sent
    to the model serving system. However, only two are under the current
    rate limit, which allows a maximum of two concurrent model serving
    requests. In this case, the rate-limiting queue for model serving
    requests first checks whether the requests received are under the
    current rate limit. Once the system has finished processing those
    two requests, it will proceed to the remaining two requests in the
    queue.</p>
</li>
<li>
<p>If we are deploying and exposing an API for a model serving service
    to our users, it's also generally a best practice to have a
    relatively small rate limit (e.g., only one request is allowed
    within 1 hour) for users with anonymous access and then ask users to
    log in to obtain a higher rate limit.</p>
</li>
<li>
<p>This system would allow the model serving system to better control
    and monitor the users' behavior and traffic so that we can take
    necessary actions to address any potential problems or
    denial-of-service attacks.</p>
</li>
<li>
<p>For example, requiring a login provides auditing to find out which
    users/events are responsible for the unexpectedly large number of
    model serving requests.</p>
</li>
</ul>
<blockquote>
<p>Model serving requests</p>
</blockquote>
<p>Ok to add to the queue?</p>
<blockquote>
<p>Model serving requests that are under rate limit:</p>
<p>2 maximum concurrent requests</p>
</blockquote>
<p>Figure 4.17 A flowchart of four model serving requests sent to the model
serving system. However, only two are under the current rate limit,
which allows a maximum of two concurrent model serving requests. Once
the system has finished processing those two requests, it will proceed
to the remaining two requests in the queue</p>
<ul>
<li>
<p>Figure 4.18 shows the previously described strategy.</p>
</li>
<li>
<p>In the diagram, the flowchart on the left side is the same as figure
    4.17 where four total model serving requests from unauthenticated
    users are sent to the model serving system.</p>
</li>
</ul>
<p>Model serving requests from authenticated users Model serving requests
from <strong>unauthenticated</strong> users</p>
<p>Ok to add to the queue? Ok to add to the queue?</p>
<blockquote>
<p>Model serving requests that are Model serving requests that are under
rate limit: under rate limit:</p>
<p>2 maximum concurrent requests 3 maximum concurrent requests for
<strong>unauthenticated</strong> users for <strong>authenticated</strong> users</p>
</blockquote>
<p>Figure 4.18 A comparison of behaviors from different rate limits applied
to authenticated and unauthenticated users</p>
<ul>
<li>However, only two can be served by the system due to the current
    rate limit, which allows a maximum of two concurrent model serving
    requests for unauthenticated users.</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   Conversely, the model serving requests in the flowchart on the right
    side all come from authenticated users. Thus, three requests can be
    processed by the model serving system since the limit of maximum
    concurrent requests for authenticated users is three.</p>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   Rate limits differ depending on whether the user is authenticated.
    Rate limits thus effectively control the traffic of the model
    serving system and prevent malicious denial-of-service attacks,
    which could cause unexpected use of computational resources from the
    shared resource pool and eventually lead to resource scarcity of
    other services that rely on it.</p>
<h5 class="unnumbered" id="points-to-note-1">Points to Note:</h5>
<ul>
<li>
<p>This pattern is not a universal solution.</p>
</li>
<li>
<p>For machine learning applications with consistent traffic---for
    example, model predictions calculated regularly based on a
    schedule---an event-driven processing approach is unnecessary as the
    system already knows when to process the requests, and there will be
    too much overhead trying to monitor this regular traffic.</p>
</li>
<li>
<p>In addition, applications that can tolerate less-accurate
    predictions can work well without being driven by events; they can
    also recalculate and provide good-enough predictions to a particular
    granularity level, such as per day or per week.</p>
</li>
<li>
<p>Event-driven processing is more suitable for applications with
    different traffic patterns that are complicated for the system to
    prepare beforehand necessary computational resources. With
    event-driven processing, the model serving system only requests a
    necessary amount of computational resources on demand.</p>
</li>
<li>
<p>The applications can also provide more accurate and real-time
    predictions since they obtain the predictions right after the users
    send requests instead of relying on precalculated prediction results
    based on a schedule.</p>
</li>
<li>
<p>From developers' perspective, one benefit of the event-driven
    processing pattern is that it's very intuitive.</p>
</li>
<li>
<p>It greatly simplifies the process of deploying code to running
    services since there is no end artifact to create or push beyond the
    source code itself. The event-driven processing pattern makes it
    simple to deploy code from our laptops or web browser to run code in
    the cloud.</p>
</li>
<li>
<p>In our case, we only need to deploy the trained machine learning
    model that may be used as a <em>function</em> to be triggered based on user
    requests.</p>
</li>
<li>
<p>Once deployed, this model serving function is then managed and
    scaled automatically without the need to allocate resources manually
    by developers.</p>
</li>
<li>
<p>In other words, as more traffic is loaded onto the service, more
    instances of the model serving function are created to handle the
    increase in traffic using the shared resource pool.</p>
</li>
<li>
<p>If the model serving function fails due to machine failures, it will
    be restarted automatically on other machines in the shared resource
    pool.</p>
</li>
<li>
<p>Given the nature of the event-driven processing pattern, each
    function that's used to process the model serving requests needs to
    be <em>stateless</em> and independent from other model serving requests.</p>
</li>
<li>
<p>Each function instance cannot have local memory, which requires all
    states to be stored in a storage service. For example, if our
    machine learning models depend heavily on the results from previous
    predictions (e.g., a time-series model), in this case, the
    event-driven processing pattern may not be suitable.</p>
</li>
</ul>
<h5 class="unnumbered" id="quiz-1">Quiz</h5>
<ol>
<li>
<p>Suppose we allocate the same amount of computational resources over
    the lifetime of the model serving system for hotel price prediction.
    What would the resource utilization rate look like over time?</p>
</li>
<li>
<p>Are the replicated services or sharded services long-running
    systems?</p>
</li>
<li>
<p>Is event-driven processing stateless or stateful?</p>
</li>
</ol>
<p>[<strong>Part
3</strong>___________________________________________________________________]{.underline}</p>
<p><strong>What we will cover:</strong></p>
<ul>
<li>
<p>Using workflows to connect machine learning system components</p>
</li>
<li>
<p>Composing complex but maintainable structures within machine
    learning workflows with the fan-in and fan-out patterns</p>
</li>
<li>
<p>Accelerating machine learning workloads with concurrent steps using
    synchronous and asynchronous patterns</p>
</li>
<li>
<p>Improving performance with the step memoization pattern</p>
</li>
</ul>
<p><strong>Outcomes:</strong></p>
<ul>
<li>
<p>Understand workflows and their use in ML systems</p>
</li>
<li>
<p>Understand step composition and workflow patterns</p>
</li>
<li>
<p>Understand how to improve scale and performance in workflows</p>
</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   Model serving is a critical step after successfully training a
    machine learning model.</p>
<ul>
<li>
<p>It is the final artifact produced by the entire machine learning
    workflow, and the results from model serving are presented to users.</p>
</li>
<li>
<p>A Workflow is an essential component in machine learning systems as
    it connects all other components in the system.</p>
</li>
<li>
<p>A machine learning workflow can be as easy as chaining data
    ingestion, model training, and model serving.</p>
</li>
<li>
<p>However, it can be very complex to handle real-world scenarios
    requiring additional steps and performance optimizations as part of
    the entire workflow.</p>
</li>
<li>
<p>We need to know what tradeoffs we may see when making design
    decisions to meet different business and performance requirements.</p>
</li>
</ul>
<p><strong>What is a workflow?</strong></p>
<ul>
<li>
<p><strong>A</strong> <em>Workflow</em> is the process of connecting multiple components or
    steps in an end-to-end machine learning system.</p>
</li>
<li>
<p>A workflow consists of arbitrary combinations of the components
    commonly seen in real-world machine learning applications, such as
    data ingestion, distributed model training, and model serving, as
    discussed in the previous chapters.</p>
</li>
<li>
<p>Figure 5.1 shows a simple machine learning workflow. This workflow
    connects multiple components or steps in an end-to-end machine
    learning system that includes the following steps:</p>
</li>
<li>
<p>Data ingestion---Consumes the Youtube-8M videos dataset</p>
</li>
<li>
<p>Model training---Trains an entity-tagging model</p>
</li>
<li>
<p>Model serving---Tags entities in unseen videos</p>
</li>
<li>
<p>Machine learning workflows are often referred to as <em>machine
    learning pipelines</em>.</p>
</li>
<li>
<p>A machine learning workflow may consist of any combination of the
    components</p>
</li>
<li>
<p>Machine learning workflows appear in different forms in different
    situations</p>
</li>
<li>
<p>Fig 5.2 shows a more complicated workflow where two separate model
    training steps are launched after a single data ingestion step, and
    then two separate model serving steps are used to serve different
    models trained via different model training steps</p>
</li>
</ul>
<blockquote>
<p>Figure 5.1 A simple machine learning workflow, including data
ingestion, model training, and model serving. The arrows indicate
directions. For example, the arrow on the right-hand side denotes the
order of the step execution (e.g., the workflow executes the model
serving step after the model training step is completed).</p>
<p>Figure 5.2 A more complicated workflow, where two separate model
training steps are launched after a single data ingestion step, and
then two separate model serving steps are used to serve different
models trained via different model training steps</p>
</blockquote>
<ul>
<li>The complexity of machine learning workflows varies, which increases
    the difficulty of building and maintaining scalable machine learning
    systems.</li>
</ul>
<p><strong>Sequential workflows and Directed Acyclic Graphs (DAGs)</strong></p>
<ul>
<li>
<p>A <em>sequential workflow</em> is a series of steps performed one after
    another until the last step in the series is complete.</p>
</li>
<li>
<p>The exact order of execution varies, but steps will always be
    sequential. Figure 5.3 is a sequential workflow with three steps
    executed sequentially.</p>
</li>
</ul>
<blockquote>
<p><strong>A sequential workflow represents a series of steps performed one
after another until the last step in the series has completed. The
exact order of execution varies, but steps will always be
sequential.</strong></p>
<p><strong>Step C executes after step B has completed.</strong></p>
<p>Figure 5.3 An example sequential workflow with three steps that
execute in the following order: A, B, and C.</p>
</blockquote>
<ul>
<li>
<p>A workflow can be seen as a DAG if it only consists of steps
    directed from one step to another but <strong><em>never form a closed
    loop</em></strong>.</p>
</li>
<li>
<p>Figure 5.3 is a valid DAG, figure 5.4, however, is not a valid DAG</p>
</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   ML Workflows, in order to meet the requirements of different use
    cases (e.g., batch retraining of the models, hyperparameter tuning
    experiments, etc.) can get really complicated.</p>
<blockquote>
<p><strong>A workflow where there's an additional step D that connects from
step C and points to step A. These connections form a closed loop and
thus the entire workflow is not a valid DAG.</strong></p>
</blockquote>
<p>Figure 5.4 An example workflow where step D connects from step C and
points to step A. These connections form a closed loop and thus the
entire workflow is not a valid DAG.</p>
<blockquote>
<p><strong>The closed loop no longer exists since this arrow is crossed out.</strong></p>
</blockquote>
<p>Figure 5.5 Workflow where the last step D does not point back to step A.
This workflow is a valid DAG since the closed loop no longer exists.
Instead, it is a simple sequential workflow similar to figure 5.3.</p>
<p><strong>Fan-in and Fan-out Patterns</strong></p>
<ul>
<li>
<p>Q: What if the original YouTube-8M dataset has been updated, and
    we'd like to train a new model from scratch using the same model
    architecture?</p>
</li>
<li>
<p>In this case, it's pretty easy to containerize each of these
    components and chain them together in a machine learning workflow
    that can be reused by re-executing the end-to-end workflow when the
    data gets updated.</p>
</li>
<li>
<p>As shown in figure 5.6, new videos are regularly being added to the
    original YouTube-8M dataset, and the workflow is executed every time
    the dataset is updated. The next model training step trains the
    entity tagging model using the most recent dataset. Then, the last
    model serving step uses the trained model to tag entities in unseen
    videos.</p>
</li>
</ul>
<blockquote>
<p>Figure 5.6 New videos are regularly added to the original YouTube-8M
dataset, and the workflow is executed every time the dataset is
updated.</p>
</blockquote>
<p><strong>Challenge:</strong> Build a machine learning workflow that trains different
models after the system has ingested data from the data source, selects
the top two models and uses the knowledge from both to provide model
serving that generates predictions for users.</p>
<ul>
<li>
<p>Building a workflow that includes the end-to-end normal process of a
    machine learning system with only data ingestion, model training,
    and model serving, where each component only appears once as an
    individual step in the workflow, is pretty straightforward.</p>
</li>
<li>
<p>Here, the workflow is more complex as we need to include multiple
    model training steps as well as multiple model serving steps.</p>
</li>
<li>
<p>Q: How do we formalize and generalize the structure of this complex
    workflow so that it can be easily packaged, reused, and distributed?</p>
</li>
</ul>
<h4 class="unnumbered" id="solution-2"><strong>Solution</strong></h4>
<ul>
<li>Fig 5.7, a basic machine learning workflow that includes only data
    ingestion, model training, and model serving, where each of these
    components only appears once as an individual step in the workflow..</li>
</ul>
<blockquote>
<p><strong>Baseline workflow that includes only data ingestion, model training,
and model serving where each of these components only appears once as
individual steps in the workflow</strong></p>
<p>Figure 5.7 A baseline workflow including only data ingestion, model
training, and model serving, where each of these components only
appears once as an individual step in the workflow</p>
</blockquote>
<ul>
<li>
<p>We need a machine learning workflow that builds and selects the top
    two best-performing models that will be used for model serving to
    give better inference results.</p>
</li>
<li>
<p>What is the motivation here?</p>
<ul>
<li>
<p>Figure 5.8 shows two models: the first model has knowledge of
    four entities, and the second model has knowledge of three
    entities. Thus, each can tag the entities it knows from the
    videos.</p>
</li>
<li>
<p>We can use both models to tag entities at the same time and then
    aggregate their results. The aggregated result is obviously more
    knowledgeable and is able to cover more entities.</p>
</li>
<li>
<p>Thus two models can be more effective and produce more
    comprehensive entity-tagging results.</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>Figure 5.8 The first model has knowledge of four entities and the
second model has knowledge of three entities. Thus, each can tag the
entities it knows from the videos. We can use both models to tag
entities at the same time and then aggregate their results. The
aggregated result covers more entities than each individual model.</p>
</blockquote>
<ul>
<li>
<p>Now that we understand the motivation behind building this complex
    workflow, let's look at an overview of the entire end-to-end
    workflow process.</p>
</li>
<li>
<p>We want to build a machine learning workflow that performs the
    following functions sequentially:</p>
</li>
<li>
<p>Ingests data from the same data source</p>
</li>
<li>
<p>Trains multiple different models, either different sets of
    hyperparameters of the same model architecture or various model
    architectures</p>
</li>
<li>
<p>Picks the two top-performing models to be used for model serving for
    each of the trained models</p>
</li>
<li>
<p>Aggregates the models' results of the two model serving systems to
    present to users</p>
</li>
<li>
<p>Let's add <strong><em>placeholders</em></strong> to the baseline workflow for multiple
    model training steps <strong><em>after</em></strong> data ingestion.</p>
</li>
<li>
<p>We can then add multiple model serving steps once the multiple model
    training steps finish.</p>
</li>
<li>
<p>A diagram of the enhanced baseline workflow is shown in figure 5.9.</p>
</li>
<li>
<p>The key difference from what we've dealt with before in the baseline
    is the presence of <strong><em>multiple model training and model serving
    components</em></strong>.</p>
</li>
<li>
<p>The steps do not have direct, one-to-one relationships i.e., each
    model training step may be connected to a single model serving step
    or not connected to any steps at all.</p>
</li>
</ul>
<p>Figure 5.9 A diagram of the enhanced baseline workflow where multiple
model training steps occur after data ingestion, followed by multiple
model serving steps</p>
<ul>
<li>Figure 5.10 shows that the models trained from the first two model
    training steps outperform the model trained from the third model
    training step. Thus, only the first two model training steps are
    connected to the model serving steps.</li>
</ul>
<blockquote>
<p><strong>The models trained from the first two model training steps
outperform the model trained from the third model training step. Thus,
only the first two model training steps are connected to model serving
steps.</strong></p>
</blockquote>
<p>Figure 5.10 The models trained from the first two model training steps
outperform the model trained from the third model training step. Thus,
only the first two model training steps are connected to the model
serving steps.</p>
<ul>
<li>
<p>We can compose this workflow as follows:</p>
</li>
<li>
<p>On successful data ingestion, multiple model training steps are
    connected to the data ingestion step so that they can use the shared
    data that's ingested and cleaned from the original data source.</p>
</li>
<li>
<p>Next, a single step is connected to the model training steps to
    select the top two performing models. It produces two model serving
    steps that use the selected models to handle model serving requests
    from users.</p>
</li>
<li>
<p>A final step at the end of this machine learning workflow is
    connected to the two model serving steps to aggregate the model
    inference results that will be presented to the users.</p>
</li>
<li>
<p>In figure 5.11, the workflow trains different models via three model
    training steps resulting in varying accuracy when tagging entities.
    A model selection step picks the top two models with at least 90%
    accuracy trained from the first two model training steps that will
    be used in the following two separate model serving steps. The
    results from the two model serving steps are then aggregated to
    present to users via a result aggregation step.</p>
</li>
</ul>
<blockquote>
<p><strong>Three model training steps train</strong></p>
<p>Figure 5.11 A machine learning workflow that trains different models
that result in varying accuracy when tagging entities and then selects
the top two models with at least 90% accuracy to be used for model
serving. The results from the two model serving steps are then
aggregated to present to users.</p>
</blockquote>
<ul>
<li>
<p>We can pick out two patterns from this complex workflow. \</p>
</li>
<li>
<p>The first one we observe is the <em>fan-out</em> pattern.</p>
<ul>
<li>Fan-out describes the process of starting multiple separate
    steps to handle input from the workflow. In our workflow, the
    fan-out pattern appears when multiple separate model training
    steps connect to the data ingestion step, as shown in figure
    5.12.</li>
</ul>
</li>
<li>
<p>There's also the <em>fan-in</em> pattern in our workflow, where we have one
    single aggregation step that combines the results from the two model
    serving steps, as shown in 5.13</p>
</li>
</ul>
<blockquote>
<p>Figure 5.12 A diagram of the fan-out pattern that appears when
multiple separate model training steps are connected to the data
ingestion step</p>
<p>figure 5.13. Fan-in describes the process of combining results from
multiple steps into one step.</p>
<p><strong>Fanning in from two model serving steps to one result aggregation
step.</strong></p>
<p>Figure 5.13 A diagram of the fan-in pattern, where we have one single
aggregation step that combines the results from the two model serving
steps</p>
</blockquote>
<ul>
<li>Formalizing these patterns would help us build and organize more
    complex workflows by using different patterns for workflows based on
    real-world requirements.</li>
</ul>
<h4 class="unnumbered" id="points-to-note-2"><strong>Points to note:</strong></h4>
<ul>
<li>
<p>Using the fan-in and fan-out patterns, the system is able to execute
    complex workflows that train multiple machine learning models and
    pick the most performant ones to provide good entity-tagging results
    in the model serving system.</p>
</li>
<li>
<p>These patterns are great abstractions that can be incorporated into
    very complex workflows to meet the increasing demand for complex
    distributed machine learning workflows in the real world.</p>
</li>
<li>
<p>In general, if both of the following applies, we can consider
    incorporating these patterns:</p>
</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   The multiple steps that we are fanning-in or fanning-out are
    independent of each other.</p>
<ul>
<li>It takes a long time for these steps to run sequentially.</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   The multiple steps need to be order-independent because we don't
    know the order in which concurrent copies of those steps will run or
    the order in which they will return.</p>
<ul>
<li>
<p>For example, if the workflow also contains a step that trains an
    ensemble of other models (also known as <em>ensemble learning</em>;
    <a href="http://mng.bz/N2vn">http://mng.bz/N2vn</a>) to provide a better-aggregated model, this
    ensemble model depends on the completion of other model training
    steps.</p>
</li>
<li>
<p>Consequently, we cannot use the fan-in pattern because the ensemble
    model training step will need to wait for other model training to
    complete before it can start running, which would require some extra
    waiting and delay the entire workflow.</p>
</li>
</ul>
<p><strong>Ensemble models</strong></p>
<ul>
<li>
<p>An ensemble model uses multiple machine learning models to obtain
    better predictive performance than could be obtained from any of the
    constituent models alone.</p>
</li>
<li>
<p>It often consists of a number of alternative models that can learn
    the relationships in the dataset from different perspectives.</p>
</li>
<li>
<p>Ensemble models tend to yield better results when diversity among
    the constituent models is significant.</p>
</li>
<li>
<p>Therefore, many ensemble approaches try to increase the diversity of
    the models they combine.</p>
</li>
</ul>
<p><strong>Workflow complexity and performance</strong></p>
<ul>
<li>
<p>The fan-in and fan-out patterns can create very complex workflows
    that meet most of the requirements of machine learning systems.</p>
</li>
<li>
<p>However, to achieve good performance on those complex workflows, we
    need to determine which parts of the workflows to run first and
    which parts of the workflows can be executed in parallel.</p>
</li>
<li>
<p>The result of this optimization is that data science teams would
    spend less time waiting for workflows to complete, thus reducing
    infrastructure costs.</p>
</li>
</ul>
<h5 id="section-2-unnumbered">{#section-2 .unnumbered}</h5>
<h5 class="unnumbered" id="questions">Questions:</h5>
<ol>
<li>
<p>If the steps are not independent of each other, can we use the
    fan-in or fan-out patterns?</p>
</li>
<li>
<p>What's the main problem when trying to build ensemble models with
    the fan-in pattern?</p>
</li>
</ol>
<p><strong>Synchronous and asynchronous patterns: Accelerating workflows with
concurrency</strong></p>
<ul>
<li>
<p>Each model training step in the system takes a long time to
    complete; however, their durations may vary across different model
    architectures or model parameters.</p>
</li>
<li>
<p>Imagine an extreme case (LLMs) where one of the model training steps
    takes two weeks to complete since it is training a complex machine
    learning model that requires a huge amount of computational
    resources.</p>
</li>
<li>
<p>All other model training steps only take one week to complete.</p>
</li>
<li>
<p>Many of the steps, such as model selection and model serving, in the
    machine learning workflow we built earlier that uses the fan-in and
    fan-out patterns will have to wait an additional week until this
    long-running model training step is completed.</p>
</li>
<li>
<p>Fig. 5.14 illustrates the duration differences among the three model
    training steps.</p>
</li>
</ul>
<blockquote>
<p><strong>One of the model training steps takes two weeks to complete since it
is training a complex machine learning model that requires a huge
amount of computational resources, whereas each of the rest of the
model training steps only takes one week to complete.</strong></p>
</blockquote>
<p>Figure 5.14 A workflow that illustrates the duration differences for the
three model training steps</p>
<ul>
<li>
<p>In this case, since the model selection step and the steps following
    it require all model training steps to finish, the model training
    step that takes two weeks to complete will slow down the workflow by
    an entire week.</p>
</li>
<li>
<p>We would rather use that additional week to re-execute all the model
    training steps that take one week to complete instead of wasting
    time waiting for one step!</p>
</li>
</ul>
<h4 id="section-3-unnumbered">{#section-3 .unnumbered}</h4>
<p><strong>The Challenge:</strong> Accelerate workflows so it will not be affected by
the duration of individual steps</p>
<ul>
<li>
<p>We want to build a machine learning workflow that trains different
    models and then selects the top two models to use for model serving,
    which generates predictions based on the knowledge of both models.</p>
</li>
<li>
<p>Due to varying completion times for each model training step in the
    existing machine learning workflow, the start of the following
    steps, such as the model selection step and the model serving,
    depends on the completion of the previous steps.</p>
</li>
<li>
<p>However, a problem occurs when at least one of the model training
    steps takes much longer to complete than the remaining steps because
    the model selection step that follows can only start after this long
    model training step has completed.</p>
</li>
<li>
<p>As a result, the entire workflow is delayed by this particularly
    long-running step. Is there a way to accelerate this workflow so it
    will not be affected by the duration of individual steps?</p>
</li>
</ul>
<h4 id="section-4-unnumbered">{#section-4 .unnumbered}</h4>
<p><strong>The solution:</strong></p>
<ul>
<li>
<p>What if we can exclude the long-running model training step
    completely?</p>
</li>
<li>
<p>Once we do that, the rest of the model training steps will have
    consistent completion times.</p>
</li>
<li>
<p>Thus, the remaining steps in the workflow can be executed without
    waiting for a particular step that's still running. A diagram of the
    updated workflow is shown in figure 5.15.</p>
</li>
<li>
<p>This naive approach may resolve our problem of extra waiting time
    for long-running steps.</p>
</li>
<li>
<p>However, our original goal was to use this type of complex workflow
    to experiment with different machine learning model architectures
    and different sets of hyperparameters of those models to select the
    best-performing models to use for model serving.</p>
</li>
<li>
<p>If we simply exclude the long-running model training step, we are
    essentially throwing away the opportunity to experiment with
    advanced models that may better capture the entities in the videos.</p>
</li>
<li>
<p>Is there a better way to speed up the workflow so that it will not
    be affected by the duration of this individual step? Let's focus on
    the model training steps that only take one week to complete.</p>
</li>
<li>
<p>What can we do when those short-running model training steps are
    complete?</p>
</li>
</ul>
<blockquote>
<p><strong>After the long-running model training step is excluded, the rest of
the model training steps will have consistent completion time. Thus,
the remaining steps in the workflow can be executed without having to
wait for any particular step that's still running.</strong></p>
</blockquote>
<p>Figure 5.15 The new workflow after the long-running model training step
has been removed</p>
<ul>
<li>
<p>When a model training step finishes, we have successfully obtained a
    trained machine learning model.</p>
</li>
<li>
<p>In fact, we can use this trained model in our model serving system
    without waiting for the rest of the model training steps to
    complete.</p>
</li>
<li>
<p>As a result, the users can see the results of tagged entities from
    their model serving requests that contain videos as soon as we have
    trained one model from one of the steps in the workflow (figure
    5.16).</p>
</li>
<li>
<p>After a second model training step finishes, we can then pass the
    two trained models directly to model serving. The aggregated
    inference results are presented to users instead of the results from
    only the model we obtained initially, as shown in figure 5.17</p>
</li>
</ul>
<p><strong>Uses the trained model from this short-running model training step
that finishes earlier directly in our model serving system without
waiting for the rest of the model training steps to complete</strong></p>
<blockquote>
<p>Figure 5.16 A workflow where the trained model from a short-running
model training step is applied directly to our model serving system
without waiting for the remaining model training steps to complete</p>
</blockquote>
<h5 id="section-5-unnumbered">{#section-5 .unnumbered}</h5>
<blockquote>
<p><strong>After a second model training step finishes, we can pass the two
trained models directly to be used for model serving, and the
aggregated inference results will be presented to users instead of the
results from only the one model that we obtained initially.</strong></p>
<p>Figure 5.17 After a second model training step finishes, we pass the
two trained models directly to model serving. The aggregated inference
results are presented to users instead of only the results from the
model that we obtained initially.</p>
</blockquote>
<ul>
<li>
<p>Note that while we can continue to use the trained models for model
    selection and model serving, the long-running model training step is
    still running.</p>
</li>
<li>
<p>In other words, the steps are executed <em>asynchronously</em>---they don't
    depend on each other's completion.</p>
</li>
<li>
<p>The workflow starts executing the next step before the previous step
    finishes.</p>
</li>
<li>
<p>Sequential steps are performed one at a time, and only when one has
    completed does the following step become unblocked. In other words,
    you must wait for a step to finish to move to the next one.</p>
</li>
<li>
<p>For example, the data ingestion step must be completed before we
    start any of the model training steps.</p>
</li>
<li>
<p>Contrary to asynchronous steps, synchronous steps can start running
    at the same time once dependencies are met.</p>
</li>
<li>
<p>For example, the model training steps can run concurrently, as soon
    as the previous data ingestion step has finished. A different model
    training step does not have to wait for another to start.</p>
</li>
<li>
<p>The synchronous pattern is typically useful when you have multiple
    similar workloads that can run concurrently and finish near the same
    time.</p>
</li>
<li>
<p>By incorporating these patterns, the entire workflow will no longer
    be blocked by the long-running model training step. Instead, it can
    continue using the already trained models from the short-running
    model training steps in the model serving system, which can start
    handling users' model serving requests.</p>
</li>
<li>
<p>The synchronous and asynchronous patterns are also extremely useful
    in other distributed systems to optimize system performance and
    maximize the use of existing computational resources---especially
    when the amount of computational resources for heavy workloads is
    limited.</p>
</li>
</ul>
<p><strong>Discussion</strong></p>
<ul>
<li>
<p>By mixing synchronous and asynchronous patterns, we can create more
    efficient machine learning workflows and avoid any delays due to
    steps that prevent others from executing, such as a long-running
    model training step.</p>
</li>
<li>
<p>However, the models trained from the short-running model training
    steps may not be very accurate.</p>
</li>
<li>
<p>That is, the models with simpler architectures may not discover as
    many entities in the videos as the more complex model of the
    long-running model training step (figure 5.18).</p>
</li>
</ul>
<blockquote>
<p>Figure 5.18 A model trained from two finished short-running model
training steps with very simple models that serve as a baseline. They
can only identify a small number of entities, whereas the model
trained from the most time-consuming step can identify many more
entities.</p>
</blockquote>
<ul>
<li>
<p>As a result, we should keep in mind that the models we get early on
    may not be the best and may only be able to tag a small number of
    entities, which may not be satisfactory to our users.</p>
</li>
<li>
<p>When we deploy this end-to-end workflow to real-world applications,
    we need to consider whether users seeing inference results faster or
    seeing better results is more important. If the goal is to allow
    users to see the inference results as soon as a new model is
    available, they may not see the results they were expecting.</p>
</li>
<li>
<p>However, if users can tolerate a certain period of delay, it's
    better to wait for more model training steps to finish.</p>
</li>
<li>
<p>Then, we can be selective about the models we've trained and pick
    the best performing models that provide very good entity-tagging
    results.</p>
</li>
<li>
<p>Whether a delay is acceptable is subject to the requirements of
    real-world applications.</p>
</li>
<li>
<p>By using synchronous and asynchronous patterns, we can organize the
    steps in machine learning workflows from structural and
    computational perspectives.</p>
</li>
<li>
<p>As a result, data science teams can spend less time waiting for
    workflows to complete to maximize performance, thus reducing
    infrastructure costs and idling computational resources.</p>
</li>
</ul>
<p><strong>Quiz:</strong></p>
<ol>
<li>
<p>What causes each step of the model training steps to start?</p>
</li>
<li>
<p>Are the steps blocking each other if they are running
    asynchronously?</p>
</li>
<li>
<p>What do we need to consider when deciding whether we want to use any
    available trained model as early as possible?</p>
</li>
</ol>
<p><strong>Step memoization pattern: Skipping redundant workloads via memoized
steps</strong></p>
<ul>
<li>
<p>The dataset does not always remain unchanged (new YouTube videos are
    becoming available and are being added to the YouTube-8M dataset
    every week)</p>
</li>
<li>
<p>if, we would like to retrain the model so that it accounts for the
    additional videos that arrive on a regular basis, we need to run the
    entire workflow regularly from scratch---from the data ingestion
    step to the model serving step---as shown in figure 5.19.</p>
</li>
<li>
<p>Say the dataset does not change, but we want to experiment with new
    model architectures or new sets of hyperparameters, which is very
    common for machine learning practitioners (figure 5.20).</p>
</li>
<li>
<p>For example, we may change the model architecture from simple linear
    models to more complex models such as tree-based models or
    convolutional neural networks. We can also stick with the particular
    model architecture we've used and only change the set of model
    hyperparameters, such as the number of layers and hidden units in
    each of those layers for neural network models or the maximum depth
    of each tree for tree-based models. For cases like these, we still
    need to run the end-to-end workflow, which includes the data
    ingestion step to re-ingest the data from the original data source
    from scratch. Performing data ingestion again is very
    time-consuming.</p>
</li>
</ul>
<p>Figure 5.19 A diagram of the entire workflow that is re-executed every
time the dataset is updated</p>
<blockquote>
<p>New model type?</p>
<p>1. Linear model</p>
<p>Figure 5.20 A diagram where the entire workflow is re-executed every
time we experiment with a new model type or hyperparameter even though
the dataset has not changed</p>
</blockquote>
<p><strong>The problem</strong></p>
<ul>
<li>If the dataset is not updated, but we want to experiment with new
    models, we still need to execute the entire workflow, including the
    data ingestion step. However, the data ingestion step can take a
    long time to complete depending on the size of the dataset. Is there
    a way to make this workflow more efficient?</li>
</ul>
<h4 class="unnumbered" id="the-solution-1"><strong>The solution</strong></h4>
<ul>
<li>We would like to execute the data ingestion step only when we know
    that the dataset has been updated, as shown in figure 5.21.</li>
</ul>
<p>*<em>The dataset has </em>not ***</p>
<p><strong>been updated yet.</strong></p>
<blockquote>
<p>Figure 5.21 A diagram where the data ingestion step is skipped when
the dataset has not been updated</p>
</blockquote>
<ul>
<li>
<p>Once we have a way to identify that, we can conditionally
    reconstruct the machine learning workflow and control whether we
    want to include a data ingestion step to be re-executed (figure
    5.21).</p>
</li>
<li>
<p>One way to identify whether the dataset has been updated is through
    the use of cache. Since our dataset is being updated regularly on a
    fixed schedule (e.g., once a month), we can create a <em>time-based
    cache</em> that stores the location of the ingested and cleaned dataset
    (assuming the dataset is located in a remote database) and the
    timestamp of its last updated time. The data ingestion step in the
    workflow will then be constructed and executed dynamically based on
    whether the last updated timestamp is within a particular window.</p>
</li>
<li>
<p>For example, if the time window is set to two weeks, we consider the
    ingested data as fresh if it has been updated within the past two
    weeks. The data ingestion step will be skipped, and the following
    model training steps will use the already-ingested dataset from the
    location that's stored in the cache.</p>
</li>
<li>
<p>Figure 5.22 illustrates the case where a workflow has been
    triggered, and we check whether the data has been updated within the
    last two weeks by accessing the cache. If the data is fresh, we skip
    the execution of the unnecessary data ingestion step and execute the
    model training step directly.</p>
</li>
</ul>
<p>Figure 5.22 The workflow has been triggered, and we check whether the
data has been updated within the last two weeks by accessing the cache.
If the data is fresh, we skip the execution of the unnecessary data
ingestion step and execute the model training step directly.</p>
<ul>
<li>
<p>The time window can be used to control how old a cache can be before
    we consider the dataset fresh enough to be used directly for model
    training instead of re-ingesting the data again from scratch.</p>
</li>
<li>
<p>Alternatively, we can store some of the important metadata about the
    data source in the cache, such as the number of records in the
    original data source currently available.</p>
</li>
<li>
<p>This type of cache is called <em>content-based cache</em> since it stores
    information extracted from a particular step, such as the input and
    output information.</p>
</li>
<li>
<p>With this type of cache, we can identify whether the data source has
    significant changes (e.g., the number of original records has
    doubled in the data source). If there's a significant change, it's
    usually a signal to re-execute the data ingestion step since the
    current dataset is very old and outdated. A workflow that
    illustrates this approach is shown in figure 5.23.</p>
</li>
</ul>
<blockquote>
<p>Figure 5.23 The workflow has been triggered, and we check whether the
metadata collected from the dataset, such as the number of records in
the dataset, has changed significantly. If it's not significant, we
then skip the execution of the unnecessary data ingestion step and
execute the model training step directly.</p>
</blockquote>
<ul>
<li>
<p><strong><em>This pattern, which uses the cache to determine whether a step
    should be executed or skipped, is called step memoization.</em></strong></p>
</li>
<li>
<p>With the help of step memoization, a workflow can identify the steps
    with redundant workloads that can be skipped without being
    re-executed and thus greatly accelerate the execution of the
    end-to-end workflow. We'll apply this pattern in section 9.4.2.</p>
</li>
</ul>
<h4 class="unnumbered" id="points-to-note-3"><strong>Points to Note:</strong></h4>
<ul>
<li>
<p>In real-world machine learning applications, many workloads besides
    data ingestion are computationally heavy and time-consuming.</p>
</li>
<li>
<p>For example, the model training step uses a lot of computational
    resources to achieve high-performance model training and can
    sometimes take weeks to complete. If we are only experimenting with
    other components that do not require updating the trained model, it
    might make sense to avoid re-executing the expensive model training
    step. The step memoization pattern comes in handy when deciding
    whether you can skip heavy and redundant steps.</p>
</li>
<li>
<p>If we are creating content-based caches, the decision about the type
    of information to extract and store in the cache may not be trivial.</p>
</li>
<li>
<p>For example, if we are trying to cache the results from a model
    training step, we may want to consider using the trained model
    artifact that includes information such as the type of machine
    learning model and the set of hyperparameters of the model.</p>
</li>
<li>
<p>When the workflow is executed again, it will decide whether to
    re-execute the model training step based on whether we are trying
    the same model.</p>
</li>
<li>
<p>Alternatively, we may store information like the performance
    statistics (e.g., accuracy, mean-squared error, etc.) to identify
    whether it's beyond a threshold and worth training a more performant
    model.</p>
</li>
<li>
<p>Furthermore, when applying the step memoization pattern in practice,
    be aware that it requires a certain level of maintenance efforts to
    manage the life cycle of the created cache. For example, if 1,000
    machine learning workflows run every day with an average of 100
    steps for each workflow being memoized, 100,000 caches will be
    created every day.</p>
</li>
<li>
<p>Depending on the type of information they store, these caches
    require a certain amount of space that can accumulate rather
    quickly.</p>
</li>
<li>
<p>To apply this pattern at scale, <strong><em>a garbage collection mechanism
    must be in place</em></strong> to delete unnecessary caches automatically to
    prevent the accumulation of caches from taking up a huge amount of
    disk space.</p>
</li>
<li>
<p>For example, one simple strategy is to record the timestamp when the
    cache is last hit and used by a step in a workflow and then scan the
    existing caches periodically to clean up those that are not used or
    hit after a long time.</p>
</li>
</ul>
<h4 class="unnumbered" id="quiz-2"><strong>Quiz:</strong></h4>
<ol>
<li>
<p>What type of steps can most benefit from step memoization?</p>
</li>
<li>
<p>How do we tell whether a step's execution can be skipped if its
    workflow has been triggered to run again?</p>
</li>
<li>
<p>What do we need to manage and maintain once we've used the pattern
    to apply the pattern at scale?</p>
</li>
</ol>
<h4 class="unnumbered" id="summary"><strong>Summary</strong></h4>
<ul>
<li>
<p>Workflow is an essential component in machine learning systems as it
    connects all other components in a machine learning system. A
    machine learning workflow can be as easy as chaining data ingestion,
    model training, and model serving.</p>
</li>
<li>
<p>The fan-in and fan-out patterns can be incorporated into complex
    workflows to make them maintainable and composable.</p>
</li>
<li>
<p>The synchronous and asynchronous patterns accelerate the machine
    learning workloads with the help of concurrency.</p>
</li>
<li>
<p>The step memoization pattern improves the performance of workflows
    by skipping duplicate workloads.</p>
</li>
</ul>
<p>[<strong>Part
5</strong>_______________________________]{.underline}</p>
<p><strong>Distributed machine learning workflow (Overview &amp; Architecture)</strong></p>
<p><strong>What is covered</strong></p>
<ul>
<li>
<p>Providing a high-level overall design of our system</p>
</li>
<li>
<p>Optimizing the data ingestion component for multiple epochs of the
    dataset</p>
</li>
<li>
<p>Deciding which distributed model training strategy best minimizes
    overhead</p>
</li>
<li>
<p>Adding model server replicas for high performance model serving</p>
</li>
<li>
<p>Accelerating the end-to-end workflow of our machine learning system</p>
</li>
</ul>
<h4 id="section-8-unnumbered">{#section-8 .unnumbered}</h4>
<h3 class="unnumbered" id="overview"><strong>Overview</strong></h3>
<ul>
<li>
<p>For this project, we will build an image classification system that
    takes raw images downloaded from the data source, performs necessary
    data cleaning steps, builds a machine learning model in a
    distributed Kubernetes cluster, and then deploys the trained model
    to the model serving system for users to use.</p>
</li>
<li>
<p>We also want to establish an end-to-end workflow that is efficient
    and reusable. Next, I will introduce the project background and the
    overall system architecture and components.</p>
</li>
</ul>
<h3 class="unnumbered" id="background"><strong>Background</strong></h3>
<ul>
<li>
<p>We will build an end-to-end machine learning system to apply what we
    learned previously.</p>
</li>
<li>
<p>We'll build a data ingestion component that downloads the
    Fashion-MNIST dataset and a model training component to train and
    optimize the image classification model. Once the final model is
    trained, we'll build a high-performance model serving system to
    start making predictions using the trained model.</p>
</li>
<li>
<p>As previously mentioned, we will use several frameworks and
    technologies to build distributed machine learning workflow
    components. For example, we'll use TensorFlow with Python to build
    the classification model on the Fashion-MNIST dataset and make
    predictions.</p>
</li>
<li>
<p>We'll use Kubeflow to run distributed machine learning model
    training on a Kubernetes cluster. Furthermore, we'll use Argo
    Workflows to build a machine learning pipeline that consists of many
    important components of a distributed machine learning system.</p>
</li>
<li>
<p>The basics of these technologies will be introduced in the next
    chapter, and you'll gain hands-on experience with them before diving
    into the actual implementation of the project in chapter 9. In the
    next section, we'll examine the project's system components.</p>
</li>
</ul>
<h4 id="section-9-unnumbered">{#section-9 .unnumbered}</h4>
<h4 class="unnumbered" id="system-components"><strong>System components</strong></h4>
<ul>
<li>
<p>Figure 7.1 is the architecture diagram of the system we will be
    building. First, we will build the data ingestion component
    responsible for ingesting data and storing the dataset in the cache
    using some of the patterns discussed in chapter 2.</p>
</li>
<li>
<p>Next, we will build three different model training steps that train
    different models and incorporate the collective communication
    pattern addressed in chapter 3. Once we finish the model training
    steps, we will build the model selection step that picks the top
    model. The selected optimal model will be used for model serving in
    the following two steps.</p>
</li>
<li>
<p>At the end of the model serving steps, we aggregate the predictions
    and present the result to users. Finally, we want to ensure all
    these steps are part of a reproducible workflow that can be executed
    at any time in any environment.</p>
</li>
<li>
<p>We'll build the system based on the architecture diagram in Figure
    7.1 and dive into the details of the individual components. We'll
    also discuss the patterns we can use to address the challenges in
    building those components.</p>
</li>
</ul>
<blockquote>
<p><strong>Three model training</strong></p>
</blockquote>
<p><strong>serving steps. to present to users.</strong></p>
<p>Figure 7.1 The architecture diagram of the end-to-end machine learning
system we will be building</p>
<h4 class="unnumbered" id="data-ingestion"><strong>Data ingestion</strong></h4>
<ul>
<li>
<p>For this project, we will use the Fashion-MNIST dataset, introduced
    in section 2.2, to build the data ingestion component, as shown in
    figure 7.2. This dataset consists of a training set of 60,000
    examples and a test set of 10,000 examples.</p>
</li>
<li>
<p>Each example is a 28 × 28 grayscale image that represents one
    Zalando's article image associated with a label from 10 classes.
    Recall that the Fashion-MNIST dataset is designed to serve as a
    direct drop-in replacement for the original MNIST dataset for
    benchmarking machine learning algorithms.</p>
</li>
<li>
<p>It shares the same image size and structure of training and testing
    splits.</p>
</li>
</ul>
<p>Figure 7.2 The data ingestion component (dark box) in the end-to-end
machine learning system</p>
<ul>
<li>
<p>Figure 7.3 is a screenshot of the collection of images for all 10
    classes (T-shirt/top, trouser, pullover, dress, coat, sandal, shirt,
    sneaker, bag, and ankle boot) from Fashion-MNIST, where each class
    takes three rows in the screenshot.</p>
</li>
<li>
<p>Figure 7.4 is a closer look at the first few example images in the
    training set together with their corresponding text labels.</p>
</li>
<li>
<p>The downloaded Fashion-MNIST dataset should only take 30 MBs on disk
    if compressed and can be fully loaded into memory.</p>
</li>
</ul>
<h4 id="section-10-unnumbered">{#section-10 .unnumbered}</h4>
<h4 class="unnumbered" id="the-challenge-reduce-sequential-training-inefficiency"><strong>The Challenge: Reduce Sequential Training Inefficiency</strong></h4>
<p><strong>Context:</strong></p>
<ul>
<li>
<p>Although the Fashion-MNIST data is not large, we may want to perform
    additional computations before feeding the dataset into the model,
    which is common for tasks that require additional transformations
    and cleaning.</p>
</li>
<li>
<p>We may want to resize, normalize, or convert the images to
    grayscale. We also may want to perform complex mathematical
    operations such as convolution operations, which can require large
    additional memory space allocations. Our available computational
    resources may or</p>
</li>
</ul>
<h5 class="unnumbered" id="data-ingestion-1">7.2 Data ingestion</h5>
<blockquote>
<p>Figure 7.3 A screenshot of the collection of images from the
Fashion-MNIST dataset for all 10 classes (T-shirt/top, trouser,
pullover, dress, coat, sandal, shirt, sneaker, bag, and ankle boot)</p>
<p><img alt="" src="../media/media/image19.jpg" />{width="5.666666666666667in"
height="1.3533333333333333in"}</p>
</blockquote>
<p>Figure 7.4 A closer look at the first few example images in the training
set with their corresponding labels in text</p>
<blockquote>
<p>may not be sufficient after we load the entire dataset in memory,
depending on the distributed cluster size.</p>
</blockquote>
<ul>
<li>In addition, the machine learning model we are training from this
    dataset requires multiple epochs on the training dataset. Suppose
    training one epoch on the entire training dataset takes 3 hours. If
    we want to train two epochs, the time needed for model training
    would double, as shown in figure 7.5.</li>
</ul>
<p>Figure 7.5 A diagram of model training for multiple epochs at time t0,
t1, etc. where we spent 3 hours for each epoch</p>
<ul>
<li>In real-world machine learning systems, a larger number of epochs is
    often needed, and training each epoch sequentially is inefficient.</li>
</ul>
<blockquote>
<p><strong><em>Question: How can we tackle that inefficiency?</em></strong></p>
</blockquote>
<h4 class="unnumbered" id="the-solution-approach-2"><strong>The Solution Approach</strong></h4>
<ul>
<li>
<p>The first challenge we have: the mathematical operations in the
    machine learning algorithms may require a lot of additional memory
    space allocations while computational resources may or may not be
    sufficient.</p>
</li>
<li>
<p>Given that we don't have too much free memory, we should <strong><em>not</em></strong>
    load the entire Fashion-MNIST dataset into memory directly.</p>
</li>
<li>
<p>Let's assume that the mathematical operations that we want to
    perform on the dataset can be performed on <strong><em>subsets</em></strong> of the
    entire dataset.</p>
</li>
<li>
<p>We can use the batching pattern introduced earlier, which would
    group a number of data records from the entire dataset into batches,
    which will be used to train the machine learning model sequentially
    on each batch.</p>
</li>
<li>
<p>To apply the batching pattern, we first divide the dataset into
    smaller subsets or mini-batches, load each individual mini-batch of
    example images, perform expensive mathematical operations on each
    batch, and then use only one mini-batch of images in each model
    training iteration.</p>
</li>
<li>
<p>For example, we can perform convolution or other heavy mathematical
    operations on the first mini-batch, which consists of only 20
    images, and then send the transformed images to the machine learning
    model for model training.</p>
</li>
<li>
<p>We then repeat the same process for the remaining mini-batches while
    continuing to perform model training.</p>
</li>
<li>
<p>Since we've divided the dataset into many small subsets
    (mini-batches), we can avoid any potential problems with running out
    of memory when performing various heavy mathematical operations on
    the entire dataset necessary for achieving an accurate
    classification model on the Fashion-MNIST dataset. We can then
    handle even larger datasets using this approach by reducing the size
    of the mini-batches.</p>
</li>
<li>
<p>With the help of the batching pattern, we are no longer concerned
    about potential out-of-memory problems when ingesting the dataset
    for model training. We don't have to load the entire dataset into
    memory at once, and instead, we are consuming the dataset batch by
    batch sequentially.\
    Fig. 7.6 illustrates this process, where the original dataset gets
    divided into two batches and processed sequentially. The first batch
    gets consumed to train the model at time t0, and the second batch
    gets consumed at time t1.</p>
</li>
</ul>
<blockquote>
<p><strong>The two batches of the dataset are consumed sequentially for model
training.</strong></p>
<p>Figure 7.6 The dataset is divided into two batches and processed
sequentially. The first batch is consumed to train the model at time
t0, and the second batch is consumed at time t1.</p>
</blockquote>
<ul>
<li>
<p>The second challenge is that we want to avoid wasting time if we
    need to train a machine learning model that involves <strong><em>iterating on
    multiple epochs of the original dataset</em></strong>.</p>
</li>
<li>
<p>The caching pattern can solve this type of problem. With the help of
    the caching pattern, we can greatly speed up the re-access to the
    dataset for the model training process that involves <strong><em>training on
    the same dataset for multiple epochs</em></strong>.</p>
</li>
<li>
<p>We can't do anything special to the first epoch since it's the first
    time the machine learning model has seen the entire training
    dataset. We can store the cache of the training examples in memory,
    making it much faster to re-access when needed for the second and
    subsequent epochs.</p>
</li>
<li>
<p>Assuming that the single laptop we use to train the model has
    sufficient computational resources such as memory and disk space,
    once the machine learning model consumes each training example from
    the entire dataset, we can hold off recycling and instead keep the
    consumed training examples in memory.</p>
</li>
<li>
<p>In figure 7.7, after we have finished fitting the model for the
    first epoch, we can store a cache for both batches used for the
    first epoch of model training. Then, we can start training the model
    for the second epoch by feeding the stored in-memory cache to the
    model directly without repeatedly reading from the data source for
    future epochs.</p>
</li>
</ul>
<p>Figure 7.7 A diagram of model training for multiple epochs at time t0,
t1, etc. using cache, making reading from the data source repeatedly
unnecessary</p>
<h4 class="unnumbered" id="quiz-5"><strong>Quiz</strong></h4>
<ol>
<li>
<p>Where do we store the cache?</p>
</li>
<li>
<p>Can we use the batching pattern when the Fashion-MNIST dataset gets
    large?</p>
</li>
</ol>
<h3 class="unnumbered" id="model-training"><strong>Model training</strong></h3>
<ul>
<li>Figure 7.8 is a diagram of the model training component in the
    overall architecture.</li>
</ul>
<blockquote>
<p>In the diagram, three different model training steps are followed by a
model selection step. These model training steps can train three
different models competing with each other for better statistical
performance. The dedicated model selection step then picks the top
model, which will be used in the subsequent components in the
end-to-end machine learning workflow.</p>
<p>In the next section, we will look more closely at the model training
component in figure 7.8 and discuss potential problems when
implementing this component.</p>
</blockquote>
<h4 id="section-11-unnumbered">{#section-11 .unnumbered}</h4>
<blockquote>
<p><strong>Three model training steps</strong></p>
<p><strong>This step picks the top two train different models. models that will
be used in the following two separate model serving steps.</strong></p>
</blockquote>
<p>Figure 7.8 The model training component (dark boxes) in the end-to-end
machine learning system</p>
<h4 class="unnumbered" id="the-challenge"><strong>The Challenge:</strong></h4>
<ul>
<li>
<p>We discussed the parameter server and the collective communication
    patterns earlier.</p>
</li>
<li>
<p>The parameter server pattern is handy when the model is too large to
    fit in a single machine, such as the one for tagging entities in the
    8 million YouTube videos.</p>
</li>
<li>
<p>The collective communication pattern is useful to speed up the
    training process for medium-sized models when the communication
    overhead is significant.</p>
</li>
</ul>
<p><strong><em>Question: Which pattern should we select for our model training
component?</em></strong></p>
<h4 class="unnumbered" id="the-solution-2"><strong>The solution</strong></h4>
<ul>
<li>
<p>With the help of parameter servers, we can effectively resolve the
    challenge of building an extremely large machine learning model that
    may not fit a single machine.</p>
</li>
<li>
<p>Even when the model is too large to fit in a single machine, we can
    still successfully train the model efficiently with parameter
    servers.</p>
</li>
<li>
<p>Figure 7.9 is an architecture diagram of the parameter server
    pattern using multiple parameter servers.</p>
</li>
<li>
<p>Each worker node takes a subset of the dataset, performs
    calculations required in each neural network layer, and sends the
    calculated gradients to update <strong><em>one model partition stored in one
    of the parameter servers</em></strong>.</p>
</li>
<li>
<p>Because all workers perform calculations in an asynchronous fashion,
    the model partitions each worker node uses to calculate the
    gradients may not be up to date.</p>
</li>
<li>
<p>For instance, two workers can block each other when sending
    gradients to the same parameter server, which makes it hard to
    gather the calculated gradients on time and requires a strategy to
    resolve the blocking problem.</p>
</li>
<li>
<p>Unfortunately, in real-world distributed training systems where
    parameter servers are incorporated, multiple workers may send the
    gradients at the same time, and thus many blocking communications
    must be resolved.</p>
</li>
</ul>
<p>Figure 7.9 A machine learning training component with multiple parameter
servers</p>
<ul>
<li>
<p>Another challenge comes when deciding the optimal ratio between the
    number of workers and the number of parameter servers.</p>
</li>
<li>
<p>For example, many workers are sending gradients to the same
    parameter server at the same time; the problem gets even worse, and
    eventually, the blocking communications between different workers or
    parameter servers become a bottleneck.</p>
</li>
<li>
<p>In the Fashion-MNIST classification model, the model we are building
    is not as large as large recommendation system models; it can easily
    fit in a single machine if we give the machine sufficient
    computational resources. It's only 30 MBs in compressed form.</p>
</li>
<li>
<p>Thus, the collective communication model is perfect for the system
    we are building.</p>
</li>
<li>
<p>Without parameter servers, <strong><em>each worker node stores a copy of the
    entire set of model parameters</em></strong> (figure 7.10).</p>
</li>
<li>
<p>Every worker consumes some portion of data and calculates the
    gradients needed to update the model parameters stored locally on
    this worker node.</p>
</li>
<li>
<p>We want to aggregate all the gradients as soon as all worker nodes
    have successfully completed their calculation of gradients.</p>
</li>
<li>
<p>We also want to make sure every worker's entire set of model
    parameters is updated based on the aggregated gradients i.e., each
    worker should <strong><em>store a copy of the exact same updated model</em></strong>.</p>
</li>
<li>
<p>Referring to figure 7.8, each model training step uses the
    collective communication pattern, taking advantage of the underlying
    network infrastructure to perform allreduce operations to
    communicate gradients between multiple workers.</p>
</li>
<li>
<p>The collective communication pattern also allows us to train
    multiple medium-sized machine learning models in a distributed
    setting.</p>
</li>
<li>
<p>Once the model is trained, we can start a separate process to pick
    the top model to be used for model serving. This step is pretty
    intuitive,</p>
</li>
</ul>
<h5 class="unnumbered" id="model-serving">7.4 Model serving</h5>
<blockquote>
<p><strong>Each of these workers contains a copy of the entire set of model
parameters and consumes partitions of data to calculate the
gradients.</strong></p>
<p>Figure 7.10 Distributed model training component with only worker
nodes, where every worker stores a copy of the entire set of model
parameters and consumes partitions of data to calculate the gradients</p>
<p><strong>Quiz:</strong></p>
</blockquote>
<ol>
<li>
<p>Why isn't the parameter server pattern a good fit for our model?</p>
</li>
<li>
<p>Does each worker store different parts of the model when using the
    collective communication pattern?</p>
</li>
</ol>
<h3 class="unnumbered" id="model-serving-1"><strong>Model serving</strong></h3>
<ul>
<li>
<p>We've talked about both the data ingestion and model training
    components of the system we are building. Next, let's discuss the
    model server component, which is essential to the end-user
    experience. Figure 7.11 shows the serving training component in the
    overall architecture.</p>
</li>
<li>
<p>Next, let's take a look at a potential problem and its solution we
    will encounter when we begin building this component.</p>
</li>
</ul>
<h4 class="unnumbered" id="the-challenge-1"><strong>The Challenge</strong></h4>
<ul>
<li>
<p>The model serving system needs to take raw images uploaded by users
    and send the requests to the model server to make inferences using
    the trained model. These model serving requests are being queued and
    waiting to be processed by the model server.</p>
</li>
<li>
<p>If the model serving system is a single-node server, it can only
    serve a limited number of model serving requests on a first-come,
    first-served basis. As the number of requests grows in the real
    world, the user experience suffers when users must wait a long time
    to receive the model serving result. In other words, all requests
    are waiting to be processed by the model serving system, but the
    computational resources are limited to this single node.</p>
</li>
</ul>
<p><strong>Question: How do we build a more efficient model serving system?</strong></p>
<blockquote>
<p><strong>The results from the two model serving steps are then aggregated via
a result aggregation step to present to users.</strong></p>
</blockquote>
<p>Figure 7.11 The model serving component (dark boxes) in the end-to-end
machine learning system</p>
<p><strong>The solution</strong></p>
<ul>
<li>
<p>The previous section lays a perfect use case for the replicated
    services pattern discussed earlier.</p>
</li>
<li>
<p>Our model serving system takes the images uploaded by users and
    sends requests to the model server.</p>
</li>
<li>
<p>In addition, unlike the simple single-server design, the system has
    multiple model server replicas to process the model serving requests
    asynchronously.</p>
</li>
<li>
<p>Each model server replica takes a single request, retrieves the
    previously trained classification model from the model training
    component, and classifies the images that don't exist in the
    Fashion-MNIST dataset.</p>
</li>
<li>
<p>With the help of the replicated services pattern, we can easily
    scale up our model server by adding model server replicas to the
    single-server model serving system. The new architecture is shown in
    figure 7.12.</p>
</li>
<li>
<p>The model server replicas can handle many requests at a time since
    each replica can process individual model serving requests
    independently.</p>
</li>
<li>
<p>Multiple model serving requests from users are sent to the model
    server replicas at the same time after we've introduced them. We
    also need to define a clear mapping relationship between the
    requests and the model server replicas, which determines which
    requests are processed by which of the model server replicas.</p>
</li>
<li>
<p>To distribute the model server requests among the replicas, we need
    to add an additional load balancer layer. For example, the load
    balancer takes multiple model serving requests from our users.</p>
</li>
<li>
<p>It then distributes the requests evenly among the model server
    replicas, which are responsible for processing individual requests,
    including model retrieval and inference on the new data in the
    request. Figure 7.13 illustrates this process.</p>
</li>
</ul>
<blockquote>
<p><strong>Users upload images and then submit requests to the model serving
system for classification.</strong></p>
</blockquote>
<p>Figure 7.12 The system architecture of the replicated model serving
services</p>
<blockquote>
<p>Figure 7.13 Load balancer distributes requests evenly across the model
server replicas</p>
</blockquote>
<ul>
<li>
<p>The load balancer uses different algorithms to determine which
    request goes to which particular model server replica. Example
    algorithms for load balancing include round robin, least-connection
    method, and hashing.</p>
</li>
<li>
<p>Note that from our original architecture diagram in figure 7.11,
    there are two individual steps for model serving, each using
    different models. Each model serving step consists of a model
    serving service with multiple replicas to handle model serving
    traffic for different models.</p>
</li>
</ul>
<h4 class="unnumbered" id="quiz-6"><strong>Quiz:</strong></h4>
<blockquote>
<p>1 What happens when we don't have a load balancer as part of the model
serving system?</p>
</blockquote>
<h3 class="unnumbered" id="end-to-end-workflow"><strong>End-to-end workflow</strong></h3>
<ul>
<li>
<p>Now that we've looked at the individual components, let's see how to
    compose an end-to-end workflow that consists of all those components
    in a scalable and efficient way.</p>
</li>
<li>
<p>Figure 7.14 is a diagram of the end-to-end workflow for the system.</p>
</li>
</ul>
<blockquote>
<p><strong>Three model training</strong></p>
</blockquote>
<p><strong>serving steps. to present to users.</strong></p>
<p>Figure 7.14 The architecture diagram of the end-to-end machine learning
system we will build</p>
<ul>
<li>We now examine the entire machine learning system, which chains all
    the components together in an end-to-end workflow.</li>
</ul>
<h4 class="unnumbered" id="the-challenges"><strong>The Challenges</strong></h4>
<ul>
<li>
<p>First, the Fashion-MNIST dataset is static and does not change over
    time. However, to design a more realistic system, let's assume we'll
    manually update the Fashion-MNIST dataset regularly.</p>
</li>
<li>
<p>Whenever the updates happen, we may want to rerun the entire machine
    learning workflow to train a fresh machine learning model that
    includes the new data, i.e., we need to execute the data ingestion
    step every time when changes happen.</p>
</li>
<li>
<p>In the meantime, when the dataset is not updated, we want to
    experiment with new machine learning models.</p>
</li>
<li>
<p>Thus, we still need to execute the entire workflow, including the
    data ingestion step.</p>
</li>
<li>
<p>The data ingestion step is usually very time consuming, especially
    for large datasets.</p>
</li>
<li>
<p><strong><em>Question: Is there a way to make this workflow more efficient?</em></strong></p>
</li>
<li>
<p>Second, we want to build a machine learning workflow that can train
    different models and then select the top model, which will be used
    in model serving to generate predictions using the knowledge from
    both models.</p>
</li>
<li>
<p>Due to the variance of completion time for each of the model
    training steps in the existing machine learning workflow, the start
    of each following step, such as model selection and model serving,
    depends on the completion of the previous steps.</p>
</li>
<li>
<p>However, this sequential execution of steps in the workflow is quite
    time-consuming and blocks the rest of the steps. For example, say
    one model training step takes much longer to complete than the rest
    of the steps.</p>
</li>
<li>
<p>The model selection step that follows can only start to execute
    after this long-running model training step has completed. As a
    result, the entire workflow is delayed by this particular step.</p>
</li>
<li>
<p><strong><em>Question: Is there a way to accelerate this workflow so it will
    not be affected by the duration of individual steps?</em></strong></p>
</li>
</ul>
<h4 class="unnumbered" id="the-solutions"><strong>The solutions</strong></h4>
<ul>
<li>
<p>For the first problem, we can use the step memoization pattern
    discussed earlier.</p>
</li>
<li>
<p>Recall that step memoization can help the system decide whether a
    step should be executed or skipped.</p>
</li>
<li>
<p>With the help of step memoization, a workflow can identify steps
    with redundant workloads that can be skipped without being
    re-executed and thus greatly accelerate the execution of the
    end-to-end workflow.</p>
</li>
<li>
<p>For instance, figure 7.15 contains a simple workflow that only
    executes the data ingestion step when we know the dataset has been
    updated ie., we don't want to re-ingest the data that's already
    collected if the new data has not been updated.</p>
</li>
<li>
<p>Many strategies can be used to determine whether the dataset has
    been updated. With a predefined strategy, we can conditionally
    reconstruct the machine learning workflow and control whether we
    would like to include a data ingestion step to be re-executed, as
    shown in figure 7.16.</p>
</li>
<li>
<p>Cache is one way to identify whether a dataset has been updated.
    Since we suppose our Fashion-MNIST dataset is being updated
    regularly on a fixed schedule (e.g., once a month), we can create a
    time-based <em>cache</em> that stores the location of the ingested and
    cleaned dataset (assuming the dataset is located in a remote
    database) and the timestamp of its last updated time.</p>
</li>
</ul>
<blockquote>
<p><strong>The dataset has <em>not</em> been updated yet.</strong></p>
</blockquote>
<p>Figure 7.15 A diagram of skipping the data ingestion step when the
dataset has not been updated</p>
<ul>
<li>
<p>As in figure 7.16, the data ingestion step in the workflow will then
    be constructed and executed dynamically based on whether the last
    updated timestamp is within a particular window. For example, if the
    time window is set to two weeks, we consider the ingested data as
    fresh if it has been updated within the past two weeks.</p>
</li>
<li>
<p>The data ingestion step will be skipped, and the following model
    training steps will use the already ingested dataset from the
    location in the cache.</p>
</li>
<li>
<p>The time window can be used to control how old a cache can be before
    we consider the dataset fresh enough to be used directly for model
    training instead of re-ingesting the data from scratch.</p>
</li>
</ul>
<p>Figure 7.16 The workflow has been triggered. We check whether the data
has been updated within the last two weeks by accessing the cache. If
the data is fresh, we can skip the unnecessary data ingestion step and
execute the model training step directly.</p>
<ul>
<li>
<p>For the second problem: sequential execution of the steps blocks the
    subsequent steps in the workflow and is inefficient.</p>
</li>
<li>
<p>The synchronous and asynchronous patterns introduced earlier can
    help.</p>
</li>
<li>
<p>When a short-running model training step finishes---for example,
    model training step 2 in figure 7.17---we successfully obtain a
    trained machine learning model.</p>
</li>
<li>
<p>In fact, we can use this already-trained model directly in our model
    serving system without waiting for the rest of the model training
    steps to complete.</p>
</li>
<li>
<p>As a result, users will be able to see the results of image
    classification from their model serving requests that contain videos
    as soon as we have trained one model from one of the steps in the
    workflow.</p>
</li>
<li>
<p>After a second model training step (figure 7.17, model training
    step 3) finishes, the two trained models are sent to model serving.
    Now, users benefit from the aggregated results obtained from both
    models.</p>
</li>
</ul>
<blockquote>
<p><strong>After a second model training step finishes, we can pass the two
trained models directly to be used for model serving, and the
aggregated inference results will be presented to users instead of the
results from only the one model that we obtained initially.</strong></p>
<p>Figure 7.17 After a second model training step finishes, we can pass
the two trained models directly to model serving. The aggregated
inference results will be presented to users instead of only the
results from the first model.</p>
</blockquote>
<ul>
<li>
<p>As a result, we can continue to use the trained models for model
    selection and model serving; in the meantime, the long-running model
    training steps are still running i.e., they execute asynchronously
    without depending on each other's completion.</p>
</li>
<li>
<p>The workflow can proceed and execute the next step before the
    previous one finishes. The long-running model training step will no
    longer block the entire workflow. Instead, it can continue to use
    the already-trained models from the short-running model training
    steps in the model serving system. Thus, it can start handling
    users' model serving requests.</p>
</li>
</ul>
<h4 class="unnumbered" id="quiz-7"><strong>Quiz:</strong></h4>
<ol>
<li>
<p>Which component can benefit the most from step memoization?</p>
</li>
<li>
<p>How do we tell whether a step's execution can be skipped if its
    workflow has been triggered to run again?</p>
</li>
</ol>
<h3 class="unnumbered" id="summary-2"><strong>Summary</strong></h3>
<ul>
<li>
<p>The data ingestion component uses the caching pattern to speed up
    the processing of multiple epochs of the dataset.</p>
</li>
<li>
<p>The model training component uses the collective communication
    pattern to avoid the potential communication overhead between
    workers and parameter servers.</p>
</li>
<li>
<p>We can use model server replicas, which are capable of handling many
    requests at one time since each replica processes individual model
    serving requests independently.</p>
</li>
<li>
<p>We can chain all our components into a workflow and use caching to
    effectively skip time-consuming components such as data ingestion.</p>
</li>
</ul>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2024 <a href="https://spanda.io"  target="_blank" rel="noopener">Spanda.io</a>

    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://ranga-rangarajan.github.io/spanda-bootcamp/" target="_blank" rel="noopener" title="ranga-rangarajan.github.io" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.caa56a14.min.js"></script>
      
    
  </body>
</html>