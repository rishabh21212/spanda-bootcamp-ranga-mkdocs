
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../13_Federated_Learning_Implementation_-_Server_Side/">
      
      
        <link rel="next" href="../15_Federated_Learning_Running_the_System_%26amp%3B_Analyzing_Results/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.7">
    
    
      
        <title>14 Federated Learning Implementation Client Side - Spanda DL Bootcamp</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.f2e4d321.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Spanda DL Bootcamp" class="md-header__button md-logo" aria-label="Spanda DL Bootcamp" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Spanda DL Bootcamp
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              14 Federated Learning Implementation   Client Side
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-hidden="true"  type="radio" name="__palette" id="__palette_2">
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  The Complete Generative AI Bootcamp

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../01_Course_Vision_and_Running_a_Local_LLM/" class="md-tabs__link">
          
  
  Lecturewise mkdocs

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Spanda DL Bootcamp" class="md-nav__button md-logo" aria-label="Spanda DL Bootcamp" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Spanda DL Bootcamp
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The Complete Generative AI Bootcamp
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Lecturewise mkdocs
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Lecturewise mkdocs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01_Course_Vision_and_Running_a_Local_LLM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    01 Course Vision and Running a Local LLM
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02_Software_Engineering_for_AI/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    02 Software Engineering for AI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03_Ops_-_DevOps%2C_MLOps%2C_AIOps/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    03 Ops   DevOps, MLOps, AIOps
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04_Data_Collection%2C_Data_Labeling%2C_Data_Management%2C_Analytics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    04 Data Collection, Data Labeling, Data Management, Analytics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05_Testing_and_Deployment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    05 Testing and Deployment
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06.5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    06.5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06_LLM_Serving_and_Reinforcement_Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    06 LLM Serving and Reinforcement Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07_Distributed_Systems_Patterns/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    07 Distributed Systems Patterns
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08_Workflow_Patterns/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    08 Workflow Patterns
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09_Scheduling_%26amp%3B_Metadata_Patterns/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    09 Scheduling &amp; Metadata Patterns
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10_Data_Lakes_%26amp%3B_Intro_to_Federated_Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10 Data Lakes &amp; Intro to Federated Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_Federated_Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11 Federated Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12_Federated_Learning_System_Details/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12 Federated Learning System Details
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13_Federated_Learning_Implementation_-_Server_Side/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13 Federated Learning Implementation   Server Side
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    14 Federated Learning Implementation   Client Side
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../15_Federated_Learning_Running_the_System_%26amp%3B_Analyzing_Results/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15 Federated Learning Running the System &amp; Analyzing Results
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../16_Introducing_Existing_Federated_Learning_Frameworks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16 Introducing Existing Federated Learning Frameworks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../17_Federated_Learning_Use_Cases_and_Case_Studies/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17 Federated Learning Use Cases and Case Studies
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18_Federated_Learning_Future_Directions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18 Federated Learning Future Directions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19_LLMs_and_NLP_Introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM's & NLP Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../20_Retrieval_Augmented_Generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Retrieval Augmented Generation (RAG)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../21_RAG_Pipeline_Implementation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RAG Pipeline Implementation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../22_From_Simple_to_Advanced_RAG/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    From Simple to Advanced RAG
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../23_RAG_Observability/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Observability Tools for RAG
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  <h1>14 Federated Learning Implementation   Client Side</h1>

<p><strong>FL Client Side Implementation</strong></p>
<ul>
<li>
<p>The client-side modules of a <strong>federated learning</strong> (<strong>FL</strong>) system
    can be implemented based on the system architecture, sequence, and
    procedure flow, as discussed earlier.</p>
</li>
<li>
<p>FL client-side functionalities can connect distributed <strong>machine
    learning</strong> (<strong>ML</strong>) applications that conduct local training and
    testing with an aggregator, through a communications module embedded
    in the client-side libraries.</p>
</li>
<li>
<p>In the example of using the FL client libraries in a local ML
    engine, the minimal engine package example will be discussed, with
    dummy ML models to understand the process of integration with the FL
    client libraries that are designed Here.</p>
</li>
<li>
<p>By following the example code about integration, you will understand
    how to actually enable the whole process related to the FL client
    side, as discussed earlier, while an analysis on what will happen
    with the minimal example will be discussed in  <em>Running the
    Federated Learning System and Analyzing the Results</em>.</p>
</li>
<li>
<p>Here, an overview of the design and implementation principle of FL
    client-side functionalities used in local ML engines will be
    discussed.  you will be able to code the FL client-side modules and
    libraries as well as distributed local ML engines, such as image
    classification with <strong>Convolutional Neural Networks</strong> (<strong>CNNs</strong>).</p>
</li>
<li>
<p>Here, we will cover the following topics:</p>
</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   An overview of FL client-side components</p>
<ul>
<li>
<p>Implementing FL client-side main functionalities</p>
</li>
<li>
<p>Designing FL client libraries</p>
</li>
<li>
<p>Local ML engine integration into an FL system</p>
</li>
<li>
<p>An example of integrating image classification into an FL system</p>
</li>
</ul>
<p>Technical requirements</p>
<p>All the code files introduced Here can be found on GitHub
(<a href="https://github.com/keshavaspanda/simple-fl">[https://github.com/keshavaspanda/simple-fl]{.underline}</a>).</p>
<p>An overview of FL client-side components</p>
<ul>
<li>
<p>The architecture of an FL client as an agent was introduced earlier.</p>
</li>
<li>
<p>Here, we will introduce code that realizes the basic functionalities
    of an FL client.</p>
</li>
<li>
<p>The client side of software architecture is simplified here, where
    only the client.py file can be used in this example, together with
    supporting functions from the lib/util folder, as shown in <em>Figure
    5.1</em>:</p>
</li>
</ul>
<p><img alt="Figure 5.1 -- Python software components for an FL client as an agent
" src="../images/media/image24.jpg" />{width="3.751388888888889in"
height="3.3020833333333335in"}</p>
<p>Figure 5.1 -- Python software components for an FL client as an agent</p>
<ul>
<li>The following section gives a brief description of the Python files
    for an agent of the FL system.</li>
</ul>
<p>Distributed agent-side code</p>
<ul>
<li>For the agent side, there is one main file, client.py, in
    the fl_main/agent directory that deals with most of the FL
    client-side functionalities.</li>
</ul>
<p>FL client code (client.py)</p>
<ul>
<li>
<p>The client.py file in the agent folder has functions to participate
    in an FL cycle, an ML model exchange framework with an aggregator,
    and <em>push</em> and <em>polling</em> mechanisms to communicate with the
    aggregator.</p>
</li>
<li>
<p>The client's functions can also serve as interfaces between the
    local ML application and the FL system itself, providing FL
    client-side libraries to the ML engine.</p>
</li>
<li>
<p>This is the main code that connects locally trained ML models to the
    FL server and aggregator.</p>
</li>
<li>
<p>You need to prepare a local ML application by yourself, and we will
    help you understand how to integrate your ML engine into an FL
    system using the FL client libraries, which is another main topic of
    This section.</p>
</li>
</ul>
<p>lib/util code</p>
<ul>
<li>An explanation of the supporting Python code
    (communication_handler.py, data_struc.py, helpers.py, messengers.py,
    and states.py) as internal libraries will be covered in <em>Appendix,
    Exploring Internal Libraries</em>.</li>
</ul>
<p>Configuration of an agent</p>
<ul>
<li>The following is an example of client-side configuration parameters
    saved as config_agent.json in the code we are using:</li>
</ul>
<blockquote>
<p>{</p>
<p>\"aggr_ip\": \"localhost\",</p>
<p>\"reg_socket\": \"8765\",</p>
<p>\"model_path\": \"./data/agents\",</p>
<p>\"local_model_file_name\": \"lms.binaryfile\",</p>
<p>\"global_model_file_name\": \"gms.binaryfile\",</p>
<p>\"state_file_name\": \"state\",</p>
<p>\"init_weights_flag\": 1,</p>
<p>\"polling\": 1</p>
<p>}</p>
</blockquote>
<ul>
<li>
<p>CopyExplain</p>
</li>
<li>
<p>The aggregator's IP (aggr_ip) and its port number (reg_socket) are
    used to get connected to the FL server, where the aggregation of the
    local models happens. In addition, the model path
    parameter, model_path, specifies the location of both the local
    model (named local_model_file_name) and the global model
    (named global_model_file_name).</p>
</li>
<li>
<p>The local and global models are stored as binary files
    (lms.binaryfile and gms.binaryfile in this example). The state file
    (named state_file_name) writes the local state of the client that
    defines waiting for the global models, training the models, sending
    the trained models, and so on. init_weights_flag is used when the
    system operator wants to initialize the global model with certain
    weights.</p>
</li>
<li>
<p>If the flag is 1, the agent will send the pre-configured model;
    otherwise, the model will be filled with zeros on the aggregator
    side. The polling flag (polling) concerns whether to utilize the
    polling method or not for communication between agents and an
    aggregator.</p>
</li>
<li>
<p>Now that we've discussed FL client-side modules, let's look into the
    actual implementation and some code to realize the functionalities
    of an FL client.</p>
</li>
</ul>
<p>Implementing FL client-side main functionalities</p>
<ul>
<li>
<p>In this section, we will explain how you can implement basic FL
    client-side code, which is described in the client.py file in
    the agent directory.</p>
</li>
<li>
<p>By learning about this client-side code, you will understand how to
    implement an agent's registration process, model exchange
    synchronization, and <em>push</em>/<em>polling</em> mechanisms, as well as the
    communication protocol between the agent and aggregator, with some
    functions that will be called from other ML applications
    as <strong>Application Programming Interfaces</strong> (<strong>APIs</strong>).</p>
</li>
<li>
<p>Let's first see what libraries will be imported for implementing FL
    client functions.</p>
</li>
</ul>
<p>Importing libraries for an agent</p>
<ul>
<li>In this client.py file example, the agent imports general libraries
    such as asyncio and time (a detailed explanation of which is out of
    scope for this book):</li>
</ul>
<blockquote>
<p>import asyncio, time, logging, sys, os</p>
<p>from typing import Dict, Any</p>
<p>from threading import Thread</p>
<p>from fl_main.lib.util.communication_handler import \</p>
<p>init_client_server, send, receive</p>
<p>from fl_main.lib.util.helpers import read_config, \</p>
<p>init_loop, save_model_file, load_model_file, \</p>
<p>read_state, write_state, generate_id, \</p>
<p>set_config_file, get_ip, compatible_data_dict_read, \</p>
<p>generate_model_id, create_data_dict_from_models, \</p>
<p>create_meta_data_dict</p>
<p>from fl_main.lib.util.states import ClientState, \</p>
<p>AggMsgType, ParticipateConfirmationMSGLocation, \</p>
<p>GMDistributionMsgLocation, IDPrefix</p>
<p>from fl_main.lib.util.messengers import \</p>
<p>generate_lmodel_update_message, \</p>
<p>generate_agent_participation_message, \</p>
<p>generate_polling_message</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>As for the communication_handler, helpers, states,
    and messengers libraries imported from fl_main.lib.util that are
    designed for enabling the FL general functionalities, please refer
    to the <em>Appendix, Exploring Internal Libraries</em>.</p>
</li>
<li>
<p>After importing the necessary libraries, you will define
    the Client class.</p>
</li>
</ul>
<p>Defining the Client class</p>
<ul>
<li>Let's define the Client class that implements the core
    functionalities of an FL client, including the participation
    mechanism of the agent itself, the model exchange framework, and a
    communication interface between the agent and an aggregator, as well
    as libraries provided for use in the agent-side local ML engine:</li>
</ul>
<blockquote>
<p>class Client:</p>
<p>\"\"\"</p>
<p>Client class instance with FL client-side functions</p>
<p>and libraries used in the agent\'s ML engine</p>
<p>\"\"\"</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>Then, you will initialize the Client class under
    the __init__ function, as discussed in the next section.</li>
</ul>
<p>Initializing the client</p>
<ul>
<li>The following code inside the __init__ constructor is an example
    of the initialization process of the client:</li>
</ul>
<blockquote>
<p>def __init__(self):</p>
<p>self.agent_name = \'default_agent\'</p>
<p>self.id = generate_id()</p>
<p>self.agent_ip = get_ip()</p>
<p>self.simulation_flag = False</p>
<p>if len(sys.argv) &gt; 1:</p>
<p>self.simulation_flag = bool(int(sys.argv[1]))</p>
<p>config_file = set_config_file(\"agent\")</p>
<p>self.config = read_config(config_file)</p>
<p>self.aggr_ip = self.config[\'aggr_ip\']</p>
<p>self.reg_socket = self.config[\'reg_socket\']</p>
<p>self.msend_socket = 0</p>
<p>self.exch_socket = 0</p>
<p>if self.simulation_flag:</p>
<p>self.exch_socket = int(sys.argv[2])</p>
<p>self.agent_name = sys.argv[3]</p>
<p>self.model_path = f\'{self.config[\"model_path\"]}</p>
<p>/{self.agent_name}\'</p>
<p>if not os.path.exists(self.model_path):</p>
<p>os.makedirs(self.model_path)</p>
<p>self.lmfile = self.config[\'local_model_file_name\']</p>
<p>self.gmfile = self.config[\'global_model_file_name\']</p>
<p>self.statefile = self.config[\'state_file_name\']</p>
<p>self.round = 0</p>
<p>self.init_weights_flag = \</p>
<p>bool(self.config[\'init_weights_flag\'])</p>
<p>self.is_polling = bool(self.config[\'polling\'])</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>First, the client generates a unique ID for itself as an identifier
    that will be used in many scenarios to conduct FL.</p>
</li>
<li>
<p>Second, the client gets its own IP address by using
    the get_ip() function.</p>
</li>
<li>
<p>Also, simulation runs are supported in this implementation exercise,
    where we can run all the FL system components of a database, server,
    and multiple agents within one machine. If simulation needs to be
    done, then the simulation_flag parameter needs to be True (refer to
    the README file on GitHub for how to set up a simulation mode).</p>
</li>
<li>
<p>Then, self.cofig reads and stores the information
    of config_agent.json.</p>
</li>
<li>
<p>The client then configures the aggregator's information to connect
    to its server, where self.aggr_ip reads the IP address of the
    aggregator machine or instance from the agent configuration file.</p>
</li>
<li>
<p>After that, the reg_socket port will be set up, where reg_socket is
    used for registration of the agent, together with an aggregator IP
    address stored as self.aggr_ip. The reg_socket value in this example
    can be read from the agent configuration file as well.</p>
</li>
<li>
<p>msend_socket, which is used in the model exchange routine to send
    the local ML models to the aggregator, will be configured after
    participating in the FL process by sending a message to the FL
    server and receiving the response.</p>
</li>
<li>
<p>exch_socket is used when communication is not in <em>polling</em> mode for
    receiving global models sent from the aggregator, together with an
    agent IP address stored as self.agent_ip.</p>
</li>
<li>
<p>exch_socket in this example can either be read from the arguments
    from the command line or decided by the aggregator, depending on the
    simulation mode.</p>
</li>
<li>
<p>In this example, when the aggregator is set to be able to push
    messages to the connected agents, which is not the case when polling
    mode is on, exch_socket can be dynamically configured by the
    aggregator.</p>
</li>
<li>
<p>self.model_path stores the path to the local and global models and
    can either be read from the agent configuration file or arguments
    from the command line, depending on the simulation mode as well. If
    there is no directory to save those model files, it makes sure to
    create the directory.</p>
</li>
<li>
<p>self.lmfile, self.gmfile, and self.statefile are the filenames for
    local models, global models, and the state of the client
    respectively, and read from the configuration file of the agent. In
    particular, in self.statefile, the value of ClientState is
    saved. ClientState is the enumeration value of the client itself
    where there is a state waiting for the global model (waiting_gm), a
    state for local training (training), a state for sending local
    models (sending), and a state for having the updated global models
    (gm_ready).</p>
</li>
<li>
<p>The round information of the FL process, defined as self.round, is
    initialized as 0 and later updated as the FL round proceeds with
    model aggregation, where the aggregator will notify the change of
    the round usually.</p>
</li>
<li>
<p>self.init_weights_flag is the flag used when a system operator wants
    to initialize a global model with certain parameters, as explained
    in the configuration of the agent.</p>
</li>
<li>
<p>The self.is_polling flag concerns whether to use the polling method
    in communication between the agents and aggregator or not. The
    polling flag must be the same as the one set up on the aggregator
    side.</p>
</li>
<li>
<p>The code about the __init__ constructor discussed here can be
    found in client.py in the fl_main/agent folder on GitHub
    (<a href="https://github.com/tie-set/simple-fl">[https://github.com/keshavaspanda/simple-fl]{.underline}</a>).</p>
</li>
<li>
<p>Now that we have discussed how to initialize a client-side module,
    in the next section, we will look into how the participation
    mechanism works with some sample code.</p>
</li>
</ul>
<p>Agent participation in an FL cycle</p>
<ul>
<li>
<p>This participation or registration process is needed for an agent to
    be able to participate in an FL process together with other agents.
    Therefore, the agent needs to be added to the list of authorized
    agents that can send locally trained ML models to an aggregator.</p>
</li>
<li>
<p>The asynchronous participate function sends the first message to an
    aggregator to join the FL cycle and will receive state and
    communication information, such as socket numbers from the
    aggregator.</p>
</li>
<li>
<p>An agent knows the IP address and port number to join the FL
    platform through the config_agent.json file. When joining the FL
    platform, an agent sends a participation message that contains the
    following information:</p>
</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   agent_name: A unique name of an agent itself.</p>
<ul>
<li>
<p>id: A unique identifier of an agent itself.</p>
</li>
<li>
<p>model_id: A unique identifier of models to be sent to an aggregator.</p>
</li>
<li>
<p>models: A dictionary of models keyed by model names. The weights of
    models need not be trained if init_flag is False, since it is only
    used by an aggregator to remember the shapes of models.</p>
</li>
<li>
<p>init_weights_flag: A Boolean flag to indicate whether the sent model
    weights should be used as a base model. If it is True and there are
    no global models ready, an aggregator sets this set of local models
    as the first global models and sends it to all agents.</p>
</li>
<li>
<p>simulation_flag: This is True if it is a simulation run; otherwise,
    it is False.</p>
</li>
<li>
<p>exch_socket: The port number waiting for global models from the
    aggregator.</p>
</li>
<li>
<p>gene_time: The time that models are generated.</p>
</li>
<li>
<p>performance_dict: Performance data related to models in a dictionary
    format.</p>
</li>
<li>
<p>agent_ip: The IP address of an agent itself.</p>
</li>
</ul>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>&lt;!-- --&gt;
</code></pre></div>
-   With all the aforementioned participation messages defined, the
    agent is ready to exchange models with the aggregator, and the code
    to realize the participation process is as follows:</p>
<blockquote>
<p>async def participate(self):</p>
<p>data_dict, performance_dict = \</p>
<p>load_model_file(self.model_path, self.lmfile)</p>
<p>_, gene_time, models, model_id = \</p>
<p>compatible_data_dict_read(data_dict)</p>
<p>msg = generate_agent_participation_message(</p>
<p>self.agent_name, self.id, model_id, models,</p>
<p>self.init_weights_flag, self.simulation_flag,</p>
<p>self.exch_socket, gene_time, performance_dict,</p>
<p>self.agent_ip)</p>
<p>resp = await send(msg, self.aggr_ip, self.reg_socket)</p>
<p>self.round = resp[ \</p>
<p>int(ParticipateConfirmaMSGLocation.round)]</p>
<p>self.exch_socket = resp[ \</p>
<p>int(ParticipateConfirmationMSGLocation.exch_socket)]</p>
<p>self.msend_socket = resp[ \</p>
<p>int(ParticipateConfirmationMSGLocation.recv_socket)]</p>
<p>self.id = resp[ \</p>
<p>int(ParticipateConfirmationMSGLocation.agent_id)]</p>
<p>self.save_model_from_message(resp, \</p>
<p>ParticipateConfirmationMSGLocation)</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>The agent reads the local models to tell the structure of the ML
    models to the aggregator, and the initial model does not necessarily
    need to be trained. data_dict and performance_dict store the models
    and their performance data respectively.</p>
</li>
<li>
<p>Then, a message, msg, containing information such as the
    ML models and its model_id, is packaged using
    the generate_agent_participation_message function.</p>
</li>
<li>
<p>When sending the message, in this example, the WebSocket is
    constructed using the aggregator's IP address (aggr_ip) and the
    registration port number (reg_socket) to be connected to the
    aggregator.</p>
</li>
<li>
<p>After sending the message to the aggregator via an
    asynchronous send function imported from communication_handler, the
    agent receives a response message, resp, from the aggregator. The
    response will include the round info, the port number to receive the
    global models' exch_socket, the port number to send the local models
    to the aggregator's msend_socket, and an updated agent ID.</p>
</li>
<li>
<p>Finally, the global model within the response message is saved
    locally by calling the save_model_from_message function.</p>
</li>
<li>
<p>The participation mechanism of an agent has been explained. In the
    next section, we will learn about the framework of model exchange
    synchronization.</p>
</li>
</ul>
<p>Model exchange synchronization</p>
<ul>
<li>Model exchange synchronization, as shown in the following code, is
    for checking the state of the agent and calling a proper function
    based on the state:</li>
</ul>
<blockquote>
<p>Async def model_exchange_routine(self):</p>
<p>while True:</p>
<p>await asyncio.sleep(5)</p>
<p>state = read_state(self.model_path, self.statefile)</p>
<p>if state == ClientState.sending:</p>
<p>await self.send_models()</p>
<p>elif state == ClientState.waiting_gm:</p>
<p>if self.is_polling == True:</p>
<p>await self.process_polling()</p>
<p>else: pass</p>
<p>elif state == ClientState.training: pass</p>
<p>elif state == ClientState.gm_ready: pass</p>
<p>else: pass</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>Basically, this process is always running while the client is alive,
    whereas the while loop is used periodically to check the
    client's state and proceed with the next steps if necessary.</p>
</li>
<li>
<p>In the while loop, after waiting a few seconds, it first checks the
    client state by the read_state function. The parameters in
    the read_state function are to locate the state file stored in the
    local environment.</p>
</li>
<li>
<p>As mentioned, ClientState has the enumeration value of the client
    state itself, defining a state for sending local models (sending), a
    state waiting for the global model (waiting_sgm), a state for local
    training (training), and a state for receiving the updated global
    models (gm_ready).</p>
</li>
<li>
<p>If the client is in the sending state (state ==
    ClientState.sending), it means it is ready to send the locally
    trained model to the aggregator. Therefore, the agent calls
    the send_models function to send the locally trained ML model to the
    aggregator.</p>
</li>
<li>
<p>When the state is waiting_gm (state == ClientState.waiting_gm), it
    either proceeds with process_polling to poll from the agent to the
    aggregator if polling mode is on, or just does nothing if polling
    mode is off.</p>
</li>
<li>
<p>If the client is in the training state (state ==
    ClientState.training), it means that the client is training the
    local model now and just waits for a few seconds, printing the
    training status if necessary. You can also add any procedure if
    needed.</p>
</li>
<li>
<p>If the client is in the gm_ready state (state ==
    ClientState.gm_ready), it means that the client received the global
    model. This state will be handled by a local ML application, and it
    does nothing but show the readiness of the global models.</p>
</li>
<li>
<p>In the next section, we will talk about how
    the <em>push</em> and <em>polling</em> mechanisms can be implemented for an FL
    cycle.</p>
</li>
</ul>
<p>Push and polling implementation</p>
<ul>
<li>
<p>Once an agent is initialized and confirmed for participation in an
    FL process, it starts waiting for the global models sent from an
    aggregator. There are two ways to receive global models from the
    aggregator: the push method and the polling method. Although
    the <strong>Secure Sockets Layer</strong> (<strong>SSL</strong>) or <strong>Transport Layer
    Security</strong> (<strong>TSL</strong>) frameworks are not implemented in FL
    client-side code here for simplification, it is recommended to
    support them to secure constant communication.</p>
</li>
<li>
<p>Let's look into the mechanism for each communication framework.</p>
</li>
<li>
<p>The push method from aggregator to agent</p>
</li>
<li>
<p>With the push method, the aggregator will push the message that
    includes global models to all the connected agents right after the
    global models are generated.</p>
</li>
<li>
<p>The following code shows the <em>push</em> mechanism accepting and saving
    global models from the aggregator:</p>
</li>
</ul>
<blockquote>
<p>async def wait_models(self, websocket, path):</p>
<p>gm_msg = await receive(websocket)</p>
<p>self.save_model_from_message( \</p>
<p>gm_msg, GMDistributionMsgLocation)</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>The wait_models asynchronous function accepts websocket as a
    parameter. When the aggregator sends a message to the agent, it
    receives the gm_msg message through await recieve(websocket) and
    saves the global models locally by calling
    the save_model_from_message function, as defined in the <em>Toward
    designing FL client libraries</em> section.</p>
</li>
<li>
<p>The polling method from agent to aggregator</p>
</li>
<li>
<p>With the polling method, an agent will keep asking (polling) an
    aggregator to see whether global models are already formed or not.
    Once it has been created and is ready to be sent to the connected
    agents, the polled message will be returned to the agent with the
    updated global models in the response.</p>
</li>
<li>
<p>The following code about the process_polling asynchronous function
    illustrates the polling method:</p>
</li>
</ul>
<p>async def process_polling(self):</p>
<p>msg = generate_polling_message(self.round, self.id)</p>
<p>resp = await send(msg, self.aggr_ip, self.msend_socket)</p>
<p>if resp[int(PollingMSGLocation.msg_type)] \</p>
<p>== AggMsgType.update:</p>
<p>self.save_model_from_message(resp, \</p>
<p>GMDistributionMsgLocation)</p>
<p>else: pass</p>
<p>CopyExplain</p>
<ul>
<li>
<p>It first generates the polling message with
    the generate_polling_message function to be sent to the aggregator.
    After receiving the response message, resp, from the aggregator, if
    the message type is AggMsgType.update, meaning the response message
    contains the updated global models, it calls
    the save_model_from_message function. Otherwise, it does nothing.</p>
</li>
<li>
<p>The aforementioned functions are the basic but core features of an
    FL client, and those functions need to be efficiently used by a
    user-side ML application as libraries.</p>
</li>
<li>
<p>Now that FL client design, including initialization, participation,
    and model exchanges, has been explained, we will learn about how to
    design FL client libraries.</p>
</li>
</ul>
<p>Designing FL client libraries</p>
<ul>
<li>
<p>In this section, we will explain how to package essential functions
    to be provided as libraries to users. In this example, the simplest
    way to package them as libraries will be discussed.</p>
</li>
<li>
<p>This will need to be expanded, depending on your needs and the
    design of your own FL client framework. By packaging FL client-side
    modules as libraries, developers will be easily able to integrate
    the FL client's functions into the local ML engine.</p>
</li>
<li>
<p>Let's start with how to define a library to start and register an FL
    client.</p>
</li>
</ul>
<p>Starting FL client core threads</p>
<ul>
<li>
<p>For local ML application developers to be able to integrate FL
    client-related functions, they sometimes need to be packaged as
    threading functions.</p>
</li>
<li>
<p>The following code to register an agent in the FL system simply puts
    a participate function into the run_until_complete function of
    an asyncio.get_event_loop function:</p>
</li>
</ul>
<blockquote>
<p>def register_client(self):</p>
<p>asyncio.get_event_loop().run_until_complete( \</p>
<p>self.participate())</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>Also, the start_wait_model_server function is packaged, as shown in
    the following code block, where the Thread function takes care of
    the constant run.</p>
</li>
<li>
<p>This way, you will be able to run the local ML module in parallel
    and receive global models in the wait_models thread when the FL
    system is in <em>push</em> communication mode:</p>
</li>
</ul>
<blockquote>
<p>def start_wait_model_server(self):</p>
<p>th = Thread(target = init_client_server, \</p>
<p>args=[self.wait_models, self.agent_ip, \</p>
<p>self.exch_socket])</p>
<p>th.start()</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>Similarly, the start_model_exhange_server function can be a thread
    to run a model exchange routine to synchronize the local and global
    models, while the local ML module is running in parallel. You can
    just call the following start_model_exchange_server function as a
    library to enable this functionality:</li>
</ul>
<blockquote>
<p>def start_model_exchange_server(self):</p>
<p>self.agent_running = True</p>
<p>th = Thread(target = init_loop, \</p>
<p>args=[self.model_exchange_routine()])</p>
<p>th.start()</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>Finally, it may be helpful to package all these three functions to
    execute at the same time when they are called outside
    the Client class. Therefore, we introduce the following code
    concerning start_fl_client that aggregates the functions of
    registering agents, waiting for global models and a model exchange
    routine to start the FL client core functions:</li>
</ul>
<blockquote>
<p>def start_fl_client(self):</p>
<p>self.register_client()</p>
<p>if self.is_polling == False:</p>
<p>self.start_wait_model_server()</p>
<p>self.start_model_exchange_server()</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>The initiation of the FL client is now packaged
    into start_fl_client. Next, we will define the libraries of saved ML
    models.</li>
</ul>
<p>Saving global models</p>
<ul>
<li>
<p>While the load and save model functions are provided by the helper
    functions in lib/util, which will be explained later in
    the <em>Appendix</em>, <em>Exploring Internal Libraries</em>, it is helpful to
    provide an interface for ML developers to save global models from a
    message sent from an aggregator.</p>
</li>
<li>
<p>The following save_model_from_message function is one that extracts
    and saves global models in an agent and also changes the client
    state to gm_ready. This function takes the message (msg) and message
    location (MSG_LOC) information as parameters:</p>
</li>
</ul>
<blockquote>
<p>def save_model_from_message(self, msg, MSG_LOC):</p>
<p>data_dict = create_data_dict_from_models( \</p>
<p>msg[int(MSG_LOC.model_id)],</p>
<p>msg[int(MSG_LOC.global_models)],</p>
<p>msg[int(MSG_LOC.aggregator_id)])</p>
<p>self.round = msg[int(MSG_LOC.round)]</p>
<p>save_model_file(data_dict, self.model_path, \</p>
<p>self.gmfile)</p>
<p>self.tran_state(ClientState.gm_ready)</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>The global models, model ID, and aggregator ID are extracted from
    the message and put into a dictionary using
    the create_data_dict_from_models library. The round information is
    also updated based on the received message.</p>
</li>
<li>
<p>Then, the received global models are saved to the local file using
    the save_model_file library, in which the data dictionary, model
    path, and global model file name are specified to save the models.</p>
</li>
<li>
<p>After receiving the global models, it changes the client state
    to gm_ready, the state indicating that the global model is ready for
    the local ML to be utilized by calling the tran_state function,
    which will be explained in the next section.</p>
</li>
<li>
<p>With the function of saving global models defined, we are ready to
    move on to how to manipulate the client state in the next section.</p>
</li>
</ul>
<p>Manipulating client state</p>
<ul>
<li>
<p>In order to manipulate the client state so that it can logically
    handle local and global models, we prepare
    the read_state and tran_state functions, which can be accessed both
    from inside and outside the code.</p>
</li>
<li>
<p>The following read_state function reads the value written
    in statefile, stored in the location specified by model_path. The
    enumeration value of ClientState is used to change the client state:</p>
</li>
</ul>
<blockquote>
<p>def read_state(self) -&gt; ClientState:</p>
<p>return read_state(self.model_path, self.statefile)</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>The following tran_state function changes the state of the agent. In
    this code sample, the state is maintained in the local state file
    only:</li>
</ul>
<blockquote>
<p>def tran_state(self, state: ClientState):</p>
<p>write_state(self.model_path, self.statefile, state)</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>Next, let's define the functions that can send local models to an
    aggregator.</li>
</ul>
<p>Sending local models to aggregator</p>
<ul>
<li>The following asynchronous send_models function is about sending
    models that have been saved locally to the aggregator:</li>
</ul>
<blockquote>
<p>async def send_models(self):</p>
<p>data_dict, performance_dict = \</p>
<p>load_model_file(self.model_path, self.lmfile)</p>
<p>, _, models, model_id = \</p>
<p>compatible_data_dict_read(data_dict)</p>
<p>msg = generate_lmodel_update_message( \</p>
<p>self.id, model_id, models, performance_dict)</p>
<p>await send(msg, self.aggr_ip, self.msend_socket)</p>
<p>self.tran_state(ClientState.waiting_gm)</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>It first extracts data_dict and performance_dict using
    the load_model_file helper function and then pulls out the models
    and their ID from data_dict, based on
    the compatible_data_dict_read function. Then, the message is
    packaged with the generate_lmodel_update_message library and sent to
    the aggregator, with the send function from communication_handler.
    After that, the client state is changed to waiting_gm by
    the tran_state function. Again, the SSL/TSL framework can be added
    to secure communication, which is not implemented here to keep the
    FL client-side coding simple.</p>
</li>
<li>
<p>The following send_initial_model function is called when you want to
    send the initial <em>base model</em> to an aggregator of the model
    architecture for registration purposes. It takes initial models, the
    number of samples, and performance value as input and
    calls setup_sending_model, which will be explained later in this
    section:</p>
</li>
</ul>
<blockquote>
<p>def send_initial_model(self, initial_models, \</p>
<p>num_samples=1, perf_val=0.0):</p>
<p>self.setup_sending_models( \</p>
<p>initial_models, num_samples, perf_val)</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>The following send_trained_model function is called when you want to
    send trained local models to the aggregator during the FL cycle. It
    takes trained models, the number of samples, and performance value
    as input and only calls setup_sending_model if the client state
    is <em>not</em> gm_ready:</li>
</ul>
<blockquote>
<p>def send_trained_model(self, models, \</p>
<p>num_samples, perf_value):</p>
<p>state = self.read_state()</p>
<p>if state == ClientState.gm_ready:</p>
<p>pass</p>
<p>else:</p>
<p>self.setup_sending_models( \</p>
<p>models, num_samples, perf_value)</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>The following setup_sending_models function is designed to serve as
    an internal library to set up sending locally trained models to the
    aggregator. It takes parameters of models as np.array, the number of
    samples as an integer, and performance data as a float value:</li>
</ul>
<blockquote>
<p>def setup_sending_models(self, models, \</p>
<p>num_samples, perf_val):</p>
<p>model_id = generate_model_id( \</p>
<p>IDPrefix.agent, self.id, time.time())</p>
<p>data_dict = create_data_dict_from_models( \</p>
<p>model_id, models, self.id)</p>
<p>meta_data_dict = create_meta_data_dict( \</p>
<p>perf_val, num_samples)</p>
<p>save_model_file(data_dict, self.model_path, \</p>
<p>self.lmfile, meta_data_dict)</p>
<p>self.tran_state(ClientState.sending)</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>Basically, this function creates a unique model ID with
    the generate_model_id helper function, data_dict to store the local
    ML models data created with the create_data_dict_from_models helper
    function, and meta_data_dict to store the performance data created
    with the create_meta_data_dict helper function. And then, all the
    aforementioned data related to the models and performance is saved
    locally with the save_model_file function, in the location specified
    with self.model_path. Then, it changes the client state
    to sending so that the mode_exchange_routine function can note the
    change in the client state and start sending trained local models to
    the aggregator.</p>
</li>
<li>
<p>Now that we know about the libraries to send ML models to the
    aggregator, let's learn about an important function to wait for a
    global model on the agent side.</p>
</li>
</ul>
<p>Waiting for global models from an aggregator</p>
<ul>
<li>The following wait_for_global_model function is very important to
    conduct an FL cycle consistently:</li>
</ul>
<blockquote>
<p>def wait_for_global_model(self):</p>
<p>while (self.read_state() != ClientState.gm_ready):</p>
<p>time.sleep(5)</p>
<p>data_dict, _ = load_model_file( \</p>
<p>self.model_path, self.gmfile)</p>
<p>global_models = data_dict[\'models\']</p>
<p>self.tran_state(ClientState.training)</p>
<p>return global_models</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>The principle is that the function waits until the client state
    becomes gm_ready. The transition of the client state
    to gm_ready happens when the global model is received on the agent
    side. Once the client state changes to gm_ready, it proceeds to load
    global models from data_dict, extracted with
    the load_model_file function, changes the client state to training,
    and returns the global models to the local ML module.</p>
</li>
<li>
<p>We have discussed how to design the libraries of FL client-side
    functions. In the next section, we will discuss how to integrate
    those libraries into a local ML process.</p>
</li>
</ul>
<p>Local ML engine integration into an FL system</p>
<ul>
<li>
<p>The successful integration of FL client libraries into a local ML
    engine is key to conducting FL in distributed environments later on.</p>
</li>
<li>
<p>The minimal_MLEngine.py file in the examples/minimal directory found
    in the GitHub repository
    at <a href="https://github.com/tie-set/simple-fl">[https://github.com/tie-set/simple-fl]{.underline}</a>,
    as shown in <em>Figure 5.2</em>, provides an example of integrating FL
    client-side libraries into a minimal ML engine package:</p>
</li>
</ul>
<p><img alt="Figure 5.2 -- The minimal ML engine package
" src="../images/media/image42.jpg" />{width="6.268055555555556in"
height="1.1395833333333334in"}</p>
<p>Figure 5.2 -- The minimal ML engine package</p>
<ul>
<li>Next, we will explain what libraries need to be imported into the
    local ML engine in the following section.</li>
</ul>
<p>Importing libraries for a local ML engine</p>
<ul>
<li>
<p>The following code shows the importing process, where general
    libraries such as numpy, time, and Dict are imported first. The key
    part of this process is that Client is imported from
    the client.py file in the fl_main.agent folder. This way, a
    developer does not need to know too much about the code inside an FL
    system and just calls the important functionalities defined as
    libraries, as discussed in the <em>Toward designing FL client
    libraries</em> section.</p>
</li>
<li>
<p>We will not cover the pip installation packaging here in this book,
    but it is possible to host the client-side code with either a
    private or public PyPI server:</p>
</li>
</ul>
<blockquote>
<p>import numpy as np</p>
<p>import time, logging, sys</p>
<p>from typing import Dict</p>
<p>from fl_main.agent.client import Client</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>After importing the necessary libraries, let's look at the functions
    defined for local training and testing.</li>
</ul>
<p>Defining the ML models, training, and test functions</p>
<ul>
<li>
<p>You first define the models, training, and testing functions to be
    integrated into the FL system. In this code example, we will use
    dummy models and training/testing functions, allowing users to be
    able to understand the minimal FL procedure without being bothered
    by specific ML complications.</p>
</li>
<li>
<p>The following function called init_models returns the templates of
    models (in a dictionary format) to inform the ML model structure.
    The models do not need to be trained necessarily. In this case, the
    models have two layers defined by model1 and model2, where
    some random NumPy array is assigned to each layer, as follows:</p>
</li>
</ul>
<blockquote>
<p>def init_models() -&gt; Dict[str,np.array]:</p>
<p>models = dict()</p>
<p>models[\'model1\'] = np.array([[1, 2, 3], [4, 5, 6]])</p>
<p>models[\'model2\'] = np.array([[1, 2], [3, 4]])</p>
<p>return models</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>After initializing the models, you will design the
    following training function that can be a placeholder function for
    each ML application:</li>
</ul>
<blockquote>
<p>def training(models: Dict[str,np.array],</p>
<p>init_flag: bool = False) -&gt; Dict[str,np.array]:</p>
<p># return templates of models to tell the structure</p>
<p># This model is not necessarily actually trained</p>
<p>if init_flag:</p>
<p>return init_models()</p>
<p># ML Training. In this example, no actual training.</p>
<p>models = dict()</p>
<p>models[\'model1\'] = np.array([[1, 2, 3], [4, 5, 6]])</p>
<p>models[\'model2\'] = np.array([[1, 2], [3, 4]])</p>
<p>return models</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>The logic of this function should be in the order of taking models
    as input, training them, and returning trained local models. As
    input parameters, it takes models with
    the Dict[str,np.array] format and the init_flag Boolean value,
    indicating whether it is the initialization step or not.</p>
</li>
<li>
<p>init_flag is True when you want to call and return the
    predefined init_models, and it is False if it's an actual training
    step.</p>
</li>
<li>
<p>Eventually, this function returns the trained models that are
    decomposed into NumPy arrays, with a dictionary
    of Dict[str,np.array] in this example.</p>
</li>
<li>
<p>In this dummy example, we are just giving you dummy models that skip
    the actual training process.</p>
</li>
<li>
<p>Then, the following compute_performance function is designed to
    compute the performance of models given a set of models and a test
    dataset:</p>
</li>
</ul>
<blockquote>
<p>def compute_performance(models: Dict[str,np.array], \</p>
<p>testdata) -&gt; float:</p>
<p># replace with actual performance computation logic</p>
<p>accuracy = 0.5</p>
<p>return</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>Again, in this example, just a dummy accuracy value is given, 0.5,
    to keep things simple.</p>
</li>
<li>
<p>Then, you may want to define the
    following judge_termination function to decide the criteria to
    finish the training process and exit from the FL process:</p>
</li>
</ul>
<blockquote>
<p>def judge_termination(training_count: int = 0,</p>
<p>global_arrival_count: int = 0) -&gt; bool:</p>
<p># Depending on termination criteria, change the return bool value</p>
<p># Call a performance tracker to check if the current models
satisfy the required performance</p>
<p>return True</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>It is up to you how to design this termination condition. This
    function takes parameters such as the number of completed training
    processes (training_count), the number of times it received global
    models (global_arrival_count), and so on, returning a Boolean value
    where the flag is True if it continues the FL process and False if
    it stops. Here, it just gives a True Boolean value, meaning the FL
    process will not stop unless the agent is forced to stop outside of
    this function.</p>
</li>
<li>
<p>If preparing the test data is needed, you can define a function such
    as prep_test_data:</p>
</li>
</ul>
<blockquote>
<p>def prep_test_data():</p>
<p>testdata = 0</p>
<p>return</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>In this example, it is just set as 0.</p>
</li>
<li>
<p>Now that the necessary functions for testing and training are
    defined, we will integrate client libraries into the local ML engine
    to run the FL agent working with the FL server-side components, such
    as an aggregator and a database.</p>
</li>
</ul>
<p>Integration of client libraries into your local ML engine</p>
<ul>
<li>
<p>Now, everything is ready to start your very first FL process,
    although the models, training, and testing functions are set with
    dummy variables.</p>
</li>
<li>
<p>The very first thing to do is to create a Client instance as follows
    so that you can call its libraries:</p>
</li>
</ul>
<blockquote>
<p># Step1: Create Client instance</p>
<p>cl = Client()</p>
<p>CopyExplain</p>
<p>Second, you create the initial_models with the training function, as
follows:</p>
<p># Step2: Create template models (to tell the shapes)</p>
<p>initial_models = training(dict(), init_flag=True)</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>After that, it sends the initial models to the FL aggregator by
    calling cl.send_initial_model, with initial_models as a parameter:</li>
</ul>
<blockquote>
<p># Step3: Send initial models</p>
<p>cl.send_initial_model(initial_model)</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>Then, let's just start the client-side FL process by
    calling cl.start_fl_client(). As explained earlier in the <em>Starting
    FL client core threads</em> section, this function can start three
    processes at the same time: registering the agent, waiting for
    global models, and the model exchange routine:</li>
</ul>
<blockquote>
<p># Step4: Start the FL client</p>
<p>cl.start_fl_client()</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>Then, we design the client-side FL cycle of local training/testing
    and sending/receiving models by effectively integrating the several
    FL client libraries, as follows:</li>
</ul>
<blockquote>
<p># Step5: Run the local FL loop</p>
<p>training_count, gm_arrival_count = 0, 0</p>
<p>while judge_termination(training_count, gm_arrival_count):</p>
<p>global_models = cl.wait_for_global_model()</p>
<p>gm_arrival_count += 1</p>
<p>global_model_performance_data = \</p>
<p>compute_performance(global_models, prep_test_data())</p>
<p>models = training(global_models)</p>
<p>training_count += 1</p>
<p>perf_value = compute_performance( \</p>
<p>models, prep_test_data())</p>
<p>cl.send_trained_model(models, 1, perf_value)</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>We use a while loop and the judge_termination function to check
    whether the system needs to leave the loop. It is up to you to
    use training_count and gm_arrival_count to judge the termination of
    the FL cycle.</p>
</li>
<li>
<p>Then, the agent proceeds to wait for the global models
    with cl.wait_for_global_model(). Upon the arrival of the global
    models from the aggregator, it extracts global_models,
    increments gm_arrival_count, and sets the client state to
    the training state in the wait_for_global_model function.</p>
</li>
<li>
<p>Next, global_model_performance_data is calculated with
    the compute_performance function,
    taking global_models and prep_test_data as input.</p>
</li>
<li>
<p>While executing training(global_models) in the training state, the
    client might receive new global models from the aggregator. This
    scenario happens when the client's local training was too slow, and
    the aggregator decided to utilize other local models to create a new
    set of global models. If the new global models have already arrived
    at the agent, the client's state is changed to gm_ready and the
    current ML model being trained will be discarded.</p>
</li>
<li>
<p>After the local training phase has finished with models generated
    by training(global_models), an agent increments training_count and
    calculates the performance data, perf_value, of the current ML model
    with the compute_performance function.</p>
</li>
<li>
<p>Then, the agent tries to upload the trained local models to the
    aggregator via cl.send_trained_model, taking the trained models and
    the performance value calculated previously as parameters.</p>
</li>
<li>
<p>In the send_trained_model function, the client state is set
    to sending. Once the client's model_exchange_routine observes the
    state transition to the sending state, it sends the trained local
    models (stored as a binary file) to the aggregator. After sending
    the models, it goes back to the waiting_gm state in
    the send_models function.</p>
</li>
<li>
<p>After sending the local models, the aggregator stores the uploaded
    local models in its buffers and waits for another round of global
    model aggregation, until enough local models are uploaded by agents.</p>
</li>
<li>
<p>In the next section, we will briefly talk about how to integrate
    image classification ML into the FL system we have discussed.</p>
</li>
</ul>
<p>An example of integrating image classification  into an FL system</p>
<ul>
<li>
<p>We learned about how to initiate an FL process with a minimal
    example. In this section, we will give a brief example of FL
    with <strong>image classification</strong> (<strong>IC</strong>) using a CNN.</p>
</li>
<li>
<p>First, the package that contains the image classification example
    code is found in the examples/image_classification/ folder in the
    GitHub repository
    at <a href="https://github.com/tie-set/simple-fl">[https://github.com/keshavaspanda/simple-fl]{.underline}</a>,
    as shown in <em>Figure 5.3</em>:</p>
</li>
</ul>
<p><img alt="Figure 5.3 -- The image classification package
" src="../images/media/image47.jpg" />{width="4.041666666666667in"
height="2.627083333333333in"}</p>
<p>Figure 5.3 -- The image classification package</p>
<ul>
<li>
<p>The main code in charge of integrating the IC algorithms into the FL
    systems is found in the classification_engine.py file.</p>
</li>
<li>
<p>When importing the libraries, we use a couple of extra files that
    include CNN models, converter functions, and data managers related
    to IC algorithms. The details are provided in the GitHub code
    at <a href="https://github.com/tie-set/simple-fl">[https://github.com/keshavaspanda/simple-fl]{.underline}</a>.</p>
</li>
<li>
<p>Next, let's import some standard ML libraries as well as client
    libraries from the FL code we discussed:</p>
</li>
</ul>
<blockquote>
<p>import logging</p>
<p>import numpy as np</p>
<p>import torch</p>
<p>import torch.nn as nn</p>
<p>import torch.optim as optim</p>
<p>from typing import Dict</p>
<p>from .cnn import Net</p>
<p>from .conversion import Converter</p>
<p>from .ic_training import DataManger, execute_ic_training</p>
<p>from fl_main.agent.client import Client</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>In this case, we define TrainingMetaData, which just gives you the
    amount of training data that will be sent to the aggregator and used
    when conducting the FedAvg algorithm. The aggregation algorithm was
    discussed in <a href="https://subscription.packtpub.com/book/data/9781803247106/7/ch07lvl1sec42/B18369_04.xhtml#_idTextAnchor085"><em>section
    4</em></a>, <em>Federated
    Learning Server Implementation with Python</em>, as well as in <a href="https://subscription.packtpub.com/book/data/9781803247106/7/ch07lvl1sec42/B18369_07.xhtml#_idTextAnchor176"><em>section
    7</em></a>, <em>Model
    Aggregation</em>:</li>
</ul>
<blockquote>
<p>class TrainingMetaData:</p>
<p># The number of training data used for each round</p>
<p># This will be used for the weighted averaging</p>
<p># Set to a natural number &gt; 0</p>
<p>num_training_data = 8000</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>The content of the init_models function is now replaced with a CNN
    that is converted into a NumPy array. It returns the template of the
    CNN in a dictionary format to inform the structure:</li>
</ul>
<blockquote>
<p>def init_models() -&gt; Dict[str,np.array]:</p>
<p>net = Net()</p>
<p>return Converter.cvtr().convert_nn_to_dict_nparray(net)</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>The training function, training, is now filled with actual training
    algorithms using the CIFAR-10 dataset. It takes the models
    and init_flag as parameters and returns the trained models
    as Dict[str,np.array]. The init_flag is a bool value, where it
    is True if it's at the initial step and False if it's an actual
    training step. When preparing for the training data, we use a
    certain threshold for training due to batch size. In this case, the
    threshold is 4.</p>
</li>
<li>
<p>Then, we create a CNN-based cluster global model with net =
    Converter.cvtr().convert_dict_nparray_to_nn(models).</p>
</li>
<li>
<p>We define the loss function and optimizer as the following:</p>
</li>
</ul>
<blockquote>
<p>criterion = nn.CrossEntropyLoss()</p>
<p>optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>
<p>Then, the actual training will be conducted with trained_net =
    execute_ic_training(DataManger.dm(), net, criterion, optimizer),
    where the actual code of the IC training can be found in
    the ic_training.py file.</p>
</li>
<li>
<p>After the training, the converted models will be returned.</p>
</li>
<li>
<p>The algorithm is summarized as follows:</p>
</li>
</ul>
<blockquote>
<p>def training(models: Dict[str,np.array], \</p>
<p>init_flag: bool=False) -&gt; Dict[str,np.array]:</p>
<p>if init_flag:</p>
<p>DataManger.dm( \</p>
<p>int(TrainingMetaData.num_training_data / 4))</p>
<p>return init_models()</p>
<p>net = \</p>
<p>Converter.cvtr().convert_dict_nparray_to_nn(models)</p>
<p>criterion = nn.CrossEntropyLoss()</p>
<p>optimizer = optim.SGD(net.parameters(), \</p>
<p>lr=0.001, momentum=0.9)</p>
<p>trained_net = execute_ic_training(DataManger.dm(), \</p>
<p>net, criterion, optimizer)</p>
<p>models = Converter.cvtr(). \</p>
<p>convert_nn_to_dict_nparray(trained_net)</p>
<p>return models</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>The following compute_performance function is filled with an
    algorithm to calculate the accuracy, which is simple enough -- just
    divide the number of correct outcomes by the number of total labels.
    With a given set of models and a test dataset, it computes the
    performance of the models, with models and testdata as parameters:</li>
</ul>
<blockquote>
<p>def compute_performance(models: Dict[str,np.array], \</p>
<p>testdata, is_local: bool) -&gt; float:</p>
<p># Convert np arrays to a CNN</p>
<p>net = \</p>
<p>Converter.cvtr().convert_dict_nparray_to_nn(models)</p>
<p>correct, total = 0, 0</p>
<p>with torch.no_grad():</p>
<p>for data in DataManger.dm().testloader:</p>
<p>images, labels = data</p>
<p>_, predicted = torch.max(net(images).data, 1)</p>
<p>total += labels.size(0)</p>
<p>correct += (predicted == labels).sum().item()</p>
<p>acc = float(correct) / total</p>
<p>return acc</p>
</blockquote>
<p>CopyExplain</p>
<ul>
<li>The judge_termination and prep_test_data functions are the same as
    the functions of the minimal examples.</li>
</ul>
<p>Integration of client libraries into the IC example</p>
<ul>
<li>
<p>Now, everything is ready to start the IC algorithm, and all the code
    to integrate the preceding functions is the same as that used in the
    previous <em>Integration of client libraries into your local ML
    engine</em> section.</p>
</li>
<li>
<p>Please look into the classification_engine.py file to make sure the
    code is the same, except for the part that shows the actual number
    of data samples that we are sending. This way, by just rewriting the
    preceding functions, you will be able to easily connect your own
    local ML application to the FL system that we have discussed here.</p>
</li>
<li>
<p>Please refer to the <em>Running image classification and its
    analysis</em> section, <em>Running the Federated Learning System and
    Analyzing the Results</em>, to check the results of running the code
    discussed in this section.</p>
</li>
</ul>
<p>Summary</p>
<ul>
<li>
<p>Here, we discussed FL client-side implementation. There are three
    basic but important functionalities when participating in the FL
    process, receiving global models sent from an aggregator with a push
    or polling mechanism, and sending local models to an aggregator
    after the local training process.</p>
</li>
<li>
<p>In order to effectively implement the client-side ML engines that
    cooperate with the FL aggregator, understanding the client state is
    important. The client states include a state waiting for the global
    model, a state indicating that local training is happening, a state
    showing the readiness to send local models, and a state for having
    the updated global models.</p>
</li>
<li>
<p>We also discussed the design philosophy of FL client-side libraries,
    where the core functions need to be effectively packaged to provide
    user-friendly interfaces for ML developers and engineers.</p>
</li>
<li>
<p>Last but not least, we learned how to actually use the FL client
    libraries to integrate a local ML engine into an FL system, where we
    used the minimal dummy example and IC example to understand the
    integration process itself.</p>
</li>
<li>
<p>In the next section, we will actually run the code that was
    introduced in this and previous sections so that we can dig into
    what is happening with the models, which are aggregated with a
    minimal example as well as an IC example.</p>
</li>
</ul>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2024 <a href="https://spanda.io"  target="_blank" rel="noopener">Spanda.io</a>

    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://ranga-rangarajan.github.io/spanda-bootcamp/" target="_blank" rel="noopener" title="ranga-rangarajan.github.io" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.caa56a14.min.js"></script>
      
    
  </body>
</html>