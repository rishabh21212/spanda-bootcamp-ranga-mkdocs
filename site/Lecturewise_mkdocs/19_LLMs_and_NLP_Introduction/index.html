
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../18_Federated_Learning_Future_Directions/">
      
      
        <link rel="next" href="../20_Retrieval_Augmented_Generation/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.7">
    
    
      
        <title>LLM's & NLP Introduction - Spanda DL Bootcamp</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.f2e4d321.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#llms-nlp-introduction" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Spanda DL Bootcamp" class="md-header__button md-logo" aria-label="Spanda DL Bootcamp" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Spanda DL Bootcamp
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              LLM's & NLP Introduction
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-hidden="true"  type="radio" name="__palette" id="__palette_2">
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  The Complete Generative AI Bootcamp

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../01_Course_Vision_and_Running_a_Local_LLM/" class="md-tabs__link">
          
  
  Lecturewise mkdocs

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Spanda DL Bootcamp" class="md-nav__button md-logo" aria-label="Spanda DL Bootcamp" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Spanda DL Bootcamp
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The Complete Generative AI Bootcamp
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Lecturewise mkdocs
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Lecturewise mkdocs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01_Course_Vision_and_Running_a_Local_LLM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    01 Course Vision and Running a Local LLM
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02_Software_Engineering_for_AI/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    02 Software Engineering for AI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03_Ops_-_DevOps%2C_MLOps%2C_AIOps/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    03 Ops   DevOps, MLOps, AIOps
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04_Data_Collection%2C_Data_Labeling%2C_Data_Management%2C_Analytics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    04 Data Collection, Data Labeling, Data Management, Analytics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05_Testing_and_Deployment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    05 Testing and Deployment
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06.5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    06.5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06_LLM_Serving_and_Reinforcement_Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    06 LLM Serving and Reinforcement Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07_Distributed_Systems_Patterns/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    07 Distributed Systems Patterns
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08_Workflow_Patterns/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    08 Workflow Patterns
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09_Scheduling_%26amp%3B_Metadata_Patterns/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    09 Scheduling &amp; Metadata Patterns
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10_Data_Lakes_%26amp%3B_Intro_to_Federated_Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10 Data Lakes &amp; Intro to Federated Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_Federated_Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11 Federated Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12_Federated_Learning_System_Details/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12 Federated Learning System Details
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13_Federated_Learning_Implementation_-_Server_Side/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13 Federated Learning Implementation   Server Side
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14_Federated_Learning_Implementation_-_Client_Side/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14 Federated Learning Implementation   Client Side
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../15_Federated_Learning_Running_the_System_%26amp%3B_Analyzing_Results/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15 Federated Learning Running the System &amp; Analyzing Results
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../16_Introducing_Existing_Federated_Learning_Frameworks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16 Introducing Existing Federated Learning Frameworks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../17_Federated_Learning_Use_Cases_and_Case_Studies/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17 Federated Learning Use Cases and Case Studies
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18_Federated_Learning_Future_Directions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18 Federated Learning Future Directions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    LLM's & NLP Introduction
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../20_Retrieval_Augmented_Generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Retrieval Augmented Generation (RAG)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../21_RAG_Pipeline_Implementation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RAG Pipeline Implementation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../22_From_Simple_to_Advanced_RAG/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    From Simple to Advanced RAG
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../23_RAG_Observability/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Observability Tools for RAG
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="llms-nlp-introduction">LLM's &amp; NLP Introduction</h1>
<h1 id="1-introduction">1 Introduction</h1>
<blockquote>
<p>In this chapter, we will lay the foundation for building a chat-to-PDF
app using Large Language Models (LLMs) with a focus on the
Retrieval-Augmented Generation approach. We'll explore the fundamental
concepts and technologies that underpin this project.</p>
</blockquote>
<h2 id="11-the-role-of-llms-in-nlp">1.1 The Role of LLMs in NLP</h2>
<blockquote>
<p>Large Language Models (LLMs) play a crucial role in Natural Language
Processing (NLP). These models have revolutionized the field of NLP by
their ability to understand and generate human-like text. With
advances in deep learning and neural networks, LLMs have become
valuable assets in various NLP tasks, including language translation,
text summarization, and chatbot development.</p>
<p>One of the key strengths of LLMs lies in their capacity to learn from
vast amounts of text data. By training on massive datasets, LLMs can
capture complex linguistic patterns and generate coherent and
contextually appropriate responses. This enables them to produce
high-quality outputs that are indistinguishable from human-generated
text.</p>
<p>LLMs are trained using a two-step process: pre-training and
fine-tuning. During pre-training, models are exposed to a large corpus
of text data and learn to predict the next word in a sentence. This
helps them develop a strong understanding of language structure and
semantics. In the fine-tuning phase, the models are further trained on
task-specific data to adapt their knowledge to specific domains or
tasks. The versatility and effectiveness of LLMs make them a powerful
tool in advancing the field of NLP. They have not only improved the
performance of existing NLP systems but have also opened up new
possibilities for developing innovative applications. With continued
research and development, LLMs are expected to further push the
boundaries of what is possible in natural language understanding and
generation.</p>
<p>Large Language Models (LLMs) represent a breakthrough in NLP, allowing
machines to understand and generate human-like text at an
unprecedented level of accuracy and fluency. Some of the key roles of
LLMs in NLP include:</p>
</blockquote>
<ol>
<li>
<p><strong>Natural Language Understanding (NLU):</strong> LLMs can comprehend the
    nuances of human language, making them adept at tasks such as
    sentiment analysis, entity recognition, and language translation.</p>
</li>
<li>
<p><strong>Text Generation:</strong> LLMs excel at generating coherent and
    contextually relevant text. This capability is invaluable for
    content generation, chatbots, and automated writing.</p>
</li>
<li>
<p><strong>Question Answering:</strong> LLMs are particularly powerful in question
    answering tasks. They can read a given text and provide accurate
    answers to questions posed in natural language.</p>
</li>
<li>
<p><strong>Summarization:</strong> LLMs can summarize lengthy documents or articles,
    distilling the most important information into a concise form.</p>
</li>
<li>
<p><strong>Conversational AI:</strong> They serve as the backbone of conversational
    AI systems, enabling chatbots and virtual assistants to engage in
    meaningful and context-aware conversations.</p>
</li>
<li>
<p><strong>Information Retrieval:</strong> LLMs can be used to retrieve relevant
    information from vast corpora of text, which is crucial for
    applications like search engines and document retrieval.</p>
</li>
<li>
<p><strong>Customization:</strong> LLMs can be fine-tuned for specific tasks or
    domains, making them adaptable to a wide range of applications.</p>
</li>
</ol>
<h2 id="12-the-importance-of-question-answering-over-pdfs">1.2 The Importance of Question Answering over PDFs</h2>
<blockquote>
<p><em>Question answering over PDF documents addresses a critical need in
information retrieval and document processing. Here, we'll explore why
it is important and how LLMs can play a pivotal role:</em></p>
<p>The Importance of Question Answering over PDFs:</p>
</blockquote>
<ol>
<li>
<p><strong>Document Accessibility:</strong> PDF is a widely used format for storing
    and sharing documents. However, extracting information from PDFs,
    especially in response to specific questions, can be challenging for
    users. Question answering over PDFs enhances document accessibility.</p>
</li>
<li>
<p><strong>Efficient Information Retrieval:</strong> For researchers, students, and
    professionals, finding answers within lengthy PDF documents can be
    time-consuming. Question-answering systems streamline this process,
    enabling users to quickly locate the information they need.</p>
</li>
<li>
<p><strong>Enhanced User Experience:</strong> In various domains, including legal,
    medical, and educational, users often need precise answers from PDF
    documents. Implementing question answering improves the user
    experience by providing direct and accurate responses.</p>
</li>
<li>
<p><strong>Automation and Productivity:</strong> By automating the process of
    extracting answers from PDFs, organizations can save time and
    resources. This automation can be particularly beneficial in
    scenarios where large volumes of documents need to be processed.</p>
</li>
<li>
<p><strong>Scalability:</strong> As the volume of digital documents continues to
    grow, scalable solutions for question answering over PDFs become
    increasingly important. LLMs can handle large datasets and diverse
    document types.</p>
</li>
</ol>
<blockquote>
<p>In various industries, there is a growing demand for efficient
information retrieval from extensive collections of PDF documents.
Take, for example, a legal firm or department collaborating with the
Federal Trade Commission (FTC) to process updated information about
legal cases and proceedings. Their task often involves processing a
substantial volume of documents, sifting through them, and extracting
relevant case information---a labour intensive process.</p>
<p><em>Background: Every year the FTC brings hundreds of cases against
individuals and companies for violating consumer protection and
competition laws that the agency enforces. These cases can involve
fraud, scams, identity theft, false advertising, privacy violations,
anti-competitive behaviour and more.</em></p>
<p>The advent of the Retrieval-Augmented Generation (RAG) approach marks
a new era in question and answering that promises to revolutionize
workflows within these industries.</p>
</blockquote>
<h2 id="13-the-retrieval-augmented-generation-approach">1.3 The Retrieval-Augmented Generation Approach</h2>
<blockquote>
<p><em>The Retrieval-Augmented Generation approach is a cutting-edge
technique that combines the strengths of information retrieval and
text generation. Let's explore this approach in detail:</em></p>
<p>The Retrieval-Augmented Generation Approach:</p>
<p>The Retrieval-Augmented Generation approach combines two fundamental
components, retrieval and generation, to create a powerful system for
question answering and content generation. Here's an overview of this
approach:</p>
</blockquote>
<ol>
<li>
<p><strong>Retrieval Component:</strong> This part of the system is responsible for
    searching and retrieving relevant information from a database of
    documents. It uses techniques such as indexing, ranking, and query
    expansion to find the most pertinent documents.</p>
</li>
<li>
<p><strong>Generation Component:</strong> Once the relevant documents are retrieved,
    the generation component takes over. It uses LLMs to process the
    retrieved information and generate coherent and contextually
    accurate responses to user queries.</p>
</li>
<li>
<p><strong>Benefits:</strong> The key advantage of this approach is its ability to
    provide answers based on existing knowledge (retrieval) while also
    generating contextually rich responses (generation). It combines the
    strengths of both worlds to deliver high-quality answers.</p>
</li>
<li>
<p><strong>Use Cases:</strong> Retrieval-Augmented Generation is particularly useful
    for question answering over large document collections, where
    traditional search engines may fall short in providing concise and
    informative answers.</p>
</li>
<li>
<p><strong>Fine-Tuning:</strong> Successful implementation of this approach often
    involves fine-tuning LLMs on domain-specific data to improve the
    quality of generated responses.</p>
</li>
</ol>
<blockquote>
<p>By understanding the role of LLMs in NLP, the importance of question
answering over PDFs, and the principles behind the Retrieval-Augmented
Generation approach, you have now laid the groundwork for building
your chat-to-PDF app using these advanced technologies. In the
following chapters, we will delve deeper into the technical aspects
and practical implementation of this innovative solution.</p>
</blockquote>
<h2 id="14-a-brief-history-of-llms">1.4 A Brief History of LLMs</h2>
<blockquote>
<p>Lately, ChatGPT, as well as DALL-E-2 and Codex, have been getting a
lot of attention. This has sparked curiosity in many who want to know
more about what's behind their impressive performance. ChatGPT and
other Generative AI (GAI) technologies fall into a category called
Artificial Intelligence Generated Content (AIGC). This means they're
all about using AI models to create content like images, music, and
written language. The whole idea behind AIGC is to make creating
conetent faster and easier.</p>
<p><em>AIGC is achieved by extracting and understanding intent information
from instructions provided by human, and generating the content
according to its knowledge and the intent information. In recent
years, large-scale models have become increasingly important in AIGC
as they provide better intent extraction and thus, improved generation
results.</em></p>
</blockquote>
<p>With more data and bigger models, these AI systems can make things that
look and sound quite realistic and high-quality. The following shows an
example of text prompting that generates images according to the
instructions, leveraging the OpenAI DALL-E-2 model.</p>
<blockquote>
<p><img alt="" src="../media/media/image50.jpg" />{width="4.857113954505687in"
height="1.3914140419947507in"}</p>
<p>Figure 1.1: Examples of AIGC in image generation. Image source</p>
<p>In the realm of Generative AI (GAI), models can typically be divided
into two categories: unimodal models and multimodal models. Unimodal
models operate by taking instructions from the same type of data as
the content they generate, while multimodal models are capable of
receiving instructions from one type of data and generating content in
another type. The following figure illustrates these two categories of
models.</p>
<p>These models have found applications across diverse industries, such
as art and design, marketing, and education. It's evident that in the
foreseeable future, AIGC will remain a prominent and continually
evolving research area with artificial intelligence.</p>
</blockquote>
<h3 id="141-foundation-models">1.4.1 Foundation Models</h3>
<blockquote>
<p>Speaking of LLMs and GenAI, we cannot overlook the significant role
played by Transformer models.</p>
<p><img alt="" src="../media/media/image49.jpg" />{width="4.857123797025372in"
height="1.893879046369204in"}</p>
<p>Figure 1.2: Overview of AIGC model types. Image source</p>
<p><em>Transformer is the backbone architecture for many state-of-the-art
models, such as GPT, DALL-E, Codex, and so on.</em></p>
<p>Transformer started out to address the limitations of traditional
models like RNNs when dealing with variable-length sequences and
context. The heart of the Transformer is its self-attention mechanism,
allowing the model to focus on different parts of an input sequence.
It comprises an encoder and a decoder. The encoder processes the input
sequence to create hidden representations, while the decoder generates
an output sequence. Each encoder and decoder layer includes multi-head
attention and feed-forward neural networks. Multi-head attention, a
key component, assigns weights to tokens based on relevance, enhancing
the model's performance in various NLP tasks. The Transformer's
inherent parallelizability minimizes inductive biases, making it ideal
for large-scale pretraining and adaptability to different downstream
tasks.</p>
<p>Transformer architecture has dominated natural language processing,
with two main types of pre-trained language models based on training
tasks: masked language modeling (e.g., BERT) and autoregressive
language modeling (e.g., GPT3). Masked language models predict masked
tokens within a sentence, while autoregressive models focus on
predicting the next token given previous ones, making them more
suitable for generative tasks. RoBERTa and XL-Net are classic examples
of masked language models and have further improved upon the BERT
architecture with additional training data and techniques.</p>
<p><img alt="" src="../media/media/image46.jpg" />{width="4.857099737532808in"
height="1.7512346894138233in"}</p>
<p>Figure 1.3: Categories of pre-trained LLMs. Image source</p>
<p>In this graph, you can see two types of information flow indicated by
lines: the black line represents bidirectional information flow, while
the gray line represents left-to-right information flow. There are
three main model categories:</p>
</blockquote>
<ol>
<li>
<p>Encoder models like BERT, which are trained with context-aware
    objectives.</p>
</li>
<li>
<p>Decoder models like GPT, which are trained with autoregressive
    objectives.</p>
</li>
<li>
<p>Encoder-decoder models like T5 and BART, which merge both
    approaches. These models use context-aware structures as encoders
    and left-to-right structures as decoders.</p>
</li>
</ol>
<h3 id="142-reinforcement-learning-from-human-feedback">1.4.2 Reinforcement Learning from Human Feedback</h3>
<blockquote>
<p>To improve AI-generated content (AIGC) alignment with user intent,
i.e., considerations in <em>usefulness</em> and <em>truthfulness</em>, reinforcement
learning from human feedback (RLHF) has been applied in models like
Sparrow, InstructGPT, and ChatGPT.</p>
<p>The RLHF pipeline involves three steps: <em>pre-training</em>, <em>reward
learning</em>, and <em>finetuning with reinforcement learning</em>. In reward
learning, human feedback on diverse responses is used to create reward
scalars. Fine-tuning is done through reinforcement learning with
Proximal Policy Optimization (PPO), aiming to maximize the learned
reward.</p>
<p>However, the field lacks benchmarks and resources for RL, which is
seen as a challenge. But this is changing day-by day. For example, an
open-source library called RL4LMs was introduced to address this gap.
Claude, a dialogue agent, uses <em>Constitutional AI</em>, where the reward
model is learned via RL from AI feedback. The focus is on reducing
harmful outputs, with guidance from a set of principles provided by
humans. See more about the topic of <em>Constitutional AI</em> in one of our
blog post here.</p>
</blockquote>
<h3 id="143-gan">1.4.3 GAN</h3>
<blockquote>
<p>Generative Adversarial Networks (GANs) are widely used for image
generation. GANs consist of a generator and a discriminator. The
generator creates new data, while the discriminator decides if the
input is real or not.</p>
<p>The design of the generator and discriminator influences GAN training
and performance. Various GAN variants have been developed, including
LAPGAN, DCGAN, Progressive GAN, SAGAN, BigGAN, StyleGAN, and methods
addressing mode collapse like D2GAN and GMAN.</p>
<p>The following graph illustrates some of the categories of vision
generative models.</p>
<p>Although GAN models are not the focus of our book, they are essential
in powering multi-modality applications such as the diffusion models.</p>
</blockquote>
<h3 id="144-applications">1.4.4 Applications</h3>
<blockquote>
<p>Chatbots are probably one of the most popular applications for LLMs.</p>
<p><img alt="" src="../media/media/image53.jpg" />{width="4.857048337707787in"
height="2.4782884951881017in"}</p>
<p>Figure 1.4: Categories of vision generative models. Image source</p>
<p>Chatbots are computer programs that mimic human conversation through
textbased interfaces. They use language models to understand and
respond to user input. Chatbots have various use cases, like customer
support and answering common questions. Our "<em>chat with your PDF
documents</em>" is a up-and-coming use case!</p>
<p>Other notable examples include Xiaoice, developed by Microsoft, which
expresses empathy, and Google's Meena, an advanced chatbot.
Microsoft's Bing now incorporates ChatGPT, opening up new
possibilities for chatbot development.</p>
<p>This graph illustrates the relationships among current research areas,
applications, and related companies. Research areas are denoted by
dark blue circles, applications by light blue circles, and companies
by green circles.</p>
<p>In addition, we have previously written about chatbots and now they
are part of history, but still worth reviewing:</p>
</blockquote>
<ul>
<li>Blog post: What Does A Chatbot Look Like Under the Hood?</li>
</ul>
<p><img alt="" src="../media/media/image54.jpg" />{width="4.857147856517935in"
height="3.0453543307086615in"}</p>
<blockquote>
<p>Figure 1.5: Knowlege Graph for Applications. Image source</p>
</blockquote>
<ul>
<li>
<p>Blogpost: What Is Behind the Scene of A Chatbot NLU?</p>
</li>
<li>
<p>Blogpost: What More Can You Do with Chatbots?</p>
</li>
</ul>
<blockquote>
<p>Of course, chatbots are not the only application. There are vast
possibilities in arts and design, music generation, education
technology, coding and beyond your imagination doesn't need to stop
here.</p>
</blockquote>
<h3 id="145-prompt-learning">1.4.5 Prompt Learning</h3>
<p>Prompt learning is a new concept in language models. Instead of
predictingùë• ùë•^‚Ä≤^ ùëÉ(ùë¶|ùë•^‚Ä≤^) ùë¶</p>
<p>given , it aims to find a template that predicts .</p>
<blockquote>
<p><em>Normally, prompt learning will freeze the language model and directly
perform few-shot or zero- shot learning on it. This enables the
language models to be pre-trained on large amount of raw text data and
be adapted to new domains without tuning it again. Hence, prompt
learning could help save much time and efforts.</em></p>
<p>Traditionally, prompt learning involves prompting the model with a
task, and it can be done in two stages: prompt engineering and answer
engineering.</p>
<p><strong>Prompt engineering:</strong> This involves creating prompts, which can be
either discrete (manually designed) or continuous (added to input
embeddings) to convey task-specific information.</p>
<p><strong>Answer engineering:</strong> After reformulating the task, the generated
answer must be mapped to the correct answer space.</p>
<p>Besides single-prompt, <em>multi-prompt methods</em> combine multiple prompts
for better predictions, and <em>prompt augmentation</em> basically beefs up
the prompt to generate better results.</p>
<p>Moreover, in-context learning, a subset of prompt learning, has gained
popularity. It enhances model performance by incorporating a
pre-trained language model and supplying input-label pairs and
task-specific instructions to improve alignment with the task.</p>
<p>Overall, in the dynamic landscape of language models, tooling and
applications, the graph below illustrates to the evolution of language
model engineering. With increasing flexibility along the x-axis and
rising complexity along the yaxis, this graph offers a bird's-eye view
of the choices and challenges faced by developers, researchers and
companies.</p>
<p><img alt="" src="../media/media/image55.jpg" />{width="4.857174103237095in"
height="2.8068908573928257in"}</p>
<p>Figure 1.6: Emerging RAG &amp; Prompt Engineering Architecture for LLMs.
Image</p>
<p>source</p>
<p>In the top-right corner, you can see the complex, yet powerful tools
like OpenAI, Cohere, and Anthropic (to-be-added), which have pushed
the boundaries of what language models can achieve. Along the
diagonal, the evolution of prompt engineering is displayed, from
static prompts to templates, prompt chaining, RAG pipelines,
autonomous agents, and prompt tuning. On the more flexible side,
options like Haystack and LangChain have excelled, presenting broader
horizons for those seeking to harness the versatility of language
models.</p>
<p>This graph serves as a snapshot of the ever-evolving landscape of
toolings in the realm of language model and prompt engineering today,
providing a roadmap for those navigating the exciting possibilities
and complexities of this field. It is likely going to be changing
every day, reflecting the continuous innovation and dynamism in the
space.</p>
<p>In the next Chapter we'll turn our focus to more details of Retrieval
Augmented Generation (RAG) pipelines. We will break down their key
components, architecture, and the key steps involved in building an
efficient retrieval system.</p>
</blockquote>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2024 <a href="https://spanda.io"  target="_blank" rel="noopener">Spanda.io</a>

    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://ranga-rangarajan.github.io/spanda-bootcamp/" target="_blank" rel="noopener" title="ranga-rangarajan.github.io" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.caa56a14.min.js"></script>
      
    
  </body>
</html>