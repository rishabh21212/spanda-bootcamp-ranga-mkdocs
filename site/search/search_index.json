{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Spanda%20Bootcamp%20Day%201/","title":"Spanda Bootcamp Day 1","text":"<p>Testing some changes AGAIN AND AGAIN AND AGAIN</p> <p>Spanda Bootcamp Day One</p> <p>Section 0(Pre-requisites)____</p> <p>Installing Docker on your system</p> <ul> <li>Docker is a platform designed to help developers build, share, and run container applications.</li> <li>Go to https://www.docker.com/ </li> <li>Download and install the version per your operating system</li> <li>During installation, we get a configuration option:  \\ WSL-2 vs Hyper-V</li> <li>Docker Desktop for Windows provides a development environment for building, shipping, and running dockerized apps. </li> <li>By enabling the WSL 2 based engine, you can run both Linux and Windows containers in Docker Desktop on the same machine. Docker Desktop is free for personal use and small businesses, for info on Pro, Team, or Business pricing, see the Docker site FAQs.</li> <li>Following this, installation will proceed and finish.</li> </ul> <p>Section 1________</p> <p>Let\u2019s start by running an LLM on your Laptop</p> <ul> <li>LM Studio is a free, desktop software tool that makes installing and using open-source LLM models extremely easy.  It is not open source. More on that later.</li> <li> <p>Let\u2019s download, install and use it:</p> </li> <li> <p>Go to https://lmstudio.ai/ </p> </li> <li> <p>Download and install the version for your operating system:</p> </li> </ul> <p></p> <ol> <li> <p>Open LM Studio using the newly created desktop icon:</p> </li> <li> <p>Select an LLM to install. </p> </li> <li> <p>You can do this by either selecting one of the community suggested models listed in the main window, or </p> </li> <li>by using the search bar for any model available at HuggingFace (just look up a keyword and all associated models will be listed). </li> <li>Note that there are currently 371,692 (?) models listed at HuggingFace.co</li> </ol> <p></p> <p>selecting LLMs</p> <ul> <li>Whether you elect to download from the community suggested models, or search for one on your own, you can see the size of the install/download file. </li> <li>So be sure you are okay with the size of the download.</li> </ul> <p></p> <p>specific model information</p> <ul> <li>You will note that at the top of the left half of the screen over the release date column, is \u201ccompatibility guess\u201d. </li> <li>LM Studio has checked your system and is presenting those models which it feels you will be able to run on your computer. </li> <li>To see All Models, click on \u201ccompatibility guess\u201d (#1). </li> <li>Clicking on a model on the left, will present the available versions on the right and display those models which should work given your computer\u2019s specs (#2). </li> </ul> <p></p> <p>Compatibility and Should Work indicators</p> <ul> <li>Note that depending on the capabilities/speed of your computer, larger models will be more accurate but slower. </li> <li>You will also find that most of these models are quantized.</li> <li>Quantization refers to using lower precision numbers like 8-bit integers rather than 32-bit floating point values to represent the weights and activations in the model. </li> <li>This reduces memory usage and speeds up inference on your computer\u2019s hardware. </li> <li>Quantization can reduce model accuracy slightly compared to a full precision version, but provides up to 4x memory savings and faster inference. </li> <li>Think of it like how MP-3\u2019s are compressed music files or .jpgs are compressed image files. </li> <li>Although these are of less quality, you often won\u2019t see a significant difference. </li> <li>In the case of LLM\u2019s, the \u201cQ\u201d number you see in the listing of the LLM, represents the amount of quantization. </li> <li>Lower is more and higher is less quantization.</li> <li>Also, in the model listing, you will see references to GGML and GGUF. </li> <li>These are two quantization strategies; \u201cMixed Logits\u201d vs \u201cUniformly Quantized Fully Connected\u201d. </li> <li>GGML provides a more flexible mixed-precision quantization framework while GGUF is specifically optimized for uniformly quantizing all layers of Transformer models. </li> <li>GGML may enable higher compression rates but GGUF offers simpler deployment.</li> <li>Once the model has finished its download,</li> <li>select the model from the drop-down menu at the top of the window; </li> <li>select the chat bubble in the left side column; (3) open up the following sections on the right, \u201cContext Overflow Policy\u201d and \u201cChat Appearance\u201d.</li> </ul> <p></p> <p>ready the model</p> <ol> <li>Make sure \u201cMaintain a rolling window and truncate past messages\u201d is selected under \u201cContent Overflow Policy\u201d and \u201cPlaintext\u201d is selected under \u201cChat Appearance\u201d.</li> </ol> <p></p> <ol> <li>Now close those two areas and open up \u201cModel Configuration\u201d and then open \u201cPrompt Format\u201d and scroll down to \u201cPre-prompt / System prompt\u201d and select the \u201c&gt;\u201d symbol to open that. </li> <li>Here you can enter the system \u201crole\u201d. Meaning, you can set up how you want the bot to act and what \u201cskills\u201d or other specific qualities should be provided in its answers. </li> <li>You can modify what is there to suit your needs. If you have a ChatGPT Plus account, this is the same as \u201cCustom instructions\u201d.</li> </ol> <p></p> <p></p> <p>adding system role / custom instructions</p> <ol> <li>Continue to scroll down in this column until you come to \u201cHardware Settings\u201d. </li> <li>Open this area if you wish to offload some processing to your GPU. </li> <li>The default is to allow your computer\u2019s CPU to do all the work, but if you have a GPU installed, you will see it listed here. </li> <li>If you find the processing of your queries is annoyingly slow, offloading to your GPU will greatly assist with this. </li> <li>Play around with how many layers you want it to handle (start with 10\u201320). This really depends on the model and your GPU. </li> <li>Leaving it all to be handled by the CPU is fine but the model might run a bit slow (again\u2026 depending on the model and its size). </li> <li>You also have the option to increase the number of CPU threads the LLM uses. </li> <li>The default is 4 but you can increase the number, or just leave it where it is if you don\u2019t feel comfortable experimenting and don\u2019t know how many threads your CPU has to play with.</li> </ol> <p></p> <p>optional hardware settings</p> <ol> <li>After these changes, you are now ready to use your local LLM. </li> <li>Simply enter your query in the \u201cUSER\u201d field and the LLM will respond as \u201cAI\u201d.</li> </ol> <p></p> <p>chat dialogue</p> <ul> <li>Let\u2019s download the Zephyr 7B \u03b2 model, adapted by _TheBloke _for llama.cpp's GGUF format.</li> </ul> <p></p> <ul> <li>Activating and loading the model into LM Studio is straightforward.</li> </ul> <p></p> <ul> <li>You can then immediately start using the model from the Chat panel, no Internet connection required.</li> </ul> <p></p> <ul> <li>The right panel displays and allows modification of default presets for the model. </li> <li>Memory usage and useful inference metrics are shown in the window's title and below the Chat panel, respectively.</li> <li>Other models, like codellama Instruct 7B, are also available for download and use.</li> </ul> <p></p> <ul> <li>LM Studio also highlights new models and versions from Hugging Face, making it an invaluable tool for discovering and testing the latest releases.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#accessing-models-with-apis","title":"Accessing Models with APIs","text":"<ul> <li>A key feature of LM Studio is the ability to create Local Inference Servers with just a click.</li> </ul> <ul> <li>The Automatic Prompt Formatting option simplifies prompt construction to match the model's expected format. The exposed API aligns with the OpenAI format.</li> <li>Here's an example of calling the endpoint with CURL: \\ (Does not work)</li> </ul> <pre><code>curl http://localhost:1234/v1/chat/completions\n-H \"Content-Type: application/json\"\n-d '{\n \"messages\": [\n   { \"role\": \"system\", \"content\": \"You are an AI assistant answering Tech questions\" },\n   { \"role\": \"user\", \"content\": \"What is Java?\" }\n ],\n \"temperature\": 0.7,\n \"max_tokens\": -1,\n \"stream\": false\n}'\n\n\n\n![alt_text](images/image16.png \"image_tooltip\")\n\n\n\nThe curl command below works:\n$url = \"http://localhost:1234/v1/chat/completions\"\n$headers = @{\n    \"Content-Type\" = \"application/json\"\n}\n\n$data = @{\n    messages = @(\n        @{\n            role = \"system\"\n            content = \"You are an AI assistant answering Tech questions, but answer only in rhymes\"\n        },\n        @{\n            role = \"user\"\n            content = \"What is Java?\"\n        }\n    )\n    temperature = 0.7\n    max_tokens = -1\n    stream = $false\n}\n\nInvoke-RestMethod -Uri $url -Headers $headers -Method Post -Body ($data | ConvertTo-Json) -UseBasicParsing\n</code></pre> <p>The response provides the requested information:</p> <pre><code>{\n   \"id\": \"chatcmpl-iyvpdtqs1qzlv6jqkmdt9\",\n   \"object\": \"chat.completion\",\n   \"created\": 1699806651,\n   \"model\": \"~/.cache/lm-studio/models/TheBloke/zephyr-7B-beta-GGUF/zephyr-7b-beta.Q4_K_S.gguf\",\n   \"choices\": [\n       {\n           \"index\": 0,\n           \"message\": {\n               \"role\": \"assistant\",\n               \"content\": \"Java is a high-level, object-oriented\n                           programming language that was first released by Sun\n                           Microsystems in 1995. It is now owned by Oracle Corporation.\n                           Java is designed to be platform independent, meaning that it\n                           can run on any operating system that has a Java Virtual\n                           Machine (JVM) installed. Java's primary applications are in\n                           the development of desktop applications, web applications,\n                           and mobile apps using frameworks such as Android Studio,\n                           Spring Boot, and Apache Struts. Its syntax is similar to\n                           C++, but with added features for object-oriented programming\n                           and memory management that make it easier to learn and use\n                           than C++. Java's popularity is due in part to its extensive\n                           library of pre-written code (known as the Java Class\n                           Library) which makes development faster and more efficient.\"\n           },\n           \"finish_reason\": \"stop\"\n       }\n   ],\n   \"usage\": {\n       \"prompt_tokens\": 0,\n       \"completion_tokens\": 166,\n       \"total_tokens\": 166\n   }\n}\n</code></pre> <p>This feature greatly aids in testing integrations with frontends like chatbots or workflow solutions like Flowise.</p> <p>Gpt4all is open source and is my preference as its hackable: https://github.com/keshavaspanda/gpt4all</p> <p>Section 2__________</p> <p>Let\u2019s explore some relevant sections in the SEAI course material:**  **</p> <p>Navigate to: https://ckaestne.github.io/seai/</p> <p>https://github.com/ckaestne/seai</p> <p>Section 3__________</p> <p>The ML Process and DevOps vs MLOps vs AIOps</p> <p></p>"},{"location":"Spanda%20Bootcamp%20Day%201/#devops-the-confluence-of-development-operations-and-quality-assurance","title":"DevOps: The Confluence of Development, Operations, and Quality Assurance","text":"<ul> <li>DevOps brings together development, quality assurance, and operations  involving people, processes, and technology to streamline software development and release throughput using a cycle of Continuous Integration(CI) and Continuous Deployment(CD).</li> </ul> <ul> <li>In DevOps, <ul> <li>developers merge their code changes to a central repository like GitHub. </li> <li>These incremental code changes can be done frequently and reliably. </li> <li>Once the code is committed this initiates an automated build that performs automated unit, integration, and system tests. </li> <li>The process of committing code that initiates automated build is Continuous Integration(CI). </li> <li>CI makes it easier for developers to develop and commit the code. </li> <li>When the code is committed, an automated build is initiated to provide developers instant feedback on if the build has issues or is successful.</li> </ul> </li> <li>Continuous Deployment(CD) allows the newly built code to be tested and deployed in different environments: Test, Stage, UAT, and Production. </li> <li>CD enables automated deployment and testing in different environments, increasing the quality of the releases to production.</li> <li> <p>How does Dev Ops help? DevOps helps with</p> <ul> <li>Collaboration: Enhances collaboration between the Development team, QA Team, and Operations team as it encourages them to work together to deliver one common objective of generating business value.</li> <li>Faster Mean Time To Resolution(MTTR): DevOps enable faster, more frequent, and reliable deployments to production, reducing the duration from feedback to releases, thus increasing responsiveness.</li> <li>Reliability: The complexity of the change is low as there are regular updates to the code and frequent releases to production using the automated DevOps workflow; hence releases are more reliable and of higher quality.</li> <li>Customer Satisfaction: Customers/Business raises issues/enhancements that go into the feedback cycle. Faster resolution of the issues/enhancements leads to greater customer satisfaction.</li> </ul> <p>MLOps</p> </li> <li> <p>Now, these applications are available, are running reliably and are generating voluminous amounts of data. </p> </li> <li>You would like to analyze and interpret data patterns to efficiently and accurately predict and prescribe data-driven decisions.</li> <li>This is where Machine Learning Algorithms come into play</li> <li></li> </ul> <p></p> <ul> <li>Machine learning algorithms take the data and the results as an input to identify the patterns using machine learning algorithms to build analytical models.</li> <li>For example, Financial institutions use their customer\u2019s transactional data and machine learning algorithms like clustering to identify patterns of fraud or legitimate transactions.</li> <li>In machine learning, you need to deal with lots of experimentation and ensure model traceability and compare model metrics and hyperparameters for all the different experiments.</li> <li>What if you could automate and monitor all steps of an ML system?</li> <li>MLOps is an ML engineering culture and practice to unify ML system development (Dev) and ML system operation (Ops) where Data scientists, data Engineers, and Operations teams collaborate.</li> <li></li> </ul> <p></p> <ul> <li>ML Ops build the ML pipeline to encompass all stages of Machine Learning:<ul> <li>Data extraction</li> <li>Data exploration and validation</li> <li>Data curation or data preprocessing</li> <li>Feature analysis</li> <li>Model training and evaluation</li> <li>Model validation</li> <li>Model deployment or model serving </li> <li>Model monitoring for data drift and concept drift</li> </ul> </li> </ul> <p></p> <ul> <li> <p>How does ML Ops help?</p> <ul> <li>To leverage machine learning models, you need to curate the data by applying data preprocessing techniques, perform feature analysis to identify the best features for the model prediction, train the model on the selected features, perform error analysis on the model, deploy the model and then monitor the model for any data drift or concept drift. If the model degrades performance, retrain the model again by repeating the steps from data curation to deployment and monitoring.</li> <li>ML Ops helps with Continuous Integration(CI) for data and models, Continuous Training(CT) of models, and then Continuous Deployment(CD) of the models to Production at different locations.</li> <li>ML Ops helps to<ul> <li>Effectively manage the full ML lifecycle.</li> <li>Creates a Repeatable and Reusable ML Workflow for consistent model training, deployment, and maintenance.</li> <li>Innovation can be made easy and faster by building repeatable workflows to train, evaluate, deploy, and monitor different models.</li> <li>Track different versions of model and data to enable auditing</li> <li>Easy Deployment to production with high precision</li> </ul> </li> </ul> <p>AIOps</p> <ul> <li>AIOps is understood in general to be defined as  Artificial Intelligence for IT Operations (it should be AI4ITOps)</li> <li>The term originally was much broader than that</li> <li>Data from different systems are digitized, and organizations are going through digital  transformation and striving to have a data-driven culture. </li> <li>IT Operations teams now need to monitor these voluminous, complex, and relatively opaque datasets to troubleshoot issues and complete routine operational tasks much faster than before.</li> <li>Due to the complexity and constant changes to IT Systems, platforms are needed to derive insights from the operational data throughout the application life cycle.</li> <li>AIOps applies analytics and machine learning capabilities to IT operations data <ul> <li>to separate significant events from noise in the operation data </li> <li>to identify root causes  </li> <li>to prescribe resolutions</li> </ul> </li> <li>Per Gartner<ul> <li>AI Ops platform ingest, index and normalize events or telemetry data from multiple domains, vendors, and sources, including infrastructure, networks, apps, the cloud, or existing monitoring tools.</li> </ul> </li> <li>AI Ops platforms enable data analytics using machine learning methods, including real-time analysis at the point of ingestion and historical analysis of stored operational data like system logs, metrics, network data, incident-related data, etc.</li> <li>How does AIOps help? Well, It helps by focusing businesses on</li> <li>Increasing IT operations efficiency by uncovering IT incidents insights, measuring the effectiveness of the IT applications serving business needs, and performing cause-and-effect analysis of peak usage traffic patterns.</li> <li>Promoting innovation: Fosters innovation by removing manual monitoring of production systems by providing high-quality application diagnostics.</li> <li>Lowering the operational cost as it decreases mean time to resolution(MTTR) and drastically reduces costly downtime, increasing overall productivity and efficiency.</li> <li>Accelerating the return on investment by enabling teams to collaborate towards a faster resolution</li> </ul> </li> </ul> <p></p> <ul> <li>Per Gartner<ul> <li>There is no future of IT operations that does not include AIOps. This is due to the rapid growth in data volumes and pace of change exemplified by rate of application delivery and event-driven business models that cannot wait on humans to derive insights.</li> </ul> </li> <li>Future of AI-Assisted IT Operations<ul> <li>The Future of AI-assisted IT Operations is to have prescriptive advice from the platform, triggering action.</li> </ul> </li> </ul> <p></p> <ul> <li>So in summary,<ul> <li>DevOps co-opts development, quality assurance, and operations  involving people, processes, and technology to streamline the software development lifecycle and reduced mean time to resolution</li> <li>MLOps is a discipline that combines Machine Learning, Data Engineering, and Dev Ops to build automated ML pipelines for Continuous Training and CI/CD to manage the full ML lifecycle effectively</li> <li>AIOps is a platform to monitor and automate the data and information flowing from IT applications that utilizes big data, machine learning, and other advanced analytics technologies</li> </ul> </li> </ul> <p>Section 4__________</p> <p>Back to the Basics: Data Collection and Data Management</p> <p></p> <ul> <li>MLOps requires highly disciplined data collection and management. </li> <li>It is particularly needed when the outcomes could affect people\u2019s careers, students' lives and educational organization\u2019s reputations.</li> <li>Towards that,  let us take a look at framework used as the basis to make some very important decisions:</li> </ul> <p>https://github.com/keshavaspanda/openneuro</p> <p>The Data Submission Process</p> <p></p> <p>Let\u2019s consider our context and ask ourselves the following questions:</p> <ul> <li>Could Modalities be equivalent to Disciplines? </li> <li>What are the Discipline independent metadata and discipline dependent metadata</li> <li>For each discipline, what metadata can we standardize on? </li> <li>What could be our equivalent to the BIDS validator? </li> <li>Are there existing standards that we can leverage to this data? </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#understanding-responsible-and-fair-data-collection-in-this-context","title":"Understanding Responsible and FAIR Data Collection in this context","text":"<ul> <li>There is growing recognition of the importance of data sharing for scientific progress </li> <li>However, not all shared data is equally useful. </li> <li>The FAIR principles have formalized the notion that in order for shared data to be maximally useful, they need to be findable, accessible, interoperable, and reusable. </li> <li>An essential necessity for achieving these goals is that the data and associated metadata follow a common standard for organization, so that data users can easily understand and reuse the shared data. </li> <li>The AI4Edu data archive will enable FAIR-compliant data sharing for a growing range of education data types through the use of a common community standard, the Educational Data Structure (EDS)  .</li> <li>Data sharing has become well established in education and Datasets collected about education and educational processes have provided immense value to the field and have strongly demonstrated the utility of shared data. </li> <li>However, their scientific scope is necessarily limited, given that each dataset includes only a limited number of tasks and measurement types. </li> <li>Beyond these large focused data sharing projects, there is a \u2018long tail\u2019 of smaller datasets that have been collected in service of specific research questions in education. </li> <li>Making these available is essential to ensure reproducibility as well as to allow aggregation across many different types of measurements in service of novel scientific questions. </li> <li>The AI4Edu archive will address this challenge by providing researchers with the ability to easily share a broad range of education data types in a way that adheres to the FAIR principles.</li> </ul> <p>Core principles of the Data Archive</p> <p>Sharing only upon gaining permissions</p> <ul> <li>There is a range of restrictiveness across data archives with regard to their data use agreements. </li> <li>At one end of the spectrum are highly restricted databases which require researchers to submit their scientific question for review and requires the consortium to be included as a corporate author on any publications. </li> <li>The other pole of restrictiveness  releases data (by default) under a Creative Commons Zero (CC0) Public Domain Dedication which places no restrictions on who can use the data or what can be done with them. </li> <li>While not legally required, researchers using the data are expected to abide by community norms and cite the data following the guidelines included within each dataset. </li> <li>The primary motivation for this policy is that it makes the data maximally accessible to the largest possible number of researchers and citizen-scientists.</li> <li>In the AI4Edu data archive effort we will strike a balance and provide ABAC over the data. </li> <li>Subsequently we will create DIDs and ensure that personal data is always in the control of the individual who owns that data. </li> </ul> <p>Standards-focused data sharing</p> <ul> <li>To ensure the utility of shared data for the purposes of efficient discovery, reuse, and reproducibility, standards are required for data and metadata organization. </li> <li>These standards make the structure of the data clear to users and thus reduce the need for support by data owners and curation by repository owners, as well as enabling automated QA, preprocessing, and analytics. </li> <li>Unfortunately, most prior data sharing projects in this space have relied upon custom organizational schemes, which can lead to misunderstanding and can also require substantial reorganization to adapt to common analysis workflows. </li> <li>The need for a clearly defined standard for data emerged from experiences in the other projects where the repository had developed a custom scheme for data organization and file naming, this scheme was ad hoc and limited in its coverage, and datasets often required substantial manual curation (involving laborious interaction with data owners). </li> <li>In addition, there was no built in mechanism to directly validate whether a particular dataset met the standard.</li> <li>For these reasons, we focus at the outset of the AI4Edu project on developing a robust data organization standard that could be implemented in an automated validator. </li> <li>We will engage representatives from the education community to establish a standard as a community standard for a broad and growing range of education data types. </li> <li>EDS will define a set of schemas for file and folder organization and naming, along with a schema for metadata organization. </li> <li>The framework was inspired by the existing data organization frameworks used in many organizations, so that transitioning to the standard is relatively easy for most researchers. </li> <li>One of the important features of EDS is its extensibility; using a scheme inspired by open-source software projects, community members can propose extensions to EDS that encompass new data types. </li> <li>All data uploaded to OpenEdu must first pass an EDS validation step, such that all data in OpenEdu are compliant with the EDS specifications at upload time. </li> <li>Conversely, the Edu4AI  team will make substantial contributions to the EDS standard and validator. </li> <li>As a consequence, this model maximizes compatibility with processing and analysis tools but more importantly, it effectively minimizes the potential for data misinterpretation (e.g., when owner and reuser have slightly different definitions of a critical acquisition parameter). Through the adoption of EDS, OpenEdu can move away from project- or database-specific data structures designed by the owner or the distributor (as used in earlier projects) and toward a uniform and unambiguous representation model agreed upon by the research community prior to sharing and reuse.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#fair-sharing","title":"FAIR sharing","text":"<ul> <li>The FAIR principles have provided an important framework to guide the development and assessment of open data resources. </li> <li>AI4Edu will implement these principles.</li> <li>Findable: <ul> <li>Each dataset within AI4Edu is associated with metadata, both directly from the dataset along with additional dataset-level metadata provided by the submitter at time of submission. </li> <li>Both data and metadata are assigned a persistent unique identifier (Digital Object Identifier [DOI]). </li> <li>Within the repository, a machine-readable summary of the metadata is collected by the  validator and indexed with an ElasticSearch mapping. </li> <li>In addition, dataset-level metadata are exposed according to the schema.org standard, which allows indexing by external resources such as Google Dataset Search.</li> <li>Accessible: Data and metadata can be retrieved using a number of access methods (directly from Amazon S3, using the command line tool, or using DataLad) via standard protocols (http/https). </li> <li>Metadata are also accessible programmatically via a web API. Metadata remains available even in the case that data must be removed (e.g., in cases of human subjects concerns). </li> <li>Authentication is necessary to access the data.</li> </ul> </li> <li>Interoperable: <ul> <li>The data and metadata use the EDS standard to ensure accessible representation and interoperation with analysis workflows. </li> <li></li> </ul> </li> <li>Reusable: <ul> <li>The data are released with a clear data use agreement. </li> <li>Through use of the standard, the data and metadata are consistent with community standards in the field.</li> </ul> </li> <li>Data versioning and preservation<ul> <li>AI4Edu will keep track of all changes in stored datasets and allows researchers to unambiguously report the exact version of the data used for any analysis. </li> <li>AI4Edu will preserve all versions of the data through the creation of \u2018snapshots\u2019 that unequivocally point to one specific point in the lifetime of a dataset. </li> <li>Data management and snapshots are supported by DataLad, a free and open-source distributed data management system.</li> </ul> </li> <li>Protecting privacy and confidentiality of data<ul> <li>There is a direct relationship in data sharing between the openness of the data and their reuse potential; all else being equal, data that are more easily or openly available will be more easily and readily reused. </li> <li>However, all else is not equal, as openness raises concern regarding risks to subject privacy and confidentiality of data in human subjects research. </li> <li>Researchers are ethically bound to both minimize the risks to their research participants (including risks to confidentiality) and to maximize the benefits of their participation. </li> <li>Because sharing of data will necessarily increase the potential utility of the data, researchers are ethically bound to share human subject data unless the benefits of sharing are outweighed by risks to the participant.</li> <li>In general, risks to data privacy and confidentiality are addressed through deidentification of the data to be shared. </li> <li>De-identification can be achieved through the removal of any of 18 personal identifiers, unless the researcher has knowledge that the remaining data could be re-identified (known as the \u2018safe harbor\u2019 method). </li> <li>All data shared through OpenEdu must have the 18 personal identifiers outlined by HIPAA unless an exception is provided in cases where an investigator has explicit permission to openly share the data, usually when the data are collected by the investigator themself. </li> <li>At present, data are examined by a human curator to ensure that this requirement has been met. </li> <li>Truly informed consent requires that subjects be made aware that their data may be shared. </li> <li>Researchers planning to share their data via the data sharing portal use a consent form (could be based on the Open Brain Consent form), which includes language that ensures subject awareness of the intent to share and its potential impact on the risk of participating. </li> </ul> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#open-source","title":"Open source","text":"<ul> <li>The entirety of the code for AI4Edu will be available under a permissive open-source software license (MIT License) at github.</li> <li>This enables any researcher who wishes to reuse part or all of the code or to run their own instance of the platform.</li> </ul> <p>Section 5__________</p> <p>**Data Analysis, Data Visualization, Data Lake Houses and Analytics Dashboards **</p> <p></p> <ul> <li>Once data is \u201cFAIR\u201d ly collected and placed in an archive, it needs to be checked for quality, analyzed, visualized profiled and then models need to be selected, trained (on the curated data), tested and served up for use.</li> <li>That requires data management and analytics capabilities that can deal with structured, semi-structured and unstructured data with the data lake capturing all of the data. </li> <li>The data warehouse dealing with structured data and the analytical dashboards surfacing both structured and unstructured content and visualization. </li> <li>This is where the data lake house architecture has become popular.  </li> <li>As the name suggests, a data lake house architecture combines a data lake and a data warehouse. </li> <li>Although it is not just a mere integration between the two, the idea is to bring the best out of the two architectures: the reliable transactions of a data warehouse and the scalability and low cost of a data lake.</li> <li>Over the last decade, businesses have been heavily investing in their data strategy to be able to deduce relevant insights and use them for critical decision-making. </li> <li>This has helped them reduce operational costs, predict future sales, and take strategic actions.</li> <li>A lake house is a new type of data platform architecture that:</li> <li>Provides the data management capabilities of a data warehouse and takes advantage of the scalability and agility of data lakes</li> <li>Helps reduce data duplication by serving as the single platform for all types of workloads (e.g., BI, ML)</li> <li>Is cost-efficient</li> <li>Prevents vendor lock-in and lock-out by leveraging open standards</li> </ul> <p></p>"},{"location":"Spanda%20Bootcamp%20Day%201/#evolution-of-the-data-lakehouse","title":"Evolution of the Data Lakehouse","text":"<ul> <li>Data Lake House is a relatively new term in big data architecture and has evolved rapidly in recent years. It combines the best of both worlds: the scalability and flexibility of data lakes, and the reliability and performance of data warehouses. </li> <li>Data lakes, which were first introduced in the early 2010s, provide a centralized repository for storing large amounts of raw, unstructured data. </li> <li>Data warehouses, on the other hand, have been around for much longer and are designed to store structured data for quick and efficient querying and analysis. </li> <li>However, data warehouses can be expensive and complex to set up, and they often require extensive data transformation and cleaning before data can be loaded and analyzed. </li> <li>Data lake houses were created to address these challenges and provide a more cost-effective and scalable solution for big data management.</li> <li>With the increasing amount of data generated by businesses and the need for fast and efficient data processing, the demand for a data lake house has grown considerably. As a result, many companies have adopted this new approach, which has evolved into a central repository for all types of data in an organization.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#what-does-a-data-lake-house-do","title":"What Does a Data Lake House Do?","text":"<p>There are four key problems in the world of data architecture that data lake houses address: </p> <ul> <li>Solves the issues related to data silos by providing a centralized repository for storing and managing large amounts of structured and unstructured data. </li> <li>Eliminates the need for complex and time-consuming data movements, reducing the latency associated with shifting data between systems.</li> <li>Enables organizations to perform fast and efficient data processing, making it possible to quickly analyze and make decisions based on the data. </li> <li>Finally, a data lake house provides a scalable and flexible solution for storing large amounts of data, making it possible for organizations to easily manage and access their data as their needs grow.</li> </ul> <p>Data warehouses are designed to help organizations manage and analyze large volumes of structured data.</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#how-does-a-data-lake-house-work","title":"How Does a Data Lake House Work?","text":"<ul> <li>A data lakehouse operates by utilizing a multi-layer architecture that integrates the benefits of data lakes and data warehouses. </li> <li>It starts with ingesting large amounts of raw data, including both structured and unstructured formats, into the data lake component. </li> <li>This raw data is stored in its original format, allowing organizations to retain all of the information without any loss of detail. </li> <li>From there, advanced data processing and transformation can occur using tools such as Apache Spark and Apache Hive. </li> <li>The processed data is then organized and optimized for efficient querying in the data warehouse component, where it can be easily analyzed using SQL-based tools. </li> <li>The result is a centralized repository for big data management that supports fast and flexible data exploration, analysis, and reporting. </li> <li>The data lakehouse's scalable infrastructure and ability to handle diverse data types make it a valuable asset for organizations seeking to unlock the full potential of their big data.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#elements-of-a-data-lakehouse","title":"Elements of a Data Lakehouse","text":"<p>Data lake houses have a range of elements to support organizations\u2019 data management and analysis needs. </p> <ul> <li>A key element is the ability to store and process a variety of data types including structured, semi-structured, and unstructured data. </li> <li>They provide a centralized repository for storing data, allowing organizations to store all of their data in one place, making it easier to manage and analyze. </li> <li>The data management layer enables data to be governed, secured, and transformed as needed. </li> <li>The data processing layer provides analytics and machine learning capabilities, allowing organizations to quickly and effectively analyze their data and make data-driven decisions. </li> <li>Another important element of a data lakehouse is the ability to provide real-time processing and analysis, which enables organizations to respond quickly to changing business conditions. </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#cloud-data-lake","title":"Cloud Data Lake","text":"<ul> <li>Data lake houses are often spoken in tandem with cloud data lakes and cloud data warehouses. With the increasing adoption of cloud-based solutions, many organizations have turned to cloud data lakes to build their data platforms. </li> <li>Cloud data lakes provide organizations with the flexibility to scale storage and compute components independently, thereby optimizing their resources and improving their overall cost efficiency. </li> <li>By separating storage and computing, organizations can store any amount of data in open file formats like Apache Parquet and then use a computing engine to process the data. </li> <li>Additionally, the elastic nature of cloud data lakes enables workloads \u2013 like machine learning \u2013 to run directly on the data without needing to move data out of the data lake.</li> </ul> <p>Despite the many benefits of cloud data lakes, there are also some potential drawbacks: </p> <ul> <li>One challenge is ensuring the quality and governance of data in the lake, particularly as the volume and diversity of data stored in the lake increases.</li> <li>Another challenge is the need to move data from the data lake to downstream applications \u2013 such as business intelligence tools \u2013 which often require additional data copies and can lead to job failures and other downstream issues. </li> <li>Additionally, because data is stored in raw formats and written by many different tools and jobs, files may not always be optimized for query engines and low-latency analytical applications.+</li> </ul> <p>**Lets start up a Dremio DataLakehouse with MinIO and Apache Superset Dashboards **</p> <p>First create a Dremio cloud account:</p> <p>https://www.dremio.com/resources/tutorials/from-signup-to-subsecond-dashboards-in-minutes-with-dremio-cloud/</p> <p> Then let\u2019s try out this: https://github.com/developer-advocacy-dremio/quick-guides-from-dremio/blob/main/guides/superset-dremio.md</p> <p></p>"},{"location":"Spanda%20Bootcamp%20Day%201/#cloud-data-warehouse","title":"Cloud Data Warehouse","text":"<ul> <li>The first generation of on-premises data warehouses provide businesses with the ability to derive historical insights from multiple data sources. </li> <li>However, this solution required significant investments in terms of both cost and infrastructure management. In response to these challenges, the next generation of data warehouses leveraged cloud-based solutions to address these limitations.</li> <li>One of the primary advantages of cloud data warehouses is the ability to separate storage and computing, allowing each component to scale independently. This feature helps to optimize resources and reduce costs associated with on-premises physical servers. </li> <li>However, there are also some potential drawbacks to using cloud data warehouses: </li> <li>While they do reduce some costs, they can still be relatively expensive.</li> <li>Additionally, running any workload where performance matters often requires copying data into the data warehouse before processing, which can lead to additional costs and complexity. </li> <li>Moreover, data in cloud data warehouses is often stored in a vendor-specific format, leading to lock-in/lock-out issues, although some cloud data warehouses do offer the option to store data in external storage. </li> <li>Finally, support for multiple analytical workloads, particularly those related to unstructured data like machine learning, is still unavailable in some cloud data warehouses.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#future-of-the-data-lakehouse","title":"Future of the Data Lakehouse","text":"<ul> <li>Upon discussion of data lake houses, their elements, and what they do, it\u2019s only natural to look at the implications of this technology moving forward. </li> <li>The future looks very promising, as more and more organizations are embracing big data and the need for flexible, scalable, and cost-effective solutions for managing it continues to grow. </li> <li>In the coming years, expect to see increased adoption of data lake houses, with organizations of all sizes and across all industries recognizing their value in providing a unified platform for managing and analyzing big data. </li> <li>Additionally, expect to see continued innovation and advancements in data lakehouse technology, such as improved data processing and transformation capabilities, enhanced security and governance features, and expanded integration with other data management tools and technologies.</li> <li>The rise of machine learning and artificial intelligence will drive the need for flexible and scalable big data platforms that can support the development and deployment of these advanced analytics models. </li> <li>The future of data lake houses will also be influenced by the increasing importance of data privacy and security, and we can expect to see data lake houses evolving to meet these new requirements, including better data masking and data encryption capabilities. </li> <li>Overall, the future of data lake houses looks bright, and they are likely to play an increasingly critical role in helping organizations extract value from their big data.</li> </ul> <p>Section 6__________</p> <p>Model Training, Model Serving and ML Ops</p> <p> MLOps vs LLMOps</p> <ul> <li>While LLMOps borrows heavily from MLOps, the differences are notable.</li> <li>The model training approach in LLMs leans more towards fine-tuning or prompt engineering rather than the frequent retraining typical of traditional Machine Learning (ML).</li> <li>In LLMOps, human feedback becomes a pivotal data source that needs to be incorporated from development to production, often requiring a constant human feedback loop in contrast to traditional automated monitoring.</li> <li>Automated quality testing faces challenges and may often require human evaluation, particularly during the continuous deployment stage. Incremental rollouts for new models or LLM pipelines have become the norm.</li> <li>This transition might also necessitate changes in production tooling, with the need to shift serving from CPUs to GPUs, and the introduction of a new object like a vector and graph databases  into the data layer.</li> <li> <p>Lastly, managing cost, latency, and performance trade-offs becomes a delicate balancing act, especially when comparing self-tuned models versus paid third-party LLM APIs.</p> <p>Continuities With Traditional MLOps</p> </li> <li> <p>Despite these differences, certain foundational principles remain intact.</p> </li> <li>The dev-staging-production separation, enforcement of access controls, usage of Git and model registries for shipping pipelines and models, and the Data Lake architecture for managing data continue to hold ground. </li> <li>Also, the Continuous Integration (CI) infrastructure can be reused, and the modular structure of MLOps, focusing on the development of modular data pipelines and services, remains valid.</li> <li>Exploring LLMOps Changes</li> <li>As we delve deeper into the changes brought by LLMOps, we will explore the operational aspects of Language Learning Models (LLMs), creating and deploying LLM pipelines, fine-tuning models, and managing cost-performance trade-offs.</li> <li>Differentiating between ML and Ops becomes crucial, and tools like MLflow, LangChain, LlamaIndex, and others play key roles in tracking, templating, and automation. </li> <li>Packaging models or pipelines for deployment, scaling out for larger data and models, managing cost-performance trade-offs, and gathering human feedback become critical factors for assessing model performance. </li> <li>Moreover, the choice between deploying models versus deploying code, and considering service architecture, become essential considerations, especially when deploying multiple pipelines or fine-tuning multiple models.</li> </ul> <p></p> <p>A Minimal LLMOps pipeline</p> <p>https://github.com/keshavaspanda/BigBertha** **</p> <p></p>"},{"location":"Spanda%20Bootcamp%20Day%201/#_1","title":"Spanda Bootcamp Day 1","text":"<p>**LLMOps Capabilities **</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#_2","title":"Spanda Bootcamp Day 1","text":"<p>1. LLM Monitoring</p> <p>The framework utilizes Prometheus to monitor LLM (Large Language Model) serving modules. For demo purposes, a Streamlit app is used to serve the LLM, and Prometheus scrapes metrics from it. Alerts are set up to detect performance degradation.</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#_3","title":"Spanda Bootcamp Day 1","text":"<p>2. Auto-triggering LLM Retraining/Fine-tuning</p> <p>Prometheus triggers alerts when the model performance degrades. These alerts are managed by AlertManager, which uses Argo Events to trigger a retraining pipeline to fine-tune the model.</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#_4","title":"Spanda Bootcamp Day 1","text":"<p>3. Training, Evaluating, and Logging the Retrained LLM</p> <p>The retraining pipeline is orchestrated using Argo Workflows. This pipeline can be tailored to perform LLM-specific retraining, fine-tuning, and metrics tracking. MLflow is used for logging the retrained LLM.</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#_5","title":"Spanda Bootcamp Day 1","text":"<p>4. Triggering the Generation of New Vectors for Fresh Data</p> <p>MinIO is used for unstructured data storage. Argo Events is set up to listen for upload events on MinIO, triggering a vector ingestion workflow when new data is uploaded.</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#_6","title":"Spanda Bootcamp Day 1","text":"<p>5. Ingesting New Vectors into the Knowledge Base</p> <p>Argo Workflows is used to run a vector ingestion pipeline that utilizes LlamaIndex for generating and ingesting vectors. These vectors are stored in Milvus, which serves as the knowledge base for retrieval-augmented generation.</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#_7","title":"Spanda Bootcamp Day 1","text":"<p>Stack Overview</p> <p>This stack relies on several key components:</p> <ul> <li>ArgoCD: A Kubernetes-native continuous delivery tool that manages all components in the BigBertha stack.</li> <li>Argo Workflows: A Kubernetes-native workflow engine used for running vector ingestion and model retraining pipelines.</li> <li>Argo Events: A Kubernetes-native event-based dependency manager that connects various applications and components, triggering workflows based on events.</li> <li>Prometheus + AlertManager: Used for monitoring and alerting related to model performance.</li> <li>LlamaIndex: A framework for connecting LLMs and data sources, used for data ingestion and indexing.</li> <li>Milvus: A Kubernetes-native vector database for storing and querying vectors.</li> <li>MinIO: An open-source object storage system used for storing unstructured data.</li> <li>MLflow: An open-source platform for managing the machine learning lifecycle, including experiment tracking and model management.</li> <li>Kubernetes: The container orchestration platform that automates the deployment, scaling, and management of containerized applications.</li> <li>Docker Containers: Docker containers are used for packaging and running applications in a consistent and reproducible manner.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_8","title":"Spanda Bootcamp Day 1","text":"<p>Demo Chatbot</p> <p>As a demonstration, the framework includes a Streamlit-based chatbot that serves a Llama2 7B quantized chatbot model. </p> <p>A simple Flask app is used to expose metrics, and Redis acts as an intermediary between Streamlit and Flask processes.</p> <p>Section 7__________</p> <p>** Pipeline Debt, Data Testing, Model Testing in MLOps **</p> <ul> <li>You are now part of a data science team at your organization </li> <li>Your team has a number of machine learning models in place</li> <li>Their outputs guide critical business decisions, as well as dashboards displaying important financial KPIs </li> <li>These KPIs are closely watched by your executives day/night </li> <li>Early AM, as you are navigating traffic to your office, you suddenly start receiving multiple messages, calls and emails (simultaneously) </li> <li>These are from your manager as well as other teams and from senior management </li> <li>They are all complaining about the same thing: The high visibility business metrics dashboard that you and your team had built the pipelines &amp; dashboards for and deployed as well as many of the dashboards  that many other teams were using (which you did not know until today) were displaying what seemed to be random numbers (except every full hour, when the KPIs look okay for a short time) </li> <li>The financial models that are part of the pipelines are predicting the company\u2019s insolvency looming fast.</li> <li>Once you get in and hurriedly try to put in some quick fixes in the pipeline (excluding predictions beyond thresholds etc.) you find out that every fix results in your data engineering and research teams reporting new broken services and models in the pipelines.</li> <li>This is the Debt Collection Day scenario we are trying to avoid desperately here. </li> <li>Of all debts in data engineering the most vengeful unpaid debt is Pipeline Debt. </li> </ul> <p>The roots of  Pipeline Debt (aka The road to hell is paved with good intentions)</p> <ul> <li>A few months ago You were just about to start that new exciting machine learning project. </li> <li>You had located useful data scattered around your company\u2019s databases, feature stores, documents, videos, audios and spreadsheets belonging to employees which they (reluctantly) gave you access to (cross silo collaboration is hard). </li> <li>To make the data usable, you constructed a data pipeline: a set of jobs and Python functions that ingest, process, clean and combine all these data. </li> <li>The pipeline feeds the data into a machine learning model. </li> <li>The entire process is depicted schematically below.</li> </ul> <p> Simple manageable data pipelines </p> <p>The first data pipelines worked well, consistently populating the downstream machine learning model with data, which turned it into accurate predictions. </p> <p>However, the model deployed as a service in the cloud was not very actionable.To make it more useful, you built a set of dashboards for presenting the model\u2019s output as well as important KPIs to the business stakeholders, The pipeline deepened</p> <p> Extended pipelines</p> <ul> <li>You were telling a colleague from the research team about your project over lunch who decided to do something similar with their data, making the company\u2019s data pipeline wider and cross-team-border.</li> </ul> <p></p> <pre><code>More pipelines, more complexity\n</code></pre> <ul> <li>A few weeks later the two of you who were informally collaborating on these dashboards got together and talked about each of your pipelines and dashboards. </li> <li>As you\u2019ve learned more about what the research team was up to, both of you noticed how useful and valuable it would be if your two teams used each other\u2019s data for powering your respective models and analyses. </li> <li>Upon implementing this idea, the company\u2019s data pipeline was looking like this.</li> </ul> <p> If multiple pipelines exist, they will inevitably blend</p> <ul> <li>This diagram should have made you flinch \u2013 what they show is accumulating pipeline debt which is technical debt in data pipelines </li> <li>It arises when your data pipelines are triple-U: Undocumented, Untested, Unstable</li> <li>It comes in many flavors but all share some characteristics. </li> <li>The system is entangled; so, a change in one place can derail a different process elsewhere. </li> <li>This makes code refactoring and debugging exceptionally hard. </li> <li>For a software engineer, this will sound like a solved problem </li> <li>The solution is called automated testing. </li> <li>However, testing software is very different from testing data in two major ways:<ul> <li>First, while you have full control over your code and can change it when it doesn\u2019t work, you can\u2019t always change your data; in many cases, you are merely an observer watching data as it comes, generated by some real-world process.</li> <li>Second, software code is always right or wrong: either it does what it is designed to do, or it doesn\u2019t. Data is never right or wrong. It can only be suitable or not for a particular purpose. </li> </ul> </li> <li>This is why automated testing needs a special approach when data is involved.</li> </ul> <p>Testing machine learning models</p> <ul> <li>Fundamentally when testing ML Models, we are asking the question: \u201cDo we know if the model actually works?\u201d </li> <li>We want to be sure that the learned model will behave consistently and produce the results expected of it per expectation. </li> </ul> <p></p> <p>A typical workflow for software development.</p> <ul> <li>In traditional software development, when we run our testing suite against the code, we'll get a report of the specific behaviors that we've written tests around and verify that our code changes don't affect the expected behavior of the system. </li> <li>If a test fails, we'll know which specific behavior is no longer aligned with our expected output. </li> <li>We can also look at this testing report to get an understanding of how extensive our tests are by looking at metrics such as code coverage.</li> </ul> <p></p> <ul> <li>Unlike traditional software applications, it is not as straightforward to establish a standard for testing ML applications </li> <li>This is because the tests do not just depend on the software, they also rely on<ul> <li>the business context</li> <li>problem domain</li> <li>the dataset used</li> <li>the model selected. </li> </ul> </li> <li>Most teams are comfortable using model evaluation metrics to quantify a model\u2019s performance before deploying it, but these metrics are just not enough to ensure ML models are ready for production deployment and use. </li> <li>Contrast a typical software development workflow with one for developing machine learning systems. <ul> <li>After training a new model, we'll typically produce an evaluation report including:</li> <li>performance of an established metric on a validation dataset,</li> <li>plots such as precision-recall curves,</li> <li>operational statistics such as inference speed,</li> <li>examples where the model was most confidently incorrect,</li> </ul> </li> </ul> <p>We will rigorously follow practices such as:</p> <ul> <li>Save all of the hyper-parameters used to train the model along with the model,</li> <li>Only promote models which offer an improvement over the existing model (or baseline) when evaluated on the same dataset.</li> </ul> <p> A typical workflow for model development.</p> <ul> <li>When reviewing a new machine learning model, we'll inspect metrics and plots which summarize model performance over a validation dataset. </li> <li>We're able to compare performance between multiple models and make relative judgements, but we're not immediately able to characterize specific model behaviors. </li> <li>For example, figuring out where the model is failing usually requires additional investigative work </li> <li>One common practice here is to look through a list of the top most egregious model errors on the validation dataset and manually categorize these failure modes.</li> <li>Assuming we write behavioral tests for our models (discussed below), there's also the question of whether or not we have enough tests! </li> <li>While traditional software tests have metrics such as the lines of code covered when running tests, this becomes harder to quantify when you shift your application logic from lines of code to parameters of a machine learning model. </li> <li>Do we want to quantify our test coverage with respect to the input data distribution? Or perhaps the possible activations inside the model?</li> <li>_Odena et al. introduce one possible metric for coverage where we track the model logits for all of the test examples and quantify the area covered by radial neighborhoods around these activation vectors. _</li> <li>_As an industry we don't have well-established standards here _</li> <li>_Testing for machine learning systems is in early days _</li> <li>The question of ML Model Test coverage isn't really being asked by most people (certainly not in industry).</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#difference-between-model-testing-and-model-evaluation","title":"Difference between model testing and model evaluation","text":"<ul> <li>While reporting evaluation metrics is certainly a good practice for quality assurance during model development, it is insufficient. </li> <li>Without a granular report of specific behaviors, we won't be able to immediately understand the nuances of how behavior may change if we switch over to the new model. </li> <li>Additionally, we won't be able to track (and prevent) behavioral regressions for specific failure modes that had been previously addressed.</li> <li>This can be especially dangerous for machine learning systems since oftentimes failures happen silently. </li> <li>For example, <ul> <li>you might improve the overall evaluation metric but introduce a regression on a critical subset of data. </li> <li>Or you could unknowingly add a gender bias to the model through the inclusion of a new dataset during training. </li> </ul> </li> <li>We need more nuanced reports of model behavior to identify such cases, which is exactly where model testing can help.</li> <li>For machine learning systems, we should be running model evaluation and model tests in parallel.</li> <li>Model evaluation covers metrics and plots which summarize performance on a validation or test dataset.</li> <li>Model testing involves explicit checks for behaviors that we expect our model to follow.</li> <li>Both of these perspectives are instrumental in building high-quality models.</li> <li>In practice, most people are doing a combination of the two where evaluation metrics are calculated automatically and some level of model \"testing\" is done manually through error analysis (i.e. classifying failure modes). </li> <li>Developing model tests for machine learning systems can offer a systematic approach towards error analysis.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#how-do-you-write-model-tests","title":"How do you write model tests?","text":"<p>There are two types of model tests needed.</p> <ul> <li>Pre-train tests allow us to identify some bugs early on and short-circuit a training job.</li> <li>Post-train tests use the trained model artifact to inspect behaviors for a variety of important scenarios that we define.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#pre-train-tests","title":"Pre-train tests","text":"<p>There are some tests that we can run without needing trained parameters. These tests include:</p> <ul> <li>check the shape of your model output and ensure it aligns with the labels in your dataset</li> <li>check the output ranges and ensure it aligns with our expectations (eg. the output of a classification model should be a distribution with class probabilities that sum to 1)</li> <li>make sure a single gradient step on a batch of data yields a decrease in your loss</li> <li>make assertions about your datasets</li> <li>check for label leakage between your training and validation datasets</li> </ul> <p>The main goal here is to identify some errors early so we can avoid a wasted training job.</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#post-train-tests","title":"Post-train tests","text":"<ul> <li>However, in order for us to be able to understand model behaviors we'll need to test against trained model artifacts. </li> <li>These tests aim to interrogate the logic learned during training and provide us with a behavioral report of model performance.</li> </ul> <p>Reference: Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</p> <ul> <li>Three different types of model tests can be used to understand behavioral attributes.</li> <li>Invariance Tests</li> <li>Invariance tests allow us to describe a set of perturbations we should be able to make to the input without affecting the model's output. </li> <li>We can use these perturbations to produce pairs of input examples (original and perturbed) and check for consistency in the model predictions. </li> <li>This is closely related to the concept of data augmentation, where we apply perturbations to inputs during training and preserve the original label.</li> <li>For example, imagine running a sentiment analysis model on the following two sentences:<ul> <li>Mark was a great instructor.</li> <li>Samantha was a great instructor.</li> </ul> </li> <li>We would expect that simply changing the name of the subject doesn't affect the model predictions.</li> <li>Directional Expectation Tests</li> <li>Directional expectation tests, on the other hand, allow us to define a set of perturbations to the input which should have a predictable effect on the model output.</li> <li>For example, if we had a housing price prediction model we might assert:</li> <li>Increasing the number of bathrooms (holding all other features constant) should not cause a drop in price.</li> <li>Lowering the square footage of the house (holding all other features constant) should not cause an increase in price.</li> <li>Let's consider a scenario where a model fails the second test - taking a random row from our validation dataset and decreasing the feature <code>house_sq_ft</code> yields a higher predicted price than the original label. <ul> <li>This is surprising as it doesn't match our intuition, so we decide to look further into it </li> <li>We realize that,_ without having a feature for the house's neighborhood/location, our model has learned that smaller units tend to be more expensive; this is due to the fact that smaller units from our dataset are more prevalent in cities where prices are generally higher. _</li> </ul> </li> <li>In this case, the selection of our dataset has influenced the model's logic in unintended ways - this isn't something we would have been able to identify simply by examining performance on a validation dataset.</li> <li>Minimum Functionality Tests (aka data unit tests)</li> <li>Just as software unit tests aim to isolate and test atomic components in your codebase, data unit tests allow us to quantify model performance for specific cases found in your data.</li> <li>This allows you to identify critical scenarios where prediction errors lead to high consequences. </li> <li>You may also decide to write data unit tests for failure modes that you uncover during error analysis; this allows you to \"automate\" searching for such errors in future models.</li> <li>Take a look at Snorkel (https://www.snorkel.org/) who have introduced a very similar approach through their concept of _slicing functions. _</li> <li>These are programmatic functions which allow us to identify subsets of a dataset which meet certain criteria. </li> <li>For example, you might write a slicing function to identify sentences less than 5 words to evaluate how the model performs on short pieces of text.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#organizing-tests","title":"Organizing tests","text":"<ul> <li>In traditional software tests, we typically organize our tests to mirror the structure of the code repository. </li> <li>However, this approach doesn't translate well to machine learning models since our logic is structured by the parameters of the model.</li> <li>The authors of the CheckList paper linked above recommend structuring your tests around the \"skills\" we expect the model to acquire while learning to perform a given task.</li> </ul> <p>For example, a sentiment analysis model might be expected to gain some understanding of:</p> <ul> <li>vocabulary and parts of speech,</li> <li>robustness to noise,</li> <li>identifying named entities,</li> <li>temporal relationships,</li> <li>and negation of words.</li> </ul> <p>For an image recognition model, we might expect the model to learn concepts such as:</p> <ul> <li>object rotation,</li> <li>partial occlusion,</li> <li>perspective shift,</li> <li>lighting conditions,</li> <li>weather artifacts (rain, snow, fog),</li> <li>and camera artifacts (ISO noise, motion blur).</li> <li></li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_9","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Model development pipeline**\n</code></pre> <ul> <li>We also need to perform thorough testing of the models to ensure they are robust enough for real-world use.</li> <li>Let's go through some of the ways we can perform testing for different scenarios.ML testing is problem-dependent. </li> <li>This is not a template approach but rather a guide to what types of test suites you might want to establish for your application based on your use case.</li> </ul> <p>**Developing, testing, and deploying machine learning models **</p> <p>Combining automated tests and manual validation for effective model testing</p> <ul> <li>To perform ML testing in their projects, this approach involves having a few levels of tests suites, as well as validation:</li> <li>Automated tests for model verification</li> <li>Manual model evaluation and validation.</li> <li>To implement automated tests in their workflow, GitOps can be used. </li> <li>Jenkins runs code quality checks and smoke tests using production-like runs in the test environment. </li> <li>A single pipeline for model code is created where every pull request goes through code reviews and automated unit tests.</li> <li>The pull requests also go through automated smoke tests. </li> <li>The automated test suites\u2019 goal was to make sure tests flagged erroneous code early in the development process.</li> <li>After the automation tests are run and passed by the model pipeline, a domain expert manually reviewed the evaluation metrics to make sure that they made sense, validated them, and marked them ready for deployment.</li> </ul> <p>Automated tests for model verification</p> <p>The workflow for the automated tests will be that whenever someone on the team made a commit, </p> <ul> <li>the smoke test would run to ensure the code worked, </li> <li>then the unit tests would run, making sure that the assertions in the code and data were met. </li> <li>Finally, the integration tests would run to ensure the model works well with other components in the pipeline.</li> </ul> <p>Automated smoke test</p> <ul> <li>Every pull request goes through automated smoke tests where the team trained models and made predictions, running the entire end-to-end pipeline on some small chunk of actual data to ensure the pipeline worked as expected and nothing broke. </li> <li>The right kind of testing for the smoke suite can give any team a chance to understand the quality of their pipeline before deploying it. </li> <li>Running the smoke test suite does not mean the entire pipeline is guaranteed to be fully working because the code passed. </li> <li>The team has to consider the unit test suite to test data and model assumptions.</li> </ul> <p>Automated unit and integration tests</p> <ul> <li>The unit and integration tests the team run will check assertions about the dataset to prevent low-quality data from entering the training pipeline and prevent problems with the data preprocessing code. </li> <li>You could think of these assertions as assumptions the team made about the data. </li> <li>For example, they would expect to see some kind of correlation in the data or see that the model\u2019s prediction bounds are non-negative.</li> <li>Unit testing machine learning code is more challenging than typical software code. </li> <li>Unit testing several aspects of the model code is very difficult for a team. </li> <li>For example, to test accurately, teams would have to train the model, and even with a modest data set, a unit test could take a long time.</li> <li>Furthermore, some of the tests could be erratic and flaky (failed at random). </li> <li>One of the challenges of running the unit tests to assert the data quality is that running these tests on sample datasets was more complex and took way less time than running them on the entire dataset. </li> <li>It was difficult to fix for the team but to address the issues. </li> <li>Some teams opt to eliminate part of the unit tests in favor of smoke tests. </li> <li>The team defines acceptance criteria and their test suite continuously evolves as they experiment by adding new tests, and removing others, gaining more knowledge on what works and what doesn't.</li> <li>The model is trained in a production-like environment on a complete dataset for each new pull request, except that they would adjust the hyperparameters at values that resulted in quick results. Finally, they would monitor the pipeline\u2019s health for any issues and catch them early.</li> </ul> <p> The MLOps toolstack including testing tools</p> <p>Manual model evaluation and validation</p> <ul> <li>Have a human-in-the-loop framework where after training the model, reports are  created with different plots showing results based on the dataset, so the domain experts could review them before the model could be shipped.</li> <li>After training the model, a domain expert generated and reviewed a model quality report. </li> <li>The expert would approve (or deny) the model through a manual auditing process before it could eventually be shipped to production by the team after getting validation and passing all previous tests.</li> </ul> <p>Stress Tests and A/B Tests</p> <ul> <li>Once the pipeline generates the build (a container image), the models are stress-tested in a production-like environment through the release pipelines. </li> <li>Depending on the use case, the team also carried out A/B tests to understand how their models performed in varying conditions before they deployed them, rather than relying purely on offline evaluation metrics. </li> <li>With what they learned from the A/B tests, they knew whether a new model improved a current model and tuned their model to optimize the business metrics better.</li> </ul> <p>Stress testing machine learning models</p> <ul> <li>Testing the model\u2019s performance under extreme workloads is crucial for business applications that typically expect high traffic from users. </li> <li>Therefore, the team performed stress tests to see how responsive and stable the model would be under an increased number of prediction requests at a given time scale. </li> <li>This way, they benchmarked the model\u2019s scalability under load and identified the breaking point of the model. In addition, the test helped them determine if the model\u2019s prediction service meets the required service-level objective (SLO) with uptime or response time metrics.</li> <li>It is worth noting that the point of stress testing the model isn\u2019t so much to see how many inference requests the model could handle as to see what would happen when users exceed such traffic. </li> <li>This way, you can understand the model\u2019s performance problems, including the load time, response time, and other bottlenecks.</li> </ul> <p>Testing model quality after deployment</p> <ul> <li>The goal of the testing production models is to ensure that the deployment of the model is successful and the model works correctly in production together with other services. For this team, testing the inference performance of the model in production was a crucial process for continuously providing business value. </li> <li>In addition, the team tested for data and model drift to make sure models could be monitored and perhaps retrained when such drift was detected. On another note, testing production models can enable teams to perform error analysis on their mission-critical models through manual inspection from domain experts.</li> </ul> <p> - An example of a dashboard showing information on data drift for a machine learning project in Azure ML Studio | Source</p> <p>Drift MonitoringExample:</p> <p>https://github.com/keshavaspanda/drift-monitoring</p> <p>** Behavioral tests for ML (Natural language processing (NLP) and classification tasks)**</p> <ul> <li>Business use case: The transaction metadata product at MonoHQ uses machine learning to classify transaction statements that are helpful for a variety of corporate customer applications such as credit application, asset planning/management, BNPL (buy now pay later), and payment. Based on the narration, the product classifies transactions for thousands of customers into different categories.</li> <li>Before deploying the model, the team conducts a behavioral test. This test consists of 3 elements:</li> <li>Prediction distribution,</li> <li>Failure rate,</li> <li>Latency.</li> <li>If the model passes the three tests, the team lists it for deployment. If the model does not pass the tests, they would have to re-work it until it passes the test. They always ensure that they set a performance threshold as a metric for these tests.</li> <li>They also perform A/B tests on their models to learn what version is better to put into the production environment.</li> </ul> <p>Behavioral tests to check for prediction quality</p> <ul> <li>This test shows how the model responds to inference data, especially NLP models. </li> <li>First, the team runs an invariance test, introducing perturbability to the input data.</li> <li>Next, they check if the slight change in the input affects the model response\u2014its ability to correctly classify the narration for a customer transaction. </li> <li>Essentially, they are trying to answer here: does a minor tweak in the dataset with a similar context produce consistent output?</li> </ul> <p>Performance testing for machine learning models</p> <ul> <li>To test the response time of the model under load, the team configures a testing environment where they would send a lot of traffic to the model service. Here\u2019s their process:</li> <li>They take a large amount of transaction dataset,</li> <li>Create a table, </li> <li>Stream the data to the model service,</li> <li>Record the inference latency,</li> <li>And finally, calculate the average response time for the entire transaction data.</li> <li>If the response time passes a specified latency threshold, it is up for deployment. If it doesn\u2019t, the team would have to rework it to improve it or devise another strategy to deploy the model to reduce the latency. </li> </ul> <p>A/B testing machine learning models</p> <ul> <li>For this test, the team containerizes two models to deploy to the production system for upstream services to consume to the production system. </li> <li>They deploy one of the models to serve traffic from a random sample of users and another to a different sample of users so they can measure the real impact of the model\u2019s results on their users. </li> <li>In addition, they can tune their models using their real customers and measure how they react to the model predictions. </li> <li>This test also helps the team avoid introducing complexity from newly trained that are difficult to maintain and add no value to their users.</li> </ul> <p>** Performing  model engineering and statistical tests for machine learning applications**</p> <ul> <li>This team performed two types of tests on their machine learning projects:</li> <li>Engineering-based tests (unit and integration tests),</li> <li>Statistical-based tests (model validation and evaluation metrics). </li> <li>The engineering team ran the unit tests and checked whether the model threw errors. </li> <li>Then, the data team would hand off (to the engineering team) a mock model with the same input-output relationship as the model they were building. </li> <li>Also, the engineering team would test this model to ensure it does not break the production system and then serve it until the correct model from the data team is ready.</li> <li>Once the data team and stakeholders evaluate and validate that the model is ready for deployment, the engineering team will run an integration test with the original model. </li> <li>Finally, they will swap the mock model with the original model in production if it works.</li> </ul> <p>Engineering-based test for machine learning models</p> <ul> <li>Unit and integration tests</li> <li>To run an initial test to check if the model will integrate well with other services in production, the data team will send a mock (or dummy) model to the engineering team. </li> <li>The mock model has the same structure as the real model, but it only returns the random output. </li> <li>The engineering team will write the service for the mock model and prepare it for testing.</li> <li>The data team will provide data and input structures to the engineering team to test whether the input-output relationships match with what they expect, if they are coming in the correct format, and do not throw any errors. </li> <li>The engineering team does not check whether that model is the correct model; they only check if it works from an engineering perspective. </li> <li>They do this to ensure that when the model goes into production, it will not break the product pipeline.</li> <li>When the data team trains and evaluates the correct model and stakeholders validate it, the data team will package it and hand it off to the engineering team. </li> <li>The engineering team will swap the mock model with the correct model and then run integration tests to ensure that it works as expected and does not throw any errors.</li> </ul> <p>Statistical-based test for machine learning models</p> <ul> <li>The data team would train, test, and validate their model on real-world data and statistical evaluation metrics. </li> <li>The head of data science audits the results and approves (or denies) the model. If there is evidence that the model is the correct model, the head of data science will report the results to the necessary stakeholders. </li> <li>He will explain the results and inner workings of the model, the risks of the model, and the errors it makes, and confirm if they are comfortable with the results or the model still needs to be re-worked. </li> <li>If the model is approved, the engineering team swaps the mock model with the original model, reruns an integration test to confirm that it does not throw any error, and then deploy it.</li> <li>Model evaluation metrics are not enough to ensure your models are ready for production. </li> <li>You also need to perform thorough testing of your models to ensure they are robust enough for real-world encounters.</li> <li>Developing tests for ML models can help teams systematically analyze model errors and detect failure modes, so resolution plans are made and implemented before deploying the models to production.</li> </ul> <p>**Automated testing with Great Expectations **</p> <p></p> <p>Automated testing tailored for data pipelines is the premise of Great Expectations, a widely used open-source Python package for data validation.</p> <p>https://medium.com/dataroots/great-expectations-tutorial-by-paolo-l%C3%A9onard-95e689d73702</p> <ul> <li>The package is built around the concept of an expectation. </li> <li>The expectation can be thought of as a unit test for data. It is a declarative statement that describes the property of a dataset and does so in a simple, human-readable language.</li> <li>For example, to assert that the values of the column \u201cnum_complaints\u201d in some table in between one and five, you can write:</li> </ul> <p>expect_column_values_to_be_between(</p> <pre><code>column=\"num_complaints\",\n\nmin_value=1,\n\nmax_value=5,\n</code></pre> <p>)</p> <ul> <li>This statement will validate your data and return a success or a failure result. </li> <li>As we have already mentioned, you do not always control your data but rather passively observe it flowing. It is often the case that an atypical value pops up in your data from time to time without necessarily being a reason for distress. Great - Expectations accommodate this via the \u201cmostly\u201d keyword which allows for describing how often should the expectation be matched.</li> </ul> <p>expect_column_values_to_be_between(</p> <pre><code>column=\"num_complaints\",\n\nmin_value=1,\n\nmax_value=5,\n\nmostly=0.95,\n</code></pre> <p>)</p> <ul> <li>The above statement will return success if at least 95% of \u201cnum_complaints\u201d values are between one and five.</li> <li>In order to understand the data well, it is crucial to have some context about why we expect certain properties from it. </li> <li>We can simply add it by passing the \u201cmeta\u201d  parameter to the expectation with any relevant information about how it came to be. Our colleagues or even our future selves will thank us for it.</li> </ul> <p>expect_column_values_to_be_between(</p> <pre><code>column=\"num_complaints\",\n\nmin_value=1,\n\nmax_value=5,\n\nmostly=0.95,\n\nmeta={\n\n    \u201ccreated_by\u201d: \u201cMichal\u201d,\n\n    \u201ccraeted_on\u201d: \u201c28.03.2022\u201d,\n\n    \u201cnotes\u201d: \u201cnumber of client complaints; more than 5 is unusual\u201d\n\n             \u201cand likely means something broke\u201d,\n\n}\n</code></pre> <p>)</p> <ul> <li>These metadata notes will also form a basis for the data documentation which Great Expectations can just generate out of thin air \u2013 but more on this later!</li> <li>The package contains several dozen expectations to use out of the box, all of them with wordy, human-readable names such as \u201cexpect_column_distinct_values_to_be_in_set\u201d, \u201cexpect_column_sum_to_be_between\u201d, or \u201cexpect_column_kl_divergence_to_be_less_than\u201d. This syntax allows one to clearly state what is expected of the data and why. </li> <li>Some expectations are applicable to column values, others to their aggregate functions or entire density distributions. Naturally, the package also makes it possible to easily create custom expectations for when a tailored solution is needed.</li> <li>Great Expectations works with many different backends. </li> <li>You can evaluate your expectations locally on a Pandas data frame just as easily as on a SQL database (via SQLAlchemy) or on an Apache Spark cluster.</li> <li>So, how do the expectations help to reduce pipeline debt? The answer to this is multifold. </li> <li> <ol> <li>The process of crafting the expectations forces us to sit and ponder about our data: its nature, sources, and what can go wrong with it. This creates a deeper understanding and improves data-related communication within the team.</li> </ol> </li> <li> <ol> <li>By clearly stating what we expect from the data, we can detect any unusual situations such as system outages early on.</li> </ol> </li> <li> <ol> <li>By validating new data against a set of pre-existing expectations we can be sure we don\u2019t feed our machine learning models garbage.</li> </ol> </li> <li> <ol> <li>Having the expectations defined brings us very close to having well-maintained data documentation in place. The list goes on and on.</li> </ol> </li> </ul> <p>A few specific use cases in which investing time in GE pays back a great deal are:</p> <p>Detecting data drift</p> <ul> <li>A notorious danger to machine learning models deployed in production is data drift. Data drift is a situation when the distribution of model inputs changes. This can happen for a multitude of reasons: data-collecting devices tend to break or have their software updated, which impacts the way data is being recorded. If the data is produced by humans, it is even more volatile as fashions and demographics evolve quickly.</li> <li>Data drift constitutes a serious problem for machine learning models. It can make the decision boundaries learned by the algorithm invalid for the new-regime data, which has a detrimental impact on the model\u2019s performance.</li> <li>Data drift may impact the model\u2019s performance</li> <li>You have collected and cleaned your data, experimented with various machine learning models and data preprocessing variants and fine-tuned your model\u2019s hyperparameters to finally come up with a solution good enough for your problem. </li> <li>Then, you\u2019ve built a robust, automatic data pipeline, wrote an API for the model, put it in a container, and deployed it to the production environment. </li> <li>You even made sure to check that the model runs smoothly and correctly in production. Finally, you're done! Or are you? </li> <li>Not even close. In fact, this is just the beginning of the journey.</li> <li>There are so many things that could go wrong with a machine learning system after it has been deployed to production! </li> <li>Broadly speaking, we can divide all these potential concerns into two buckets: statistical issues and infrastructure issues. </li> <li>The latter comprise things like computing resources and memory (are there enough?), latency (is the model responding quickly enough?), throughput (can we answer all the incoming requests?), and so on. </li> <li>Here, we\u2019ll focus on the former: the statistical issues, which come in two main flavors: data drift and concept drift.</li> <li>Enters data validation. </li> <li>In situations where data drift could be of concern, just create expectations about the model input features that validate their long-term trend, average values, or historic range and volatility. </li> <li>As soon as the world changes and your incoming data starts to look differently, GE will alert you by spitting out an array of failed tests!</li> </ul> <p>Preventing outliers from distorting model outputs</p> <ul> <li>Another threat to models deployed in production, slightly similar to the data drift, are outliers. </li> <li>What happens to a model\u2019s output when it gets an unusual value as input, typically very high or very low? </li> <li>If the model has not seen such an extreme value during training, an honest answer for it would be to say: I don\u2019t know what the prediction should be!</li> <li>Unfortunately, machine learning models are not this honest. Much to the contrary: the model will likely produce some output that will be highly unreliable without any warning.</li> <li>Fortunately, one can easily prevent it with a proper expectations suite! Just set allowed ranges for the model\u2019s input features based on what it has seen in training to make sure you are not making predictions based on outliers.</li> </ul> <p>Preventing pipeline failures from spilling over</p> <ul> <li>Data pipelines do fail sometimes. You might have missed a corner case. Or the power might have gone off for a moment in your server room. </li> <li>Whatever the reason, it happens that a data processing job expecting new files to appear somewhere suddenly finds none.</li> <li>If this makes the code fail, that\u2019s not necessarily bad.  </li> <li>But often it doesn\u2019t: the job succeeds, announcing happily to the downstream systems that your website had 0 visits on the previous day. </li> <li>These data points are then shown on KPI dashboards or even worse, are fed into models that automatically retrain. </li> <li>Q: How do we prevent such a scenario? Expect recent data \u2013 for instance, with a fresh enough timestamp \u2013 to be there.</li> </ul> <p>Detecting harmful biases</p> <ul> <li>Bias in machine learning models is a topic that has seen increasing awareness and interest recently. </li> <li>This is crucial, considering how profoundly the models can impact people\u2019s lives. The open question is how to detect and prevent these biases from doing charm.</li> <li>While by no means do they provide an ultimate answer, Great Expectations can at least help us in detecting dangerous biases. </li> </ul> <p>Fairness</p> <ul> <li>Fairness in machine learning is a vast and complex topic, so let us focus on two small parts of the big picture: the training data that goes into the model, and the predictions produced by it for different test inputs.</li> <li>When it comes to the training data, we want it to be fair and unbiased, whatever that means in our particular case. </li> <li>If the data is about users, for instance, you might want to include users from various geographies in appropriate proportions, matching their global population. Whether or not this is the case can be checked by validating each batch of training data against an appropriate expectations suite before the data is allowed to be used for training.</li> <li>As for the model\u2019s output, we might want it, for instance, to produce the same predictions for both women and men if their remaining characteristics are the same. To ensure this, just test the model on a hold-out test set and run the results against a pre-crafted suite of expectations.</li> </ul> <p>Improving team communication and data understanding.</p> <ul> <li>Finally, we could start off by creating an empty expectations suite, that is: list all the columns, but don\u2019t impose any checks on their values yet. </li> <li>Then, get together people who own the data or the business processes involved and ask them:</li> <li>What is the maximal monthly churn rate that is worrisome? </li> <li>How low does the website stickiness have to fall to trigger an alert? Such conversations can improve the data-related communication between the teams and the understanding of the data themselves in the company</li> </ul> <p>Resources</p> <ul> <li>Great Expectations official documentation. </li> </ul> <p>Testing LLMs</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#testing-large-language-models-with-wb-and-giskard","title":"Testing Large Language Models with W&amp;B and Giskard","text":"<ul> <li>Combining W&amp;B with Giskard to deeply understand LLM behavior and avoid common pitfalls like hallucinations and injection attacks</li> <li>According to the Open Worldwide Application Security Project, some of the most critical vulnerabilities that affect LLMs are prompt injection (when LLMs are manipulated to behave as the attacker wishes), sensitive information disclosure (when LLMs inadvertently leak confidential information), and hallucination (when LLMs generate inaccurate or inappropriate content).</li> <li>Giskard's scan feature ensures the identification of these vulnerabilities\u2014and many others. </li> <li>The library generates a comprehensive report which quantifies these into interpretable metrics. The Giskard/W&amp;B integration allows the logging of both the report and metrics into W&amp;B, which in conjunction with the tracing, creates the ideal combination for building and debugging LLM apps.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#giskards-vulnerability-scanning-for-llms","title":"Giskard's vulnerability scanning for LLMs","text":"<ul> <li>Giskard is an open-source testing framework dedicated to ML models, covering any Python model, from tabular to LLMs.</li> <li>Testing machine learning applications can be tedious: Where to start testing? Which tests to implement? What issues to cover? How do we implement the tests?</li> <li>With Giskard, data scientists can scan their model to find dozens of hidden vulnerabilities, instantaneously generate domain-specific tests, and leverage the Quality Assurance best practices of the open-source community.</li> <li>For more information, you can check Giskard's documentation following this link.</li> <li>Watch: https://www.youtube.com/watch?v=KeY6qPAvyq0</li> <li>The better developer version: https://www.youtube.com/watch?v=rkjFFx_nXhU</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#wb-traces-for-debugging-llms","title":"**W&amp;B Traces for Debugging LLMs: **","text":"<ul> <li>Weights &amp; Biases, often referred to as wandb or even simply W&amp;B, is an MLOps platform that helps AI developers streamline their ML workflow from end to end.</li> <li>With W&amp;B, developers can monitor the progress of training their models in real-time, log key metrics and hyperparameters, and visualize results through interactive dashboards. It simplifies collaboration by enabling team members to share experiments and compare model performance. For more information, you can check W&amp;B's documentation following this link.</li> <li>In the context of LLMs, earlier this year, W&amp;B introduced a new debugging tool \u201cW&amp;B Traces\u201d designed to support ML practitioners working on prompt engineering for LLMs. </li> <li>It lets users visualize and drill down into every component and activity throughout the trace of the LLM pipeline execution. In addition, it enables the review of past results, identification and debugging of errors, gathering insights about the LLM\u2019s behavior, and sharing insights.</li> <li>Tracing is invaluable, but how do we measure the quality of the outputs throughout the pipeline? </li> <li>Could there be hidden vulnerabilities that our carefully-crafted prompts may have inadvertently failed to counter? Is there a way to detect such vulnerabilities automatically? Would it be possible to log these issues into W&amp;B to complement the tracing?</li> <li>In a nutshell, the answer to all these questions is \"yes.\" That's precisely the capability that Giskard brings to the table.</li> <li>Combining Weights &amp; Biases and Giskard, makes it possible to overcome this very challenge with this example available as a Google Colab notebook!</li> </ul> <p>Using LLMs to perform Data Quality:</p> <ul> <li>BirdiDQ https://github.com/keshavaspanda/BirdiDQ  is a simple, intuitive and user-friendly data quality application that allows you to run data quality checks on top of python based great expectation open source library using natural language queries. </li> <li>The idea is to type in your requests, and BirdiDQ will generate the appropriate GE method, run the quality control and return the results along with data docs you need. Demo</li> </ul> <p>Using Large Language Models for Efficient Data Annotation and Model Fine-Tuning with Iterative Active Learning</p> <p></p> <p>https://github.com/keshavaspanda/llm-data-annotation</p> <p>Additional Resource for us to try out:</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#httpsgithubcomkeshavaspandallama-in-a-container","title":"https://github.com/keshavaspanda/llama-in-a-container","text":"<p>Section 8__________</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#training-tuning-and-serving-up-llms-in-production","title":"Training, Tuning and Serving up LLMs in production:","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#serving-up-llms-in-production","title":"Serving up LLMs in production:","text":"<ul> <li>OpenLLM is an open-source platform designed to facilitate the deployment and operation of large language models (LLMs) in real-world applications. </li> <li>With OpenLLM, you can run inference on any open-source LLM, deploy them on the cloud or on-premises, and build powerful AI applications.</li> <li>https://github.com/bentoml/OpenLLM?tab=readme-ov-file</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#trainingfine-tuning-large-language-models-llms-the-first-pass","title":"Training/Fine-Tuning Large Language Models (LLMs) - The First Pass","text":"<pre><code>Comparison of the number of parameters of models. Just look at how big GPT-3 is. And nobody knows about GPT-4\u2026\n</code></pre> <pre><code>LLMs capabilities\n</code></pre> <ul> <li>Creating a local large language model (LLM) is a significant undertaking.</li> <li>It requires substantial computational resources and expertise in machine learning. </li> <li>It was not feasible to run local LLMs on your own local system because of the computational costs involved. </li> <li>However, with the advent of new software, GPT4All and LM-Studio can be used to create complete software packages that work locally. </li> <li>But let\u2019s start with a HuggingFace Transformers source code example that shows you how to use the HuggingFace Libraries and PyTorch for LLMs (cloud-based, not local in this case):</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#consider-the-huggingface-transformers-example","title":"Consider the HuggingFace Transformers Example","text":"<ul> <li>This is a complete program that uses the GPT-2 model, GPT-2 tokenizer, and is fine-tuned on the AG NEWS dataset (a small dataset used for utility purposes) is given below and explained in code snippets. </li> <li>We can leverage the power of pre-trained models and fine-tune them on specific tasks.</li> <li> <p>Importing necessary libraries and modules: </p> <ul> <li>The script starts by importing the necessary libraries and modules. AG_NEWS is a news classification dataset from the \u201ctorchtext.datasets\u201d package. AutoModelWithLMHead and AdamW are imported from the transformers library. </li> <li>AutoModelWithLMHead is a class that provides automatic access to pre-trained models with a language modeling head, and AdamW is a class that implements the AdamW optimizer, a variant of the Adam optimizer with weight decay.</li> </ul> <p>```</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#from-torchtextdatasets-import-ag_news","title":"from torchtext.datasets import AG_NEWS","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#from-transformers-import-automodelwithlmhead-adamw","title":"from transformers import AutoModelWithLMHead, AdamW","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#from-transformers-import-autotokenizer","title":"from transformers import AutoTokenizer","text":"<pre><code>```\n\n\n* **Setting up the tokenizer:** The script uses the AutoTokenizer class from the transformers library to load the tokenizer associated with the \u201cgpt2\u201d model. The tokenizer is responsible for converting input text into a format that the model can understand. This includes splitting the text into tokens (words, subwords, or characters), mapping the tokens to their corresponding IDs in the model\u2019s vocabulary, and creating the necessary inputs for the model (like attention masks).\n* tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n* **Setting the number of epochs:** The script sets the number of epochs for training to 50. An epoch is one complete pass through the entire training dataset. The number of epochs is a hyperparameter that you can tune. Training for more epochs can lead to better results, but it also increases the risk of overfitting and requires more computational resources.\n\n```\n</code></pre>"},{"location":"Spanda%20Bootcamp%20Day%201/#epochs-50","title":"EPOCHS = 50","text":"<pre><code>```\n\n\n* **Preprocessing the data: **The preprocess_data function is defined to preprocess the data. It takes an iterator over the data and encodes the text in each item using the tokenizer. The AG_NEWS dataset is then loaded and preprocessed. The dataset is split into \u2018train\u2019 and the text from each item is encoded. Encoding the text involves splitting it into tokens, mapping the tokens to their IDs in the model\u2019s vocabulary, and creating the necessary inputs for the model.\n\n```\n</code></pre>"},{"location":"Spanda%20Bootcamp%20Day%201/#def-preprocess_datadata_iter","title":"def preprocess_data(data_iter):","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#data-tokenizerencodetext-for-_-text-in-data_iter","title":"data = [tokenizer.encode(text) for _, text in data_iter]","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#return-data","title":"return data","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#train_iter-ag_newssplittrain","title":"train_iter = AG_NEWS(split='train')","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#train_data-preprocess_datatrain_iter","title":"train_data = preprocess_data(train_iter)","text":"<pre><code>```\n</code></pre> <ul> <li> <p>Setting up the model and optimizer: </p> <ul> <li>The script loads the pre-trained \u201cgpt2\u201d model using the AutoModelWithLMHead class and sets up the AdamW optimizer with the model\u2019s parameters. The model is a transformer-based model with a language modeling head, which means it\u2019s designed to generate text. The AdamW optimizer is a variant of the Adam optimizer with weight decay, which can help prevent overfitting.</li> </ul> <p>```</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#model-automodelwithlmheadfrom_pretrainedgpt2","title":"model = AutoModelWithLMHead.from_pretrained(\"gpt2\")","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#optimizer-adamwmodelparameters","title":"optimizer = AdamW(model.parameters())","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#modeltrain","title":"model.train()","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#for-epoch-in-rangeepochs","title":"for epoch in range(EPOCHS):","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#for-batch-in-train_data","title":"for batch in train_data:","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#outputs-modelbatch","title":"outputs = model(batch)","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#loss-outputsloss","title":"loss = outputs.loss","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#lossbackward","title":"loss.backward()","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#optimizerstep","title":"optimizer.step()","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#optimizerzero_grad","title":"optimizer.zero_grad()","text":"<pre><code>```\n</code></pre> <ul> <li>Training the model: The script trains the model for the specified number of epochs. In each epoch, it iterates over the batches of training data, feeds each batch to the model, computes the loss, performs backpropagation with loss.backward(), and updates the model\u2019s parameters with optimizer.step(). It also resets the gradients with optimizer.zero_grad(). This is a standard training loop for PyTorch models.</li> <li> <p>Generating text: After training, the script uses the model to generate text. It starts by encoding a prompt using the tokenizer, then feeds this encoded prompt to the model\u2019s generate method. The output of the generate() method is a sequence of token IDs, which is then decoded back into text using the tokenizer.</p> <p>```</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#prompt-tokenizerencodewrite-a-summary-of-the-new-features-in-the-latest-release-of-the-julia-programming-language-return_tensorspt","title":"prompt = tokenizer.encode(\"Write a summary of the new features in the latest release of the Julia Programming Language\", return_tensors=\"pt\")","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#generated-modelgenerateprompt","title":"generated = model.generate(prompt)","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#generated_text-tokenizerdecodegenerated0","title":"generated_text = tokenizer.decode(generated[0])","text":"<pre><code>```\n</code></pre> <ul> <li> <p>Saving the generated text: Finally, the script saves the generated text to a file named \u201cgenerated.txt\u201d. This is done using Python\u2019s built-in file handling functions.</p> <p>```</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#with-opengeneratedtxt-w-as-f","title":"with open(\"generated.txt\", \"w\") as f:","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#fwritegenerated_text","title":"f.write(generated_text)","text":"<pre><code>```\n</code></pre> <ul> <li>This script is a good example of how to fine-tune a pre-trained language model on a specific task. </li> <li>Fine-tuning a large model like GPT-2 can be computationally intensive and may require a powerful machine or cloud-based resources. </li> <li>This script doesn\u2019t include some important steps like splitting the data into training and validation sets, shuffling the data, and batching the data. _These steps are crucial for training a robust model. _</li> <li> <p>The entire program is given below:</p> <p>```</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#from-torchtextdatasets-import-ag_news_1","title":"from torchtext.datasets import AG_NEWS","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#from-transformers-import-automodelwithlmhead-adamw_1","title":"from transformers import AutoModelWithLMHead, AdamW","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#from-transformers-import-autotokenizer_1","title":"from transformers import AutoTokenizer","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#tokenizer-autotokenizerfrom_pretrainedgpt2","title":"tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#epochs-50_1","title":"EPOCHS = 50","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#def-preprocess_datadata_iter_1","title":"def preprocess_data(data_iter):","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#data-tokenizerencodetext-for-_-text-in-data_iter_1","title":"data = [tokenizer.encode(text) for _, text in data_iter]","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#return-data_1","title":"return data","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#train_iter-ag_newssplittrain_1","title":"train_iter = AG_NEWS(split='train')","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#train_data-preprocess_datatrain_iter_1","title":"train_data = preprocess_data(train_iter)","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#model-automodelwithlmheadfrom_pretrainedgpt2_1","title":"model = AutoModelWithLMHead.from_pretrained(\"gpt2\")","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#optimizer-adamwmodelparameters_1","title":"optimizer = AdamW(model.parameters())","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#modeltrain_1","title":"model.train()","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#for-epoch-in-rangeepochs_1","title":"for epoch in range(EPOCHS):","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#for-batch-in-train_data_1","title":"for batch in train_data:","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#outputs-modelbatch_1","title":"outputs = model(batch)","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#loss-outputsloss_1","title":"loss = outputs.loss","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#lossbackward_1","title":"loss.backward()","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#optimizerstep_1","title":"optimizer.step()","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#optimizerzero_grad_1","title":"optimizer.zero_grad()","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#prompt-tokenizerencodewrite-a-summary-of-the-new-features-in-the-latest-release-of-the-julia-programming-language-return_tensorspt_1","title":"prompt = tokenizer.encode(\"Write a summary of the new features in the latest release of the Julia Programming Language\", return_tensors=\"pt\")","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#generated-modelgenerateprompt_1","title":"generated = model.generate(prompt)","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#generated_text-tokenizerdecodegenerated0_1","title":"generated_text = tokenizer.decode(generated[0])","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#with-opengeneratedtxt-w-as-f_1","title":"with open(\"generated.txt\", \"w\") as f:","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#fwritegenerated_text_1","title":"f.write(generated_text)","text":"<pre><code>```\n</code></pre>"},{"location":"Spanda%20Bootcamp%20Day%201/#there-are-two-packaged-solutions-for-local-llms-and-many-more-popping-up-everyday-two-of-them-are-the-best-one-is-lm-studio-the-other-is-httpsgpt4allioindexhtml","title":"There are two packaged solutions for Local LLMs (and many more popping up, everyday). Two of them are the best. One is LM-Studio. The other is https://gpt4all.io/index.html","text":"<ul> <li>This is the best for those if you want a completely open-source on-premises system. But you need to have at least 32 GB of local RAM, 16 GB GPU RAM, a 3+ Ghz multicore(the more, the better) processor, and a local SSD.  LLMs are computationally, extremely expensive!</li> <li>There\u2019s a lot more to LLM models than just chat</li> <li>Given the expensive;y daunting computational requirements for fine-tuning musical and pictures and audio for LLMs we are not going to run them. </li> <li>Some popular, already built and ready-to-go solutions as well as some interesting source material are:</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#audio-llms","title":"Audio LLMs","text":"<ul> <li>https://www.assemblyai.com/docs/guides/processing-audio-with-llms-using-lemur</li> <li>AudioGPT Research Paper \u2014 https://arxiv.org/abs/2304.12995</li> <li>Tango https://tango-web.github.io/</li> <li>https://blog.google/technology/ai/musiclm-google-ai-test-kitchen/</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#image-llms","title":"Image LLMs","text":"<ul> <li>https://www.linkedin.com/pulse/generating-images-large-language-model-gill-arun-krishnan</li> <li>Stable Diffusion</li> <li>DALL E-1,2,3</li> <li>MidJourney</li> <li>Bing Image Creator</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#multimodal-llms","title":"Multimodal LLMs","text":"<ul> <li>https://arxiv.org/abs/2306.09093 Macaw-LLM research paper. </li> <li>https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models</li> <li>https://openai.com/research/gpt-4 </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#general-llm-resources","title":"General LLM Resources","text":"<ul> <li>https://beebom.com/best-large-language-models-llms/</li> <li>https://roadmap.sh/guides/free-resources-to-learn-llms</li> <li>https://github.com/Hannibal046/Awesome-LLM</li> <li>https://medium.com/@abonia/best-llm-and-llmops-resources-for-2023-75e96ac37feb</li> <li>https://learn.deeplearning.ai/ </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_10","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Fine-Tuning Your LLM - A Revisit**\n</code></pre> <ul> <li>Again. fine-tuning is the process of continuing the training of a pre-trained LLM on a specific dataset. </li> <li>You might ask why we need to train the model further if we can already add data using RAG. </li> <li> <p>The simple answer is that only fine-tuning can tailor your model to understand a specific domain or define its \u201cstyle\u201d. </p> <p>:</p> </li> </ul> <p></p> <pre><code>Classical approach of fine-tuning on domain specific data (all icons from [flaticon](http://flaticon.com/))\n</code></pre> <ol> <li>Take a trained LLM, sometimes called Base LLM. You can download them from HuggingFace.</li> <li>Prepare your training data. You only need to compile instructions and responses. Here\u2019s an example of such a dataset. You can also generate synthetic data using GPT-4.</li> <li>Choose a suitable fine-tuning method. LoRA and QLoRA are currently popular.</li> <li>Fine-tune the model on new data.</li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%201/#_11","title":"Spanda Bootcamp Day 1","text":"<pre><code>**When to Use**\n</code></pre> <ul> <li>Niche Applications: When the application deals with specialized or unconventional topics. For example, legal document applications that need to understand and handle legal jargon.</li> <li>Custom Language Styles: For applications requiring a specific tone or style. For example, creating an AI character whether it\u2019s a celebrity or a character from a book.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_12","title":"Spanda Bootcamp Day 1","text":"<pre><code>**When NOT to Use**\n</code></pre> <ul> <li>Broad Applications: Where the scope of the application is general and doesn\u2019t require specialized knowledge.</li> <li>Limited Data: Fine-tuning requires a significant amount of relevant data. However, you can always generate them with another LLM. For example, the Alpaca dataset of 52k LLM-generated instruction-response pairs was used to create the first finetuning Llama v1 model earlier this year.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_13","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Fine-tuning LLM**\n\n\nLet us look at a high-level library, [Lit-GPT](https://github.com/Lightning-AI/lit-gpt), which hides all complexities, hence doesn\u2019t allow for much customization of the training process, but one can quickly conduct experiments and get initial results.\n\n\nYou\u2019ll need just a few lines of code:\n\n\n```\n# 1. Download the model:\npython scripts/download.py --repo_id meta-llama/Llama-2-7b\n\n# 2. Convert the checkpoint to the lit-gpt format:\npython scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/llama\n\n# 3. Generate an instruction tuning dataset:\npython scripts/prepare_alpaca.py  # it should be your dataset\n\n# 4. Run the finetuning script\npython finetune/lora.py \\\n   --checkpoint_dir checkpoints/llama/\n   --data_dir your_data_folder/\n   --out_dir my_finetuned_model/\n```\n\n\n\nAnd that\u2019s it! Your training process will start:\n</code></pre> <pre><code>_This  takes approximately **10 hours** and **30 GB** memory to fine-tune Falcon-7B on a single A100 GPU._\n</code></pre> <ul> <li>The fine-tuning process is quite complex and to get better results, you\u2019ll need to understand various adapters, their parameters, and much more. </li> <li>However, even after such a simple iteration, you will have a new model that follows your instructions.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_14","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Some References to chase down:**\n</code></pre> <ul> <li>Create a Clone of Yourself With a Fine-tuned LLM \u2014 an article about collecting datasets, using parameters, and  useful tips on fine-tuning.</li> <li>Understanding Parameter-Efficient Fine-tuning of Large Language Models \u2014 an excellent tutorial to get into the details of the concept of fine-tuning and popular parameter-efficient alternatives.</li> <li>Fine-tuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments \u2014 one of my favorite articles for understanding the capabilities of LoRA.</li> <li>OpenAI Fine-tuning \u2014 if you want to fine-tune GPT-3.5 with minimal effort.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_15","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Deploying Your LLM Application in Production**\n</code></pre> <ul> <li>There are a huge number of frameworks that specialize in deploying large language models with</li> <li>Lots of pre-built wrappers and integrations.</li> <li>A vast selection of available models.</li> <li>A multitude of internal optimizations.</li> <li>Rapid prototyping.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_16","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Choosing the Right Framework**\n</code></pre> <ul> <li>The choice of framework for deploying an LLM application depends on various factors, including the size of the model, the scalability requirements of the application, and the deployment environment. </li> <li>Heres a  cheat sheet:</li> </ul> <pre><code>You can get a more detailed overview of the existing solutions here [7 Frameworks for Serving LLMs](https://medium.com/better-programming/frameworks-for-serving-llms-60b7f7b23407)\n</code></pre> <pre><code>    Comparison of frameworks for LLMs inference\n</code></pre>"},{"location":"Spanda%20Bootcamp%20Day%201/#_17","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Example Code for Deployment**\n</code></pre> <ul> <li>Let\u2019s move from theory to practice and try to deploy LLaMA-2 using Text Generation Inference. </li> <li> <p>And, as you might have guessed, you\u2019ll need just a few lines of code:</p> <pre><code># 1. Create a folder where your model will be stored:\nmkdir data\n\n# 2. Run Docker container (launch RestAPI service):\ndocker run --gpus all --shm-size 1g -p 8080:80 \\\n   -v $volume:/data \\\n   ghcr.io/huggingface/text-generation-inference:1.1.0\n   --model-id meta-llama/Llama-2-7b\n\n# 3. And now you can make requests:\ncurl 127.0.0.1:8080/generate \\\n   -X POST \\\n   -d '{\"inputs\":\"Tell me a joke!\",\"parameters\":{\"max_new_tokens\":20}}' \\\n   -H 'Content-Type: application/json'\n</code></pre> <ul> <li>That\u2019s it! You\u2019ve set up a RestAPI service with built-in logging, Prometheus endpoint for monitoring, token streaming, and your model is fully optimized. </li> </ul> </li> </ul> <p></p> <pre><code>API Documentation\n</code></pre>"},{"location":"Spanda%20Bootcamp%20Day%201/#_18","title":"Spanda Bootcamp Day 1","text":"<pre><code>**References:**\n</code></pre> <ul> <li>7 Frameworks for Serving LLMs \u2014 comprehensive guide into LLMs inference and serving with detailed comparison.</li> <li>Inference Endpoints \u2014 a product from HuggingFace that will allow you to deploy any LLMs in a few clicks. A good choice when you need rapid prototyping.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_19","title":"Spanda Bootcamp Day 1","text":"<pre><code>**To get in a little deeper**\n</code></pre> <ul> <li>We\u2019ve covered the basic concepts needed for developing LLM-based applications, there are still some aspects you\u2019ll likely encounter in the future. Here are  a few useful reference:</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_20","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Optimization**\n</code></pre> <ul> <li>When you launch your first model, you inevitably find it\u2019s not as fast as you\u2019d like and consumes a lot of resources and you\u2019ll need to understand how it can be optimized.</li> <li>7 Ways To Speed Up Inference of Your Hosted LLMs \u2014 techniques to speed up inference of LLMs to increase token generation speed and reduce memory consumption.</li> <li>Optimizing Memory Usage for Training LLMs in PyTorch \u2014 article provides a series of techniques that can reduce memory consumption in PyTorch by approximately 20x without sacrificing modeling performance and prediction accuracy.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_21","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Evaluating**\n</code></pre> <ul> <li>Suppose you have a fine-tuned model you need to be sure that its quality has improved.  What metrics should we use to check quality?</li> <li>All about evaluating Large language models \u2014 a good overview article about benchmarks and metrics.</li> <li>evals \u2014 the most popular framework for evaluating LLMs and LLM systems.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_22","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Vector Databases**\n</code></pre> <ul> <li>If you work with RAG, at some point, you\u2019ll move from storing vectors in memory to a database. </li> <li>For this, it\u2019s important to understand what\u2019s currently on the market and its limitations.</li> <li>All You Need to Know about Vector Databases \u2014 a step-by-step guide by  \\ Dominik Polzer \\  to discover and harness the power of vector databases.</li> <li>Picking a vector database: a comparison and guide for 2023 \u2014 comparison of Pinecone, Weviate, Milvus, Qdrant, Chroma, Elasticsearch and PGvector databases.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_23","title":"Spanda Bootcamp Day 1","text":"<pre><code>**LLM Agents**\n</code></pre> <ul> <li>One of  the most promising developments in LLMs are LLM Agents i f you want multiple models to work together. </li> <li>The following links are worth going through</li> <li>A Survey on LLM-based Autonomous Agents \u2014 this is probably the most comprehensive overview of LLM based agents.</li> <li>autogen \u2014 is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks.</li> <li>OpenAgents \u2014 an open platform for using and hosting language agents in the wild.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_24","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Reinforcement Learning from Human Feedback (RLHF)**\n</code></pre> <ul> <li>As soon as you allow users access to your model, you start taking responsibility. </li> <li>What if it responds rudely? Or reveals bomb-making ingredients? To avoid this, check out these articles:</li> <li>Illustrating Reinforcement Learning from Human Feedback (RLHF) \u2014 an overview article that details the RLHF technology.</li> <li>RL4LMs \u2014 a modular RL library to fine-tune language models to human preferences.</li> <li>TRL \u2014 a set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step.</li> </ul> <p>Summary:</p> <p></p> <ul> <li>The material covered today is broad but is the technology of the future. </li> <li>Junior Programmers, Artists, ML Engineers, Data Processing Analysts, Beginner Data Scientists, and practically every other digital job should be learning this technology. There is a lot of scope and opportunity. </li> <li>Generative AI is the future of the Digital Media World. Artists are feeling the impact today. A similar situation is looming for junior-level software engineers. </li> <li>But the solution is simple: Skill up! Help someone else Skill up! Regain Control!</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%203/","title":"Spanda Bootcamp Day 3","text":"<p>[Section 1: Overview_____________________]{.underline}</p> <p>Data Lakes, Federated Learning and Big Data</p> <ul> <li> <p>Data Lakes as a concept have played a key role in ushering in the     massive scales we see when working with data today as they give     companies the ability to store arbitrary types of data observed     during operations.</p> </li> <li> <p>While this freedom allows data lakes to maintain the maximum     potential of the data generated by the company, it also can lead to     a key problem---complacency in understanding the collected data.</p> </li> <li> <p>The ease of storing different types of data in an unstructured     manner can actually lead to a\u00a0store now, sort out later\u00a0mentality.</p> </li> <li> <p>The true difficulty of working with unstructured data actually stems     from its processing; thus, the delayed processing mentality has the     potential to lead to data lakes that become highly cumbersome to     sift through and work with due to unrestricted growth from the     collection of data.</p> </li> <li> <p>Raw data is only as valuable as the models and insights that can be     derived from it.</p> </li> <li> <p>The central data lake approach leads to cases where derivation from     the data is limited by a lack of structure, leading to issues     ranging from storage inefficiency to actual intelligence     inefficiency due to extraction difficulties.</p> </li> <li> <p>On the other side, approaches preceding data lakes suffered from a     simple lack of access to the amount of data potentially available.</p> </li> <li> <p>The fact that Federated Learning (FL) allows for both classes of     problems to be avoided is the key\u00a0driving support for FL as the     vehicle that will advance big data into the collective intelligence     era.</p> </li> <li> <p>This claim is substantiated by the fact that FL flips the big data     flow from collect \u2192 derive intelligence to derive intelligence \u2192     collect.</p> </li> <li> <p>For humans, intelligence can be thought of as the condensed form of     large swaths of experience.</p> </li> <li> <p>In a similar way, the derivation of intelligence at the source of     the generated data--- done by training a model on the data at the     source location---succinctly summarizes the data in a format that     maximizes accessibility for practical applications.</p> </li> <li> <p>The late collection step of FL leads to the creation of the desired     global intelligence with maximal data access and data storage     efficiency.</p> </li> <li> <p>Even cases with partial usage of the generated data sources can     still greatly benefit from the joint storage of intelligence and     data by greatly reducing the number of data formats entering the     residual data lake.</p> </li> </ul> <p>Abundance, Acceptance &amp; Ambivalence</p> <ul> <li> <p>While many\u00a0definitions have been proposed with emphasis on different     aspects, Oxford professor Viktor Mayer-Sch\u00f6nberger and\u00a0The     Economist\u00a0senior editor Kenneth Cukier brilliantly elucidated the     nature of big data.</p> </li> <li> <p>It is not about how big the data in a server is;\u00a0big data is about     three major shifts of a mindset that are interlinked and hence     reinforce one another.</p> </li> <li> <p>Their\u00a0argument can be summarized as the\u00a0Triple-A mindset for big     data*, which consists of an Abundance of observations,     Acceptance of messiness, and Ambivalence of causality***.</p> </li> </ul> <p>Abundance of observations</p> <ul> <li> <p>Big data doesn't have to be\u00a0big\u00a0in terms of columns and rows     or file size.</p> </li> <li> <p>Big data has a number of observations, commonly denominated as\u00a0n,     close or equal to the size of the population\u00a0of interest.</p> </li> <li> <p>In traditional statistics, collecting data from the entire     population---for example, people interested in fitness in New     York---was not possible or feasible, and researchers would have to     randomly select a sample from the population---for example, 1,000     people interested in fitness in New York.</p> </li> <li> <p>Random sampling is often difficult to perform and so is justifying     the narrow focus on particular subgroups: surveying people around     gyms would miss those who run in parks and practice yoga at home,     and why gym goers rather than runners and yoga fans?</p> </li> <li> <p>Thanks to the development and sophistication of\u00a0Information and     Communications Technology\u00a0(ICT) systems, however, researchers     today can access the data of approximately\u00a0all of the population     through multiple sources---for example, records of Google searches     about fitness.</p> </li> <li> <p>This paradigm of\u00a0abundance\u00a0or\u00a0n = all\u00a0is advantageous since what     the data says can be interpreted as a true statement about the     population, whereas the older methods could only\u00a0infer\u00a0such truth     with a significant level of confidence expressed in a\u00a0p-value,     typically supposed to be under 0.05.</p> </li> <li> <p>Small data provides statistics; big data proves states.</p> </li> </ul> <p>Acceptance of messiness</p> <ul> <li> <p>Big data is messy.</p> </li> <li> <p>If we use Google search data as a proxy for someone's interest---for     example---we could mistakenly attribute some of the searches made by     their family or friends on their devices to them, and the estimated     interest will be inaccurate to the degree of the ratio of such     unowned-device searches.</p> </li> <li> <p>In some devices, a significant amount of searches may be made by     multiple users, such as shared computers at an office or a     smartphone belonging to a child whose younger siblings are yet to     own one.</p> </li> <li> <p>Otherwise, people may search for words that pop up in a conversation     with someone else, rather than self-talk, which does not necessarily     reflect their own interests.</p> </li> <li> <p>In studies using traditional methods, researchers would have to make     sure that such devices are not included in their sample data because     the\u00a0mess\u00a0can affect the quality of inference\u00a0significantly, as the     number of observations would be small.</p> </li> <li> <p>This is not the case in big data studies. Researchers would be     willing to accept the\u00a0mess\u00a0as its effect diminishes proportionally     as the number of observations becomes large enough toward\u00a0n = all.</p> </li> <li> <p>In most devices, Google searches would be made by the owner     autonomously most of the time, and the impact of searches in other     contexts would not matter.</p> </li> </ul> <p>Ambivalence of causality</p> <ul> <li> <p>Big data is often used to study correlation but not     causation---in other words, it usually does not tell\u00a0why\u00a0but     only\u00a0what.</p> </li> <li> <p>For many practical questions, correlation alone can provide the     answer. Mayer-Sch\u00f6nberger and Cukier give several examples in     the\u00a0Big Data: A Revolution That Will Transform How We Live, Work,     and Think\u00a0book, among which\u00a0is Fair Isaac Corporation's\u00a0Medication     Adherence Score\u00a0established in 2011.</p> </li> <li> <p>In an era where people's behavioral patterns are\u00a0datafied,     collecting\u00a0n = all\u00a0observations for the variables of interest is     possible, and the correlation found among them is powerful enough to     direct our decision-making. There is no need to know people's     psychological scores     of\u00a0consistency\u00a0or\u00a0conformity\u00a0that\u00a0cause\u00a0their adherence to     medical prescriptions; by looking at how they behave in other     aspects of life, we can directly predict whether they will follow     the prescription or not.</p> </li> <li> <p>By embracing the triple mindset of abundance, acceptance, and     ambivalence, enterprises and governments have generated intelligence     across tasks from pricing services to recommending products,     optimizing transportation routes, and identifying crime suspects.</p> </li> <li> <p>Nevertheless, that mindset has been challenged in recent years, as     shown in the following sections.</p> </li> <li> <p>First, let's glimpse into how the abundance of observations often     taken for granted is currently under pressure.</p> </li> </ul> <p>AI and Data Privacy</p> <ul> <li> <p>FL is often said to be\u00a0one of the most popular privacy-preserving     AI technologies because private data does not have to be     collected or shared with third-party entities to generate     high-quality intelligence.</p> </li> <li> <p>Therefore, in this section, we discuss the data privacy that has     been a bottleneck that FL tries to resolve to create high-quality     intelligence.</p> </li> </ul> <p>Data privacy:</p> <ul> <li> <p>Data privacy, also known as\u00a0information privacy, is the right     of individuals to control how their personal information is\u00a0used,     which mandates third parties to handle, process, store, and use such     information properly in accordance with the\u00a0law.</p> <ul> <li> <p>In May 2021, HCA Healthcare announced that the company had     struck a deal to\u00a0share its patient records and real-time medical     data with Google.</p> </li> <li> <p>Various media quickly responded by warning the public about the     deal, as Google had been mentioned for its\u00a0Project     Nightingale\u00a0where the tech giant allegedly exploited the     sensitive data of millions of American patients.</p> </li> </ul> </li> <li> <p>Given above 80% of the public believes that the potential risks in     data collection by companies outweigh the benefits, according     to a 2019 poll by Pew Research Center, data sharing projects of such     a scale are naturally seen as a threat to people's\u00a0data privacy.</p> </li> <li> <p>It is often confused with\u00a0data security, which ensures that data     is accurate, reliable, and accessible only to authorized users.</p> </li> <li> <p>In the case of Google accounts, data privacy regulates how the     company can use the account holders' information, while data     security requires them to deploy measures such as password     protection and\u00a02-step verification.</p> </li> <li> <p>In explaining these two concepts, the data privacy managers use an     analogy of a\u00a0window\u00a0for security\u00a0and a\u00a0curtain\u00a0for privacy: data     security\u00a0is a prerequisite for data privacy. Put together, they     comprise\u00a0data protection, as shown in the following diagram:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"2.182638888888889in\"}</p> <p>Figure 1.2 -- Data security versus data privacy</p> <ul> <li> <p>We can see from the preceding diagram that while data security     limits\u00a0who\u00a0can access data, data privacy limits\u00a0what\u00a0can be in     the data.</p> </li> <li> <p>Understanding this distinction is very important\u00a0because data     privacy can multiply the consequences of failures in data security.     Let's look into how.</p> </li> </ul> <p>Privata Data Risks</p> <ul> <li> <p>Failing in data protection is costly. According to IBM's\u00a0Cost of a     Data Breach Report 2021, the global average cost of a data breach     in the year marked\u00a0US dollars\u00a0(USD) $4.24 million, which is     considerably higher than $3.86 million a year earlier and is     the\u00a0highest amount in the 17-year history of the report; an     increased number of people working remotely in the aftermath of the     COVID-19 outbreak is considered a major reason for the spike.</p> </li> <li> <p>The top five industries for average total cost are\u00a0healthcare,     finance, pharmaceuticals, technology, and energy.</p> </li> <li> <p>Nearly half of breaches in the year included customer\u00a0personally     identifiable information\u00a0(PII), which costs $180 per record     on average.</p> </li> <li> <p>Once customer PII is breached, negativities such as system downtime     during the response, loss of customers, need for acquiring new     customers, reputation losses, and diminished goodwill ensue; hence,     the hefty cost.</p> </li> <li> <p>The IBM study also found that failing to comply with regulations for     data protection was top among the factors that amplify data breach     costs     ([https://www.ibm.com/downloads/cas/ojdvqgry]{.underline}).</p> </li> </ul> <p>Increased Data Protection Regulations</p> <ul> <li> <p>As technology\u00a0advances, the need to protect customer data has become     more critical. Consumers require and expect privacy protection     during every transaction; many simple activities can risk personal     data, whether online banking or using a phone app.</p> </li> <li> <p>Governments worldwide were initially slow to react by creating laws     and regulations to protect personal data from identity theft,     cybercrime, and data privacy violations.</p> </li> <li> <p>However, times are now changing as data protection laws are     beginning to take shape globally.</p> </li> <li> <p>There are several drivers for the increase in regulations.</p> </li> <li> <p>These include the growth of enormous amounts of data, and we need     more data security and privacy to protect users from nefarious     activities such as identity theft.</p> </li> <li> <p>Let's look at some of the measures taken toward data privacy     in the following subsections.</p> </li> </ul> <p>General Data Protection Regulation (GDPR)</p> <ul> <li> <p>The\u00a0General Data Protection Regulation\u00a0(GDPR) by\u00a0European     Union\u00a0is regarded as\u00a0the first data protection regulation in the     modern data economy and was emulated by many countries to craft     their own.</p> </li> <li> <p>GDPR was proposed in 2012, adapted by the EU Council and Parliament     in 2016, and enforced in May 2018. It superseded the Data Protection     Directive that had been adopted in 1995.</p> </li> <li> <p>What makes GDPR epoch-making is its stress on the protection of PII,     including people's names, locations, racial or ethnic origin,     political or sexual orientation, religious beliefs, association     memberships, and genetic/biometric/health information. Organizations     and individuals both in and outside the EU have to follow the     regulation when dealing with the personal data of EU residents.     There are seven principles of GDPR, among which six were inherited     from the Data Protection Directive; the new principle     is\u00a0accountability, which demands data users maintain documentation     about the purpose and procedure of personal data usage.</p> </li> <li> <p>GDPR has shown the public what the consequences of its violation can     be.</p> </li> <li> <p>Depending on the severity of non-compliance, the GDPR fine can go     from 2% of global annual turnover or \u20ac10 million, whichever is     higher, or 4% of global annual turnover or \u20ac20 million, whichever is     higher.</p> </li> <li> <p>In May 2018, thousands of Europeans filed a complaint against     Amazon.com Inc. through the French organization La Quadrature du     Net, also known as\u00a0Squaring the Net\u00a0in English, accusing the     company of using its\u00a0advertisement targeting system without customer     consent.</p> </li> <li> <p>After 3 years of investigation, Luxembourg's\u00a0National Commission     for Data Protection\u00a0(CNDP) made headlines around the world: it     issued\u00a0Amazon a \u20ac746 million fine.</p> </li> <li> <p>Similarly, WhatsApp was fined by Ireland's Data Protection     Commission in September 2021 for GDPR infringement; again, the     investigation had taken 3 years, and the fine amounted to \u20ac225     million.</p> </li> <li> <p>Currently, in the US, a majority of states have privacy protections     in place or soon will. Additionally, several states have     strengthened existing regulations, such as California, Colorado, and     Virginia.</p> </li> <li> <p>Let's look at each to get an idea of these changes.</p> </li> </ul> <p>California Consumer Privacy Act (CCPA)</p> <ul> <li> <p>The state of California followed suit. The\u00a0California Consumer     Privacy Act\u00a0(CCPA) became effective on January 1, 2020.</p> </li> <li> <p>As the name suggests, the aim of the regulation is to\u00a0protect     consumers' PII just as GDPR does.</p> </li> <li> <p>Compared to GDPR, the scope of the CCPA is significantly limited.</p> </li> <li> <p>The CCPA is applicable only to for-profit organizations that collect     data from over 50,000 points (residents, households, or devices in     the state) in a year, generate annual revenue over $25 million, or     make half of their annual revenue by selling such information.</p> </li> <li> <p>However, CCPA infringement can be much more costly than GDPR     infringement since the former has no ceiling for its fine ($2,500     per record for each unintentional violation; $7,500 per record for     each intentional violation).</p> </li> </ul> <p>Colorado Privacy Act (CPA)</p> <ul> <li>Under the\u00a0Colorado Privacy Act\u00a0(CPA), starting July 1, 2024,     data collectors and controllers\u00a0will have to follow universal     opt-outs that users have selected for generating targeted     advertising and sales. This rule protects residents in Colorado from     targeted sales and advertising as well as certain types of     profiling.</li> </ul> <p>Virginia Consumer Data Protection Act (CDPA)</p> <ul> <li> <p>Virginia's\u00a0Consumer Data Protection Act\u00a0(CDPA) will make     several changes to increase security\u00a0and privacy on January 1, 2023.     These changes will be applicable to organizations that do business     in Virginia or with residents in Virginia. Data collectors need to     obtain approval to utilize their private data. These changes also     try to determine the adequacy of privacy and security of AI vendors,     which may require the removal of that data.</p> </li> <li> <p>These are just a few simple examples of how data regulations will     take shape in the US. What does this look like for the rest of the     world? Some estimate that by 2024, 75% of the global population will     have personal data covered by privacy regulations of one type or     another.</p> </li> <li> <p>Another example of major data protection regulation is     Brazil's\u00a0Lei Geral de Prote\u00e7\u00e3o de Dados Pessoais\u00a0(LGPD)     which has been in force since September 2020.</p> </li> <li> <p>It replaced\u00a0dozens of laws in the country related to data privacy.     LGPD was modeled after GDPR, and the contents are almost identical.</p> </li> <li> <p>In Asia, Japan was the first country to introduce a data protection     regulation: the\u00a0Act on the Protection of Personal     Information\u00a0(APPI) was adopted\u00a0in 2003 and amended in 2015. In     April 2022, the latest version of APPI was put in force to address     modern concerns over data privacy.</p> </li> <li> <p>FL has been\u00a0identified as a critical technology that can work     well with privacy regulations and regulatory compliance in different     domains.</p> </li> </ul> <p>Data Minimalism</p> <ul> <li> <p>Organizations have been acclimatizing to these regulations.</p> </li> <li> <p>TrustArc's\u00a0Global Privacy Benchmarks Survey 2021\u00a0found that the     number of enterprises with a dedicated privacy office is increasing:     83% of respondents in the survey had a privacy office, whereas the     rate was only 67% in 2020. 85% had a strategic and reportable     privacy management program in place, yet 73% of them believed that     they could do more to protect privacy.</p> </li> <li> <p>Their eagerness is hardly surprising as 34% of the respondents     claimed that they had faced a data breach in the previous 3 years,     the\u00a0costly consequences of which was mentioned previously Here.</p> </li> <li> <p>A privacy\u00a0office would be led by a\u00a0data protection     officer\u00a0(DPO) who is responsible for the company's\u00a0Data     Protection Impact Assessment\u00a0(DPIA) in order to comply with     regulations such as GDPR that demand accountability and     documentation of personal data handling.</p> </li> <li> <p>DPOs are also responsible for monitoring and ensuring that personal     data is treated by their organizations in compliance with the law,     and the top management and board are supposed to provide necessary     support and resources to DPOs to allow them to complete their task.</p> </li> <li> <p>In the face of GDPR, the current trend in data protection is     shifting toward\u00a0data minimalism.</p> </li> <li> <p>Data minimalism in this context does not necessarily encourage     minimization of the size\u00a0of data; it pertains more directly to     minimizing PII factors in data so that individuals cannot be     identified with its data points.</p> </li> <li> <p>Therefore, data minimalism affects AI sectors in their ability to     create a high-performing AI application because a shortage in data     variety for the ML process simply generates ML model biases with     unsatisfying performance in prediction.</p> </li> <li> <p>The abundance mindset for big data introduced at the beginning of     the section has thus been disciplined by the public concern over     data privacy. The risk of being fined for violating data protection     regulations, coupled with the wasteful cost of having a data     graveyard, calls for practicing data minimalism rather than data     abundance.</p> </li> <li> <p>That is why FL is becoming a\u00a0must-have\u00a0solution for many AI     solution providers such as medical sectors that are struggling with     public concerns and data privacy, which basically becomes\u00a0an issue     when a third-party entity needs to collect private data for     improving the quality of ML models and their applications.</p> </li> <li> <p>As mentioned, FL is a promising framework for privacy-preserving AI     because learning of the data can happen anywhere; even if the data     is not available for the AI service providers, all we have to do is     collect and aggregate trained ML models in a consistent way.</p> </li> <li> <p>Now, let's consider another facet of the Triple-A mindset for big     data being challenged: acceptance of messy data.</p> </li> </ul> <p>Data Volume, Training Data and Model Bias</p> <ul> <li> <p>Does the sheer volume of big data truly annihilate the treacherous     reality of\u00a0garbage in, garbage out?</p> </li> <li> <p>In\u00a0fact, the messiness of data can only be accepted if\u00a0enough data     from a variety of sources and distributions can be fully learned     without causing any biases in the outcomes of the learning.</p> </li> <li> <p>The actual training of the big data in a centralized location does     take a lot of time and huge computational resources and storage.</p> </li> <li> <p>Also, we would probably have to find methods to measure and reduce     model bias without directly collecting and accessing sensitive and     private data, which would conflict with some of the privacy     regulations discussed previously.</p> </li> <li> <p>FL also has an aspect of distributed and collaborative learning,     which becomes critical to eliminate data and model bias to absorb     the messiness of the data.</p> </li> <li> <p>With collaborative and distributed learning, we could significantly     increase the data accessibility and efficiency of an entire learning     process that is often very expensive and time-consuming.</p> </li> <li> <p>It gives us a chance to break through the limitation that big data     training used to have, as discussed in the following sections.</p> </li> </ul> <p>Training Costs</p> <ul> <li> <p>According to the report:     https://www.flexera.com/blog/cloud/cloud-computing-trends-2022-state-of-the-cloud-report,     37% of enterprises annually spend more than $12 million and 80%     spend over $1.2 million\u00a0per year for public cloud.</p> </li> <li> <p>The training cost over the cloud is not cheap, and it can easily be     assumed that this cost is going to boost significantly, together     with the increasing demand for AI and ML.</p> </li> <li> <p>Sometimes, big data\u00a0cannot be fully trained for ML because\u00a0of the     following issues:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Big data storage: Big data storage is an architecture for     compute and storage that collects and manages large amounts of     datasets for AI applications or real-time analytics. Worldwide     enterprise companies are paying more than $100 billion just for     cloud storage and data center costs     (https://a16z.com/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-repatriation-optimization/).     While some of the datasets are critical for the\u00a0applications they     provide, what they really want is often\u00a0business     intelligence\u00a0that can be extracted from the data, not just the     data itself.</p> <ul> <li> <p>Significant training time: Building and training an ML model     that can be delivered as an authentic product basically takes a     significant amount of time, not only for the training process but     also for the preparation of the ML pipelines. Therefore, in many     cases, the true value of the intelligence is going to be lost by the     time the ML model is delivered.</p> </li> <li> <p>Huge computation: Training of an ML model often consumes     significant computational resources. For example, an ML task of     manipulating pieces such as a Rubik's Cube using a robotic hand     could sometimes require more than 1,000 computers. It could also     take a dozen machines just to run some specialized graphics chips     for several months.</p> </li> <li> <p>Communications latency: To form big data, especially in the     cloud, a significant amount of data needs to be transferred to the     server, which in itself causes communications latency. In most use     cases, FL requires much less data to be transferred from local     devices or learning environments to a server called an aggregator     that is there to synthesize the local ML models collected from those     devices.</p> </li> <li> <p>Scalability: In traditional centralized systems, scalability     becomes an issue because of the complexity of big data and its     costly infrastructures such as huge storage and computing resources     in the cloud server environment. In an FL server, only an     aggregation is conducted to synthesize the multiple local models     that have been trained to update the global model. Therefore, both     the system and\u00a0learning scalability increase significantly as ML     training is conducted on edge devices in a distributed manner, not     only in a single centralized learning server.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   FL effectively utilizes distributed computational resources that can     be used for light training of the ML models.</p> <ul> <li> <p>Whether training happens on actual physical devices or virtual     instances of the cloud system, parallelizing the model training     process into distributed environments often accelerates the speed of     learning itself.</p> </li> <li> <p>In addition, once the trained\u00a0models are collected, the FL system     can quickly synthesize them to generate an updated ML model called a     global model that absorbs enough learnings at the edge sides, and     thus delivering the intelligence in near real time is possible.</p> </li> </ul> <p>Model bias and training data</p> <ul> <li> <p>ML bias happens when an ML\u00a0algorithm generates results that are     systemically prejudiced because\u00a0of erroneous assumptions in\u00a0the ML     process. ML bias is also\u00a0sometimes called algorithm bias or AI bias.</p> </li> <li> <p>Yann LeCun, the 2018 Turing Award winner for his outstanding     contribution to the development of DL, says \"ML systems are biased     when data is biased\"     ([https://twitter.com/ylecun/status/1274782757907030016]{.underline}).</p> </li> <li> <p>This comes from a\u00a0computer vision\u00a0(CV) model trained with     the\u00a0Flickr-Faces-HQ\u00a0dataset compiled by the Nvidia team.</p> </li> <li> <p>Based on the face upsampling system, many people are classified     as\u00a0white as the network was pre-trained on\u00a0Flickr-Faces-HQ\u00a0data     mainly containing pictures of white people.</p> </li> <li> <p>For this problem of misclassification of the people, the     architecture of the model is not the issue that mandates this     output.</p> </li> <li> <p>Hence, the conclusion is that a racially skewed dataset generated a     neutral model to produce biased outcomes.</p> </li> <li> <p>Productive conversations about AI and ML biases have been led by the     former lead of AI Ethics at Google.</p> </li> <li> <p>The 2018 publication of the\u00a0Gender Shades\u00a0paper demonstrated race     and gender bias in major facial recognition models, and lawmakers in     Congress have sought to prohibit the use of the technology by the US     federal government.</p> </li> <li> <p>Tech companies including Amazon, IBM, and Microsoft also agreed to     suspend or terminate sales of facial recognition models to the     police.</p> </li> <li> <p>They are encouraged to use an interventionist approach to data     collection by advising scientists and engineers to specify the     objectives of model development, form a strict policy for data     collection, and conduct a thorough appraisal of collected data to     avoid biases---details are available on the\u00a0FATE/CV\u00a0website     (https://sites.google.com/view/fatecv-tutorial/home).</p> </li> <li> <p>FL could be one of the most promising ML technologies to overcome     data-silo issues.</p> </li> <li> <p>Very often, the data is not even be accessible or usable for the     training, causing a significant bias in data\u00a0and models.</p> </li> <li> <p>Naturally, FL is useful for overcoming bias by\u00a0resolving the issues     of data privacy and silos that become the bottleneck to     fundamentally avoiding data bias.</p> </li> <li> <p>In this context, FL is becoming a breakthrough in the implementation     of big data services and applications, as thoroughly investigated     in\u00a0[https://arxiv.org/pdf/2110.04160.pdf]{.underline}.</p> </li> <li> <p>Also, there are several techniques that try to mitigate model bias     in FL itself, such as\u00a0Reweighing\u00a0and\u00a0Prejudice Remover, both     detailed     in\u00a0[https://arxiv.org/pdf/2012.02447.pdf]{.underline}.</p> </li> </ul> <p>Model drift &amp; performance degradation</p> <ul> <li> <p>Model drift\u00a0is generally\u00a0about the degradation of ML model     performance because of\u00a0changes in data and relationships     between\u00a0input and output\u00a0(I/O) variables, known as\u00a0model     decay, as well.</p> </li> <li> <p>Model\u00a0drift can be addressed by continuous learning to adapt to     the\u00a0latest changes in datasets or environments in near real time.</p> </li> <li> <p>One of the important aspects of FL is realizing a continuous     learning framework by updating an ML model instantly whenever the     learning happens in the local distributed environment anytime, in a     consistent manner.</p> </li> <li> <p>That way, FL could resolve the situation often seen in enterprise AI     applications where the intelligence is useless by the time it is     delivered for production.</p> </li> <li> <p>We will now touch\u00a0on how models could get degraded or stop working,     and then some of the current efforts of\u00a0model     operations\u00a0(ModelOps) to continuously improve the performance     of models and achieve sustainable AI operations.</p> </li> </ul> <p>How models can stop working</p> <ul> <li> <p>Any AI and ML model with\u00a0fixed parameters, or\u00a0weights, generated     from the training data and adjusted to the test data can perform     fairly well when deployed in an environment where the model receives     data similar to the training and test data.</p> </li> <li> <p>If an autonomous driving model is well trained with data recorded     during sunny daytime, the model can drive vehicles safely on sunny     days because it is doing what it has been trained to do.</p> </li> <li> <p>On a rainy night, however, nobody should be in or near the vehicle     if it is autonomously driven: the model is fed with totally     unfamiliar, dark, and blurry images; its decisions will not be     reliable at all. In such a situation, the\u00a0model's decision will be     far off the track, hence the name\u00a0model drift.</p> </li> <li> <p>Again, model drift is not likely to happen\u00a0if the model is deployed     in an environment similar to the training and testing environment     and if the environment does not change significantly over time.</p> </li> <li> <p>But in many business situations, that assumption does not always     hold, and model drift becomes a serious issue.</p> </li> <li> <p>There are\u00a0two types of model drifts:\u00a0data drift\u00a0and\u00a0concept     drift.</p> </li> <li> <p>Data drift happens when input\u00a0data to a deployed model is     significantly different from the data the\u00a0model has been trained     with. In other words, changes in\u00a0data distribution\u00a0are the cause     of data drift. The aforementioned diurnal autonomous vehicle model     not performing well in the nighttime is an example of data drift.     Another example would be an ice-cream sale prediction model trained     in California being deployed in New Zealand; seasonality in the     southern hemisphere is opposite to that in the northern hemisphere,     and the estimated sales of ice cream will be low for\u00a0summer\u00a0and     high for\u00a0winter, on the contrary to the actual sales volume.</p> </li> <li> <p>Concept drift, on the other hand, is a result of changes in how     variables correlate with each other. In the terminology of     statistics, this implies that the\u00a0data-generating process\u00a0has been     altered. And this is what\u00a0Google Flu Trends (GFT)\u00a0suffered from,     as the author of\u00a0The Undercover Economist\u00a0put it in the     following\u00a0Financial Times\u00a0article:     https://www.ft.com/content/21a6e7d8-b479-11e3-a09a-00144feabdc0#axzz30qfdzLCB.</p> </li> <li> <p>Prior to the period, search queries were meaningfully correlated     with the spread of flu as mainly people who suspected that they were     infected typed those words in the browser, and therefore the model     worked successfully.</p> </li> <li> <p>This may no longer have been the case in 2013 since people in other     categories, such as those who were precocious about a potential     pandemic or those who were just curious, were searching for\u00a0those     words, and they may have been led to do so by Google's     recommendations.</p> </li> <li> <p>This concept drift likely made GFT overestimate the spread vis-\u00e0-vis     medical reports provided by the\u00a0Centers for Disease Control and     Prevention\u00a0(CDC).</p> </li> <li> <p>Either by data or by concept, model drift causes model performance     degradation, and it occurs because of our focus on correlation.</p> </li> <li> <p>The\u00a0ground truth\u00a0in data science parlance does not mean something     like the universal truth in hard science such as physics and     chemistry---that is, causation.</p> </li> <li> <p>It is merely a true statement about how variables in given data     correlate with each other in a particular environment, and it     provides no guarantee that the correlation holds when the     environment changes or differs.</p> </li> <li> <p>This is to say that what we estimate as the\u00a0ground truth\u00a0can vary     over time and locations, just like the\u00a0ground\u00a0has been reshaped by     seismic events throughout history and geography.</p> </li> </ul> <p>Continuous monitoring</p> <ul> <li> <p>In a survey commissioned by Redis Labs     (https://venturebeat.com/business/redis-survey-finds-ai-is-stressing-it-infrastructure-to-breaking-point/),     about half of the respondents cited model reliability (48%), model     performance (44%), accuracy over time (57%), and latency of running     the model (51%) as the top challenges for getting models deployed.</p> </li> <li> <p>Given the risk and concern of model drift, AI and ML model     stakeholders need to\u00a0work on two additional tasks after deployment.     First, model performance must be continuously monitored to detect     model drift. Both data drift and concept drift can take place     gradually or suddenly. Once model drift is detected, the model needs     to be retrained with new training data, and when concept drift     occurs, even the use of a new model architecture may be necessary to     upgrade the model.</p> </li> <li> <p>In order to address\u00a0these requirements, a new ML principle     called\u00a0Continuous Delivery for Machine Learning\u00a0(CD4ML) has     been proposed.</p> </li> <li> <p>In the framework of CD4ML,</p> <ul> <li> <p>A model is coded and trained with training data in the first     step.</p> </li> <li> <p>The model is then tested with a separate dataset and evaluated     based on some metrics, and more often than not, the best model     is selected from multiple candidates.</p> </li> <li> <p>Next, the selected model is productionized with a further test     to make sure that the model performs well after the deployment,     and once it passes the test, it is deployed.</p> </li> <li> <p>Here, the monitoring process starts.</p> </li> <li> <p>When model drift is observed, the model will be retrained with     new data or given a new architecture, depending on the severity     of the drift.</p> </li> <li> <p>If you are familiar with software\u00a0engineering, you might have     noticed that CD4ML is the adoption of\u00a0continuous     integration/continuous delivery\u00a0(CI/CD) in the field of     ML.</p> </li> <li> <p>In a similar vein,\u00a0ModelOps, an AI and ML\u00a0operational     framework stemming from     the\u00a0development-operations\u00a0(DevOps) software engineering     framework is gaining popularity.</p> </li> <li> <p>ModelOps bridges\u00a0ML operations\u00a0(MLOps: the integration     of\u00a0data engineering and data science) and application     engineering; it can be seen as the enabler of CD4ML.</p> </li> </ul> </li> <li> <p>The third factor of the\u00a0Triple-A mindset for big data\u00a0lets us     focus on correlation and has helped in building AI and ML models     rapidly over the last decade.</p> </li> <li> <p>Finding correlation is much easier than discovering causation.</p> </li> <li> <p>For many AI and ML models that have been telling us what we need to     know from people's Google search patterns over years, we have to     check if it still works today. And so do we tomorrow.</p> </li> <li> <p>That is why FL is one of the important approaches for continuous     learning. When creating and operating an FL system, it is also     important to develop the system with ModelOps functionalities,     as\u00a0the critical role of FL is to keep improving models constantly     from various learning environments in a collaborative manner.</p> </li> <li> <p>It is even possible to realize a\u00a0crowdsourced learning\u00a0framework     with FL so that people in the platform can take the\u00a0desired ML model     to adapt and train it locally and return an updated model to the FL     server with an aggregator.</p> </li> <li> <p>With an advanced model aggregation framework to filter out poisonous     ML models that could potentially degrade the current models, FL can     consistently integrate other learnings, and thus realize a     sustainable continuous learning operation that is key for the     platform with ModelOps functionalities.</p> </li> </ul> <p>The FL Solution Approach</p> <ul> <li> <p>We confirmed that big data has issues to be addressed.</p> </li> <li> <p>Data privacy must be\u00a0preserved in order to protect not only     individuals but also data users who would face risks of data     breaches and subsequent fines.</p> </li> <li> <p>Biases in a set of big data can affect inference significantly     through proxies, even when factors about gender and race are     omitted, and focus on correlation rather than causation makes     predictive models vulnerable to model drift.</p> </li> <li> <p>Let us discuss\u00a0the difference between a\u00a0traditional big data ML     system and an FL system in terms of their architectures, processes,     issues, and benefits.</p> </li> <li> <p>The following diagram depicts a visual comparison between a     traditional big data ML system and an FL system:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.716666666666667in\"}</p> <p>Figure 1.3 -- Comparison between traditional big data ML system and FL system</p> <ul> <li> <p>In the traditional\u00a0big data system, data is\u00a0gathered to create large     data stores. These large data stores are used to solve a specific     problem using ML. The resulting model displays strong     generalizability due to the volume of data it is trained on and is     eventually deployed.</p> </li> <li> <p>However, continuous data collection uses large amounts of     communication bandwidth. In privacy-focused applications, the     transmission of data may be banned entirely, making model creation     impossible. Training large ML models on big data stores is     computationally expensive, and traditional centralized training     efficiency is limited by single-machine performance. Slow training     processes lead to long delays between incremental model updates,     leading to a lack of flexibility in accommodating new data trends.</p> </li> <li> <p>On the other hand, in an FL system, ML training is performed     directly at the location of the data. The resulting\u00a0trained models     are collected at the central server. Aggregation algorithms are used     to produce an aggregated model from the collected models. The     aggregated model is sent back to the data locations for further     training.</p> </li> <li> <p>FL approaches often incur overhead to set up and maintain training     performance with distributed-system settings. However, even with a     bit more complicated architecture and settings, there are benefits     that exceed its complexity drawbacks.</p> </li> <li> <p>Training is performed at the data location, so data is never     transmitted, maintaining data privacy.</p> </li> <li> <p>Training can be performed asynchronously across a variable number of     nodes, which results in efficient and easily scalable distributed     learning.</p> </li> <li> <p>Only model weights are transmitted between server and nodes, thus FL     is efficient in communication.</p> </li> <li> <p>Advanced aggregation algorithms can maintain training performance     even in restricted scenarios and increase efficiency in standard ML     scenarios too.</p> </li> <li> <p>The vast majority of all AI projects do not seem to be delivered, or     simply fall short altogether.</p> </li> <li> <p>To deliver an authentic AI application and product, all the issues     discussed previously need to be considered seriously. It is obvious     that FL, together with other key\u00a0technologies to deal with local     data processed by the ML pipeline and engine, is getting to be a     critical solution to resolve data-related problems in a continuous     and collaborative manner.</p> </li> <li> <p>How can we harness the power of AI and ML to optimize the technical     system for society in its entirety---that is, bring about a more     joyous, comfortable, convenient, and safe world while being data     minimalistic and ethical, as well as delivering improvements     continuously?</p> </li> <li> <p>We contend that the key is a\u00a0collective     intelligence\u00a0or\u00a0intelligence-centric\u00a0platform.</p> </li> <li> <p>In subsequent sections, we introduce the concept, design, and     implementation of an FL system as a promising technology for     orchestrating collective intelligence with networks of AI and ML     models to fulfill those requirements discussed so far.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>This section provided an overview of how FL could potentially solve     many of the big data issues by first understanding the definition of     big data and its nature, involving an abundance of observations,     acceptance of messiness, and ambivalence of causality.</p> </li> <li> <p>We have learned about privacy regulations in a variety of forms from     many regions and the risk of data breaches and privacy violations     that eventually lead to loss of profits, as well as a bottleneck in     creating authentic AI applications. Federated learning, by design,     will not collect any raw data and can preserve data privacy and     follow those regulations.</p> </li> <li> <p>In addition, with an FL framework, we can reduce inherent bias that     affects the performance of ML models and minimize model drift with a     continuous learning framework. Thus, a distributed and collaborative     learning framework such as FL is required for a more cost-effective     and efficient approach based on FL.</p> </li> <li> <p>This introductory section concluded with the potential of FL as a     primary solution for the aforementioned big data problems based on     the paradigm-shifting idea of collective intelligence that could     potentially replace the current mainstream data-centric platforms.</p> </li> <li> <p>In the next section, we will see where in the landscape of data     science FL fits and how it can open a new era of ML.</p> </li> </ul> <p>[Section 2: All about FL __________________]{.underline}</p> <p>What Is Federated Learning?</p> <ul> <li> <p>This section frames\u00a0federated learning\u00a0(FL) as the answer to     the desire for this new ML approach.</p> </li> <li> <p>In a nutshell, FL is an approach to ML that allows models to be     trained in parallel across data sources without the transmission of     any data.</p> </li> <li> <p>The goal of This section is to build up the case for the FL     approach, with explanations of the necessary conceptual building     blocks in order to ensure that you can achieve a similar     understanding of the technical aspects and practical usage of FL.</p> </li> <li> <p>After going through this section, you should have a high-level     understanding of the FL process and should be able to visualize     where the approach slots into real-world problem domains.</p> </li> </ul> <p>Topics covered:</p> <ul> <li> <p>Understanding the current state of ML</p> </li> <li> <p>Distributed learning nature -- toward scalable AI</p> </li> <li> <p>Understanding FL</p> </li> <li> <p>FL system considerations</p> </li> </ul> <p>ML - Current State</p> <ul> <li>To understand why the\u00a0benefits derived from the application of FL     can outweigh the increased complexity of this approach, it is     necessary to understand how ML is currently practiced and the     associated limitations. The goal of this section is to provide you     with this context.</li> </ul> <p>The Model</p> <ul> <li> <p>The term \"model\" finds usage across numerous different disciplines;     however, the generalized definition we are interested in can be     narrowed down to a working representation of the dynamics within     some desired system.</p> </li> <li> <p>Simply put, we develop a model B of some\u00a0phenomenon A as a means of     better understanding A through the increased interaction offered by     B.</p> </li> <li> <p>Consider the phenomenon of an object being dropped from some point     in a vacuum.</p> </li> <li> <p>Using kinematic equations, we can compute exactly how long it will     take for the object to hit the ground -- this is a model of the     aforementioned phenomenon.</p> </li> <li> <p>The power of this approach is the ability to observe results from     the created model without having to explicitly interact with the     phenomenon in question.</p> </li> <li> <p>For example, the model of the falling object allows us to determine     the difference in fall time between a 10 kg object and a 50 kg     object at some height without having to physically drop real objects     from said height in a real vacuum.</p> </li> <li> <p>Evidently, the modeling of natural phenomena plays a key role in     being able to claim a true understanding of said phenomena.</p> </li> <li> <p>Removing the need for the comprehensive observation of a phenomenon     allows for true generalization in the decision-making process.</p> </li> <li> <p>The concept of a model is greatly narrowed down within the context     of computer science. In this context, models are algorithms that     allow for some key values of a phenomenon to be output given some     initial characterization of the phenomenon in question.</p> </li> <li> <p>Going back to the falling object example, a computer science model     could entail the computation of values such as the time to hit the     ground and the maximum speed given the mass of the object and the     height from which it is dropped.</p> </li> <li> <p>These computer science models are uniquely powerful due to the     superhuman ability of computers to compute the output from countless     starting phenomenon configurations, offering us even greater     understanding and generalization.</p> </li> <li> <p>So, how do we\u00a0create such models?</p> </li> <li> <p>The first and simplest approach is building rule-based systems     or\u00a0white-box\u00a0models.</p> </li> <li> <p>A white-box (also known as\u00a0glass-box or clear-box) model is made\u00a0by     writing down the underlying functions of a system of interest     explicitly.</p> </li> <li> <p>This is only possible when information about the system is     available\u00a0a priori.</p> </li> <li> <p>Naturally, in this case, the underlying functions are relatively     simple.</p> </li> <li> <p>One such example is the problem of classifying a randomly selected     integer as odd or even; we can easily write an algorithm to do this     by\u00a0checking the remainder after dividing the integer by two.</p> </li> <li> <p>If you want to see how much it costs to fill up your gas tank, given     how empty the tank is and the price per gallon, you can just     multiply those values together.</p> </li> <li> <p>Despite their simplicity, these examples illustrate that simple     models can have a lot of practical applications in various fields.</p> </li> <li> <p>Unfortunately, the white-box modeling of underlying functions can     quickly become too complex to perform directly. In general, systems     are often too complex for us to be able to construct a\u00a0white-box     model for.</p> </li> <li> <p>For example, let's say you want to predict the future values of your     property.</p> </li> <li> <p>You have a lot of metrics about the property, such as the area, how     old it is, its location, and interest rate to name but a few.</p> </li> <li> <p>You believe that there is likely a linear relationship between the     property value and all of those metrics, such that the weighted sum     of all of them would give you the property value.</p> </li> <li> <p>Now, if you actually try to build a white-box model under that     assumption, you will have to directly figure out what the parameter     (weight) for each metric is, which implies that you must know the     underlying function of the real\u00a0estate pricing system. Usually, this     is not the case.</p> </li> <li> <p>Therefore, we need another approach:\u00a0black box\u00a0modeling.</p> </li> </ul> <p>Automating Model Creation</p> <ul> <li> <p>The concept of a black box\u00a0system was first developed in the field     of electric circuits during the WWII period.</p> </li> <li> <p>It was the famous cybernetician Norbert Wiener who began treating     the black box as an abstract concept, and a general theory was     established by Mario Augusto Bunge in the 1960s.</p> </li> <li> <p>The function for estimating future property values, as illustrated     earlier, is a good example of a black box.</p> </li> <li> <p>As you might expect, the function is complex enough that it is     unreasonable for us to try to write a white-box model to represent     it.</p> </li> <li> <p>This is where ML comes in, allowing us to create a model as a black     box.</p> </li> </ul> <p>Reference</p> <ul> <li> <p>You might be aware that black box modeling has been criticized for     its lack of interpretability, an important concept outside the scope     of this tutorial; Interpretable Machine Learning covers this area.</p> </li> <li> <p>ML is a type of artificial intelligence that is used to     automatically generate model parameters for making decisions and     predictions.\u00a0</p> </li> <li> <p>Figure 2.1\u00a0illustrates this in a very simple way: those cases     where the known values and the unknown value have a linear     relationship allow a popular algorithm, called\u00a0ordinary least     squares\u00a0(OLS), to be applied. OLS computes\u00a0the unknown     parameters of the linear relationship by finding the set of     parameters that produces the closest predictions on some set of     known examples (pairs of input feature value sets and the true     output value):</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.7034722222222225in\"}</p> <p>Figure 2.1 -- ML determining model parameters</p> <ul> <li> <p>The preceding diagram\u00a0displays a simple two-dimensional linear     regression problem with one feature/input variable and one output     variable. In this toy two-dimensional case, it might be relatively     straightforward for us to come up with the parameters representing     the best-fit relationship directly, either through implicit     knowledge or through testing different values.</p> </li> <li> <p>However, it should be clear that this approach quickly becomes     intractable as the number of feature variables increases.</p> </li> <li> <p>OLS allows us to attack this problem from the reverse direction:     instead of producing linear relationships and evaluating them on the     data, we can use the data to compute the parameters of the best-fit     relationship directly instead.</p> </li> <li> <p>Revisiting the real estate problem, let's assume that we have     collected a large number of property valuation data points,     consisting of the associated metric values and the sale price.</p> </li> <li> <p>We can apply OLS to take these points and find the relationship     between each metric and the sale\u00a0price for any property (still under     the assumption that the true relationship is linear).</p> </li> <li> <p>From this, we can pass in the metric values of our property and get     the predicted sale price.</p> </li> <li> <p>The power of this approach is the abstraction of this relationship     computation from any implicit knowledge of the problem.</p> </li> <li> <p>The OLS algorithm doesn't care what the data represents -- it just     finds the best line for the data it is given.</p> </li> <li> <p>This class of approaches is exactly what ML entails, granting the     power to create models of phenomena without any required knowledge     of the internal relationship, given a sufficient amount of data.</p> </li> <li> <p>In a nutshell, ML lets us program algorithms that can learn to     create models from data, and our motivation to do so is to     approximate complex systems.</p> </li> <li> <p>It is important to keep in mind that the underlying functions of a     complex system can change over time due to outside factors, quickly     making models created from old data obsolete.</p> </li> <li> <p>For example, the preceding linear regression model might not work to     estimate property values in a far distant future or a faraway     district.</p> </li> <li> <p>Variance in such a macroscopic scale is not taken into account in a     model containing only a few dozen parameters, and we would need     different models for separate groups of adjacent data points --     unless we employ even more sophisticated ML approaches such     as\u00a0deep learning.</p> </li> </ul> <p>Deep Learning (DL)</p> <ul> <li> <p>So, how did deep learning\u00a0become synonymous with ML in common usage?</p> </li> <li> <p>Deep learning\u00a0involves the application of a\u00a0deep neural     network\u00a0(DNN), which is a type of highly-parameterized model     inspired by the transmission of signals between neurons in the     brain.</p> </li> <li> <p>The foundation of deep learning was established in the early 1960s     by Frank Rosenblatt, who is known as the\u00a0father of deep learning.</p> </li> <li> <p>His work was further developed in the 1970s and 1980s by computer     scientists including Geoffrey Hinton, Yann LeCun, and Yoshua Bengio,     and the term\u00a0deep learning\u00a0was popularized by the University of     California, Irvine's distinguished Professor Rina Dechter. Deep     learning can conduct much more complex tasks compared to simpler ML     algorithms such as linear regression.</p> </li> <li> <p>While the specifics are beyond the scope of this book, the key     problem that deep learning was able to solve was the modeling of     complex non-linear relationships, pushing ML as a whole to the     forefront of numerous fields due to the increased modeling ability     it provided.</p> </li> <li> <p>This ability has been\u00a0mathematically proven via specific universal     approximation theorems for different model size cases.</p> </li> <li> <p>Over the past decade, ever-increasingly powerful models have been     built by tech giants against the backdrop of big data.</p> </li> <li> <p>If we look at the state-of-the-art deep learning models today, they     could have up to trillions of parameters; expectedly, this gives     them unparalleled flexibility in modeling complex functions.</p> </li> <li> <p>The reason deep learning models can be scaled up to arbitrarily     increase performance, unlike other ML model types used previously,     is due to\u00a0a phenomenon called\u00a0double descent.</p> </li> <li> <p>This refers to the ability for a certain parameterization/training     threshold to overcome the standard bias-variance trade-off (where     increasing complexity leads to fine-tuning on training data,     reducing bias but increasing variance) and continuing to increase     performance.</p> </li> <li> <p>The key takeaway is that the performance of deep learning models can     be considered to be limited by just the available compute power and     data, two factors that have surged in growth in the past 10 years     due to advances in computing and the ever-increasing number of     devices and software collecting data, respectively.</p> </li> <li> <p>Deep learning has become intertwined with ML, with deep learning     playing a significant role within the current state of ML and big     data.</p> </li> <li> <p>This section focused on establishing a case for the importance of     the modeling performed by current ML techniques. In a sense, this     can be considered the\u00a0what\u00a0-- what exactly FL is trying to do.</p> </li> <li> <p>Next, we will focus on the\u00a0where\u00a0in terms of the desired setting     for numerous ML applications.</p> </li> </ul> <p>Distributed Learning and Scalability</p> <ul> <li> <p>In this section, we introduce the distributed computing setting and     discuss the intersection of this setting with ML approaches to fully     establish the support for why FL is necessary.</p> </li> <li> <p>The goal of the section is for the user to understand both the     benefits and limitations imposed by the distributed computing     setting, in order to understand how FL addresses some of these     limitations.</p> </li> </ul> <p>Distributed computing</p> <ul> <li> <p>The past several years have\u00a0shown a large but predictable rise in     the development of new approaches and the conversion of existing     server infrastructure within the lens of distributed computing.</p> </li> <li> <p>To generalize further, distributed approaches themselves have     shifted more and more from research implementations to extensive use     in production settings; one significant example of this phenomenon     is the usage of cloud computing platforms such as AWS     from\u00a0Amazon,\u00a0Google Cloud Platform\u00a0(GCP) from Google, and     Azure from Microsoft. It turns out that the flexibility of     on-demand resources allows for cost-saving and efficiency in     numerous applications that would, otherwise, be bottlenecked by     on-premise servers and computational power.</p> </li> <li> <p>While a parallel cannot exactly be drawn between cloud computing and     the concept of distributed computing, the key benefits stemming from     the distributed nature are similar.</p> </li> <li> <p>At a high level, distributed computing involves spreading the work     necessary for some computational task over a number of computational     agents in a way that allows each to act near-autonomously.</p> </li> <li> <p>The following figure shows the difference between centralized and     distributed approaches in the high-level context of answering     questions:</p> </li> </ul> <p>Figure 2.2 -- Centralized versus distributed question answering</p> <ul> <li> <p>In this simple example, the centralized approach involves processing     the input questions sequentially, whereas the distributed\u00a0approach     is able to process each question at the same time. It should be     clear that the parallel approach trades off computational resource     usage for increased answering speed.</p> </li> <li> <p>The question, then, is whether this trade-off is beneficial for     real-world applications.</p> </li> </ul> <p>A Real-world example</p> <ul> <li> <p>To understand the\u00a0practical benefits of distributed computing     approaches, let's analyze an example business problem through a     traditional and a distributed computing lens.</p> </li> <li> <p>Consider an e-commerce business that is trying to host its website     using on-premise servers.</p> </li> <li> <p>The traditional way to do this would be to perform enough analysis     on the business side to determine the expected volume of traffic at     some future time and invest in one or a couple of server machines     large enough to handle that calculated volume.</p> </li> <li> <p>Several cases immediately lend themselves to showing the flaws of     such an approach.</p> </li> <li> <p>Consider a scenario where usage of the websites greatly exceeds the     initial projections.</p> </li> <li> <p>A fixed number of servers means that all upgrades must be hardware     upgrades, resulting in old hardware that had to be purchased and is     no longer used.</p> </li> <li> <p>Going further, there are no guarantees that the now-increased usage     will stay fixed. Further increases in usage will result in more     scaling-up costs, while decreases in usage will lead to wasted     resources (maintaining large servers when smaller machines would be     sufficient).</p> </li> <li> <p>A key point is that the integration of additional servers is     non-trivial due to the single-machine approach used to manage     hosting.</p> </li> <li> <p>Additionally, we have to consider the hardware limitations of     handling large numbers of requests in parallel with one or a few     machines.</p> </li> <li> <p>The ability to handle requests in parallel is limited for each     machine -- significant volumes of traffic would be almost guaranteed     to eventually be bottlenecked regardless of the power available to     each server.</p> </li> <li> <p>In comparison, consider the distributed computing-based solution for     this problem.</p> </li> <li> <p>Based on the initial business projections, a number of smaller     server machines are purchased and each is set up to handle some     fixed volume of traffic.</p> </li> <li> <p>If the scenario of incoming traffic\u00a0exceeding projects arises, no     modification to the existing machines is necessary; instead, more     similarly-sized servers can be purchased and configured to handle     their designated volume of new traffic.</p> </li> <li> <p>If the incoming traffic decreases, the equivalent number of servers     can be shut down or shifted to handle other tasks. This means that     the same hardware can be used for variable volumes of traffic.</p> </li> <li> <p>This ability to scale quickly to handle the necessary computational     task at any moment is precisely due to how distributed computing     approaches allow for computational agents to seamlessly start and     stop working on said task.</p> </li> <li> <p>In addition, the use of many smaller machines in parallel, versus     using fewer larger machines, means that the number of requests that     can be handled at the same time is notably higher. It is clear that     a distributed computing approach, in this case, lends itself to     cost-saving and flexibility that cannot be matched with more     traditional methods.</p> </li> </ul> <p>Distributed Computing - The benefits</p> <ul> <li> <p>In general, distributed computing\u00a0approaches offer three main     benefits for any computational task -- scalability, throughput, and     resilience.</p> </li> <li> <p>In the previous case of web hosting, scalability referred to the     ability to scale the number of servers deployed based on the amount     of incoming traffic, whereas throughput refers to the ability to     reduce request processing latency through the inherent parallelism     of smaller servers.</p> </li> <li> <p>In this example, resilience could refer to the ability of other     deployed servers to take on the load from a server that stops     working, allowing the hosting to continue relatively unfazed.</p> </li> <li> <p>Distributed computing often finds uses when working with large     stores of data, especially when attempting to perform analyses on     the data using a single machine would be computationally infeasible     or otherwise undesirable.</p> </li> <li> <p>In these cases, scalability allows for the deployment of a variable     number of agents based on factors such as the desired runtime and     amount of data at any given time, whereas the ability of each agent     to\u00a0autonomously work on processing a subset of the data in parallel     allows for processing throughput that would be impossible for a     single high-power machine to achieve.</p> </li> <li> <p>It turns out that this lack of reliance on cutting-edge hardware     leads to further cost savings, as hardware price-to-performance     ratios are often not linear.</p> </li> <li> <p>While the development of parallelized software to operate in a     distributed computing setting is non-trivial, hopefully, it is clear     that many practical computational tasks greatly benefit from the     scalability and throughput achieved by such approaches.</p> </li> </ul> <p>Distributed ML</p> <ul> <li> <p>When thinking about the types of computational tasks that have     proven to be valuable in practical applications\u00a0and that might be     directly benefited from increased scalability and throughput, it is     clear that the rapidly growing field of ML is near the top.</p> </li> <li> <p>In fact, we can frame ML tasks as a specific example of the     aforementioned tasks of analyzing large stores of data, placing     emphasis on the data being processed and the nature of the analysis     being performed.</p> </li> <li> <p>The joint growth of cheap computational power (for example, smart     devices) and the proven benefits of data analysis and modeling have     led to companies with both the storage of excessive amounts of data     and the desire to extract meaningful insights and predictions from     said data.</p> </li> <li> <p>The second part is exactly what ML is geared to solve, and large     amounts of work have already been completed to do so in various     domains.</p> </li> <li> <p>However, like other computational tasks, performing ML on large     stores of data often leads to a time-computational power trade-off     in which more powerful machines are needed to perform such tasks in     reasonable amounts of time.</p> </li> <li> <p>As ML algorithms become more computationally and memory-intensive,     such as recent state-of-the-art deep learning models with billions     of parameters, hardware bottlenecks make increasing the     computational power infeasible.</p> </li> <li> <p>As a result, current ML tasks must apply distributed computing     approaches to stay cutting-edge while producing results in usable     timeframes.</p> </li> </ul> <p>ML at the Edge</p> <ul> <li> <p>Although the prevalence of deep learning described earlier, besides     the paradigm shift from big data to collective\u00a0intelligence gives     enough motivation for distributed ML, its physical foundation came     from the recent development of\u00a0edge computing.</p> </li> <li> <p>The\u00a0edge\u00a0represents the close\u00a0proximity around deployed solutions;     it follows that edge computing refers to processing data at or near     the location of the data source.</p> </li> <li> <p>Extending the concept of computation to ML leads to the idea     of\u00a0edge AI, where models\u00a0are integrated directly into edge     devices.</p> </li> <li> <p>A few popular examples would be Amazon Alexa, where edge AI takes     care of speech recognition, and self-driving cars that collect     real-world data and incrementally improve with edge AI.</p> </li> <li> <p>The most ubiquitous example is the smartphone -- some potential uses     are the recommendation of content to the user, searches with voice     assistance and auto-complete, auto-sorting of pictures into an album     and gallery search, and more.</p> </li> <li> <p>To capitalize on this potential, smartphone manufacturers have     already begun integrating ML-focused processor components into the     chips they integrate with their newest phones, such as the\u00a0Neural     Processing Unit\u00a0from\u00a0Samsung\u00a0and the\u00a0Tensor Processing Unit\u00a0on     the\u00a0Google Tensor chip.</p> </li> <li> <p>Google has also worked to develop ML-focused APIs for Android     applications through their\u00a0Android ML Kit SDK.</p> </li> <li> <p>From this, it should be clear that ML applications are shifting     toward the edge computing paradigm.</p> </li> <li> <p>Let's say that smartphones need to use a deep learning model for     word recommendation. This is so that when you type words on your     phone, it gives you suggestions for the next word, with the goal     being to save you some time.</p> </li> <li> <p>In the scheme of a centralized computing process, the central server     is the only component that has access to this text prediction model     and none of the phones have the model stored locally.</p> </li> <li> <p>The central server handles all of the requests sent from the phones     to return word recommendations.</p> </li> <li> <p>As you type, your phone has to send what has been typed along with     some personal information about you, all the way to the central     server. The server receives this information, makes a prediction     using the deep learning model, and then sends the result back to the     phone.</p> </li> <li> <p>The following figure reflects this scenario:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"2.5076388888888888in\"}</p> <p>Figure 2.3 -- Centralized inference scenario</p> <ul> <li> <p>There are a few problems that become apparent when you look at this     scenario.</p> </li> <li> <p>First, even a half to one second of latency makes the recommendation     slower than typing everything yourself, making the system useless.</p> </li> <li> <p>Furthermore, if there is no internet connection, the recommendation     simply does not work.</p> </li> <li> <p>Another restriction of this scheme is the need\u00a0for the central     server to process all of these requests. Imagine how many     smartphones are being used in the world, and you will realize a lack     of feasibility due to the extreme scale of this solution.</p> </li> <li> <p>Now, let's look at the same problem from the edge computing     perspective.</p> </li> <li> <p>What if the smartphones themselves contain the deep learning model?</p> </li> <li> <p>The central server is only in charge of managing the latest trained     model and communicating this model with each phone.</p> </li> <li> <p>Now, whenever you start typing, your phone can use the received     model locally to make recommendations from what you typed. The     following figure reflects this scenario:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"2.704861111111111in\"}</p> <p>Figure 2.4 -- Edge inference scenario</p> <ul> <li> <p>This removes both the latency problem and prevents the need to     handle the incoming inference requests\u00a0at a central location.</p> </li> <li> <p>In addition, the phones no longer have to maintain a connection with     the server to make a recommendation.</p> </li> <li> <p>Each phone is in charge of fulfilling requests from its user.</p> </li> <li> <p>This is the core benefit of edge computing:\u00a0we have moved the     computing load from the central server to the edge devices/servers.</p> </li> </ul> <p>Training at the Edge</p> <ul> <li> <p>The distinction between\u00a0centralized and decentralized computing can     be extended to the concept of model training.</p> </li> <li> <p>Let's stick to the smartphone example but think about how we would     train the predictive model instead.</p> </li> <li> <p>First, in the centralized ML process, all of the data used to train     the recommendation model must be collected from the users' devices     and stored on the central server.</p> </li> <li> <p>Then, the collected data is used to train a model, which is     eventually sent to all the phones.</p> </li> <li> <p>This means that the central server still has to be able to handle     the large volume of user data coming in and store it in an efficient     way to be able to train the model.</p> </li> <li> <p>This design leads to the problems found in the centralized computing     approach: as the number of phones connected to the server increases,     the server's ability to work with the incoming data needs to scale     in order to maintain the training process. In addition, since the     data needs to be transmitted and stored centrally in this approach,     there is always the possibility of the interception of transmissions     or even attacks on the stored data.</p> </li> <li> <p>There are several cases where data confidentiality and privacy are     required or strongly\u00a0desired; for example, applications in the     financial and medical industries.\u00a0</p> </li> <li> <p>Centralized model training thus limits use cases, and an     alternative way to work with data directly on edge devices is     required. This exact setting is the motivation for FL.</p> </li> </ul> <p>FL - The technical bits</p> <ul> <li> <p>This section focuses\u00a0on providing a high-level technical     understanding of how FL actually slots in as a solution to the     problem setting described in the previous section.</p> </li> <li> <p>The goal of this section is for you to understand how FL fits as a     solution, and to provide a conceptual basis that will be filled in     by the subsequent sections.</p> </li> </ul> <p>FL Definition</p> <ul> <li> <p>Federated learning is a method to synthesize global models from     local models trained on the edge.</p> </li> <li> <p>FL was first developed by Google in 2016 for their Gboard     application, which incorporates the context of an Android user's     typing history to suggest corrections and propose candidates for     subsequent words.</p> </li> <li> <p>Indeed, this is the exact word recommendation\u00a0problem discussed in     the\u00a0Edge inference\u00a0and\u00a0Edge training\u00a0sections.</p> </li> <li> <p>The solution that Google produced was a decentralized training     approach where an iterative process would compute model training     updates at the edge, aggregating these updates to produce the global     update to be applied to the model.</p> </li> <li> <p>This core concept of aggregating model updates was key in allowing     for a single, performant model to be produced from edge training.</p> </li> <li> <p>Let's break this concept down further.</p> </li> <li> <p>The desired model is distributed across the edge and is trained on     data collected locally at the edge.</p> </li> <li> <p>Of course, we can expect that a model trained on one specific data     source is not going to be representative of the entire dataset.</p> </li> <li> <p>As a result, we dub such models trained with limited data\u00a0local     models. One\u00a0immediate benefit of this approach is the enabling of     ML on data that would otherwise not be collected in the centralized     case, due to issues with privacy and efficiency.</p> </li> <li> <p>Aggregation, the key\u00a0theoretical step of FL, allows for our desired     single\u00a0global model\u00a0to be created from the set of local models     produced at some iteration.</p> </li> <li> <p>The most well-known aggregation algorithm, popular for its     simplicity\u00a0and surprising performance, is called\u00a0federated     averaging\u00a0(FedAvg).</p> </li> <li> <p>FedAvg is performed on a set of local models by computing the     parameter-wise arithmetic mean across the models, producing an     aggregate model.</p> </li> <li> <p>It is important to understand that performing aggregation once     is not enough to produce a good global aggregate model; instead,     it is the iterative process of locally training the previous     global model and aggregating the produced local models into a     new global model that allows for global training progress to be     made.</p> </li> </ul> <p>The FL process</p> <p>To better understand FL from an iterative process perspective, we break it down into the core constituent\u00a0steps of a single iteration, or\u00a0round.</p> <p>The steps for a round can be described as follows:</p> <ol> <li> <p>The aggregate global model parameters are sent to each user's     device.</p> </li> <li> <p>The received ML models located on the user devices are trained with     local data.</p> </li> <li> <p>After a certain amount of training, the local model parameters are     sent to the central server.</p> </li> <li> <p>The central server aggregates the local models by applying an     aggregation function, producing a new aggregate global model.</p> </li> </ol> <p>These steps are depicted in\u00a0Figure 2.5:</p> <p>{width=\"6.268055555555556in\" height=\"4.667361111111111in\"}</p> <p>Figure 2.5 -- FL steps</p> <ul> <li> <p>The flow from\u00a0steps 1 to 4\u00a0constitutes\u00a0a single round of FL.</p> </li> <li> <p>The next round begins as the user servers/devices receive the newly     created aggregate model and start training on the local data.</p> </li> <li> <p>Let's revisit Google's word recommendation for Gboard.</p> </li> <li> <p>At some point in time, each phone stores a sufficient amount of its     user's typing data.</p> </li> <li> <p>The edge training process can create a local model from it, and the     parameters will be sent to the central server.</p> </li> <li> <p>After receiving parameters from a certain number of phones,     the server aggregates them to create a global model and sends it to     the phones.</p> </li> <li> <p>This way, every phone connected to the server receives a model that     reflects local data in all of the phones without ever transmitting     the data from them.</p> </li> <li> <p>In turn, each phone retrains the model when another batch of     sufficient data is collected, sends the model to the server, and     receives a new global model.</p> </li> <li> <p>This cycle repeats itself over and over according to the     configuration of the FL system, resulting in the continuous     monitoring and updating of the global model.</p> </li> <li> <p>Note that the user data never leaves the edge, only the model     parameters; nor is there a need to put all the data in a central     server to generate a global model, allowing for data minimalism.</p> </li> <li> <p>Moreover, model bias can be mitigated with FL methods, as discussed.</p> </li> <li> <p>That is why FL can be\u00a0regarded as a solution to the three issues of     big data, which were introduced earlier (Triple A).</p> </li> </ul> <p>Transfer learning</p> <ul> <li> <p>FL is closely related to an ML concept called\u00a0transfer     learning\u00a0(TL).</p> </li> <li> <p>TL allows us to use large deep learning\u00a0models that have been     trained by researchers using plentiful compute power and resources     on very generalized datasets.</p> </li> <li> <p>These\u00a0models can be applied to more specific problems.</p> </li> <li> <p>For example, we can take an object detection model trained to locate     and name specific objects in images and retrain it on a limited     dataset containing specific objects we are interested in, which were     not included in the original data.</p> </li> <li> <p>If you were to take the original data, add to it the data of those     objects of our interest, and then train a model from scratch, a lot     of computational time and power would be required.</p> </li> <li> <p>With TL, you can quicken the process by leveraging a key fact about     those existing large, generalized models.</p> </li> <li> <p>There is a tendency for the intermediate layers of large DNNs to be     excellent at extracting features, used by the following layers for     the specific ML task.</p> </li> <li> <p>We can maintain its learned ability to extract features by     preserving the parameters in those layers.</p> </li> <li> <p>In other words, parameters in certain layers of existing pre-trained     models can be preserved and used to detect new objects -- we do     not need to reinvent the wheel.</p> </li> <li> <p>This technique is called\u00a0parameter freezing.</p> </li> <li> <p>In FL, model training often takes place in local devices/servers     with limited\u00a0computational power.</p> </li> <li> <p>One example using the Gboard scenario is performing parameter     freezing on a pre trained word embedding layer to allow training to     focus on task-specific information, leveraging prior training of the     embeddings to greatly reduce the trainable parameter count.</p> </li> <li> <p>Taking this concept further, the\u00a0intersection of FL and TL is     called\u00a0federated transfer learning\u00a0(FTL).</p> </li> <li> <p>FTL allows for the FL approach to be applied in cases where the     local datasets differ in structure by performing FL on a shared     subset of the model that can later be extended for specific tasks.</p> </li> <li> <p>For example, a sentiment analysis model and a text summarization     model could both share a sentence encoding component, which can be     trained using FL and used for both tasks.</p> </li> <li> <p>TL (and, by extension, FTL) are key concepts that allow for training     efficiency and incremental improvement to be realized in FL.</p> </li> </ul> <p>Personalization</p> <ul> <li> <p>When edge devices are dealing\u00a0with data that is not\u00a0independent     and identically distributed\u00a0(IID), each device can customize     the global model.</p> </li> <li> <p>This is an idea called\u00a0personalization, which can be\u00a0considered     as fine-tuning the global model\u00a0with local data, or the strategic     use of bias in the data.</p> </li> <li> <p>For example, consider a shopping mall chain that operates in two     areas with distinct local demographics (that is, the chain deals     with non-IID data).</p> </li> <li> <p>If the chain seeks tenant recommendations for both locations using     FL, each of the locations can be better served with personalized     models than a single global model, helping attract local customers.     Since the personalized model is fine-tuned or\u00a0biased\u00a0with local     data, we can expect that its performance on general data would not     be as good as that of the global model.</p> </li> <li> <p>On the other hand, we can also expect that the personalized model     performs better than the global model on the local data for which     the model is personalized.</p> </li> <li> <p>There is a trade-off between user-specific performance and     generalizability, and the power of an FL system comes from its     flexibility to balance them according to the requirements.</p> </li> </ul> <p>Horizontal and Vertical FL</p> <ul> <li> <p>There are two\u00a0types of FL:\u00a0horizontal\u00a0or\u00a0homogeneous     FL\u00a0and\u00a0vertical\u00a0or\u00a0heterogeneous FL.</p> </li> <li> <p>Horizontal\u00a0FL, also called\u00a0sample-based FL, is applicable when     all local datasets\u00a0connected with the aggregator server have the     same features but\u00a0contain different samples.</p> </li> <li> <p>The Gboard application discussed earlier is a good example of     horizontal FL in the\u00a0form of\u00a0cross-device FL, that is, local     training taking place in edge devices.</p> </li> <li> <p>The datasets in all Android phones have identical formats but unique     contents that reflect their user's typing history.</p> </li> <li> <p>On the other hand, vertical FL, or\u00a0feature-based FL, is a\u00a0more     advanced technology that allows parties holding different features     for the same samples to cooperatively generate a global model.</p> </li> <li> <p>For example, a bank and an e-commerce company might both store the     data of residents in a city but their features would differ: the     former knows the credit and expenditure patterns of the citizens,     the latter their shopping behavior.</p> </li> <li> <p>Both of them can benefit by sharing valuable insights without     sharing customer data.</p> </li> <li> <p>First, the bank and e-commerce company can identify their common     users with a technique called\u00a0private set intersection\u00a0(PSI)     while\u00a0preserving data privacy     using\u00a0Rivest-Shamir-Adleman\u00a0(RSA) encryption.</p> </li> <li> <p>Next, each party trains a preliminary model\u00a0with local data     containing unique features.</p> </li> <li> <p>Those models are then aggregated to construct a global model.</p> </li> <li> <p>Usually, vertical FL involves multiple data silos, and when that is     the case, it is also\u00a0called\u00a0cross-silo FL.</p> </li> <li> <p>In China,\u00a0federated Ai ecosystem\u00a0(FATE) is well known for     its seminal demonstration of vertical FL involving WeBank. If you     are interested in\u00a0further conceptual details of FL, there is a very     illustrative and well-written report by Cloudera Fast Forward Labs,     at https://federated.fastforwardlabs.com/.</p> </li> <li> <p>The information on FL contained in this section should be sufficient     to understand the following parts, which examine, in further depth,     some of the key concepts introduced here.</p> </li> <li> <p>The final section aims to cover some of the auxiliary concepts     focused on the practical application of FL.</p> </li> </ul> <p>System Considerations for FL</p> <ul> <li> <p>This section mainly focuses on the multi-party\u00a0computation aspects     of FL, including theoretical security measures and full     decentralization approaches.</p> </li> <li> <p>The goal of this section is for you to be aware of some of the more     practical considerations that should be taken into account for     practical FL applications.</p> </li> </ul> <p>Security Considerations for FL</p> <ul> <li> <p>Despite the\u00a0nascency of the technology, experimental usage of FL has     emerged in a few sectors.</p> </li> <li> <p>Specifically,\u00a0anti-money laundering\u00a0(AML) in the financial     industry and\u00a0drug discovery and diagnosis in the medical industry     have seen promising results, as proofs of concepts in those fields     have been successfully conducted by companies such as Consilient and     Owkin.</p> </li> <li> <p>In AML use cases, banks can cooperate with one another to identify     fraudulent transactions efficiently without sharing their account     data; and hospitals can keep their patient data to themselves while     improving ML models for detecting health issues.</p> </li> <li> <p>These solutions exploit the power of relatively simple horizontal     cross-silo FL, as explained in the\u00a0Understanding FL\u00a0section, and     its application is spreading to other areas.</p> </li> <li> <p>For example, Edgify is a UK-based company contributing to the     automation of cashiers at retail stores in collaboration with Intel     and Hewlett Packard.</p> </li> <li> <p>In Munich, Germany, another UK-based company, Fetch.ai, is     developing a smart city infrastructure with their FL-based     technology. It is clear that the practical application of FL is     rapidly growing.</p> </li> <li> <p>Although FL can circumvent the concern over data privacy thanks to     its privacy-by-design (model parameters do not expose privacy) and     data minimalist (data is not collected in the central server)     approach, there are potential obstructions against its     implementation; one such example is\u00a0mistrust\u00a0among the     participants of an FL project.</p> </li> <li> <p>Consider a situation where\u00a0Bank A\u00a0and\u00a0Bank B\u00a0agree to use FL for     developing a collaborative AML solution.</p> </li> <li> <p>They decide on the common model architecture so that each can train     a local model with their own data and aggregate the results to     create a global model to be used by both.</p> </li> <li> <p>Na\u00efve implementations of FL might allow for one bank to reconstruct     the local model from the other bank, using their local model and the     aggregate model.</p> </li> <li> <p>From this, the bank might be able to extract key information on the     data used to train the other bank's model.</p> </li> <li> <p>As a result, there might be a dispute regarding which party should     host the server to\u00a0aggregate the local models.</p> </li> <li> <p>A possible solution is having a third party host the server and take     responsibility for model aggregation.</p> </li> <li> <p>Yet, how would\u00a0Bank A\u00a0know that the third party is not colluding     with\u00a0Bank B, and vice versa?</p> </li> <li> <p>Going further, the integration of an FL system into a     security-focused domain leads to new concerns regarding the security     and stability of each system component.</p> </li> <li> <p>Known security issues tied to different FL system approaches might     incur an additional potential weakness to adversarial attacks that     outweighs the benefits of the approach.</p> </li> <li> <p>There are several security measures to allow FL collaboration     without forcing the participants to trust one\u00a0another.</p> </li> <li> <p>With a statistical method called\u00a0differential privacy\u00a0(DP),     each participant can add random noise to their local model     parameters to prevent the ability to glean information on the     training data distribution or specific elements from the transmitted     parameters.</p> </li> <li> <p>By sampling the random noise from a symmetric distribution with zero     mean and relatively low variance (for example, Gaussian, Laplace),     the random differences added to the local models are expected to     cancel out when aggregation is performed.</p> </li> <li> <p>As a result, the global model is expected to be very similar to what     would have been generated without DP.</p> </li> <li> <p>However, there is a critical limitation to this approach; for the     sum of the added random noise to converge to zero, a sufficient     number of parties must participate in the coalition.</p> </li> <li> <p>This might not be the case for projects involving only a few banks     or hospitals, and using DP in such cases would harm the global     model's integrity.</p> </li> <li> <p>Some additional measures would be necessary, for example, each     participant sending multiple copies of their local model to increase     the number of models so that the noise will be offset.</p> </li> <li> <p>Another possibility in certain\u00a0fully-decentralized FL systems     is\u00a0secure multi-party computation\u00a0(MPC).</p> </li> <li> <p>MPC-based aggregation allows agents to communicate among themselves     and compute the aggregate model without involving a trusted     third-party server, maintaining model parameter privacy.</p> </li> <li> <p>How could the participants secure the system from outside attacks?\u00a0</p> </li> <li> <p>Homomorphic encryption\u00a0(HE), which preserves the\u00a0effects of     addition and multiplication on data across encryption, allows the     local models to be aggregated into the global model in an encrypted     form.</p> </li> <li> <p>This precludes the exposure of model parameters to outsiders who do     not possess the key for decryption.</p> </li> <li> <p>Yet, HE's effectiveness in securing the communication between     the\u00a0participants comes with a prohibitively high computational cost:     processing the operation on data with the HE algorithm can take     hundreds of trillions of times longer than otherwise!</p> </li> <li> <p>A solution to mitigate this challenge is the use of partial HE,     which is compatible with only one of the additive or multiplicative     operations across encryption; therefore it is computationally much     lighter than the fully homomorphic counterpart.</p> </li> <li> <p>Using this scheme, each participant in a coalition can encrypt and     send their local model to the aggregator, which then sums up all     local models and sends the aggregated model back to the     participants, who, in turn, decrypt the model and divide its     parameters by the number of participants to receive the global     model.</p> </li> <li> <p>Both HE and DP are essential technology for the practical     application of FL.</p> </li> <li> <p>Those interested in the implementation of FL in real-world scenarios     can learn a great deal from\u00a0Federated AI for Real-World Business     Scenarios\u00a0written by IBM Research Fellow Dinesh C. Verma.</p> </li> </ul> <p>Decentralized FL and Blockchain</p> <ul> <li> <p>The architecture of FL discussed so far is based on client-server     networks, that is, edge devices exchanging models with a central     aggregator server.</p> </li> <li> <p>Due to the issues surrounding trust between the participants of FL     coalitions discussed earlier; however, building a system with an     aggregator as a separate and central entity can be problematic.</p> </li> <li> <p>It can be difficult for the\u00a0host of an aggregator to be impartial     and unbiased toward their own data.</p> </li> <li> <p>Also, having a central server inevitably leads to a single point of     failure in the FL system, which results in low resilience.</p> </li> <li> <p>Furthermore, if the aggregator is set up in a cloud server, the     implementation of such an FL system would require a skilled DevOps     engineer, who might be difficult to find and expensive to hire.</p> </li> <li> <p>Given these concerns, Kiyoshi Nakayama co-authored an article about     the first-ever experimentation of a fully decentralized FL system     using blockchain technology     ([http://www.kiyoshi-nakayama.com/publications/BAFFLE.pdf]{.underline}).</p> </li> <li> <p>Leveraging\u00a0smart contracts\u00a0to coordinate model updates and     aggregation, a\u00a0private Ethereum network was constructed to perform     FL in a\u00a0serverless manner.</p> </li> <li> <p>The results of the experiment showed that a peer-to-peer,     decentralized FL can be much more efficient and scalable than an     aggregator-based, centralized FL.</p> </li> <li> <p>The superiority of decentralized architecture was confirmed in a     more recent experiment conducted by Hewlett Packard and German     research\u00a0institutes who gave a unique name to decentralized FL with     blockchain technology:\u00a0swarm learning.</p> </li> <li> <p>While research\u00a0and development in the field of FL are shifting to a     decentralized model, the rest of this book assumes centralized     architecture with an aggregator server. There are two reasons for     this design. First, blockchain is still a nascent technology that AI     and ML researchers are not necessarily familiar with. Incorporating     a peer-to-peer communication scheme can overcomplicate the subject     matter. And second, the logic of FL itself is independent of the     network architecture, and there is no problem with the centralized     model to illustrate how FL works.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>Here, we covered the two key developments that have resulted from     the recent growth in accessible computational power at all levels.</p> </li> <li> <p>First, we looked at the importance of models and how this has     enabled ML to grow considerably in practical usage, with increases     in computational power allowing stronger models that surpass     manually created white-box systems to continuously be produced.</p> </li> <li> <p>We called this the\u00a0what\u00a0of FL -- ML is what we are trying to     perform using FL.</p> </li> <li> <p>Then, we took a step back to look at how edge devices are reaching a     stage where complex computations can be performed within reasonable     timeframes for real-world applications, such as the text     recommendation models on our phones.</p> </li> <li> <p>We called this the\u00a0where\u00a0of FL -- the setting where we want to     perform ML.</p> </li> <li> <p>From the what and the where, we get the intersection of these two     developments -- the usage of ML models directly on edge devices.</p> </li> <li> <p>Remember that the standard central training approach for ML models     greatly suffers from the need to centrally collect all of the data     in the edge ML case, as this prevents applications requiring     efficient communication or data privacy from being possible.</p> </li> <li> <p>We showed that\u00a0FL\u00a0directly addresses this problem by performing     all training at the edge to produce\u00a0local models, at the same     location as the requisite data stores.\u00a0Aggregation\u00a0algorithms take     these local models and produce a\u00a0global model. By iteratively     switching between local training and aggregation,</p> </li> <li> <p>FL allows for the creation of a model that has effectively been     trained across all data stores without ever needing to centrally     collect the data.</p> </li> <li> <p>We concluded the part by stepping outside the theory behind     effective aggregation, looking at system and architecture design     considerations regarding aspects such as model privacy and full     decentralization.</p> </li> <li> <p>After reading, it should be clear that the current state of ML, edge     computing, and fledgling growth in practical FL applications makes     it clear that FL is poised for serious growth in the near future.</p> </li> <li> <p>In the next part, we will examine the implementation of FL from a     system-level perspective.</p> </li> </ul> <p>[Section 3: FL - The System Details_______]{.underline}</p> <p>Federated Learning Systems</p> <ul> <li> <p>This section will provide an overview of the architecture, procedure     flow, sequence of messages, and basics of model aggregation of     the\u00a0federated learning\u00a0(FL) system.</p> </li> <li> <p>As discussed earlier, the basics of the FL framework is quite easy     to understand. However, the real implementation of the FL framework     needs to come with a good understanding of both AI and distributed     systems.</p> </li> <li> <p>The content of this section is based on the most standard foundation     of FL systems, which is used in hands-on exercises later.</p> </li> <li> <p>First, we will introduce the building blocks of FL systems, such as     an aggregator with an FL server, an agent with an FL client, a     database server, and communication between these components.</p> </li> <li> <p>The architecture introduced here is designed in a decoupled way so     that further enhancement to the system will be relatively easier     than with an FL system that contains everything on one machine.</p> </li> <li> <p>Then, an explanation of the flow of the operation of FL from     initialization to aggregation will follow.</p> </li> <li> <p>Finally, we will examine the way an FL system is scaled with a     horizontal design of decentralized FL setups.</p> </li> </ul> <p>This section covers the following topics:</p> <ul> <li> <p>FL system architecture</p> </li> <li> <p>Understanding the FL system flow -- from initialization to     continuous operation</p> </li> <li> <p>Basics of model aggregation</p> </li> <li> <p>Furthering scalability with horizontal design</p> </li> </ul> <p>The System Architecture</p> <ul> <li> <p>FL systems are distributed\u00a0systems that are comprised of servers and     distributed clients.</p> </li> <li> <p>Here, we will define a representative architecture of an FL system     with the following components: an aggregator with an FL server, an     agent with an FL client, and a database:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Cluster aggregator\u00a0(or\u00a0aggregator): A\u00a0system\u00a0with an FL     server that collects and\u00a0aggregates\u00a0machine learning\u00a0(ML)     models that are trained at multiple distributed agents (defined     shortly) and creates global ML models that are sent back to the     agents. This system serves as a\u00a0cluster aggregator, or more     simply, an\u00a0aggregator\u00a0of FL systems.</p> <ul> <li> <p>Distributed agent\u00a0(or\u00a0agent): A\u00a0distributed     learning\u00a0environment with an FL client such as a local edge device,     mobile application, tablet, or any distributed cloud environment     where ML models are trained in a distributed manner and sent to an     aggregator. The agent can be connected to an FL server of the     aggregator through the FL client-side communications module. The FL     client-side codes contain a collection of libraries that can be     integrated into the local ML application, which is designed and     implemented by individual ML engineers and data scientists.</p> </li> <li> <p>Database server\u00a0(or\u00a0database): A database and its server to     store the data related to the aggregators, agents, and global and     local ML models and their performance metrics. The database server     handles the incoming queries from the aggregators and sends the     necessary data back to the aggregators. Agents do not have to be     connected to the database server directly for the simplicity of the     FL system design.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Figure 3.1\u00a0shows the typical overall architecture consisting of a     single cluster aggregator and a database server, as well as multiple     distributed agents:</p> <p>{width=\"6.268055555555556in\" height=\"4.414583333333334in\"}</p> <p>Figure 3.1 -- Overall architecture of an FL system</p> <ul> <li> <p>One advantage\u00a0of the FL system's architecture is that users do not     have to send private raw data to the server, especially that owned     by a third party. Instead, they only have to send locally trained     models to the aggregator.</p> </li> <li> <p>The locally trained models can be in a variety of formats such as     the weights of the entire ML models, the changes of weights     (gradients), or even a subset of them.</p> </li> <li> <p>Another advantage includes reducing the communication load because     the users only have to exchange models that are usually much lighter     than raw data.</p> </li> </ul> <p>The Cluster aggregators</p> <ul> <li> <p>A cluster aggregator\u00a0consists\u00a0of an FL server module, FL state     manager module, and model aggregation module, as in\u00a0Figure 3.1.</p> </li> <li> <p>We just call a cluster aggregator with an FL server an aggregator.</p> </li> <li> <p>While these modules are the foundation of the aggregator, advanced     modules can be added to ensure further security and flexibility of     the aggregation of ML models.</p> </li> <li> <p>Some of the advanced modules are not\u00a0implemented in     the\u00a0simple-fl\u00a0GitHub repository provided with exercises because the     main purpose of this effort is to understand the basic structure and     system flow of the FL system.</p> </li> <li> <p>In the aggregator system, the following modules related to the FL     server, the state manager of FL, and model aggregation are the keys     to implementing the aggregator-side functionalities.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   FL server module: There\u00a0are three primary\u00a0functionalities for     the FL server module, which include the communication handler,     system configuration handler, and model synthesis routine:</p> <pre><code>-   **Communication handler**: Serves\u00a0as a module of the aggregator\n    that supports\u00a0*communications with agents and the database*.\n    Usually, this module accepts polling messages from agents and\n    sends responses back to them. The types of messages they receive\n    include the registration of agents themselves with secure\n    credentials and authentication mechanisms, the initialization of\n    the ML model that serves as an\u00a0*initial model*\u00a0for the future\n    aggregation process, confirmation about whether or not agents\n    participate in a round, and local ML models that are retrained\n    at distributed agents such as mobile devices and local edge\n    machines. The communication handler can also query the database\n    server in order to access the system data and ML models in the\n    database, as well as push and store this data and those models\n    once the aggregator receives or creates new models. This module\n    can utilize HTTP, WebSocket, or any other communication\n    framework for its implementation.\n\n-   **System configuration handler**: Deals with the\u00a0*registration\n    of agents*\u00a0and tracking the\u00a0connected agents and their statuses.\n    The aggregator needs to be aware of the connections and\n    registration statuses of the agents. If the agents are\n    registered with an established authentication mechanism, they\n    will accept the messages and process them accordingly.\n    Otherwise, this module\u00a0will go through the authentication\n    process, such as validating the token sent from the agent, so\n    that next time this agent is connected to the FL server, the\n    system will recognize the agent properly.\n\n-   **Model synthesis routine**: Supports\u00a0checking the collection\n    status of the local ML models and aggregating them once the\n    collection criteria are satisfied. Collection criteria include\n    the number of local models collected by the connected agents.\n    For example, aggregation can happen when 80% of the connected\n    agents send the trained local models to the aggregator. One of\n    the design patterns to do so is to periodically check the number\n    of ML models uploaded by the agents, which keep running while\n    the FL server is up and running. The model synthesis routine\n    will access the database or local buffer periodically to check\n    the status of the local model collection and aggregate those\n    models, to produce the global model that will be stored in the\n    database server and sent back to the agents.\n</code></pre> <ul> <li> <p>FL state manager: A\u00a0state manager keeps track of the\u00a0state     information of an aggregator and connected agents. It stores     volatile information for an aggregator, such as local and global     models delivered by agents, cluster models pulled from the database,     FL round information, or agents connected to the aggregator. The     buffered local models are used by the model aggregation module to     generate a global model that is sent back to each active agent     connected to the aggregator.</p> </li> <li> <p>Model aggregation module: The\u00a0model aggregation module\u00a0is a     collection of the model aggregation algorithms introduced in     the\u00a0Basics of model aggregation\u00a0section here and in\u00a0Model     Aggregation, in further depth. The most typical aggregation     algorithm is\u00a0federated averaging, which averages the weights of     the collected ML models, considering the number of samples that each     model has used for its local training.</p> </li> </ul> <p>Distributed agents</p> <ul> <li>A distributed agent\u00a0consists\u00a0of an FL client module that includes     the communication handler and client libraries as well as local ML     applications connected to the FL system through the FL client     libraries:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   FL client module: There\u00a0are primarily\u00a0four key functionalities     for the FL client module, which include a communication handler,     agent participation handler, model exchange routine, and client     libraries:</p> <pre><code>-   **Communication handler**: Serves as a\u00a0channel to communicate\n    with the aggregator that is assigned to the agent. The message\n    sent to the aggregator includes the registration payload of the\n    agent itself and an initial model that will be the basis of\n    aggregated models. The message also contains locally trained\n    models together with the performance data of those models. This\n    module supports both\u00a0*push*\u00a0and\u00a0*polling*\u00a0mechanisms and can\n    utilize HTTP or WebSocket frameworks for its implementation.\n\n-   **FL participation handler**: Deals\u00a0with the agent participation\n    in the FL process and cycle by sending an aggregator a message\n    including the agent information itself to be registered in the\n    FL platform and initialize the FL process if needed. The\n    response message will set the agent up for the continuous and\n    ongoing FL process and often includes the most updated global\n    model for the agent to utilize and train locally.\n\n-   **Model exchange routine**: Supports a synchronizing\n    functionality that constantly checks whether a new global model\n    is available or not. If the new global model is available, this\n    module downloads the global model from the aggregator and the\n    global model replaces the local model if needed. This module\n    also checks the client state and sends the retrained model if\n    the local training process is done.\n\n-   **Client libraries**: Include administrative libraries and\n    general FL client libraries:\n\n    -   The administrative libraries are used when registering the\n        initial model that will be used by other agents. Any\n        configuration changes for FL systems can be also requested\n        by administrative agents that have higher control\n        capabilities.\n\n    -   General FL client libraries provide basic functionalities\n        such as starting FL client core threads, sending local\n        models to an aggregator, saving models in some specific\n        location on the local machine, manipulating the client\n        state, and downloading the global models. This book mainly\n        talks about this general type of library.\n</code></pre> <ul> <li>Local ML engine and data pipelines: These parts are designed by     individual\u00a0ML engineers and\u00a0scientists and can be independent of the     FL client functionalities. This module has an ML model itself that     can be put into play immediately by the user for potentially more     accurate inference, a training and testing environment that can be     plugged into the FL client libraries, and for the implementation of     data pipelines. While the aforementioned module and libraries can be     generalized and provided as\u00a0application programming     interfaces\u00a0(APIs) or libraries for any ML applications, this     module is\u00a0unique depending on the requirements of AI applications to     be developed.</li> </ul> <p>The Database Components</p> <ul> <li> <p>A\u00a0database server\u00a0consists of a database query handler and a     database, as storage.</p> </li> <li> <p>The database server can reside on the server side, such as on the     cloud, and is tied closely to aggregators, while the recommended     design is to separate this database server\u00a0from aggregator servers     to decouple the functionalities to enhance the system's simplicity     and resilience.</p> </li> <li> <p>The functionality of the database query handler and sample database     tables are as follows:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Database query handler: Accepts the incoming requests from an     aggregator and sends the requested data and ML models back to the     aggregator.</p> <ul> <li> <p>Database: Stores\u00a0all the related information to FL processes. We     list some potential entries for the database here:</p> <ul> <li> <p>Aggregator information: This\u00a0aggregator-related information     includes the ID of the aggregator itself, the IP address and     various port numbers, system registered and updated times, and     system status. In addition, this entry can include model     aggregation-related information, such as the round of FL and its     information and aggregation criteria.</p> </li> <li> <p>Agent information: This\u00a0agent-related information includes     the ID of the agent itself, the IP address and various port     numbers, system registered and updated times, and system status.     This entry can also contain the opt-in/out status that is used     for synchronous FL (explained in the\u00a0Synchronous and     asynchronous FL\u00a0section Here) and a flag to record whether the     agent has been a bad actor in the past (for example, involved in     poisoning attacks, or very slow at returning results).</p> </li> <li> <p>Base model information: Base\u00a0model information is used for     the registration of initial ML models whose architecture and     information are used for the entire process of FL rounds.</p> </li> <li> <p>Local models: The information of local models includes the     model ID that is unique to individual ML models, generated time     of the model, agent ID that uploaded the model, aggregator ID     that received the model from the agent, and so on. Usually, the     model ID is uniquely mapped to the location of the actual ML     model file that can be stored in the database server or in some     cloud storage services such as S3 buckets of Amazon Web     Services, and so on.</p> </li> <li> <p>Cluster global models: The information of the cluster global     models is similar to what local models could record in the     database including the model ID, aggregator ID, generated time     of the model, and so on. Once the aggregated model is created by     an aggregator, the\u00a0database server will accept the global models     and store them in the database server or any cloud storage     services. Any global model can be requested by an aggregator.</p> </li> <li> <p>Performance data: The\u00a0performance of the local and global     models can be tracked, as metadata attached to those models.     This performance data will be used to ensure that the aggregated     model performs well enough before it is actually deployed to the     user ML application.</p> </li> </ul> </li> </ul> <p>Note</p> <ul> <li> <p>In the code sample of the\u00a0simple-fl\u00a0repository, only the database     tables related to the local models and cluster models are covered to     simplify the explanation of the entire FL process.</p> </li> <li> <p>Now that the basic architecture of the FL system has been     introduced, next, we will talk about how to enhance the FL system's     architecture if the computation resources are limited on the     agent-side devices.</p> </li> </ul> <p>Low Computational Capacity Agent Devices and Intermediate servers</p> <ul> <li> <p>Sometimes, the\u00a0computational capability of local user devices is     limited -- ML training may be difficult in those devices, but     inference or predictions can be made\u00a0possible by just downloading     the global model. In these cases, an FL platform may be able to set     up an additional intermediate server layer, such as with     smartphones, tablets, or edge servers.</p> </li> <li> <p>For example, in a healthcare AI application, users manage their     health information on their smart watches, which can be transferred     to their smart tablets or synched with laptops. In those devices, it     is easy to retrain ML models and integrate the distributed agent     functionalities.</p> </li> <li> <p>Therefore, the system architecture needs to be modified or     redesigned depending on the applications into which the FL system is     integrated, and the concept of intermediate servers can be applied     using distributed agents to realize FL processes.</p> </li> <li> <p>We do not have to modify the interactions and communication     mechanisms between the\u00a0aggregators and the intermediate servers.     Just by implementing\u00a0APIs between the user devices and the     intermediate servers, FL will be possible in most use cases.</p> </li> </ul> <p>Figure 3.2\u00a0illustrates the interaction between the aggregators, intermediate servers, and user devices:</p> <p>{width=\"6.268055555555556in\" height=\"3.623611111111111in\"}</p> <p>Figure 3.2 -- An FL system with intermediate servers</p> <ul> <li>Now that we have learned about the basic architecture and components     of an FL system, let us look into how an FL system operates in the     following section.</li> </ul> <p>FL System Process -- from initialization to continuous operation</p> <ul> <li> <p>Each distributed\u00a0agent belongs to an aggregator that is managed by     an FL server, where ML model aggregation is conducted to synthesize     a global model that is going to be sent back to the agents.</p> </li> <li> <p>An agent uses its local data to train an ML model and then uploads     the trained model to the corresponding aggregator. The concept     sounds straightforward, so we will look into a bit more detail to     realize the entire flow of those processes.</p> </li> <li> <p>We also define a\u00a0cluster global model, which we simply call     a\u00a0cluster model\u00a0or\u00a0global model, which is an aggregated ML     model of local models collected from distributed agents.</p> </li> </ul> <p>Note</p> <ul> <li> <p>In the next two sections, we will guide you on how to implement the     procedure and sequence of messages discussed Here.</p> </li> <li> <p>However, some of the system operation perspectives, such as an     aggregator or agent system registration in the database, are not     introduced in the code sample of the\u00a0simple-fl\u00a0repository in order     to simplify the explanation of the entire FL process.</p> </li> </ul> <p>Database, Aggregator, and Agent initialization</p> <ul> <li> <p>The\u00a0sequence\u00a0of the initialization processes is quite simple. The     initialization and registration processes need to happen in the     order of database, aggregator, and agents.</p> </li> <li> <p>The overall registration sequence of an aggregator and an agent with     a database is depicted in\u00a0Figure 3.3\u00a0as follows:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"6.290972222222222in\"}</p> <p>Figure 3.3 -- The process of aggregator and agent registration in the database server</p> <p>Here is the\u00a0initialization and registration procedure of each component in the FL system:</p> <ul> <li> <p>Database server initialization: The\u00a0first step of the operation     of an FL system is to initiate the database server. There are some     simple frameworks that are provided by multiple organizations that     do not include databases or database servers. However, in order to     maintain the process of federating the ML models, it is recommended     that you use a database, even a lightweight one such as an SQLite     database.</p> </li> <li> <p>Aggregator initialization and registration: An aggregator should     be set up and\u00a0running before any agents start running and uploading     the ML models. When the aggregator starts running and first gets     connected to the database server, the registration process happens     automatically by also checking whether the aggregator is safe to be     connected. If it fails to go through the registration process, it     receives the registration failure message sent back from the     database. Also, in case the aggregator is trying to connect to     the\u00a0database again after losing the connection to the database, the     database server always checks whether the aggregator has already     been registered or not. If this is the case, the response from the     database server includes the system information of the registered     aggregator so that the aggregator can start from the point where it     left off. The aggregator may need to publish an IP address and port     number for agents to be connected if it uses HTTP or WebSocket.</p> </li> <li> <p>Agent initialization and registration: Usually, if an agent     knows the aggregator\u00a0that the agent wants to connect to, the     registration is similar to how an aggregator connects to a database     server. The connection process should be straightforward enough to     just send a participation message to that aggregator using an IP     address, the port number of the aggregator (if we are using some     frameworks such as HTTP or WebSocket), and an authentication token.     In case the agent is trying to connect to the aggregator again after     losing the connection to the aggregator, the database server always     checks whether the agent already has been registered or not. If the     agent is already registered, the response from the database server     includes the system information of the registered agent so that the     agent can start from the point where it was disconnected from the     aggregator.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   In particular, when it receives the participation message from the     agent, the aggregator goes through the following procedure, as     in\u00a0Figure 3.4.</p> <ul> <li> <p>The key process after receiving the participation request is (i)     checking whether the agent is trusted or not, or whether the agent     is already registered or not, and (ii) checking\u00a0whether the initial     global model is already registered or not. If (i) is met, the     registration process keeps going. If the (initial) global model is     already registered, the agent will be able to receive the global     model and start\u00a0using that global model for the local training     process on the agent side.</p> </li> <li> <p>The agent participation and registration process at an aggregator     side is depicted in\u00a0Figure 3.4:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"6.81875in\"}</p> <p>Figure 3.4 -- The registration process of an agent by an aggregator</p> <ul> <li>Now that we\u00a0understand the initialization and registration process     of the FL system components, let us move on to the basic     configuration of the ongoing FL process, which is about uploading     the initial ML model.</li> </ul> <p>Initial model upload process by initial agent</p> <ul> <li> <p>The\u00a0next step in running an FL process is\u00a0to register the initial ML     model whose architecture will be used in the entire and continuous     process of FL by all the aggregators and agents.</p> </li> <li> <p>The initial model can be distributed by the company that owns the ML     application and FL servers.</p> </li> <li> <p>They'll likely provide the initial base model as part of the     aggregator configuration.</p> </li> <li> <p>We call the initial ML model used as a reference for model     aggregation a\u00a0base model.</p> </li> <li> <p>We also call the agent that uploads the initial base model     an\u00a0initial agent. The base model info could include the ML model     itself as well as the time it was generated and the initial     performance data.</p> </li> <li> <p>That being said, the process of initializing the base model can be     seen in\u00a0Figure 3.5:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"5.909722222222222in\"}</p> <p>Figure 3.5 -- Base model upload process for the initial agent</p> <ul> <li>Now, the FL process is ready to be conducted. Next, we will learn     about the FL cycle, which is a very core part of the FL process.</li> </ul> <p>Overall FL System Process Sequence</p> <ul> <li> <p>In this section, we\u00a0will only give an example with a single agent     and aggregator, but in real cases and operations, the agent     environments are various and dispersed into distributed devices.</p> </li> <li> <p>The following is the list\u00a0of the processes for how the local models     are uploaded, aggregated, stored, and sent back to agents as a     global model:</p> </li> <li> <p>The agents other than the initial agent will request the global     model, which is an updated aggregated ML model, in order to deploy     it to their own applications.</p> </li> <li> <p>Once the agent gets the updated model from the aggregator and     deploys it, the agent retrains the ML model locally with new data     that is obtained afterward to reflect the freshness and timeliness     of the data. An agent can also participate in multiple rounds with     different data to absorb its local examples and tendencies. Again,     this local data will not be shared with the aggregator and stays     local to the distributed devices.</p> </li> <li> <p>After retraining the local ML model (which, of course, has the same     architecture as the global or base model of the FL), the agent calls     an FL client API to send the model to the aggregator.</p> </li> <li> <p>The aggregator receives the local ML model and pushes the model to     the database. The aggregator keeps track of the number of collected     local models and will keep accepting the local models as long as the     federation round is open. The round can be closed with any defined     criteria, such as the aggregator receiving enough ML models to be     aggregated. When the criteria are met, the aggregator aggregates the     local models and produces an updated global model that is ready to     be sent back to the agent.</p> </li> <li> <p>During that process, agents constantly poll the aggregator on     whether the global model is ready or not, or in some cases, the     aggregator may push the global model to the agents that are     connected to the aggregator, depending on the communications system     design and network constraints. Then, the updated model is sent back     to the agent.</p> </li> <li> <p>After receiving the updated global model, the agent deploys and     retrains the global model locally whenever it is ready. The whole     process described is repeated until the termination criteria are met     for the FL to end. In some cases, there are no termination     conditions to stop this FL cycle and retraining process\u00a0so that the     global model constantly keeps learning about the latest phenomena,     current trends, or user-related tendencies. FL rounds can just be     stopped\u00a0manually in preparation for some evaluation before a     rollout.</p> </li> </ul> <p>Figure 3.6\u00a0shows the overall process of how FL is continuously conducted between an agent, an aggregator, and a database typically:</p> <p>{width=\"6.268055555555556in\" height=\"6.572222222222222in\"}</p> <p>Figure 3.6 -- Overview of the continuous FL cycle</p> <ul> <li>Now that we understand the overall procedure of the FL process, we     will look into the different round management approaches in the FL     process next: synchronous FL and asynchronous FL.</li> </ul> <p>Synchronous and asynchronous FL</p> <ul> <li> <p>When the model aggregation happens at the aggregator, there are     multiple criteria related to how many local models it needs to     collect from which agents.</p> </li> <li> <p>In this section, we will briefly talk about the differences between     synchronous and asynchronous FL, which have been discussed in a lot     of literature, such as     https://iqua.ece.toronto.edu/papers/ningxinsu-iwqos22.pdf, so please     refer to it to learn about these concepts further.</p> </li> </ul> <p>Synchronous FL</p> <ul> <li> <p>Synchronous FL\u00a0requires the aggregator to select the\u00a0agents that     need to send the local models for each round in order to proceed     with the model aggregation.</p> <ul> <li> <p>This synchronous FL approach is simple to design and implement     and suitable for FL applications that require a clear selection     of agents.</p> </li> <li> <p>However, if the number of agents becomes too large, the     aggregator may have to wait for a long time to wrap up the     current round, as the computational capability of the agents     could vary and some of them may have problems uploading or fail     to upload their local models.</p> </li> <li> <p>Thus, some of the agents can become slow or totally     dysfunctional when sending their models to the aggregator.</p> </li> <li> <p>These slow agents are known as\u00a0stragglers\u00a0in distributed ML,     which motivates us to use the asynchronous FL mode.</p> </li> </ul> </li> </ul> <p>Asynchronous FL</p> <ul> <li> <p>Asynchronous FL\u00a0does not require the aggregator\u00a0to select the agents     that have to upload their local models. Instead, it opens the door     for any trusted agents to upload the model anytime.</p> </li> <li> <p>Furthermore, it is fine to wrap up the federation round whenever the     aggregator wants to generate the global model, with or without     criteria such as the minimum number of local models that needs to be     collected, or some predefined interval or deadline for which the     aggregator needs to wait to receive the local models from the agents     until the aggregation for that round happens.</p> </li> <li> <p>This asynchronous FL approach gives the FL system much more     flexibility for model aggregation for each FL round, but the design     may be more complicated than the simple synchronous aggregation     framework.</p> </li> <li> <p>When managing the FL rounds, you need to consider the practicalities     of running rounds, such as scheduling and dealing with delayed     responses, the minimum levels of participation required, the details     of example stores, using the downloaded or trained models for     improved inference in the applications on the edge devices, and     dealing with bad or slow agents.</p> </li> <li> <p>We will look into the FL process and procedure flow next, focusing     on the aggregator side.</p> </li> </ul> <p>FL cycle and process- The aggregator perspective</p> <ul> <li>An\u00a0aggregator has two threads\u00a0running to accept and cache the local     models and aggregate the collected local ML models. In this section,     we describe those procedures.</li> </ul> <p>Accepting and caching local ML models</p> <ul> <li> <p>The aggregator side process\u00a0of accepting\u00a0and caching local ML models     is depicted in\u00a0Figure 3.7\u00a0and explained as follows:</p> </li> <li> <p>The aggregator will wait for a local ML model to be uploaded by an     agent. This method sounds like asynchronous FL. However, if the     aggregator has already decided which agents to accept models from,     it just needs to exclude the model uploads sent by undesired agents.     Some other system or module may have already told the undesired     agents not to participate in the round as well.</p> </li> <li> <p>Once an ML model is received, the aggregator checks whether the     model is uploaded by the trusted agents or not. Also, if the agent     that uploads the local model is not listed in the agents that the FL     operator wants to accept, the aggregator will discard the model.     Furthermore, an aggregator needs to have a mechanism to only filter     the valid models -- otherwise, there is a risk of poisoning the     global model and messing up the entire FL process.</p> </li> <li> <p>If the uploaded local ML model is valid, the aggregator will push     the model to the database. If the database resides on a different     server, the aggregator will package the model and send it to the     database server.</p> </li> <li> <p>While the\u00a0uploaded\u00a0models are stored in the database, they should be     buffered in the memory of the state manager of the aggregator in an     appropriate format, such as NumPy arrays.</p> </li> <li> <p>This procedure keeps running until the termination conditions are     satisfied or the operator of the FL system opts to stop the     process.\u00a0Figure 3.7\u00a0depicts the procedure of accepting and caching     local ML models:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"6.972916666666666in\"}</p> <p>Figure 3.7 -- Procedure for accepting and caching local ML models</p> <ul> <li>Once the\u00a0local ML models are\u00a0accepted and cached, the FL system     moves on to the next procedure: aggregating the local models.</li> </ul> <p>Aggregating local ML models</p> <ul> <li> <p>The\u00a0aggregator-side procedure of aggregating local ML models     depicted in\u00a0Figure 3.8\u00a0is as follows:</p> </li> <li> <p>The aggregator constantly checks whether the aggregation criteria     are satisfied. The typical aggregation criteria are as follows:</p> <ul> <li> <p>The number of local models collected so far in this FL round.     For example, if the number of agents is 10 nodes, after 8 nodes     (meaning 80% nodes) report the locally trained models, the     aggregator starts aggregating the models.</p> </li> <li> <p>The combination of the number of collected models and the time     that the FL round has spent. This can automate the aggregation     process and prevent systems from getting stuck.</p> </li> </ul> </li> <li> <p>Once\u00a0the aggregation criteria are met, the aggregator starts a model     aggregation process. Usually, federated averaging is a very typical     but powerful aggregation method. Further explanation of the model     aggregation methods is in the\u00a0Basics of model aggregation\u00a0section     of This section and in\u00a0section     7,\u00a0Model     Aggregation. The aggregated model is defined as a global model in     this FL round.</p> </li> <li> <p>In a case where time for the FL round has expired and not enough     agents that participated in the round have uploaded a model, the     round can be abandoned or forced to conduct aggregation for the     local models collected so far.</p> </li> <li> <p>Once the model aggregation is complete, the aggregator pushes the     aggregated global model to the database. If the database resides on     a different server, the aggregator will package the global model and     send it to the database server.</p> </li> <li> <p>Then, the aggregator sends the global model to all the agents, or     when the agents poll to check whether the global model is ready, the     aggregator will notify the agent that the global model is ready and     put it in the response message to the agents.</p> </li> <li> <p>After the whole process of model aggregation, the aggregator updates     the number of the FL round by just incrementing it.</p> </li> <li> <p>Figure 3.8\u00a0shows the aggregator's process from checking the     aggregation criteria to synthesizing the global model when enough     models are collected:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"8.41388888888889in\"}</p> <p>Figure 3.8 -- Model synthesis routine: aggregating local ML models</p> <ul> <li>Aggregating\u00a0local models to generate the global model has been     explained. Now, let us look into the agent-side FL cycle, including     the retraining process of the local ML models.</li> </ul> <p>Local Retraining Cycle - The agent-perspective</p> <ul> <li> <p>In the\u00a0distributed agent, the\u00a0following state transition happens and     is repeated for the continuous operation of the FL cycle:</p> </li> <li> <p>In the state of\u00a0waiting_gm, the agent polls the aggregator to     receive any updates related to the global model. Basically, a     polling method is used to regularly query the updated global model.     However, under some specific settings, an aggregator can push the     updated global model to all agents.</p> </li> <li> <p>gm_ready\u00a0is the state after the global model is formed by the     aggregator and downloaded by the agent. The model parameters are     cached in the agent device. The agent replaces its local ML model     with the downloaded global model. Before completely replacing the     local model with the downloaded model, the agent can check whether     the output of the global model is sufficiently performant for the     local ML engine. If the performance is not what is expected, the     user can keep using the old model locally until it receives the     global model that has the desired performance.</p> </li> <li> <p>Next, in the\u00a0training\u00a0state, the agent can locally train the model     in order to maximize its performance. The trained model is saved in     a local data storage where training examples are kept. The FL client     libraries of the agent ascertain its readiness to manipulate the     local model that can be protected with asynchronous function access.</p> </li> <li> <p>After the local model is trained, the agent checks whether the new     global model has been sent to the agent or not. If the global model     has arrived, then the locally trained ML model is discarded and goes     back to the\u00a0gm_ready\u00a0state.</p> </li> <li> <p>After local training, the agent proceeds with the\u00a0sending\u00a0state to     send the updated local model back to the aggregator, and then, the     agent goes back to the\u00a0waiting_gm\u00a0state.</p> </li> <li> <p>Figure 3.9\u00a0depicts the state transition of an agent to adapt and     update the ML model:</p> </li> </ul> <p>{width=\"5.709722222222222in\" height=\"7.125in\"}</p> <p>Figure 3.9 -- Agent-side state transition to adapt and update the ML model</p> <ul> <li>Next, we\u00a0touch on a model interpretation based on deviation from the     baseline outputs that are used for anomaly detection and preventing     model degradation.</li> </ul> <p>Model interpretation based on deviation from baseline outputs</p> <ul> <li> <p>We can also\u00a0provide an interpretation framework by looking at the     output of each local model. The following procedure can be     considered to ensure the local model is always good to use and can     be deployed in production:</p> </li> <li> <p>Obtain the most recent ML output generated by an agent as well as a     baseline output that can be a typical desired output prepared by     users. The baseline output could include an average output based on     the past windows or reference points defined by an operator, subject     expert, or rule-based algorithm.</p> </li> <li> <p>The deviation between the output of the local model and the baseline     output is computed.</p> </li> <li> <p>An anomaly or performance degradation can be detected by checking     whether the deviation exceeds the operator-specified threshold. If     an anomaly is detected, an alarm can be sent to an operator to     indicate a fault or that the ML model is in an anomalous state.</p> </li> <li> <p>Now that the process of the FL has been explained, let us look into     the basics of model aggregation, which comprise the critical part of     FL.</p> </li> </ul> <p>Model Aggregation - The Basics</p> <ul> <li> <p>Aggregation\u00a0is a\u00a0core concept within FL. In fact, the strategies     employed to aggregate models are the key theoretical driver for the     performance of FL systems.</p> </li> <li> <p>The purpose of this section is to introduce the high-level concepts     of aggregation within the context of an FL system -- the underlying     theory and examples of advanced aggregation strategies will be     discussed in greater depth later when we discuss\u00a0Model     Aggregation.</p> </li> </ul> <p>What does aggregation of models mean?</p> <ul> <li> <p>Let's revisit the\u00a0aggregator-side cycle discussed in earlier, at the     point in the process where the agents assigned to a certain     aggregator have finished training locally and have transmitted these     models back to this aggregator.</p> </li> <li> <p>The goal of any aggregation strategy, or any way of aggregating     these models together, is to produce new models that gradually     increase in performance across all of the data collected by the     constituent agents.</p> </li> <li> <p>An important point\u00a0to remember is that FL is, by definition, a     restricted version of the distributed learning setting, in which the     data collected locally by each agent cannot be directly accessed by     other agents.</p> </li> <li> <p>If this restriction were not in place, a model could be made to     perform well trivially on all of the data by collecting the data     from each agent and training on the joint dataset; thus, it makes     sense to treat this\u00a0centrally-trained\u00a0model as the target model     for an FL approach.</p> </li> <li> <p>At a high level, we can consider this unrestricted distributed     learning scenario as aggregation before model training (where in     this case, aggregation refers to combining the data from each     agent).</p> </li> <li> <p>Because FL does not allow data to be accessed by other agents, we     consider the scenario as aggregation after model training instead;     in this context, aggregation refers to the combination of the     intelligence captured by each of the trained models from their     differing local datasets.</p> </li> <li> <p>To summarize, the goal of an aggregation strategy is to combine     models in a way that eventually leads to a generalized model whose     performance approaches that of the respective centrally trained     model.</p> </li> </ul> <p>FedAvg -- Federated averaging</p> <ul> <li>To make\u00a0some of these\u00a0ideas more concrete, let's take an initial     look into one of the most well-known and straightforward aggregation     strategies, known as\u00a0Federated Averaging\u00a0(FedAvg). The     FedAvg algorithm is performed as follows:     let\u00a0{width=\"1.2670089676290464in\"     height=\"0.2487379702537183in\"}\u00a0be the parameters of the models     from\u00a0{width=\"0.20833333333333334in\"     height=\"0.20833333333333334in\"}\u00a0agents, each with a local dataset     size of\u00a0{width=\"1.1828608923884514in\"     height=\"0.2213538932633421in\"}.     Also,\u00a0{width=\"0.20833333333333334in\"     height=\"0.20833333333333334in\"}\u00a0is the total dataset size defined     as\u00a0{width=\"2.617024278215223in\"     height=\"0.29885826771653545in\"}. Then, FedAvg returns the following     ML model as the aggregated model:</li> </ul> <p>{width=\"4.697647637795275in\" height=\"0.54582239720035in\"}</p> <ul> <li> <p>Essentially, we\u00a0perform FedAvg over a set\u00a0of models by taking the     weighted average of the models, with weights proportional to the     size of the dataset used to train the model.</p> </li> <li> <p>As a result, the types of models to which FedAvg can be applied are     models that can be represented as some set of parameter values.</p> </li> <li> <p>Deep neural networks are currently the most notable of these kinds     of models -- most of the results analyzing the performance of FedAvg     work with deep learning models.</p> </li> <li> <p>It is rather surprising that this relatively simple approach can     lead to generalization in the resulting model.</p> </li> <li> <p>We can visually examine what FedAvg looks like within a toy     two-dimensional parameter space to observe the benefits of the     aggregation strategy:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.636111111111111in\"}</p> <p>Figure 3.10 -- Two-dimensional parameter space with local models from two agents (the circle and square) and a target model (the black x)</p> <ul> <li> <p>Let's consider a case where we have two newly initialized models     (the circle and square points) belonging to separate agents.</p> </li> <li> <p>The space in the preceding figure represents the parameter space of     the models, where each toy model is defined by two parameters.</p> </li> <li> <p>As\u00a0the models are trained, these points will move in the parameter     space -- the goal is to approach a local optimum in the parameter     space, generally corresponding to the aforementioned centrally     trained model:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.461111111111111in\"}</p> <p>Figure 3.11 -- Change in local model parameters without aggregation</p> <ul> <li> <p>Each model\u00a0converges to separate dataset-specific optima (two x     points from the circle and square) that do not generalize.</p> </li> <li> <p>Because each agent only has access to a subset of the data, the     local optima reached by training each model locally will differ from     the true local optima; this difference depends on how similar the     underlying data distributions are for each agent.</p> </li> <li> <p>If the models are only trained locally, the resulting models will     likely not generalize over all of the data:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.631944444444445in\"}</p> <p>Figure 3.12 -- Adding aggregation moves the local model parameters to the average for both models at each step, leading to convergence at the target model</p> <ul> <li> <p>Applying\u00a0FedAvg at each movement\u00a0step allows us to create an     aggregate model that eventually comes close to the true local optima     in the parameter space.</p> </li> <li> <p>This example\u00a0displays the basic capability of FedAvg to produce     generalized models.</p> </li> <li> <p>However, working with real models (such as highly parameterized deep     learning models) introduces additional complexity that is handled by     FedAvg but not by simpler approaches.</p> </li> <li> <p>For example, we might wonder why we don't simply fully train each     local model and only average at the end; while this approach would     work in this toy case, it has been observed that only averaging once     with real models leads to poor performance across all of the data.</p> </li> <li> <p>The FedAvg process allows for a more robust way to reach the     generalized model within high-dimension parameter spaces.</p> </li> <li> <p>This section only aims to give an overview of aggregation in     FL;\u00a0the\u00a0Model Aggregation section, contains more detailed     explanations and examples for aggregation in different scenarios.</p> </li> <li> <p>We now understand the entire process of how the FL system works with     basic model aggregation. In some applications, the FL system may     have to support a huge number of agents to realize its scalability.</p> </li> <li> <p>The following section will give you some idea about how to scale     more smoothly, especially with a decentralized horizontal design.</p> </li> </ul> <p>Horizontal Design for Enhanced Scalability</p> <ul> <li> <p>In this section, we will look into how to further scalability when     we need to support a large number of devices and users.</p> </li> <li> <p>There are\u00a0practical cases where control, ease of maintenance and     deployment, and low communication overhead are provided by     centralized FL. If the number of agents is not large, it makes more     sense to stick to centralized FL than decentralized FL.</p> </li> <li> <p>However, when the number of participating agents becomes quite     large, it may be worth looking into horizontal scaling with a     decentralized FL architecture.</p> </li> <li> <p>The latest developments of auto-scaling frameworks these days, such     as\u00a0the\u00a0Kubernetes\u00a0framework (https://kubernetes.io/, can be a     nice integration with the topic that is discussed in this section,     although actual integration and implementation with Kubernetes is     beyond the scope of this material.</p> </li> </ul> <p>Horizontal design with semi-global model</p> <ul> <li> <p>There will be some\u00a0use cases where many aggregators are needed to     cluster groups of agents and create a global model on top of those     many aggregators.</p> </li> <li> <p>Google uses a centralized approach for this, as in the     paper\u00a0Towards Federated Learning at Scale, while setting up a     centralized node for managing multiple aggregators may have some     resilience issues.</p> </li> <li> <p>The idea is simple: periodically aggregate all the cluster models at     some central master node.</p> </li> <li> <p>On the other hand, we can realize the decentralized way of     aggregating cluster models created by multiple aggregators. The     architecture for that is based on two crucial ideas:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Model aggregation conducted among individual cluster aggregators     without master nodes</p> <ul> <li>Semi-global model synthesis to aggregate cluster models generated by     other aggregators</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   To create semi-global models, decentralized cluster aggregators     exchange their aggregated cluster models with each other and     approximate optimal global models.</p> <ul> <li> <p>The cluster aggregators can also use a database to periodically     collect other cluster models to generate the semi-global models.</p> </li> <li> <p>This framework allows for the absorption of training results from     diverse sets of users dispersed across many aggregators by     synthesizing the most updated global models without a master node     concept.</p> </li> <li> <p>Based on\u00a0this decentralized architecture, the robustness of the     entire FL system can be enhanced, as the semi-global model can be     independently computed at each cluster aggregator.</p> </li> <li> <p>The FL system can be scaled further, as each cluster aggregator is     responsible for creating its own semi-global model by itself -- not     via the master node of those aggregators -- and therefore,     decentralized semi-global model formation comes with resiliency and     mobility.</p> </li> <li> <p>We can even decouple the database that stores the uploaded local     models, cluster global models, and semi-global models.</p> </li> <li> <p>By introducing a distributed database into the FL system, the entire     system could be made more scalable, resilient, and secure together     with some failover mechanism.</p> </li> <li> <p>For example, each cluster aggregator stores the cluster model in a     distributed database.</p> </li> <li> <p>The cluster aggregators can retrieve cluster models of other     aggregators by pulling the models periodically from the databases.     At each cluster aggregator, a semi-global ML model is generated by     synthesizing the pulled models.</p> </li> </ul> <p>Figure 3.13\u00a0illustrates the overall architecture of the decentralized horizontal design of a multi-aggregator FL system:</p> <p>{width=\"6.268055555555556in\" height=\"2.0625in\"}</p> <p>Figure 3.13 -- Architecture of a decentralized FL system with multiple aggregators (horizontal design)</p> <ul> <li>Now that we have\u00a0discussed how to enhance the FL system with a     horizontal design using the semi-global model concept, next, we will     look at distributed database frameworks to further ensure     scalability and resiliency.</li> </ul> <p>Distributed database</p> <ul> <li> <p>Furthermore, the\u00a0accountability of the model updates can be provided     by storing historical model data in a data-driven distributed     database.</p> </li> <li> <p>The\u00a0InterPlanetary File System\u00a0(IPFS) and Blockchain     are\u00a0well-known distributed databases that ensure the accountability     of global model updates.</p> </li> <li> <p>After a cluster aggregator generates a semi-global model based on     other cluster models, the semi-global model is stored in a     distributed database.</p> </li> <li> <p>The distributed database manages the information of those models     with a unique identifier.</p> </li> <li> <p>To maintain all the models consistently, including local, cluster,     and semi-global models, each ML model is assigned a globally unique     identifier, such as a hash value, which could be realized using the     concept of\u00a0a\u00a0Chord Distributed Hash Table\u00a0(Chord DHT).</p> </li> <li> <p>The Chord DHT is a scalable peer-to-peer lookup protocol for     internet applications.</p> </li> <li> <p>The cluster aggregator can store metadata on the cluster models,     such as timestamps and hash identifiers.</p> </li> <li> <p>This gives us further accountability for model synthesis by ensuring     the cluster models haven\\'t been altered.</p> </li> <li> <p>It is also possible to identify a set of aggregators that are     sending harmful cluster models to destroy the semi-global models     once the malicious models are detectable.</p> </li> <li> <p>These models can be filtered by analyzing the patterns of the     weights of the cluster model or deviation from the other cluster     models when the difference is too big to rely on.</p> </li> <li> <p>The nature of the distributed database is to store all the volatile     state information of the distributed FL system.</p> </li> <li> <p>The FL system can restore from the distributed database in the case     of failure.</p> </li> <li> <p>The cluster aggregators also exchange their cluster models based on     a certain interval defined by the system operator.</p> </li> <li> <p>Therefore, the mapping table between cluster models and aggregators     needs to be logged in the database together with meta-information on     the local, cluster, and semi-global models, such as the generation     time of those models and the size of training samples.</p> </li> </ul> <p>Asynchronous agent participation in a multiple-aggregator scenario</p> <ul> <li> <p>Distributed agents\u00a0can broadcast participation messages to     connectable aggregators when they want to join their FL process.</p> </li> <li> <p>The participation messages can contain the unique ID of the agent.     One of the cluster aggregators then returns a cluster aggregator ID,     potentially the value generated based on a common hash function, to     which the agent should belong.\u00a0</p> </li> <li> <p>Figure 3.14\u00a0depicts how the agent is assigned to a certain cluster     aggregator using a hash function:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"7.308333333333334in\"}</p> <p>Figure 3.14 -- The sequence of an agent joining one of the cluster aggregators in an FL system</p> <ul> <li>In the following section, we will look into how the semi-global     model is generated based on aggregating the multiple cluster global     models.</li> </ul> <p>Semi-global model synthesis</p> <ul> <li> <p>After the\u00a0agent is assigned to a specific cluster aggregator, the     agent starts to participate in the FL process.</p> </li> <li> <p>It\u00a0requests a base ML model if it is registered -- otherwise, it     needs to upload the base model to start local training.</p> </li> <li> <p>The procedure of uploading local models and generating cluster and     semi-global models will continue until the agent or aggregator is     disconnected from the system.</p> </li> <li> <p>The sequence of the local and cluster model upload process,     aggregation process, and semi-global model synthesis and pulling is     illustrated in\u00a0Figure 3.15:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.134027777777778in\"}</p> <p>Figure 3.15 -- The sequence of the semi-global model synthesis processes from uploading local models to pulling semi-global models</p> <ul> <li> <p>Let's look at semi-global model synthesis using the flowchart     between the agent, aggregator, and distributed database.</p> </li> <li> <p>The aggregator receives a local model from an agent. When receiving     the local model, the model filtering process will decide whether to     accept the uploaded model or not.</p> </li> <li> <p>This framework can be implemented using many different methods, such     as a basic scheme of checking the difference between the weights of     the global and local models. If the model is not valid, just discard     the local model.</p> </li> <li> <p>Then, a cluster model is created by aggregating all the accepted     local models.</p> </li> <li> <p>The aggregator stores the cluster model in a database, as well as     simultaneously retrieving the cluster models generated by other     cluster aggregators.</p> </li> <li> <p>A semi-global model is then synthesized from those cluster models     and will be used in the agents that are assigned to the cluster     aggregator.</p> </li> <li> <p>Figure 3.16\u00a0shows\u00a0how the\u00a0cluster aggregator proceeds with cluster     and semi-global model synthesis using a distributed database:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"8.567361111111111in\"}</p> <p>Figure 3.16 -- The procedure and flow of semi-global model synthesis</p> <ul> <li> <p>An\u00a0aggregator does not\u00a0need to retrieve all the cluster models     generated at each round to create a semi-global model. To synthesize     a semi-global model, the global model can eventually converge based     on the subset of models randomly selected by each aggregator.</p> </li> <li> <p>Using this approach, the robustness and\u00a0independence of aggregators     will be enhanced by compromising on the conditions to create the     global model at every update.</p> </li> <li> <p>This\u00a0framework can also resolve the bottlenecks in terms of     computation and communication typical to centralized FL systems.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>Here, we discussed the potential architecture, procedure flow, and     message sequences within an FL system.</p> </li> <li> <p>The typical FL system architecture consists of an aggregator,     agents, and a database server.</p> </li> <li> <p>These three components are constantly communicating with each other     to exchange system information and ML models to achieve model     aggregation.</p> </li> <li> <p>The key to implementing a good FL system is decoupling the critical     components and carefully designing the interfaces between them.</p> </li> <li> <p>We focused on the aspect of the simplicity of its design so that     further enhancement can be achieved by just adding additional     components to the systems. Horizontal decentralized design can also     help implement a scalable FL system.</p> </li> <li> <p>In the following section, we will discuss the implementation details     of achieving FL on the server side.</p> </li> <li> <p>As some critical aspects of the functionalities have been introduced     Here, you will be able to implement the basic system and smoothly     run the simulation with some ML applications.</p> </li> </ul> <p>[Section 4: FL Server - Python Implementation]{.underline}</p> <ul> <li> <p>The server-side implementation of a\u00a0federated learning\u00a0(FL)     system is critical for realizing authentic FL-enabled applications.</p> </li> <li> <p>We have discussed the basic system architecture and flow in the     previous section. Here, more hands-on implementation will be     discussed so that you can create a simple server and aggregator of     the FL system that various\u00a0machine learning\u00a0(ML)     applications can be connected to and tested on.</p> </li> <li> <p>This section describes an actual implementation aspect of FL     server-side components discussed in\u00a0section 3.</p> </li> <li> <p>Based on the understanding of how the entire process of the FL     system works, you will be able to go one step further to make it     happen with example code provided here and on GitHub.</p> </li> <li> <p>Once you understand the basic implementation principles using the     example code, it is a fun aspect to be able enhance the FL server     functionalities based on your own design.</p> </li> <li> <p>Here, we're going to cover the following topics:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Main software components of the aggregator</p> <ul> <li> <p>Implementing FL server-side functionalities</p> </li> <li> <p>Maintaining models for aggregation with the state manager</p> </li> <li> <p>Aggregating local models</p> </li> <li> <p>Running the FL server</p> </li> <li> <p>Implementing and running the database server</p> </li> <li> <p>Potential enhancements to the FL server</p> </li> </ul> <p>Technical requirements</p> <ul> <li>All the code files introduced Here can be found on GitHub     here:\u00a0[https://github.com/keshavaspanda/simple-fl]{.underline}</li> </ul> <p>Main software components of the aggregator \u2028and database</p> <ul> <li> <p>The architecture\u00a0of an aggregator with the FL server was introduced     in the previous section. Here, we will introduce the code that     realizes the basic functionalities of an FL system.</p> </li> <li> <p>The aggregator and database-side Python-based software components     are listed in the\u00a0aggregator\u00a0directory of\u00a0fl_main, as well     as\u00a0lib/util\u00a0and\u00a0pseudodb\u00a0folders, as in\u00a0Figure 4.1:</p> </li> </ul> <p>{width=\"3.407120516185477in\" height=\"3.9518897637795276in\"}</p> <p>Figure 4.1 -- Python software components for the aggregator as well as internal libraries and pseudo database</p> <ul> <li>The following is a brief description of the Python code files in the     aggregator.</li> </ul> <p>The Aggregator-side code</p> <ul> <li> <p>In this section, we\u00a0will touch on the\u00a0main Python files of the     aggregator-side related to the FL server thread, FL state manager,     and model aggregation itself.</p> </li> <li> <p>These aggregator-side code files are found in the\u00a0aggregator\u00a0folder.     The code in the repo only captures the model aggregation     perspective, not the entire engineering aspects of creating a     thorough FL platform.</p> </li> </ul> <p>FL server code (server_th.py)</p> <ul> <li> <p>This is the\u00a0main code that realizes\u00a0the whole basic flow of the FL     process from the communication processes between an aggregator     itself, agents, and a database to coordinating agent participation     and the aggregation of the ML models.</p> </li> <li> <p>It also initializes the global cluster model sent from the first     connected agent. It manages receiving local models and the cluster     model synthesis routine in which the cluster global model is formed     after collecting enough local models.</p> </li> </ul> <p>FL state manager (state_manager.py)</p> <ul> <li> <p>The state\u00a0manager\u00a0buffers the local model and cluster model data     that is needed for aggregation processes.</p> </li> <li> <p>The buffers will be filled out when the aggregator receives local     models from the agents and cleared when proceeding to the next round     of the FL process.</p> </li> <li> <p>The checking function of the aggregation criteria is also defined in     this file.</p> </li> </ul> <p>Aggregation code (aggregation.py)</p> <ul> <li> <p>The\u00a0aggregation\u00a0Python code will list the basic algorithms for     aggregating the model.</p> </li> <li> <p>In the code example used here Here, we will only introduce the     averaging method called\u00a0federated averaging\u00a0(FedAvg),     which\u00a0averages the weights of the collected local models considering     local dataset sizes to generate a cluster global model.</p> </li> </ul> <p>lib/util codes</p> <ul> <li>The Python\u00a0files for the internal\u00a0libraries     (communication_handler.py,\u00a0data_struc.py,\u00a0helpers.py,\u00a0messengers.py,     and\u00a0states.py) will be explained in the\u00a0Appendix,\u00a0Exploring     Internal Libraries.</li> </ul> <p>Database-side code</p> <ul> <li> <p>Database-side code consists of the pseudo database and the SQLite     database Python code files that can be found in the\u00a0pseudodb\u00a0folder.</p> </li> <li> <p>The pseudo database code is hosting a server to receive messages     from the aggregator and parse them to process as the ML model data     that can be utilized for the FL process.</p> </li> </ul> <p>Pseudo database code (pseudo_db.py)</p> <ul> <li>The function of pseudo database Python code is to accept the     messages related to the local and global cluster models from the     aggregator and push the information to the database. It also saves     the ML model binary files in the local file system.</li> </ul> <p>SQLite database code (sqlite_db.py)</p> <ul> <li> <p>The SQLite database Python code creates an actual SQLite database at     the specified path. It also has the function to insert data entries     related to the local and global cluster models into the database.</p> </li> <li> <p>Now that the aggregator and database-side software components are     defined, let\\'s move on to the configuration of the aggregator.</p> </li> </ul> <p>Configuring the aggregator</p> <ul> <li>The following\u00a0code is an example of the aggregator-side     configuration parameters defined in the\u00a0config_aggregator.json\u00a0file,     which can be found in the\u00a0setups\u00a0folder:</li> </ul> <p>{</p> <p>\\\"aggr_ip\\\": \\\"localhost\\\",</p> <p>\\\"db_ip\\\": \\\"localhost\\\",</p> <p>\\\"reg_socket\\\": \\\"8765\\\",</p> <p>\\\"exch_socket\\\": \\\"7890\\\",</p> <p>\\\"recv_socket\\\": \\\"4321\\\",</p> <p>\\\"db_socket\\\": \\\"9017\\\",</p> <p>\\\"round_interval\\\": 5,</p> <p>\\\"aggregation_threshold\\\": 1.0,</p> <p>\\\"polling\\\": 1</p> <p>}</p> <p>CopyExplain</p> <ul> <li> <p>The parameters include the aggregator's IP (the FL server's IP), the     database server's IP, and the various port numbers of the database     and agents.</p> </li> <li> <p>The round interval is the time of the interval at which the criteria     of aggregation are checked and the aggregation threshold defines the     percentage of collected local ML models needed to start the     aggregation process. The polling flag is related to whether to     utilize the\u00a0polling\u00a0method for communications between the aggregator     and agents or not.</p> </li> <li> <p>Now that we have covered the concept of the configuration file for     the aggregator side, let's move on to how the code is designed and     implemented.</p> </li> </ul> <p>FL server-side functions</p> <ul> <li> <p>In this section, we\u00a0will explain how you can implement the very     first version of an aggregator with an FL server system using the     actual code examples, which are in\u00a0server_th.py\u00a0in     the\u00a0aggregator\u00a0directory.</p> </li> <li> <p>This way, you will understand the core functionalities of the FL     server system and how they are implemented so that you can further     enhance a lot more functionalities on your own.</p> </li> <li> <p>Therefore, we will only cover the important and core functionalities     that are critical to conducting a simple FL process. The potential     enhancements will be listed in the later section of This     section,\u00a0Potential enhancements to the FL server.</p> </li> <li> <p>server_th.py\u00a0handles all the aspects of basic functionalities     related to the FL server side, so let's look into that in the     following section.</p> </li> </ul> <p>FL Server Library Imports</p> <ul> <li> <p>The FL server-side\u00a0code starts with importing the necessary     libraries. In particular,\u00a0lib.util\u00a0handles the basic supporting     functionalities to make the implementation of FL easy. The details     of the code can be found in the GitHub repository.</p> </li> <li> <p>The server code imports\u00a0StateManager\u00a0and\u00a0Aggregator\u00a0for the FL     processes. The code about the state manager and aggregation will be     discussed in later sections Here about\u00a0Maintaining models for     aggregation with the state manager\u00a0and\u00a0Aggregating local models.</p> </li> <li> <p>Here is the code for importing the necessary libraries:</p> </li> </ul> <p>import asyncio, logging, time, numpy as np</p> <p>from typing import List, Dict, Any</p> <p>from fl_main.lib.util.communication_handler import init_fl_server, send, send_websocket, receive</p> <p>from fl_main.lib.util.data_struc import convert_LDict_to_Dict</p> <p>from fl_main.lib.util.helpers import read_config, set_config_file</p> <p>from fl_main.lib.util.messengers import generate_db_push_message, generate_ack_message, generate_cluster_model_dist_message, generate_agent_participation_confirmation_message</p> <p>from fl_main.lib.util.states import ParticipateMSGLocation, ModelUpMSGLocation, PollingMSGLocation, ModelType, AgentMsgType</p> <p>from .state_manager import StateManager</p> <p>from .aggregation import Aggregator</p> <p>CopyExplain</p> <ul> <li>After we import\u00a0the necessary libraries, let us move on to designing     an FL\u00a0Server\u00a0class.</li> </ul> <p>Defining the FL Server class</p> <ul> <li>In practice, it is\u00a0wise to define the\u00a0Server\u00a0class, using which you     can create an instance of the FL server that has the functionalities     discussed in\u00a0earlier, as follows:</li> </ul> <p>class Server:</p> <p>\\\"\\\"\\\"</p> <p>FL Server class defining the functionalities of</p> <p>agent registration, global model synthesis, and</p> <p>handling mechanisms of messages by agents.</p> <p>\\\"\\\"\\\"</p> <p>CopyExplain</p> <ul> <li> <p>Again, the\u00a0server\u00a0class primarily provides the functionalities of     agent registration and global model synthesis and handles the     mechanisms of uploaded local models and polling messages sent from     agents. It also serves as the interface between the aggregator and     database and between the aggregator and agents.</p> </li> <li> <p>The FL server class functionality is now clear -- next is     initializing and configuring the server.</p> </li> </ul> <p>Initializing the FL server</p> <ul> <li>The\u00a0following code inside the\u00a0__init__\u00a0constructor is an example     of the initialization process of the\u00a0Server\u00a0instance:</li> </ul> <p>def __init__(self):</p> <p>config_file = set_config_file(\\\"aggregator\\\")</p> <p>self.config = read_config(config_file)</p> <p>self.sm = StateManager()</p> <p>self.agg = Aggregator(self.sm)</p> <p>self.aggr_ip = self.config[\\'aggr_ip\\']</p> <p>self.reg_socket = self.config[\\'reg_socket\\']</p> <p>self.recv_socket = self.config[\\'recv_socket\\']</p> <p>self.exch_socket = self.config[\\'exch_socket\\']</p> <p>self.db_ip = self.config[\\'db_ip\\']</p> <p>self.db_socket = self.config[\\'db_socket\\']</p> <p>self.round_interval = self.config[\\'round_interval\\']</p> <p>self.is_polling = bool(self.config[\\'polling\\'])</p> <p>self.sm.agg_threshold =</p> <p>self.config[\\'aggregation_threshold\\']</p> <p>CopyExplain</p> <ul> <li> <p>Then,\u00a0self.config\u00a0stores the information from     the\u00a0config_aggregator.json\u00a0file discussed in the preceding code     block.</p> </li> <li> <p>self.sm\u00a0and\u00a0self.agg\u00a0have instances of the state manager class and     aggregator class discussed as follows, respectively.</p> </li> <li> <p>self.aggr_ip\u00a0reads an IP address from the aggregator's configuration     file.</p> </li> <li> <p>Then,\u00a0reg_socket\u00a0and\u00a0recv_socket\u00a0will be set up, where\u00a0reg_socket\u00a0is     used for agents to register themselves together with an aggregator     IP address stored as\u00a0self.aggr_ip, and\u00a0recv_socket\u00a0is used for     receiving local models from agents, together with an aggregator IP     address stored as\u00a0self.aggr_ip. Both\u00a0reg_socket\u00a0and\u00a0recv_socket\u00a0in     this example code can be read from the aggregator's configuration     file.</p> </li> <li> <p>The\u00a0exch_socket\u00a0is the port number used to send the global model     back to the agent together with the agent IP address, which is     initialized with the configuration parameter in the initialization     process.</p> </li> <li> <p>The information to get connected to the database server will then be     configured, where\u00a0dp_ip\u00a0and\u00a0db_socket\u00a0will be the IP address and the     port number of the database server, respectively, all read from     the\u00a0config_aggregator.json\u00a0file.</p> </li> <li> <p>round_interval\u00a0is an interval time to check whether the aggregation     criteria for starting the model aggregation process are met or not.</p> </li> <li> <p>The\u00a0is_polling\u00a0flag is related to whether to use the\u00a0polling\u00a0method     from the agents or not. The polling flag must be the same as the one     used in the agent-side configuration file.</p> </li> <li> <p>agg_threshold\u00a0is also the percentage over the number of collected     local models that is used in     the\u00a0ready_for_local_aggregation\u00a0function where if the percentage of     the collected models is equal to or more than\u00a0agg_threshold, the FL     server starts the aggregation process of the local models.</p> </li> <li> <p>Both\u00a0self.round_interval\u00a0and\u00a0self.agg_threshold\u00a0are read from the     configuration file in this example code too.</p> </li> <li> <p>Now that the\u00a0configuration has been set up, we will talk about how     to register agents that are trying to participate in the FL process.</p> </li> </ul> <p>Agent Registration</p> <ul> <li> <p>In this\u00a0section, the simplified and asynchronous\u00a0register\u00a0function     is described to receive the participation message specifying the     model structures and return socket information for future model     exchanges. It also sends the welcome message back to the agent as a     response.</p> </li> <li> <p>The registration process of agents is described in the following     example code:</p> </li> </ul> <p>async def register(self, websocket: str, path):\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0</p> <p>msg = await receive(websocket)</p> <p>es = self._get_exch_socket(msg)</p> <p>agent_nm = msg[int(ParticipateMSGLocation.agent_name)]</p> <p>agent_id = msg[int(ParticipateMSGLocation.agent_id)]</p> <p>ip = msg[int(ParticipateMSGLocation.agent_ip)]</p> <p>id, es = self.sm.add_agent(agent_nm, agent_id, ip, es)</p> <p>if self.sm.round == 0:</p> <p>await self._initialize_fl(msg)</p> <p>await self._send_updated_global_model( \\</p> <p>websocket, id, es)</p> <p>CopyExplain</p> <ul> <li> <p>In this example code, the received message from an agent, defined     here as\u00a0msg, is decoded by the\u00a0receive\u00a0function imported from     the\u00a0communication_handler\u00a0code.</p> </li> <li> <p>In particular, the\u00a0self.sm.add_agent(agent_name, agent_id, addr,     es)\u00a0function takes the agent name, agent ID, agent IP address, and     the\u00a0exch_socket\u00a0number included in the\u00a0msg\u00a0message in order to     accept the messages from this agent, even if the agent is     temporarily disconnected and then connected again.</p> </li> <li> <p>After that, the registration function checks whether it should move     on to the process of initial models or not, depending on the FL     round that is tracked with\u00a0self.sm.round. If the FL process is not     happening yet, that is, if\u00a0self.sm.round\u00a0is\u00a00, it calls     the\u00a0_initialize_fl(msg)\u00a0function in order to initialize the FL     process.</p> </li> <li> <p>Then, the FL server sends the updated global model back to the agent     by calling the\u00a0_send_updated_global_model(websocket, id,     es)\u00a0function. The function takes the WebSocket, agent ID,     and\u00a0exch_socket\u00a0as parameters and creates a reply\u00a0message to the     agent to notify it whether the participation message has been     accepted or not.</p> </li> <li> <p>The registration process of agents with the FL server is simplified     in this example code here. In a production environment, all the     system information from the agent will be pushed to the database so     that an agent that loses the connection to the FL server can be     recovered anytime by reconnecting to the FL server.</p> </li> <li> <p>Usually, if the FL server is installed in the cloud and agents are     connected to the FL server from their local environment, this     push-back mechanism from the aggregator to agents will not work     because of security settings such as firewalls.</p> </li> <li> <p>We do not discuss the topic of security issues in detail, so you are     encouraged to use the\u00a0polling\u00a0method implemented in     the\u00a0simple-fl\u00a0code to communicate between the\u00a0cloud-based aggregator     and local agents.</p> </li> </ul> <p>Getting socket information to push the global model back to agents</p> <ul> <li>The following\u00a0function\u00a0called\u00a0_get_exch_socket\u00a0takes     a\u00a0participation\u00a0message from the agent and decides which port to use     to reach out to the agent depending on the simulation flag in the     message:</li> </ul> <p>def _get_exch_socket(self, msg):</p> <p>if msg[int(ParticipateMSGLocation.sim_flag)]:</p> <p>es = msg[int(ParticipateMSGLocation.exch_socket)]</p> <p>else:</p> <p>es = self.exch_socket</p> <p>return es</p> <p>CopyExplain</p> <ul> <li> <p>We support a simulation run in this implementation exercise by which     you can run all the FL system components of a database, aggregator,     and multiple agents in one machine.</p> </li> <li> <p>Initializing the FL process if necessary</p> </li> <li> <p>The\u00a0asynchronous\u00a0_initialize_fl\u00a0function is for initializing an FL     process that is only called when the round of FL is\u00a00. The following     is the code to do so:</p> </li> </ul> <p>async def _initialize_fl(self, msg):</p> <p>agent_id = msg[int(ParticipateMSGLocation.agent_id)]</p> <p>model_id = msg[int(ParticipateMSGLocation.model_id)]</p> <p>gene_time = msg[int(ParticipateMSGLocation.gene_time)]</p> <p>lmodels = msg[int(ParticipateMSGLocation.lmodels)]</p> <p>perf_val = msg[int(ParticipateMSGLocation.meta_data)]</p> <p>init_flag = \\</p> <p>bool(msg[int(ParticipateMSGLocation.init_flag)])</p> <p>self.sm.initialize_model_info(lmodels, init_flag)</p> <p>await self._push_local_models( \\</p> <p>agent_id, model_id, lmodels, gene_time, perf_val)</p> <p>self.sm.increment_round()</p> <p>CopyExplain</p> <ul> <li> <p>After extracting the agent ID (agent_id), the model ID (model_id),     local models from an agent (lmodels), the generated time of the     model (gene_time), the performance data (perf_val), and the value     of\u00a0init_flag\u00a0from the received message,     the\u00a0initialize_model_info\u00a0function of the state manager code is     called, which is explained in a later section of This section.</p> </li> <li> <p>This function then pushes the local model to the database by calling     the\u00a0_push_local_models\u00a0function, which is also described in this     section. You can refer to\u00a0the\u00a0Functions to push the local and     global models to the database\u00a0section.</p> </li> <li> <p>After that, the round is incremented to proceed to the first round     in FL.</p> </li> </ul> <p>Confirming agent participation with an updated global model</p> <ul> <li>After\u00a0initializing\u00a0the (cluster) global model, the global models     need to be sent to the agent connected to the aggregator through     this registration process. The     asynchronous\u00a0_send_updated_global_model\u00a0function as follows handles     the process of sending the global models to the agent by taking the     WebSocket information, agent ID, and the port to use to reach out to     the agent as parameters. The following code block describes the     procedure:</li> </ul> <p>async def _send_updated_global_model( \\</p> <p>self, websocket, agent_id, exch_socket):</p> <p>model_id = self.sm.cluster_model_ids[-1]</p> <p>cluster_models = \\</p> <p>convert_LDict_to_Dict(self.sm.cluster_models)</p> <p>reply = generate_agent_participation_confirm_message(</p> <p>self.sm.id, model_id, cluster_models, self.sm.round,</p> <p>agent_id, exch_socket, self.recv_socket)</p> <p>await send_websocket(reply, websocket)</p> <p>CopyExplain</p> <ul> <li> <p>If the FL process has already started, that is, the\u00a0self.sm.round\u00a0is     more than 0 already, we get the cluster models from their buffer and     convert them into a dictionary format with     the\u00a0convert_LDict_to_Dict\u00a0library function.</p> </li> <li> <p>Then, the reply message is packaged using     the\u00a0generate_\u00a0agent_participation_confirm_message\u00a0function and sent     to the agent that just connected or reconnected to the aggregator by     calling the\u00a0send_websocket(reply, websocket)\u00a0function. Please also     refer to the\u00a0Functions to send the global models to the     agents\u00a0section.</p> </li> <li> <p>Now that\u00a0we\u00a0understand the agents' registration process, let's move     on to the implementation of handling the local ML models and polling     messages.</p> </li> </ul> <p>The server for handling messages from local agents</p> <ul> <li>The\u00a0asynchronous\u00a0receive_msg_from_agent\u00a0process\u00a0at the FL server is     constantly running to receive local model updates and to push them     to the database and the memory buffer temporally saving local     models. It also responds to the polling messages from the local     agents. The following code explains this functionality:</li> </ul> <p>async def receive_msg_from_agent(self, websocket, path):</p> <p>msg = await receive(websocket)</p> <p>if msg[int(ModelUpMSGLocation.msg_type)] == \\</p> <p>AgentMsgType.update:</p> <p>await self._process_lmodel_upload(msg)</p> <p>elif msg[int(PollingMSGLocation.msg_type)] == \\</p> <p>AgentMsgType.polling:</p> <p>await self._process_polling(msg, websocket)\u00a0\u00a0</p> <p>CopyExplain</p> <ul> <li>We will then look into the two functions called by     the\u00a0receive_msg_from_agent\u00a0function as\u00a0shown in the\u00a0preceding code     blocks, which are     the\u00a0_process_lmodel_upload\u00a0and\u00a0_process_polling\u00a0functions.</li> </ul> <p>Processing a model upload by local agents</p> <ul> <li>The\u00a0asynchronous\u00a0_process_lmodel_upload\u00a0function deals\u00a0with     the\u00a0AgentMsgType.update\u00a0message. The following code block is about     the function related to receiving the local ML models and putting     them into the buffer in the state manager:</li> </ul> <p>async def _process_lmodel_upload(self, msg):</p> <p>lmodels = msg[int(ModelUpMSGLocation.lmodels)]</p> <p>agent_id = msg[int(ModelUpMSGLocation.agent_id)]</p> <p>model_id = msg[int(ModelUpMSGLocation.model_id)]</p> <p>gene_time = msg[int(ModelUpMSGLocation.gene_time)]</p> <p>perf_val = msg[int(ModelUpMSGLocation.meta_data)]</p> <p>await self._push_local_models( \\</p> <p>agent_id, model_id, lmodels, gene_time, perf_val)</p> <p>self.sm.buffer_local_models( \\</p> <p>lmodels, participate=False, meta_data=perf_val)</p> <p>CopyExplain</p> <ul> <li> <p>First, it extracts the agent ID (agent_id), the model ID (model_id),     local models from an agent (lmodels), the generated time of the     model (gene_time), and the performance data (perf_val) from the     received message, and then calls the\u00a0_push_local_models\u00a0function to     push the local models to the database.</p> </li> <li> <p>The\u00a0buffer_local_models\u00a0function is then called to save the local     models (lmodels) in the memory buffer.     The\u00a0buffer_local_models\u00a0function is described in the\u00a0Maintaining     models for aggregation with the state manager\u00a0section.</p> </li> </ul> <p>Processing polling by agents</p> <ul> <li>The following\u00a0asynchronous\u00a0_process_polling\u00a0function\u00a0deals with     the\u00a0AgentMsgType.polling\u00a0message:</li> </ul> <p>async def _process_polling(self, msg, websocket):</p> <p>if self.sm.round &gt; \\</p> <p>int(msg[int(PollingMSGLocation.round)]):</p> <p>model_id = self.sm.cluster_model_ids[-1]</p> <p>cluster_models = \\</p> <p>convert_LDict_to_Dict(self.sm.cluster_models)</p> <p>msg = generate_cluster_model_dist_message( \\</p> <p>self.sm.id, model_id, self.sm.round, \\</p> <p>cluster_models)</p> <p>await send_websocket(msg, websocket)</p> <p>else:</p> <p>msg = generate_ack_message()</p> <p>await send_websocket(msg, websocket)\u00a0\u00a0</p> <p>CopyExplain</p> <ul> <li> <p>If the FL round (self.sm.round) is greater than the local FL round     included in the received message that is maintained by the local     agent itself, it means that the model aggregation is done during the     period between the time when the agent polled to the aggregator last     time and now.</p> </li> <li> <p>In this case,\u00a0cluster_models\u00a0that are converted into a dictionary     format are packaged into a response message     by\u00a0generate_cluster_model_dist_message\u00a0and sent back to the agent     via the\u00a0send_websocket\u00a0function.</p> </li> <li> <p>Otherwise, the aggregator just returns the\u00a0ACK\u00a0message to the     agent, generated by the\u00a0generate_ack_message\u00a0function.</p> </li> <li> <p>Now we are ready to aggregate the local models received from the     agents, so let us look into the model aggregation routine.</p> </li> </ul> <p>The global model synthesis routine</p> <ul> <li> <p>The global\u00a0model synthesis\u00a0routine process designed in\u00a0async def     model_synthesis_routine(self)\u00a0in the FL server periodically checks     the number of stored models and executes global model synthesis if     there are enough local models collected to meet the aggregation     threshold.</p> </li> <li> <p>The following code describes the model synthesis routine process     that periodically checks the aggregation criteria and executes model     synthesis:</p> </li> </ul> <p>async def model_synthesis_routine(self):</p> <p>while True:</p> <p>await asyncio.sleep(self.round_interval)</p> <p>if self.sm.ready_for_local_aggregation():\u00a0\u00a0</p> <p>self.agg.aggregate_local_models()</p> <p>await self._push_cluster_models()</p> <p>if self.is_polling == False:</p> <p>await self._send_cluster_models_to_all()</p> <p>self.sm.increment_round()</p> <p>CopyExplain</p> <ul> <li> <p>This process is asynchronous, running with a\u00a0while\u00a0loop.</p> </li> <li> <p>In particular, once the criteria set     by\u00a0ready_for_local_aggregation\u00a0(explained in the\u00a0Maintaining models     for aggregation with the state manager\u00a0section) are met,     the\u00a0aggregate_local_models\u00a0function imported from     the\u00a0aggregator.py\u00a0file is called, where this function averages the     weights of the collected local models based on\u00a0FedAvg. Further     explanation of the\u00a0aggregate_local_models\u00a0function can be found in     the\u00a0Aggregating local models\u00a0section.</p> </li> <li> <p>Then,\u00a0await self._push_cluster_models()\u00a0is called to push the     aggregated cluster global model to the database.</p> </li> <li> <p>await self._send_cluster_models_to_all()\u00a0is for sending the updated     global model to all the agents connected to the aggregator if     the\u00a0polling\u00a0method is not used.</p> </li> <li> <p>Last but not least, the FL round is incremented     by\u00a0self.sm.increment_round().</p> </li> <li> <p>Once the cluster global model is generated, the models need to be     sent to the connected\u00a0agents with\u00a0the functions described in the     following section.</p> </li> </ul> <p>Functions to send the global models to the agents</p> <ul> <li>The\u00a0functionality\u00a0of sending global models to the connected agents     is dealt with by the\u00a0_send_cluster_models_to_all\u00a0function. This is     an asynchronous function to send out cluster global models to all     agents under this aggregator as follows:</li> </ul> <p>async def _send_cluster_models_to_all(self):</p> <p>model_id = self.sm.cluster_model_ids[-1]</p> <p>cluster_models = \\</p> <p>convert_LDict_to_Dict(self.sm.cluster_models)</p> <p>msg = generate_cluster_model_dist_message( \\</p> <p>self.sm.id, model_id, self.sm.round, \\</p> <p>cluster_models)</p> <p>for agent in self.sm.agent_set:</p> <p>await send(msg, agent[\\'agent_ip\\'], agent[\\'socket\\'])</p> <p>CopyExplain</p> <ul> <li> <p>After getting the cluster models' information, it creates the     message including the cluster models, round, model ID, and     aggregator ID information using     the\u00a0generate_cluster_model_dist_message\u00a0function and calls     the\u00a0send\u00a0function from the\u00a0communication_handler\u00a0libraries to send     the global models to all the agents in the\u00a0agent_set\u00a0registered     through the agent participation process.</p> </li> <li> <p>Sending the cluster global models to the connected agents has now     been explained. Next, we explain\u00a0how to push the local and cluster     models to the database.</p> </li> </ul> <p>Functions to push the local and global models to the database</p> <ul> <li>The\u00a0_push_local_models\u00a0and\u00a0_push_cluster_models\u00a0functions are both     called internally to push and send the local models and cluster     global models to the database.</li> </ul> <p>Pushing local models to the database</p> <ul> <li>Here is\u00a0the\u00a0_push_local_models\u00a0function\u00a0for pushing a given set of     local models to the database:</li> </ul> <p>async def _push_local_models(self, agent_id: str, \\</p> <p>model_id: str, local_models: Dict[str, np.array], \\</p> <p>gene_time: float, performance: Dict[str, float]) \\</p> <p>-&gt; List[Any]:</p> <p>return await self._push_models(</p> <p>agent_id, ModelType.local, local_models, \\</p> <p>model_id, gene_time, performance)</p> <p>CopyExplain</p> <ul> <li> <p>The\u00a0_push_local_models\u00a0function takes parameters such as the agent     ID, local models, the model ID, the generated time of the model, and     the performance data, and returns a response message if there is     one.</p> </li> <li> <p>Pushing cluster models to the database</p> </li> <li> <p>The\u00a0following\u00a0_push_cluster_models\u00a0function\u00a0is for pushing the     cluster global models to the database:</p> </li> </ul> <p>async def _push_cluster_models(self) -&gt; List[Any]:</p> <p>model_id = self.sm.cluster_model_ids[-1]</p> <p>models = convert_LDict_to_Dict(self.sm.cluster_models)</p> <p>meta_dict = dict({ \\</p> <p>\\\"num_samples\\\" : self.sm.own_cluster_num_samples})</p> <p>return await self._push_models( \\</p> <p>self.sm.id, ModelType.cluster, models, model_id, \\</p> <p>time.time(), meta_dict)</p> <p>CopyExplain</p> <ul> <li> <p>_push_cluster_models\u00a0in this code does not take any parameters, as     those parameters can be obtained from the instance information and     buffered memory data of the state manager.</p> </li> <li> <p>For example,\u00a0self.sm.cluster_model_ids[-1]\u00a0obtains the ID of the     latest cluster model, and\u00a0self.sm.cluster_models\u00a0stores the latest     cluster model itself, which is converted into\u00a0models\u00a0with a     dictionary format to be sent\u00a0to the\u00a0database. It also     creates\u00a0mata_dict\u00a0to store the number of samples.</p> </li> <li> <p>Pushing ML models to the database</p> </li> <li> <p>Both the\u00a0preceding\u00a0functions call the\u00a0_push_models\u00a0function as     follows:</p> </li> </ul> <p>async def _push_models(</p> <p>self, component_id: str, model_type: ModelType,</p> <p>models: Dict[str, np.array], model_id: str,</p> <p>gene_time: float, performance_dict: Dict[str, float])</p> <p>-&gt; List[Any]:</p> <p>msg = generate_db_push_message(component_id, \\</p> <p>self.sm.round, model_type, models, model_id, \\</p> <p>gene_time, performance_dict)</p> <p>resp = await send(msg, self.db_ip, self.db_socket)</p> <p>return resp</p> <p>CopyExplain</p> <ul> <li> <p>In this code example, the\u00a0_push_models\u00a0function takes parameters     such as\u00a0component_id\u00a0(the ID of the aggregator or     agent),\u00a0model_type, such as local or cluster     model,\u00a0models\u00a0themselves,\u00a0model_id,\u00a0gene_time\u00a0(the time the model is     created), and\u00a0performance_dict\u00a0as the performance metrics of the     models.</p> </li> <li> <p>Then, the message to be sent to the database (using     the\u00a0send\u00a0function) is created by     the\u00a0generate_db_push_message\u00a0function, taking these parameters     together with the FL round information. It returns a response     message from the database.</p> </li> <li> <p>Now that we have explained all the core functionalities related to     the FL server, let us look into the\u00a0role of\u00a0the state manager, which     maintains all the models needed for the aggregation process.</p> </li> </ul> <p>Maintaining models for aggregation with the \u2028state manager</p> <ul> <li>In this section, we\u00a0will explain\u00a0state_manager.py,\u00a0which handles     maintaining the models and necessary volatile information related to     the aggregation of local models.</li> </ul> <p>State Manager Library Imports</p> <ul> <li>This code imports\u00a0the\u00a0following. The internal libraries     for\u00a0data_struc,\u00a0helpers, and\u00a0states\u00a0are introduced in     the\u00a0Appendix,\u00a0Exploring Internal Libraries:</li> </ul> <p>import numpy as np</p> <p>import logging</p> <p>import time</p> <p>from typing import Dict, Any</p> <p>from fl_main.lib.util.data_struc import LimitedDict</p> <p>from fl_main.lib.util.helpers import generate_id, generate_model_id</p> <p>from fl_main.lib.util.states import IDPrefix</p> <p>CopyExplain</p> <ul> <li>After importing the necessary libraries, let's define the state     manager class.</li> </ul> <p>Defining the state manager class</p> <ul> <li>The state\u00a0manager\u00a0class (Class StateManager), as seen     in\u00a0state_manager.py, is defined in the following code:</li> </ul> <p>class StateManager:</p> <p>\\\"\\\"\\\"</p> <p>StateManager instance keeps the state of an aggregator.</p> <p>Functions are listed with this indentation.</p> <p>\\\"\\\"\\\"</p> <p>CopyExplain</p> <ul> <li> <p>This keeps track of the state information of an aggregator. The     volatile state of an aggregator and agents should also be stored,     such as local models, agents' info connected to the aggregator,     cluster models generated by the aggregation process, and the current     round number.</p> </li> <li> <p>After defining the state manager, let us move on to initializing the     state manager.</p> </li> </ul> <p>Initializing the state manager</p> <ul> <li>In the\u00a0__init__\u00a0constructor, the\u00a0information\u00a0related to the FL     process is\u00a0configured. The following code is an example of how to     construct the state manager:</li> </ul> <p>def __init__(self):</p> <p>self.id = generate_id()</p> <p>self.agent_set = list()</p> <p>self.mnames = list()</p> <p>self.round = 0</p> <p>self.local_model_buffers = LimitedDict(self.mnames)</p> <p>self.local_model_num_samples = list()</p> <p>self.cluster_models = LimitedDict(self.mnames)</p> <p>self.cluster_model_ids = list()</p> <p>self.initialized = False</p> <p>self.agg_threshold = 1.0</p> <p>CopyExplain</p> <ul> <li> <p>The ID of the\u00a0self.id\u00a0aggregator can be generated randomly using     the\u00a0generate_id()\u00a0function from the\u00a0util.helpers\u00a0library.</p> </li> <li> <p>self.agent_set\u00a0is a set of agents connected to the aggregator where     the format of the set is a collection of dictionary information,     related to agents in this case.</p> </li> <li> <p>self.mnames\u00a0stores the names of each layer of the ML models to be     aggregated in a list format.</p> </li> <li> <p>self.round\u00a0is initialized to be\u00a00\u00a0so that the round of FL is     initialized.</p> </li> <li> <p>local_model_buffers\u00a0is a list of local models collected by agents     stored in the memory\u00a0space.\u00a0local_model_buffers\u00a0accepts the local     models sent from the agents for\u00a0each FL round, and once the round is     completed by the aggregation process, this buffer is cleared and     starts accepting the next round's local models.</p> </li> <li> <p>self.local_model_num_samples\u00a0is a list that stores the number of     data samples for the models that are collected in the buffer.</p> </li> <li> <p>self.cluster_models\u00a0is a collection of global cluster models in     the\u00a0LimitedDict\u00a0format, and\u00a0self.cluster_model_ids\u00a0is a list of IDs     of cluster models.</p> </li> <li> <p>self.initialized\u00a0becomes\u00a0True\u00a0once the initial global model is set     and is\u00a0False\u00a0otherwise.</p> </li> <li> <p>self.agg_threshold\u00a0is initialized to be\u00a01.0, which is overwritten by     the value specified in the\u00a0config_aggregator.json\u00a0file.</p> </li> <li> <p>After initializing the state manager, let us investigate     initializing a global model next.</p> </li> </ul> <p>Initializing a global model</p> <ul> <li>The\u00a0following\u00a0initialize_model_info\u00a0function\u00a0sets up the initial     global\u00a0model to be used by the other agents:</li> </ul> <p>def initialize_model_info(self, lmodels, \\</p> <p>init_weights_flag):</p> <p>for key in lmodels.keys():</p> <p>self.mnames.append(key)</p> <p>self.local_model_buffers = LimitedDict(self.mnames)</p> <p>self.cluster_models = LimitedDict(self.mnames)</p> <p>self.clear_lmodel_buffers()</p> <p>if init_weights_flag:</p> <p>self.initialize_models(lmodels, \\</p> <p>weight_keep=init_weights_flag)</p> <p>else:</p> <p>self.initialize_models(lmodels, weight_keep=False)</p> <p>CopyExplain</p> <ul> <li> <p>It fills up the model names (self.mnames) extracted from the local     models (lmodels) sent from an initial agent.</p> </li> <li> <p>Together with the model     names,\u00a0local_model_buffers\u00a0and\u00a0cluster_models\u00a0are re-initialized     too. After clearing the local model buffers, it calls     the\u00a0initialize_models\u00a0function.</p> </li> <li> <p>The following\u00a0initialize_models\u00a0function initializes the structure     of neural networks (numpy.array) based on the initial base models     received as parameters of models\u00a0with a dictionary format     (str\u00a0or\u00a0np.array):</p> </li> </ul> <p>def initialize_models(self, models: Dict[str, np.array], \\</p> <p>weight_keep: bool = False):</p> <p>self.clear_saved_models()</p> <p>for mname in self.mnames:</p> <p>if weight_keep:</p> <p>m = models[mname]</p> <p>else:</p> <p>m = np.zeros_like(models[mname])</p> <p>self.cluster_models[mname].append(m)</p> <p>id = generate_model_id(IDPrefix.aggregator, \\</p> <p>self.id, time.time())</p> <p>self.cluster_model_ids.append(id)</p> <p>self.initialized = True</p> <p>CopyExplain</p> <ul> <li> <p>For each layer of the model, defined here as model names, this     function fills out the model parameters. Depending on     the\u00a0weight_keep\u00a0flag, the model is initialized with zeros or     parameters that are received.</p> </li> <li> <p>This way, the initial cluster global model is constructed together     with the randomized model ID. If an agent sends a different ML model     than the model architecture defined here, the aggregator rejects the     acceptance of the model or gives an error message to the agent.     Nothing is returned.</p> </li> <li> <p>So, we have\u00a0covered\u00a0initializing the global model. In the following     section, we will explain the core part of the FL process, which is     checking aggregation criteria.</p> </li> </ul> <p>Checking the aggregation criteria</p> <ul> <li>The\u00a0following\u00a0code, called\u00a0ready_for_local_aggregation, is     for\u00a0checking the aggregation criteria:</li> </ul> <p>def ready_for_local_aggregation(self) -&gt; bool:</p> <p>if len(self.mnames) == 0:</p> <p>return False</p> <p>num_agents = int(self.agg_threshold * \\</p> <p>len(self.agent_set))</p> <p>if num_agents == 0: num_agents = 1</p> <p>num_collected_lmodels = \\</p> <p>len(self.local_model_buffers[self.mnames[0]])</p> <p>if num_collected_lmodels &gt;= num_agents:</p> <p>return True</p> <p>else:</p> <p>return False\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0</p> <p>CopyExplain</p> <ul> <li> <p>This\u00a0ready_for_local_aggregation\u00a0function returns a\u00a0bool\u00a0value to     identify whether the aggregator can start the aggregation process.     It returns\u00a0True\u00a0if it satisfies the aggregation criteria (such as     collecting enough local models to aggregate) and\u00a0False\u00a0otherwise.     The aggregation threshold,\u00a0agg_threshold, is     configured\u00a0in\u00a0the\u00a0config_aggregator.json\u00a0file.</p> </li> <li> <p>The following section is about buffering the local models that are     used for the aggregation process.</p> </li> </ul> <p>Buffering the local models</p> <ul> <li>The following\u00a0code\u00a0on\u00a0buffer_local_models\u00a0stores local models from     an agent in the local model buffer:</li> </ul> <p>def buffer_local_models(self, models: Dict[str, np.array],</p> <p>participate=False, meta_data: Dict[Any, Any] = {}):</p> <p>if not participate:\u00a0\u00a0</p> <p>for key, model in models.items():</p> <p>self.local_model_buffers[key].append(model)</p> <p>try:</p> <p>num_samples = meta_data[\\\"num_samples\\\"]</p> <p>except:</p> <p>num_samples = 1</p> <p>self.local_model_num_samples.append( \\</p> <p>int(num_samples))</p> <p>else:\u00a0\u00a0</p> <p>pass</p> <p>if not self.initialized:</p> <p>self.initialize_models(models)</p> <p>CopyExplain</p> <ul> <li> <p>The parameters include the local\u00a0models\u00a0formatted as a dictionary as     well as meta-information such as the number of samples.</p> </li> <li> <p>First, this function checks whether the local model sent from an     agent is either the initial model or not by checking the     participation flag. If it is an initial model, it calls     the\u00a0initialize_model\u00a0function, as shown in the preceding code block.</p> </li> <li> <p>Otherwise, for each layer of the model defined with model names, it     stores the\u00a0numpy\u00a0array in the\u00a0self.local_model_buffers. The\u00a0key\u00a0is     the model name and\u00a0model\u00a0mentioned in the preceding code are the     actual parameters of the model.</p> </li> <li> <p>Optionally, it can accept the number of samples or data sources that     the agent has used for the retraining process and push it to     the\u00a0self.\u00a0local_model_num_samples\u00a0buffer.</p> </li> <li> <p>This function is called when the FL server receives the local models     from an agent during the\u00a0receive_msg_from_agent\u00a0routine.</p> </li> <li> <p>With that, the local model buffer has been explained. Next, we will     explain how to clear the saved\u00a0models\u00a0so that aggregation can     continue without having to store unnecessary models in the buffer.</p> </li> </ul> <p>Clearing the saved models</p> <ul> <li>The\u00a0following\u00a0clear_saved_models\u00a0function clears all cluster models     stored in this\u00a0round:</li> </ul> <p>def clear_saved_models(self):</p> <p>for mname in self.mnames:</p> <p>self.cluster_models[mname].clear()</p> <p>CopyExplain</p> <ul> <li> <p>This function is called when initializing the FL process at the very     beginning and the cluster global model is emptied to start a fresh     FL round again.</p> </li> <li> <p>The following function, the\u00a0clear_lmodel_buffers\u00a0function, clears     all the buffered local models to prepare for the next FL round:</p> </li> </ul> <p>def clear_lmodel_buffers(self):</p> <p>for mname in self.mnames:</p> <p>self.local_model_buffers[mname].clear()</p> <p>self.local_model_num_samples = list()</p> <p>CopyExplain</p> <ul> <li> <p>Clearing the local models in\u00a0local_model_buffers\u00a0is critical when     proceeding to the next FL round. Without this process, the models to     be aggregated are mixed up with the non-relevant models from other     rounds, and eventually, the performance\u00a0of the\u00a0FL is sometimes     degraded.</p> </li> <li> <p>Next, we will explain the basic framework of adding agents during     the FL process.</p> </li> </ul> <p>Adding agents</p> <ul> <li>This\u00a0add_agent\u00a0function\u00a0deals\u00a0with brief agent registration using     system memory:</li> </ul> <p>def add_agent(self, agent_name: str, agent_id: str, \\</p> <p>agent_ip: str, socket: str):</p> <p>for agent in self.agent_set:</p> <p>if agent_name == agent[\\'agent_name\\']:</p> <p>return agent[\\'agent_id\\'], agent[\\'socket\\']</p> <p>agent = {</p> <p>\\'agent_name\\': agent_name,</p> <p>\\'agent_id\\': agent_id,</p> <p>\\'agent_ip\\': agent_ip,</p> <p>\\'socket\\': socket</p> <p>}</p> <p>self.agent_set.append(agent)</p> <p>return agent_id, socket</p> <p>CopyExplain</p> <ul> <li> <p>This function just adds agent-related information to     the\u00a0self.agent_set\u00a0list. The agent information includes the agent     name, agent ID, agent IP address, and the\u00a0socket\u00a0number to reach out     to the agent.</p> </li> <li> <p>The\u00a0socket\u00a0number can be used when sending the cluster global model     to the agent connected to the aggregator and when the\u00a0push\u00a0method is     used for communication between an aggregator and an agent.</p> </li> <li> <p>This function is only called during the agent registration process     and returns the agent ID and the\u00a0socket\u00a0number.</p> </li> <li> <p>If the agent is already registered, which means there is already an     agent with the same name in\u00a0agent_set, it returns the agent ID and     the\u00a0socket\u00a0number of the existing agent.</p> </li> <li> <p>Again, this\u00a0push\u00a0communication method from an aggregator to agents     does not work under certain security circumstances. It is     recommended to use the\u00a0polling\u00a0method that the agents use to     constantly check whether the aggregator has an updated global model     or not.</p> </li> <li> <p>The agent\u00a0registration\u00a0mechanism can be expanded using a database,     which will give you better management of the distributed systems.</p> </li> <li> <p>Next, we will touch on incrementing the FL round.</p> </li> </ul> <p>Incrementing the FL round</p> <ul> <li>The\u00a0increment_round\u00a0function\u00a0just\u00a0increments the round number     precisely managed by the state manager:</li> </ul> <p>def increment_round(self):</p> <p>self.round += 1</p> <p>CopyExplain</p> <ul> <li> <p>Incrementing rounds is a critical part of the FL process for     supporting the continuous learning operation. This function is only     called after registering the initial global model or after each     model aggregation process.</p> </li> <li> <p>Now that we understand how the FL works with the state manager, in     the following section, we will talk about the model aggregation     framework.</p> </li> </ul> <p>Aggregating local models</p> <ul> <li>The\u00a0aggregation.py\u00a0code\u00a0handles aggregating local models with a     bunch of aggregation algorithms. In the code example, we only     support\u00a0FedAvg, as discussed in the following sections.</li> </ul> <p>Importing the libraries for the aggregator</p> <ul> <li>The\u00a0aggregation.py\u00a0code\u00a0imports the following:</li> </ul> <p>import logging</p> <p>import time</p> <p>import numpy as np</p> <p>from typing import List</p> <p>from .state_manager import StateManager</p> <p>from fl_main.lib.util.helpers import generate_model_id</p> <p>from fl_main.lib.util.states import IDPrefix</p> <p>CopyExplain</p> <ul> <li>The imported state manager's role and functionalities are discussed     in the\u00a0Maintaining models for aggregation with the state     manager\u00a0section, and the\u00a0helpers\u00a0and\u00a0states\u00a0libraries are     introduced in the\u00a0Appendix,\u00a0Exploring Internal Libraries.</li> </ul> <p>- After importing the necessary libraries, let's define the aggregator class.</p> <p>Defining and initializing the aggregator class</p> <ul> <li>The following\u00a0code for\u00a0class Aggregator\u00a0defines the core process of     the aggregator, which provides a set of mathematical functions for     computing the aggregated models:</li> </ul> <p>class Aggregator:</p> <p>\\\"\\\"\\\"</p> <p>Aggregator class instance provides a set of</p> <p>mathematical functions to compute aggregated models.</p> <p>\\\"\\\"\\\"</p> <p>CopyExplain</p> <ul> <li>The following\u00a0__init__\u00a0function just sets up the state manager     of the aggregator to access the model buffers:</li> </ul> <p>def __init__(self, sm: StateManager):</p> <p>self.sm = sm</p> <p>CopyExplain</p> <ul> <li>Once the\u00a0aggregator class is defined and initialized, let's look at     the actual FedAvg algorithm implementation.</li> </ul> <p>Defining the aggregate_local_models function</p> <ul> <li>The following\u00a0aggregate_local_models\u00a0function is the code for     aggregating the local models:</li> </ul> <p>def aggregate_local_models(self):</p> <p>for mname in self.sm.mnames:</p> <p>self.sm.cluster_models[mname][0] \\</p> <p>= self._average_aggregate( \\</p> <p>self.sm.local_model_buffers[mname], \\</p> <p>self.sm.local_model_num_samples)</p> <p>self.sm.own_cluster_num_samples = \\</p> <p>sum(self.sm.local_model_num_samples)</p> <p>id = generate_model_id( \\</p> <p>IDPrefix.aggregator, self.sm.id, time.time())</p> <p>self.sm.cluster_model_ids.append(id)</p> <p>self.sm.clear_lmodel_buffers()</p> <p>CopyExplain</p> <ul> <li> <p>This function can be called after the aggregation criteria are     satisfied, such as the aggregation threshold defined in     the\u00a0config_aggregator.json\u00a0file. The aggregation process uses local     ML models buffered in the memory of the state manager.</p> </li> <li> <p>Those local ML models are sent from the registered agents. For each     layer of the models defined by\u00a0mname, the weights of the model are     averaged by the\u00a0_average_aggregate\u00a0function as follows to realize     FedAvg. After averaging the model parameters of all the     layers,\u00a0cluster_models\u00a0is updated, which is sent to all the agents.</p> </li> <li> <p>Then, the local\u00a0model buffer is cleared to be ready for the next     round of the FL process.</p> </li> </ul> <p>The FedAvg function</p> <ul> <li>The following\u00a0function,\u00a0_average_aggregate, called by the     preceding\u00a0aggregate_local_models\u00a0function, is the code that realizes     the\u00a0FedAvg\u00a0aggregation method:</li> </ul> <p>def _average_aggregate(self, buffer: List[np.array],</p> <p>num_samples: List[int]) -&gt; np.array:</p> <p>denominator = sum(num_samples)</p> <p>model = float(num_samples[0])/denominator * buffer[0]</p> <p>for i in range(1, len(buffer)):</p> <p>model += float(num_samples[i]) /</p> <p>denominator * buffer[i]</p> <p>return model</p> <p>CopyExplain</p> <ul> <li> <p>In the\u00a0_average_aggregate\u00a0function, the computation is simple     enough that, for each buffer of the given list of ML models, it     takes averaged parameters for the models.</p> </li> <li> <p>The basics of model aggregation are discussed earlier. It returns     the weighted aggregated models with\u00a0np.array.</p> </li> <li> <p>Now that we have covered all the essential functionalities of the FL     server and aggregator, next, we will talk about how to run the FL     server itself.</p> </li> </ul> <p>Running the FL server</p> <ul> <li>Here is an\u00a0example of running the FL server. In order to run the FL     server, you will just execute the following code:</li> </ul> <p>if __name__ == \\\"__main__\\\":</p> <p>s = Server()</p> <p>init_fl_server(s.register,</p> <p>s.receive_msg_from_agent,</p> <p>s.model_synthesis_routine(),</p> <p>s.aggr_ip, s.reg_socket, s.recv_socket)</p> <p>CopyExplain</p> <ul> <li> <p>The\u00a0register,\u00a0receive_msg_from_agnet,     and\u00a0model_synthesis_routine\u00a0functions of the instance of the FL     server are for starting the registration process of the agents,     receiving messages from the agents, and starting the model synthesis     process to create a global model, which are all started using     the\u00a0init_fl_server\u00a0function from     the\u00a0communication_handler\u00a0libraries.</p> </li> <li> <p>We have covered\u00a0all the core modules of the aggregator with the FL     server. They can work with the database server, which will be     discussed in the following section.</p> </li> </ul> <p>Implementing and running the database server</p> <ul> <li> <p>The database\u00a0server\u00a0can be hosted either on the same machine as the     aggregator server or separately from the aggregator server.</p> </li> <li> <p>Whether the database server is hosted on the same machine or not,     the code introduced here is still applicable to both cases.</p> </li> <li> <p>The database-related code is found in the\u00a0fl_main/pseudodb\u00a0folder of     the GitHub repository provided alongside this book.</p> </li> </ul> <p>Configuring the database</p> <ul> <li>The following\u00a0code is an example of the database-side configuration     parameters saved as\u00a0config_db.json:</li> </ul> <p>{</p> <p>\\\"db_ip\\\": \\\"localhost\\\",</p> <p>\\\"db_socket\\\": \\\"9017\\\",</p> <p>\\\"db_name\\\": \\\"sample_data\\\",</p> <p>\\\"db_data_path\\\": \\\"./db\\\",</p> <p>\\\"db_model_path\\\": \\\"./db/models\\\"</p> <p>}</p> <p>CopyExplain</p> <ul> <li> <p>In particular,\u00a0db_data_path\u00a0is the location of the SQLite database     and\u00a0db_model_path\u00a0is the location of the ML model binary files.</p> </li> <li> <p>The\u00a0config_db.json\u00a0file\u00a0can be found in the\u00a0setup\u00a0folder.</p> </li> <li> <p>Next, let's define the database server and import the necessary     libraries.</p> </li> </ul> <p>Defining the database server</p> <ul> <li> <p>The main\u00a0functionality of the\u00a0pseudo_db.py\u00a0code is accepting     messages that contain local and cluster global models.</p> </li> <li> <p>Importing the libraries for the pseudo database</p> </li> <li> <p>First, the\u00a0pseudo_db.py\u00a0code\u00a0imports\u00a0the following:</p> </li> </ul> <p>import pickle, logging, time, os</p> <p>from typing import Any, List</p> <p>from .sqlite_db import SQLiteDBHandler</p> <p>from fl_main.lib.util.helpers import generate_id, read_config, set_config_file</p> <p>from fl_main.lib.util.states import DBMsgType, DBPushMsgLocation, ModelType</p> <p>from fl_main.lib.util.communication_handler import init_db_server, send_websocket, receive</p> <p>CopyExplain</p> <ul> <li>It imports the basic general libraries as well     as\u00a0SQLiteDBHandler\u00a0(discussed later in the\u00a0Defining the database     with SQLite\u00a0section) and the functions from the\u00a0lib/util\u00a0libraries     that are discussed in the\u00a0Appendix,\u00a0Exploring Internal     Libraries.</li> </ul> <p>Defining the PseudoDB class</p> <ul> <li>The\u00a0PseudoDB\u00a0class\u00a0is then defined to\u00a0create an instance that     receives models and their data from an aggregator and pushes them to     an actual database (SQLite, in this case):</li> </ul> <p>class PseudoDB:</p> <p>\\\"\\\"\\\"</p> <p>PseudoDB class instance receives models and their data</p> <p>from an aggregator, and pushes them to database</p> <p>\\\"\\\"\\\"</p> <p>CopyExplain</p> <ul> <li>Now, let us move on to initializing the instance of\u00a0PseudoDB.</li> </ul> <p>Initializing PseudoDB</p> <ul> <li>Then, the\u00a0initialization process,\u00a0__init__, is\u00a0defined as     follows:</li> </ul> <p>def __init__(self):</p> <p>self.id = generate_id()</p> <p>self.config = read_config(set_config_file(\\\"db\\\"))</p> <p>self.db_ip = self.config[\\'db_ip\\']</p> <p>self.db_socket = self.config[\\'db_socket\\']</p> <p>self.data_path = self.config[\\'db_data_path\\']</p> <p>if not os.path.exists(self.data_path):</p> <p>os.makedirs(self.data_path)</p> <p>self.db_file = \\</p> <p>f\\'{self.data_path}/model_data{time.time()}.db\\'</p> <p>self.dbhandler = SQLiteDBHandler(self.db_file)</p> <p>self.dbhandler.initialize_DB()</p> <p>self.db_model_path = self.config[\\'db_model_path\\']</p> <p>if not os.path.exists(self.db_model_path):</p> <p>os.makedirs(self.db_model_path)</p> <p>CopyExplain</p> <ul> <li> <p>The initialization process generates the ID of the instance and sets     up various parameters such as the database socket (db_socket), the     database IP address (db_ip), the path to the database (data_path),     and the database file (db_file), all configured from\u00a0config_db.json.</p> </li> <li> <p>dbhandler\u00a0stores the instance of\u00a0SQLiteDBHandler\u00a0and calls     the\u00a0initialize_DB\u00a0function to create an SQLite database.</p> </li> <li> <p>Folders for\u00a0data_path\u00a0and\u00a0db_model_path\u00a0are created if they do not     already exist.</p> </li> <li> <p>After the initialization process of\u00a0PseudoDB, we need to design the     communication module that accepts the messages from the aggregators.</p> </li> <li> <p>We again use WebSocket for communicating with an aggregator and     start this module as a server to accept and respond to\u00a0messages from     an aggregator.</p> </li> <li> <p>In\u00a0this design, we do not push messages from the database server to     an aggregator or agents in order to make the FL mechanism simpler.</p> </li> <li> <p>Handling messages from the aggregator</p> </li> <li> <p>The following\u00a0code for the\u00a0async def handler\u00a0function,     which\u00a0takes\u00a0websocket\u00a0as a parameter, receives messages from the     aggregator and returns the requested information:</p> </li> </ul> <p>async def handler(self, websocket, path):</p> <p>msg = await receive(websocket)</p> <p>msg_type = msg[DBPushMsgLocation.msg_type]</p> <p>reply = list()</p> <p>if msg_type == DBMsgType.push:</p> <p>self._push_all_data_to_db(msg)</p> <p>reply.append(\\'confirmation\\')</p> <p>else:</p> <p>raise TypeError(f\\'Undefined DB Message Type: \\</p> <p>{msg_type}.\\')</p> <p>await send_websocket(reply, websocket)</p> <p>CopyExplain</p> <ul> <li> <p>In the\u00a0handler\u00a0function, once it decodes the received message from     an aggregator, the\u00a0handler\u00a0function checks whether the message type     is\u00a0push\u00a0or not.</p> </li> <li> <p>If so, it tries to push the local or cluster models to the database     by calling the _push_all_data_to_db\u00a0function.</p> </li> <li> <p>Otherwise, it will show an error message. The confirmation message     about pushing the models to the database can then be sent back to     the aggregator.</p> </li> <li> <p>Here, we only\u00a0defined the type of the\u00a0push\u00a0message, but\u00a0you can     define as many types as possible, together with the enhancement of     the database schema and design.</p> </li> </ul> <p>Pushing all the data to the database</p> <ul> <li>The\u00a0following code for\u00a0_push_all_data_to_db\u00a0pushes the models'     information to the database:</li> </ul> <p>def _push_all_data_to_db(self, msg: List[Any]):</p> <p>pm = self._parse_message(msg)</p> <p>self.dbhandler.insert_an_entry(*pm)</p> <p>model_id = msg[int(DBPushMsgLocation.model_id)]</p> <p>models = msg[int(DBPushMsgLocation.models)]</p> <p>fname = f\\'{self.db_model_path}/{model_id}.binaryfile\\'</p> <p>with open(fname, \\'wb\\') as f:</p> <p>pickle.dump(models, f)</p> <p>CopyExplain</p> <ul> <li> <p>The models' information is extracted by the\u00a0_parse_message\u00a0function     and passed to the\u00a0_insert_an_entry\u00a0function.</p> </li> <li> <p>Then, the actual models are saved in the local server filesystems,     where the filename of the models and the path are defined     by\u00a0db_model_path\u00a0and\u00a0fname\u00a0here.</p> </li> </ul> <p>Parsing the message</p> <ul> <li>The\u00a0_parse_message\u00a0function\u00a0just extracts the\u00a0parameters from the     received message:</li> </ul> <p>def _parse_message(self, msg: List[Any]):</p> <p>component_id = msg[int(DBPushMsgLocation.component_id)]</p> <p>r = msg[int(DBPushMsgLocation.round)]</p> <p>mt = msg[int(DBPushMsgLocation.model_type)]</p> <p>model_id = msg[int(DBPushMsgLocation.model_id)]</p> <p>gene_time = msg[int(DBPushMsgLocation.gene_time)]</p> <p>meta_data = msg[int(DBPushMsgLocation.meta_data)]</p> <p>local_prfmc = 0.0</p> <p>if mt == ModelType.local:</p> <p>try: local_prfmc = meta_data[\\\"accuracy\\\"]</p> <p>except: pass</p> <p>num_samples = 0</p> <p>try: num_samples = meta_data[\\\"num_samples\\\"]</p> <p>except: pass</p> <p>return component_id, r, mt, model_id, gene_time, \\</p> <p>local_prfmc, num_samples</p> <p>CopyExplain</p> <ul> <li> <p>This function parses the received message into parameters related to     agent ID or aggregator ID (component_id), round number (r), message     type (mt),\u00a0model_id, time of generation of the models (gene_time),     and performance data as a dictionary format (meta_data).</p> </li> <li> <p>The local performance data,\u00a0local_prfmc, is extracted when the model     type is local. The amount of sample data used at the local device is     also extracted from\u00a0meta_dect.</p> </li> <li> <p>All these extracted parameters are returned at\u00a0the end.</p> </li> <li> <p>In the following section, we will explain the database     implementation using the SQLite framework.</p> </li> </ul> <p>Defining the database with SQLite</p> <ul> <li> <p>The\u00a0sqlite_db.py\u00a0code\u00a0creates the SQLite database and deals with     storing and retrieving data from the database.</p> </li> <li> <p>Importing libraries for the SQLite database</p> </li> <li> <p>sqlite_db.py\u00a0imports\u00a0the basic general libraries and\u00a0ModelType\u00a0as     follows:</p> </li> </ul> <p>import sqlite3</p> <p>import datetime</p> <p>import logging</p> <p>from fl_main.lib.util.states import ModelType</p> <p>CopyExplain</p> <ul> <li> <p>The\u00a0ModelType\u00a0from\u00a0lib/util\u00a0defines the type of the models: local     models and (global) cluster models.</p> </li> <li> <p>Defining and initializing the SQLiteDBHandler class</p> </li> <li> <p>Then, the\u00a0following code related to     the\u00a0SQLiteDBHandler\u00a0class\u00a0creates and initializes the SQLite     database and inserts models into the SQLite database:</p> </li> </ul> <p>class SQLiteDBHandler:</p> <p>\\\"\\\"\\\"</p> <p>SQLiteDB Handler class that creates and initialize</p> <p>SQLite DB, and inserts models to the SQLiteDB</p> <p>\\\"\\\"\\\"</p> <p>CopyExplain</p> <ul> <li>The initialization is very simple -- just setting     the\u00a0db_file\u00a0parameter passed from the\u00a0PseudoDB\u00a0instance     to\u00a0self.db_file:</li> </ul> <p>def __init__(self, db_file):</p> <p>self.db_file = db_file</p> <p>CopyExplain</p> <p>Initializing the database</p> <ul> <li>In the\u00a0following\u00a0initialize_DB\u00a0function, the database tables are     defined with local and cluster models using SQLite (sqlite3):</li> </ul> <p>def initialize_DB(self):</p> <p>conn = sqlite3.connect(f\\'{self.db_file}\\')</p> <p>c = conn.cursor()</p> <p>c.execute(\\'\\'\\'CREATE TABLE local_models(model_id, \\</p> <p>generation_time, agent_id, round, performance, \\</p> <p>num_samples)\\'\\'\\')</p> <p>c.execute(\\'\\'\\'CREATE TABLE cluster_models(model_id, \\</p> <p>generation_time, aggregator_id, round, \\</p> <p>num_samples)\\'\\'\\')</p> <p>conn.commit()</p> <p>conn.close()</p> <p>CopyExplain</p> <ul> <li> <p>The tables are simplified in this example so that you can easily     follow the uploaded local models and their performance as well as     the global models created by an aggregator.</p> </li> <li> <p>The\u00a0local_models\u00a0table has a model ID (model_id), the time the model     is generated (generation_time), an agent ID uploaded of the local     model (agent_id), round information (round), the performance data of     the local model (performance), and the number of samples used for     FedAvg aggregation (num_samples).</p> </li> <li> <p>cluster_models\u00a0has a model ID (model_id), the time the model is     generated (generation_time), an aggregator ID (aggregator_id), round     information (round), and the number of samples (num_samples).</p> </li> </ul> <p>Inserting an entry into the database</p> <ul> <li>The following\u00a0code for\u00a0insert_an_entry\u00a0inserts the data received as     parameters using\u00a0sqlite3\u00a0libraries:</li> </ul> <p>def insert_an_entry(self, component_id: str, r: int, mt: \\</p> <p>ModelType, model_id: str, gtime: float, local_prfmc: \\</p> <p>float, num_samples: int):</p> <p>conn = sqlite3.connect(self.db_file)</p> <p>c = conn.cursor()</p> <p>t = datetime.datetime.fromtimestamp(gtime)</p> <p>gene_time = t.strftime(\\'%m/%d/%Y %H:%M:%S\\')</p> <p>if mt == ModelType.local:</p> <p>c.execute(\\'\\'\\'INSERT INTO local_models VALUES \\</p> <p>(?, ?, ?, ?, ?, ?);\\'\\'\\', (model_id, gene_time, \\</p> <p>component_id, r, local_prfmc, num_samples))</p> <p>elif mt == ModelType.cluster:</p> <p>c.execute(\\'\\'\\'INSERT INTO cluster_models VALUES \\</p> <p>(?, ?, ?, ?, ?);\\'\\'\\', (model_id, gene_time, \\</p> <p>component_id, r, num_samples))</p> <p>conn.commit()</p> <p>conn.close()</p> <p>CopyExplain</p> <ul> <li> <p>This function takes\u00a0the parameters of\u00a0component_id\u00a0(agent ID or     aggregator ID), round number (r), message type (mt), model ID     (model_id), the time the model is generated (gtime), the local     model's performance data (local_prfmc), and the number of samples     (num_samples) to insert an entry with the\u00a0execute\u00a0function of the     SQLite library.</p> </li> <li> <p>If the model type is\u00a0local, the information of the models is     inserted into the\u00a0local_models\u00a0table. If the model type     is\u00a0cluster, the information of the models is inserted into     the\u00a0cluster_models\u00a0table.</p> </li> <li> <p>Other functions, such as updating and deleting data from the     database, are not implemented in this example code and it's up to     you to write those additional functions.</p> </li> <li> <p>In the following section, we will explain how to run the database     server.</p> </li> </ul> <p>Running the database server</p> <ul> <li>Here is the code for\u00a0running the database server with the SQLite     database:</li> </ul> <p>if __name__ == \\\"__main__\\\":</p> <p>pdb = PseudoDB()</p> <p>init_db_server(pdb.handler, pdb.db_ip, pdb.db_socket)</p> <p>CopyExplain</p> <ul> <li> <p>The instance of\u00a0PseudoDB\u00a0class is created as\u00a0pdb. The\u00a0pdb.handler,     the database's IP address (pdb.db_ip), and the database socket     (pdb.db_socket) are used to start the process of receiving local and     cluster models from an aggregator enabled by\u00a0init_db_server\u00a0from     the\u00a0communication_handler\u00a0library in the\u00a0util/lib\u00a0folder.</p> </li> <li> <p>Now, we understand how to implement and run the database server. The     database tables and schema discussed here are minimally designed so     that we can understand the fundamentals of the FL server's     procedure. In the following section, we will discuss potential     enhancements to the FL server.</p> </li> </ul> <p>FL Server Potential enhancements</p> <ul> <li>Here are some\u00a0of the key potential enhancements to the FL server     discussed Here.</li> </ul> <p>Redesigning the database</p> <ul> <li> <p>The database was\u00a0intentionally designed with minimal table     information in this book and needs to be extended, such as by having     tables of the aggregator itself, agents, the initial base model, and     the project info, among other things, in the database.</p> </li> <li> <p>For example, the FL system described here does not support the     termination and restart of the server and agent processes.</p> </li> <li> <p>Thus, the FL server implementation is not complete, as it loses most     of the information when any of the systems is stopped or failed.</p> </li> </ul> <p>Automating the registry of an initial model</p> <ul> <li> <p>In order to\u00a0simplify the explanation of the process of registering     the initial model, we defined the layers of the ML models using     model names.</p> </li> <li> <p>This registration of the model in the system can be automated so     that just loading a certain ML model, such as PyTorch or Keras     models, with file extensions such as\u00a0.pt/.pth\u00a0and\u00a0.h5, will be     enough for the users of the FL systems to start the process.</p> </li> </ul> <p>Performance metrics for local and global models</p> <ul> <li> <p>Again, to\u00a0simplify\u00a0the explanation of the\u00a0FL server and the     database-side functionalities, an accuracy value is just used as one     of the performance criteria of the models.</p> </li> <li> <p>Usually, ML applications have many more metrics to keep track of as     performance data and they need to be enhanced together with the     database and communications protocol design.</p> </li> </ul> <p>Fine-tuned aggregation</p> <ul> <li> <p>In order to simplify\u00a0the process of\u00a0aggregating the local models, we     just used FedAvg, a weighted averaging method.</p> </li> <li> <p>The number of samples can dynamically change depending on the local     environment, and that aspect is enhanced by you.</p> </li> <li> <p>There are also a variety of model aggregation methods, which will be     explained later,\u00a0Model Aggregation, of this work so that you can     accommodate the best aggregation method depending on the ML     applications to be created and integrated into the FL system.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>Here, the basics and principles of FL server-side implementation     were explained with actual code examples. Having followed the     contents of This section, you should now be able to construct the FL     server-side functionalities with model aggregation mechanisms.</p> </li> <li> <p>The server-side components that were introduced here involve basic     communications and the registration of the agents and initial     models, managing state information used for the aggregation, and the     aggregation mechanisms for creating the global cluster models.</p> </li> <li> <p>In addition, we discussed the implementation of the database to just     store the information of the ML models.</p> </li> <li> <p>The code was simplified so that you were able to understand the     principles of server-side functionalities. Further enhancements to     many other aspects of constructing a more sustainable, resilient,     and scalable FL system are up to you.</p> </li> <li> <p>In the next section, we will discuss the principle of implementing     the functionalities of the FL client and agent. The client side     needs to provide some well-designed APIs for the ML applications for     plugin use. Therefore, the section will discuss the FL client\\'s     core functionalities and libraries as well as the library     integration into the very simple ML applications to enable the whole     FL process.</p> </li> </ul> <p>[Section 5: FL Client Side Implementation]{.underline}</p> <ul> <li> <p>The client-side\u00a0modules of a\u00a0federated learning\u00a0(FL) system     can be implemented based on the system architecture, sequence, and     procedure flow, as discussed earlier.</p> </li> <li> <p>FL client-side\u00a0functionalities can connect distributed\u00a0machine     learning\u00a0(ML) applications that conduct local training and     testing with an aggregator, through a communications module embedded     in the client-side libraries.</p> </li> <li> <p>In the example of using the FL client libraries in a local ML     engine, the minimal engine package example will be discussed, with     dummy ML models to understand the process of integration with the FL     client libraries that are designed Here.</p> </li> <li> <p>By following the example code about integration, you will understand     how to actually enable the whole process related to the FL client     side, as discussed earlier, while an analysis on what will happen     with the minimal example will be discussed in\u00a0\u00a0Running the     Federated Learning System and Analyzing the Results.</p> </li> <li> <p>Here, an overview of the design and implementation principle of FL     client-side functionalities used in local ML engines will be     discussed. \u00a0you will be able to code the FL client-side modules and     libraries as well as\u00a0distributed\u00a0local ML engines, such as image     classification with\u00a0Convolutional Neural Networks\u00a0(CNNs).</p> </li> <li> <p>Here, we will cover the following topics:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   An overview of FL client-side components</p> <ul> <li> <p>Implementing FL client-side main functionalities</p> </li> <li> <p>Designing FL client libraries</p> </li> <li> <p>Local ML engine integration into an FL system</p> </li> <li> <p>An example of integrating image classification into an FL system</p> </li> </ul> <p>Technical requirements</p> <p>All the code files introduced Here can be found on GitHub ([https://github.com/keshavaspanda/simple-fl]{.underline}).</p> <p>An overview of FL client-side components</p> <ul> <li> <p>The architecture\u00a0of an FL client as an agent was introduced earlier.</p> </li> <li> <p>Here, we will introduce code that realizes the basic functionalities     of an FL client.</p> </li> <li> <p>The client side of software architecture is simplified here, where     only the\u00a0client.py\u00a0file can be used in this example, together with     supporting functions from the\u00a0lib/util\u00a0folder, as shown in\u00a0Figure     5.1:</p> </li> </ul> <p>{width=\"3.751388888888889in\" height=\"3.3020833333333335in\"}</p> <p>Figure 5.1 -- Python software components for an FL client as an agent</p> <ul> <li>The following section gives a brief description of the Python files     for an agent of the FL system.</li> </ul> <p>Distributed agent-side code</p> <ul> <li>For the\u00a0agent side, there is\u00a0one main file,\u00a0client.py, in     the\u00a0fl_main/agent\u00a0directory that deals with most of the FL     client-side functionalities.</li> </ul> <p>FL client code (client.py)</p> <ul> <li> <p>The\u00a0client.py\u00a0file in the\u00a0agent\u00a0folder has functions to participate     in an FL cycle, an ML\u00a0model exchange framework with an aggregator,     and\u00a0push\u00a0and\u00a0polling\u00a0mechanisms to communicate with the     aggregator.</p> </li> <li> <p>The client's functions can also serve as interfaces between the     local ML application and the FL system itself, providing FL     client-side libraries to the ML engine.</p> </li> <li> <p>This is the main code that connects locally trained ML models to the     FL server and aggregator.</p> </li> <li> <p>You need to prepare a local ML application by yourself, and we will     help you understand how to integrate your ML engine into an FL     system using the FL client libraries, which is another main topic of     This section.</p> </li> </ul> <p>lib/util code</p> <ul> <li>An explanation\u00a0of the supporting Python code     (communication_handler.py,\u00a0data_struc.py,\u00a0helpers.py,\u00a0messengers.py,     and\u00a0states.py) as internal libraries will be covered in\u00a0Appendix,     Exploring Internal Libraries.</li> </ul> <p>Configuration of an agent</p> <ul> <li>The following\u00a0is an example of client-side configuration parameters     saved as\u00a0config_agent.json\u00a0in the code we are using:</li> </ul> <p>{</p> <p>\\\"aggr_ip\\\": \\\"localhost\\\",</p> <p>\\\"reg_socket\\\": \\\"8765\\\",</p> <p>\\\"model_path\\\": \\\"./data/agents\\\",</p> <p>\\\"local_model_file_name\\\": \\\"lms.binaryfile\\\",</p> <p>\\\"global_model_file_name\\\": \\\"gms.binaryfile\\\",</p> <p>\\\"state_file_name\\\": \\\"state\\\",</p> <p>\\\"init_weights_flag\\\": 1,</p> <p>\\\"polling\\\": 1</p> <p>}</p> <ul> <li> <p>CopyExplain</p> </li> <li> <p>The aggregator's IP (aggr_ip) and its port number (reg_socket) are     used to get connected to the FL server, where the aggregation of the     local models happens. In addition, the model path     parameter,\u00a0model_path, specifies the location of both the local     model (named\u00a0local_model_file_name) and the global model     (named\u00a0global_model_file_name).</p> </li> <li> <p>The local and global models are stored as binary files     (lms.binaryfile\u00a0and\u00a0gms.binaryfile\u00a0in this example). The state file     (named\u00a0state_file_name) writes the local state of the client that     defines waiting for the global models, training the models, sending     the trained models, and so on.\u00a0init_weights_flag\u00a0is used when the     system operator wants to initialize the global model\u00a0with certain     weights.</p> </li> <li> <p>If the flag is\u00a01, the agent will send the pre-configured model;     otherwise, the model will be filled with zeros on the aggregator     side. The polling flag (polling) concerns whether to utilize the     polling method or not for communication between agents and an     aggregator.</p> </li> <li> <p>Now that we've discussed FL client-side modules, let's look into the     actual implementation and some code to realize the functionalities     of an FL client.</p> </li> </ul> <p>Implementing FL client-side main functionalities</p> <ul> <li> <p>In this section, we will explain how you can implement basic FL     client-side code, which is\u00a0described in the\u00a0client.py\u00a0file in     the\u00a0agent\u00a0directory.</p> </li> <li> <p>By learning about this client-side code, you will understand how to     implement an agent's registration process, model exchange     synchronization, and\u00a0push/polling\u00a0mechanisms, as well as the     communication protocol\u00a0between the agent and aggregator, with some     functions that will be called from other ML applications     as\u00a0Application Programming Interfaces\u00a0(APIs).</p> </li> <li> <p>Let's first see what libraries will be imported for implementing FL     client functions.</p> </li> </ul> <p>Importing libraries for an agent</p> <ul> <li>In this\u00a0client.py\u00a0file\u00a0example, the agent imports general libraries     such as\u00a0asyncio\u00a0and\u00a0time\u00a0(a detailed explanation of which is out of     scope for this book):</li> </ul> <p>import asyncio, time, logging, sys, os</p> <p>from typing import Dict, Any</p> <p>from threading import Thread</p> <p>from fl_main.lib.util.communication_handler import \\</p> <p>init_client_server, send, receive</p> <p>from fl_main.lib.util.helpers import read_config, \\</p> <p>init_loop, save_model_file, load_model_file, \\</p> <p>read_state, write_state, generate_id, \\</p> <p>set_config_file, get_ip, compatible_data_dict_read, \\</p> <p>generate_model_id, create_data_dict_from_models, \\</p> <p>create_meta_data_dict</p> <p>from fl_main.lib.util.states import ClientState, \\</p> <p>AggMsgType, ParticipateConfirmationMSGLocation, \\</p> <p>GMDistributionMsgLocation, IDPrefix</p> <p>from fl_main.lib.util.messengers import \\</p> <p>generate_lmodel_update_message, \\</p> <p>generate_agent_participation_message, \\</p> <p>generate_polling_message</p> <p>CopyExplain</p> <ul> <li> <p>As for the\u00a0communication_handler,\u00a0helpers,\u00a0states,     and\u00a0messengers\u00a0libraries imported from\u00a0fl_main.lib.util\u00a0that are     designed for enabling the FL general functionalities, please refer     to the\u00a0Appendix, Exploring Internal Libraries.</p> </li> <li> <p>After importing the necessary libraries, you will define     the\u00a0Client\u00a0class.</p> </li> </ul> <p>Defining the Client class</p> <ul> <li>Let's\u00a0define the\u00a0Client\u00a0class that implements the core     functionalities of an FL client, including the participation     mechanism of the agent itself, the model exchange framework, and a     communication interface between the agent and an aggregator, as well     as libraries provided for use in the agent-side local ML engine:</li> </ul> <p>class Client:</p> <p>\\\"\\\"\\\"</p> <p>Client class instance with FL client-side functions</p> <p>and libraries used in the agent\\'s ML engine</p> <p>\\\"\\\"\\\"</p> <p>CopyExplain</p> <ul> <li>Then, you will initialize the\u00a0Client\u00a0class under     the\u00a0__init__\u00a0function, as discussed in the next section.</li> </ul> <p>Initializing the client</p> <ul> <li>The\u00a0following code inside the\u00a0__init__\u00a0constructor is an example     of the initialization process of the client:</li> </ul> <p>def __init__(self):</p> <p>self.agent_name = \\'default_agent\\'</p> <p>self.id = generate_id()</p> <p>self.agent_ip = get_ip()</p> <p>self.simulation_flag = False</p> <p>if len(sys.argv) &gt; 1:</p> <p>self.simulation_flag = bool(int(sys.argv[1]))</p> <p>config_file = set_config_file(\\\"agent\\\")</p> <p>self.config = read_config(config_file)</p> <p>self.aggr_ip = self.config[\\'aggr_ip\\']</p> <p>self.reg_socket = self.config[\\'reg_socket\\']</p> <p>self.msend_socket = 0</p> <p>self.exch_socket = 0</p> <p>if self.simulation_flag:</p> <p>self.exch_socket = int(sys.argv[2])</p> <p>self.agent_name = sys.argv[3]</p> <p>self.model_path = f\\'{self.config[\\\"model_path\\\"]}</p> <p>/{self.agent_name}\\'</p> <p>if not os.path.exists(self.model_path):</p> <p>os.makedirs(self.model_path)</p> <p>self.lmfile = self.config[\\'local_model_file_name\\']</p> <p>self.gmfile = self.config[\\'global_model_file_name\\']</p> <p>self.statefile = self.config[\\'state_file_name\\']</p> <p>self.round = 0</p> <p>self.init_weights_flag = \\</p> <p>bool(self.config[\\'init_weights_flag\\'])</p> <p>self.is_polling = bool(self.config[\\'polling\\'])</p> <p>CopyExplain</p> <ul> <li> <p>First, the client generates a unique ID for itself as an identifier     that will be used in many\u00a0scenarios to conduct FL.</p> </li> <li> <p>Second, the client gets its own IP address by using     the\u00a0get_ip()\u00a0function.</p> </li> <li> <p>Also, simulation runs are supported in this implementation exercise,     where we can run all the FL system components of a database, server,     and multiple agents within one machine. If simulation needs to be     done, then the\u00a0simulation_flag\u00a0parameter needs to be\u00a0True\u00a0(refer to     the\u00a0README\u00a0file on GitHub for how to set up a simulation mode).</p> </li> <li> <p>Then,\u00a0self.cofig\u00a0reads and stores the information     of\u00a0config_agent.json.</p> </li> <li> <p>The client then configures the aggregator's information to connect     to its server, where\u00a0self.aggr_ip\u00a0reads the IP address of the     aggregator machine or instance from the agent configuration file.</p> </li> <li> <p>After that, the\u00a0reg_socket\u00a0port will be set up, where\u00a0reg_socket\u00a0is     used for registration of the agent, together with an aggregator IP     address stored as\u00a0self.aggr_ip. The\u00a0reg_socket\u00a0value in this example     can be read from the agent configuration file as well.</p> </li> <li> <p>msend_socket, which is used in the model exchange routine to send     the local ML models to the aggregator, will be configured after     participating in the FL process by sending a message to the FL     server and receiving the response.</p> </li> <li> <p>exch_socket\u00a0is used when communication is not in\u00a0polling\u00a0mode for     receiving global models sent from the aggregator, together with an     agent IP address stored as\u00a0self.agent_ip.</p> </li> <li> <p>exch_socket\u00a0in this example can either be read from the arguments     from the command line or decided by the aggregator, depending on the     simulation mode.</p> </li> <li> <p>In this\u00a0example, when the aggregator is set to be able to push     messages to the connected agents, which is not the case when polling     mode is on,\u00a0exch_socket\u00a0can be dynamically configured by the     aggregator.</p> </li> <li> <p>self.model_path\u00a0stores the path to the local and global models and     can either be read from the agent configuration file or arguments     from the command line, depending on the simulation mode as well. If     there is no directory to save those model files, it makes sure to     create the directory.</p> </li> <li> <p>self.lmfile,\u00a0self.gmfile, and\u00a0self.statefile\u00a0are the filenames for     local models, global models, and the state of the client     respectively, and read from the configuration file of the agent. In     particular, in\u00a0self.statefile, the value of\u00a0ClientState\u00a0is     saved.\u00a0ClientState\u00a0is the enumeration value of the client itself     where there is a state waiting for the global model (waiting_gm), a     state for local training (training), a state for sending local     models (sending), and a state for having the updated global models     (gm_ready).</p> </li> <li> <p>The round information of the FL process, defined as\u00a0self.round, is     initialized as\u00a00\u00a0and later updated as the FL round proceeds with     model aggregation, where the aggregator will notify the change of     the round usually.</p> </li> <li> <p>self.init_weights_flag\u00a0is the flag used when a system operator wants     to initialize a global model with certain parameters, as explained     in the configuration of the agent.</p> </li> <li> <p>The\u00a0self.is_polling\u00a0flag concerns whether to use the polling method     in communication between the agents and aggregator or not. The     polling flag must be the\u00a0same as the one set up on the aggregator     side.</p> </li> <li> <p>The code about the\u00a0__init__\u00a0constructor discussed here can be     found in\u00a0client.py\u00a0in the\u00a0fl_main/agent\u00a0folder on GitHub     ([https://github.com/keshavaspanda/simple-fl]{.underline}).</p> </li> <li> <p>Now that we have discussed how to initialize a client-side module,     in the next section, we will look into how the participation     mechanism works with some sample code.</p> </li> </ul> <p>Agent participation in an FL cycle</p> <ul> <li> <p>This participation or registration process is needed for an agent to     be able to participate\u00a0in an FL process together with other agents.     Therefore, the agent needs to be added to the list of authorized     agents that can send locally trained ML models to an aggregator.</p> </li> <li> <p>The asynchronous\u00a0participate\u00a0function sends the first message to an     aggregator to join the FL cycle and will receive state and     communication information, such as socket numbers from the     aggregator.</p> </li> <li> <p>An agent knows the IP address and port number to join the FL     platform through the\u00a0config_agent.json\u00a0file. When joining the FL     platform, an agent sends a participation message that contains the     following information:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   agent_name: A unique name of an agent itself.</p> <ul> <li> <p>id: A unique identifier of an agent itself.</p> </li> <li> <p>model_id: A unique identifier of models to be sent to an aggregator.</p> </li> <li> <p>models: A dictionary of models keyed by model names. The weights of     models need not be trained if\u00a0init_flag\u00a0is\u00a0False, since it is only     used by an aggregator to remember the shapes of models.</p> </li> <li> <p>init_weights_flag: A Boolean flag to indicate whether the sent model     weights should be used as a base model. If it is\u00a0True\u00a0and there are     no global models ready, an aggregator sets this set of local models     as the first global models and sends it to all agents.</p> </li> <li> <p>simulation_flag: This is\u00a0True\u00a0if it is a simulation run; otherwise,     it is\u00a0False.</p> </li> <li> <p>exch_socket: The port number waiting for global models from the     aggregator.</p> </li> <li> <p>gene_time: The time that models are generated.</p> </li> <li> <p>performance_dict: Performance data related to models in a dictionary     format.</p> </li> <li> <p>agent_ip: The IP address of an agent itself.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   With all the aforementioned participation messages defined, the     agent is ready to exchange\u00a0models with the aggregator, and the code     to realize the participation process is as follows:</p> <p>async def participate(self):</p> <p>data_dict, performance_dict = \\</p> <p>load_model_file(self.model_path, self.lmfile)</p> <p>_, gene_time, models, model_id = \\</p> <p>compatible_data_dict_read(data_dict)</p> <p>msg = generate_agent_participation_message(</p> <p>self.agent_name, self.id, model_id, models,</p> <p>self.init_weights_flag, self.simulation_flag,</p> <p>self.exch_socket, gene_time, performance_dict,</p> <p>self.agent_ip)</p> <p>resp = await send(msg, self.aggr_ip, self.reg_socket)</p> <p>self.round = resp[ \\</p> <p>int(ParticipateConfirmaMSGLocation.round)]</p> <p>self.exch_socket = resp[ \\</p> <p>int(ParticipateConfirmationMSGLocation.exch_socket)]</p> <p>self.msend_socket = resp[ \\</p> <p>int(ParticipateConfirmationMSGLocation.recv_socket)]</p> <p>self.id = resp[ \\</p> <p>int(ParticipateConfirmationMSGLocation.agent_id)]</p> <p>self.save_model_from_message(resp, \\</p> <p>ParticipateConfirmationMSGLocation)</p> <p>CopyExplain</p> <ul> <li> <p>The\u00a0agent reads the local models to tell the structure of the ML     models to the aggregator, and the initial model does not necessarily     need to be trained.\u00a0data_dict\u00a0and\u00a0performance_dict\u00a0store the models     and their performance data respectively.</p> </li> <li> <p>Then, a message,\u00a0msg, containing information such as the     ML\u00a0models\u00a0and its\u00a0model_id, is packaged using     the\u00a0generate_agent_participation_message\u00a0function.</p> </li> <li> <p>When sending the message, in this example, the WebSocket is     constructed using the aggregator's IP address (aggr_ip) and the     registration port number (reg_socket) to be connected to the     aggregator.</p> </li> <li> <p>After sending the message to the aggregator via an     asynchronous\u00a0send\u00a0function imported from\u00a0communication_handler, the     agent receives a response message,\u00a0resp, from the aggregator. The     response will include the round info, the port number to receive the     global models'\u00a0exch_socket, the port number to send the local models     to the aggregator's\u00a0msend_socket, and an updated agent ID.</p> </li> <li> <p>Finally, the global model within the response message is saved     locally by calling the\u00a0save_model_from_message\u00a0function.</p> </li> <li> <p>The participation mechanism of an agent has been explained. In the     next section, we will learn about the framework of model exchange     synchronization.</p> </li> </ul> <p>Model exchange synchronization</p> <ul> <li>Model\u00a0exchange synchronization, as shown in the following code, is     for checking the state of the agent and calling a proper function     based on the state:</li> </ul> <p>Async def model_exchange_routine(self):</p> <p>while True:</p> <p>await asyncio.sleep(5)</p> <p>state = read_state(self.model_path, self.statefile)</p> <p>if state == ClientState.sending:</p> <p>await self.send_models()</p> <p>elif state == ClientState.waiting_gm:</p> <p>if self.is_polling == True:</p> <p>await self.process_polling()</p> <p>else: pass</p> <p>elif state == ClientState.training: pass</p> <p>elif state == ClientState.gm_ready: pass</p> <p>else: pass</p> <p>CopyExplain</p> <ul> <li> <p>Basically, this process is always running while the client is alive,     whereas the\u00a0while\u00a0loop is used periodically to check the     client's\u00a0state\u00a0and proceed with the next steps if necessary.</p> </li> <li> <p>In the\u00a0while\u00a0loop, after waiting a few seconds, it first checks the     client state by the\u00a0read_state\u00a0function. The parameters in     the\u00a0read_state\u00a0function are to locate the\u00a0state\u00a0file stored in the     local environment.</p> </li> <li> <p>As mentioned,\u00a0ClientState\u00a0has the enumeration value of the client     state itself, defining a state for sending local models (sending), a     state waiting for the global model (waiting_sgm), a state for local     training (training), and a state for receiving the updated global     models (gm_ready).</p> </li> <li> <p>If the client is in the\u00a0sending\u00a0state (state ==     ClientState.sending), it means it is ready to send the locally     trained model to the aggregator. Therefore, the agent calls     the\u00a0send_models\u00a0function to send the locally trained ML model to the     aggregator.</p> </li> <li> <p>When the state is\u00a0waiting_gm\u00a0(state == ClientState.waiting_gm), it     either proceeds with\u00a0process_polling\u00a0to poll from the agent to the     aggregator if polling mode is on, or just does nothing if polling     mode is off.</p> </li> <li> <p>If the\u00a0client is in the\u00a0training\u00a0state (state ==     ClientState.training), it means that the client is training the     local model now and just waits for a few seconds, printing the     training status if necessary. You can also add any procedure if     needed.</p> </li> <li> <p>If the client is in the\u00a0gm_ready\u00a0state (state ==     ClientState.gm_ready), it means that the client received the global     model. This state will be handled by a local ML application, and it     does nothing but show the readiness of the global models.</p> </li> <li> <p>In the next section, we will talk about how     the\u00a0push\u00a0and\u00a0polling\u00a0mechanisms can be implemented for an FL     cycle.</p> </li> </ul> <p>Push and polling implementation</p> <ul> <li> <p>Once an\u00a0agent is initialized\u00a0and confirmed for participation in an     FL process, it starts waiting for the global models sent from an     aggregator. There are two ways\u00a0to receive\u00a0global models from the     aggregator: the\u00a0push\u00a0method\u00a0and the\u00a0polling\u00a0method. Although     the\u00a0Secure Sockets Layer\u00a0(SSL) or\u00a0Transport Layer     Security\u00a0(TSL) frameworks are not\u00a0implemented in FL     client-side code here for simplification, it is recommended to     support them to secure constant communication.</p> </li> <li> <p>Let's look into the mechanism for each communication framework.</p> </li> <li> <p>The push method from aggregator to agent</p> </li> <li> <p>With the\u00a0push\u00a0method, the\u00a0aggregator will push the message that     includes global models to all the connected agents right after the     global models are generated.</p> </li> <li> <p>The following code shows the\u00a0push\u00a0mechanism accepting and saving     global models from the aggregator:</p> </li> </ul> <p>async def wait_models(self, websocket, path):</p> <p>gm_msg = await receive(websocket)</p> <p>self.save_model_from_message( \\</p> <p>gm_msg, GMDistributionMsgLocation)</p> <p>CopyExplain</p> <ul> <li> <p>The\u00a0wait_models\u00a0asynchronous function accepts\u00a0websocket\u00a0as a     parameter. When\u00a0the aggregator sends a message to the agent, it     receives the\u00a0gm_msg\u00a0message through\u00a0await recieve(websocket)\u00a0and     saves the global models locally by calling     the\u00a0save_model_from_message\u00a0function, as defined in the\u00a0Toward     designing FL client libraries\u00a0section.</p> </li> <li> <p>The polling method from agent to aggregator</p> </li> <li> <p>With the\u00a0polling\u00a0method, an agent will keep asking (polling) an     aggregator to see whether\u00a0global models are already formed or not.     Once it has been created and is ready to be sent to the connected     agents, the polled message will be returned to the agent with the     updated global models in the response.</p> </li> <li> <p>The following code about the\u00a0process_polling\u00a0asynchronous function     illustrates the\u00a0polling\u00a0method:</p> </li> </ul> <p>async def process_polling(self):</p> <p>msg = generate_polling_message(self.round, self.id)</p> <p>resp = await send(msg, self.aggr_ip, self.msend_socket)</p> <p>if resp[int(PollingMSGLocation.msg_type)] \\</p> <p>== AggMsgType.update:</p> <p>self.save_model_from_message(resp, \\</p> <p>GMDistributionMsgLocation)</p> <p>else: pass</p> <p>CopyExplain</p> <ul> <li> <p>It first generates the polling message with     the\u00a0generate_polling_message\u00a0function to be sent to the aggregator.     After receiving the response message,\u00a0resp, from the aggregator, if     the message type is\u00a0AggMsgType.update, meaning the response message     contains the updated global models, it calls     the\u00a0save_model_from_message\u00a0function. Otherwise, it does nothing.</p> </li> <li> <p>The aforementioned functions are the basic but core features of an     FL client, and those functions need to be efficiently used by a     user-side ML application as libraries.</p> </li> <li> <p>Now that FL client design, including initialization, participation,     and model exchanges, has been explained, we will learn about how to     design FL client libraries.</p> </li> </ul> <p>Designing FL client libraries</p> <ul> <li> <p>In this section, we will explain how to package essential functions     to be provided as libraries to\u00a0users. In this example, the simplest     way to package them as libraries will be discussed.</p> </li> <li> <p>This will need to be expanded, depending on your needs and the     design of your own FL client framework. By packaging FL client-side     modules as libraries, developers will be easily able to integrate     the FL client's functions into the local ML engine.</p> </li> <li> <p>Let's start with how to define a library to start and register an FL     client.</p> </li> </ul> <p>Starting FL client core threads</p> <ul> <li> <p>For local ML application developers to be able to integrate FL     client-related functions, they sometimes need to be packaged as     threading functions.</p> </li> <li> <p>The\u00a0following code to register an agent in the FL system simply puts     a\u00a0participate\u00a0function into the\u00a0run_until_complete\u00a0function of     an\u00a0asyncio.get_event_loop\u00a0function:</p> </li> </ul> <p>def register_client(self):</p> <p>asyncio.get_event_loop().run_until_complete( \\</p> <p>self.participate())</p> <p>CopyExplain</p> <ul> <li> <p>Also, the\u00a0start_wait_model_server\u00a0function is packaged, as shown in     the following code block, where the\u00a0Thread\u00a0function takes care of     the constant run.</p> </li> <li> <p>This way, you will be able to run the local ML module in parallel     and receive global models in the\u00a0wait_models\u00a0thread when the FL     system is in\u00a0push\u00a0communication mode:</p> </li> </ul> <p>def start_wait_model_server(self):</p> <p>th = Thread(target = init_client_server, \\</p> <p>args=[self.wait_models, self.agent_ip, \\</p> <p>self.exch_socket])</p> <p>th.start()</p> <p>CopyExplain</p> <ul> <li>Similarly, the\u00a0start_model_exhange_server\u00a0function can be a thread     to run a model\u00a0exchange routine to synchronize the local and global     models, while the local ML module is running in parallel. You can     just call the following\u00a0start_model_exchange_server\u00a0function as a     library to enable this functionality:</li> </ul> <p>def start_model_exchange_server(self):</p> <p>self.agent_running = True</p> <p>th = Thread(target = init_loop, \\</p> <p>args=[self.model_exchange_routine()])</p> <p>th.start()</p> <p>CopyExplain</p> <ul> <li>Finally, it may be helpful to package all these three functions to     execute at the same time when they are called outside     the\u00a0Client\u00a0class. Therefore, we introduce the following code     concerning\u00a0start_fl_client\u00a0that aggregates the functions of     registering agents, waiting for global models and a model exchange     routine to start the FL client core functions:</li> </ul> <p>def start_fl_client(self):</p> <p>self.register_client()</p> <p>if self.is_polling == False:</p> <p>self.start_wait_model_server()</p> <p>self.start_model_exchange_server()</p> <p>CopyExplain</p> <ul> <li>The initiation of the FL client is now packaged     into\u00a0start_fl_client. Next, we will define the libraries of saved ML     models.</li> </ul> <p>Saving global models</p> <ul> <li> <p>While the\u00a0load\u00a0and\u00a0save\u00a0model functions are provided by the helper     functions in\u00a0lib/util, which will be explained later in     the\u00a0Appendix,\u00a0Exploring Internal Libraries, it is helpful to     provide an\u00a0interface for ML developers to save global models from a     message sent from an aggregator.</p> </li> <li> <p>The following\u00a0save_model_from_message\u00a0function is one that extracts     and saves global models in an agent and also changes the client     state to\u00a0gm_ready. This function takes the message (msg) and message     location (MSG_LOC) information as parameters:</p> </li> </ul> <p>def save_model_from_message(self, msg, MSG_LOC):</p> <p>data_dict = create_data_dict_from_models( \\</p> <p>msg[int(MSG_LOC.model_id)],</p> <p>msg[int(MSG_LOC.global_models)],</p> <p>msg[int(MSG_LOC.aggregator_id)])</p> <p>self.round = msg[int(MSG_LOC.round)]</p> <p>save_model_file(data_dict, self.model_path, \\</p> <p>self.gmfile)</p> <p>self.tran_state(ClientState.gm_ready)</p> <p>CopyExplain</p> <ul> <li> <p>The global models, model ID, and aggregator ID are extracted from     the message and put into a dictionary using     the\u00a0create_data_dict_from_models\u00a0library. The round information is     also updated based on the received message.</p> </li> <li> <p>Then, the received global models are saved to the local file using     the\u00a0save_model_file\u00a0library, in which the data dictionary, model     path, and global model file name are specified to save the models.</p> </li> <li> <p>After receiving the global models, it changes the client state     to\u00a0gm_ready, the state indicating that the global model is ready for     the local ML to be utilized by calling the\u00a0tran_state\u00a0function,     which will be explained in the next section.</p> </li> <li> <p>With the function of saving global models defined, we are ready to     move on to how to manipulate the client state in the next section.</p> </li> </ul> <p>Manipulating client state</p> <ul> <li> <p>In order to manipulate the client state so that it can logically     handle local and global models, we prepare     the\u00a0read_state\u00a0and\u00a0tran_state\u00a0functions, which can be accessed\u00a0both     from inside and outside the code.</p> </li> <li> <p>The following\u00a0read_state\u00a0function reads the value written     in\u00a0statefile, stored in the location specified by\u00a0model_path. The     enumeration value of\u00a0ClientState\u00a0is used to change the client state:</p> </li> </ul> <p>def read_state(self) -&gt; ClientState:</p> <p>return read_state(self.model_path, self.statefile)</p> <p>CopyExplain</p> <ul> <li>The following\u00a0tran_state\u00a0function changes the state of the agent. In     this code sample, the state is maintained in the local\u00a0state\u00a0file     only:</li> </ul> <p>def tran_state(self, state: ClientState):</p> <p>write_state(self.model_path, self.statefile, state)</p> <p>CopyExplain</p> <ul> <li>Next, let's define the functions that can send local models to an     aggregator.</li> </ul> <p>Sending local models to aggregator</p> <ul> <li>The\u00a0following asynchronous\u00a0send_models\u00a0function is about sending     models that have been saved locally to the aggregator:</li> </ul> <p>async def send_models(self):</p> <p>data_dict, performance_dict = \\</p> <p>load_model_file(self.model_path, self.lmfile)</p> <p>, _, models, model_id = \\</p> <p>compatible_data_dict_read(data_dict)</p> <p>msg = generate_lmodel_update_message( \\</p> <p>self.id, model_id, models, performance_dict)</p> <p>await send(msg, self.aggr_ip, self.msend_socket)</p> <p>self.tran_state(ClientState.waiting_gm)</p> <p>CopyExplain</p> <ul> <li> <p>It first extracts\u00a0data_dict\u00a0and\u00a0performance_dict\u00a0using     the\u00a0load_model_file\u00a0helper function and then pulls out the models     and their ID from\u00a0data_dict, based on     the\u00a0compatible_data_dict_read\u00a0function. Then, the message is     packaged with the\u00a0generate_lmodel_update_message\u00a0library and sent to     the aggregator, with the\u00a0send\u00a0function from\u00a0communication_handler.     After that, the client state is changed to\u00a0waiting_gm\u00a0by     the\u00a0tran_state\u00a0function. Again, the SSL/TSL framework can be added     to secure communication, which is not implemented here to keep the     FL client-side coding simple.</p> </li> <li> <p>The\u00a0following\u00a0send_initial_model\u00a0function is called when you want to     send the initial\u00a0base model\u00a0to an aggregator of the model     architecture for registration purposes. It takes initial models, the     number of samples, and performance value as input and     calls\u00a0setup_sending_model, which will be explained later in this     section:</p> </li> </ul> <p>def send_initial_model(self, initial_models, \\</p> <p>num_samples=1, perf_val=0.0):</p> <p>self.setup_sending_models( \\</p> <p>initial_models, num_samples, perf_val)</p> <p>CopyExplain</p> <ul> <li>The following\u00a0send_trained_model\u00a0function is called when you want to     send trained local models to the aggregator during the FL cycle. It     takes trained models, the number of samples, and performance value     as input and only calls\u00a0setup_sending_model\u00a0if the client state     is\u00a0not\u00a0gm_ready:</li> </ul> <p>def send_trained_model(self, models, \\</p> <p>num_samples, perf_value):</p> <p>state = self.read_state()</p> <p>if state == ClientState.gm_ready:</p> <p>pass</p> <p>else:</p> <p>self.setup_sending_models( \\</p> <p>models, num_samples, perf_value)</p> <p>CopyExplain</p> <ul> <li>The following\u00a0setup_sending_models\u00a0function is designed to serve as     an internal library to set up sending locally trained models to the     aggregator. It takes parameters\u00a0of models as\u00a0np.array, the number of     samples as an integer, and performance data as a float value:</li> </ul> <p>def setup_sending_models(self, models, \\</p> <p>num_samples, perf_val):</p> <p>model_id = generate_model_id( \\</p> <p>IDPrefix.agent, self.id, time.time())</p> <p>data_dict = create_data_dict_from_models( \\</p> <p>model_id, models, self.id)</p> <p>meta_data_dict = create_meta_data_dict( \\</p> <p>perf_val, num_samples)</p> <p>save_model_file(data_dict, self.model_path, \\</p> <p>self.lmfile, meta_data_dict)</p> <p>self.tran_state(ClientState.sending)</p> <p>CopyExplain</p> <ul> <li> <p>Basically, this function creates a unique model ID with     the\u00a0generate_model_id\u00a0helper function,\u00a0data_dict\u00a0to store the local     ML models data created with the\u00a0create_data_dict_from_models\u00a0helper     function, and\u00a0meta_data_dict\u00a0to store the performance data created     with the\u00a0create_meta_data_dict\u00a0helper function. And then, all the     aforementioned data related to the models and performance is saved     locally with the\u00a0save_model_file\u00a0function, in the location specified     with\u00a0self.model_path. Then, it changes the client state     to\u00a0sending\u00a0so that the\u00a0mode_exchange_routine\u00a0function can note the     change in the client state and start sending trained local models to     the aggregator.</p> </li> <li> <p>Now that we know about the libraries to send ML models to the     aggregator, let's learn about an important function to wait for a     global model on the agent side.</p> </li> </ul> <p>Waiting for global models from an aggregator</p> <ul> <li>The\u00a0following\u00a0wait_for_global_model\u00a0function is very important to     conduct an FL cycle consistently:</li> </ul> <p>def wait_for_global_model(self):</p> <p>while (self.read_state() != ClientState.gm_ready):</p> <p>time.sleep(5)</p> <p>data_dict, _ = load_model_file( \\</p> <p>self.model_path, self.gmfile)</p> <p>global_models = data_dict[\\'models\\']</p> <p>self.tran_state(ClientState.training)</p> <p>return global_models</p> <p>CopyExplain</p> <ul> <li> <p>The principle is that the function waits until the client state     becomes\u00a0gm_ready. The transition of the client state     to\u00a0gm_ready\u00a0happens when the global model is received on the agent     side. Once the client state changes to\u00a0gm_ready, it proceeds to load     global models from\u00a0data_dict, extracted with     the\u00a0load_model_file\u00a0function, changes the client state to\u00a0training,     and returns the global models to the local ML module.</p> </li> <li> <p>We have discussed how to design the libraries of FL client-side     functions. In the next section, we will discuss how to integrate     those libraries into a local ML process.</p> </li> </ul> <p>Local ML engine integration into an FL system</p> <ul> <li> <p>The\u00a0successful integration of FL client libraries\u00a0into a local ML     engine is key to conducting FL in distributed environments later on.</p> </li> <li> <p>The\u00a0minimal_MLEngine.py\u00a0file in the\u00a0examples/minimal\u00a0directory found     in the GitHub repository     at\u00a0[https://github.com/tie-set/simple-fl]{.underline},     as shown in\u00a0Figure 5.2, provides an example of integrating FL     client-side libraries into a minimal ML engine package:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"1.1395833333333334in\"}</p> <p>Figure 5.2 -- The minimal ML engine package</p> <ul> <li>Next, we\u00a0will explain what libraries\u00a0need to be imported into the     local ML engine in the following section.</li> </ul> <p>Importing libraries for a local ML engine</p> <ul> <li> <p>The following\u00a0code shows the importing process, where general     libraries such as\u00a0numpy,\u00a0time, and\u00a0Dict\u00a0are imported first. The key     part of this process is that\u00a0Client\u00a0is imported from     the\u00a0client.py\u00a0file in the\u00a0fl_main.agent\u00a0folder. This way, a     developer does not need to know too much about the code inside an FL     system and just calls the important functionalities defined as     libraries, as discussed in the\u00a0Toward designing FL client     libraries\u00a0section.</p> </li> <li> <p>We will not cover the\u00a0pip\u00a0installation packaging here in this book,     but it is possible to host the client-side code with either a     private or public PyPI server:</p> </li> </ul> <p>import numpy as np</p> <p>import time, logging, sys</p> <p>from typing import Dict</p> <p>from fl_main.agent.client import Client</p> <p>CopyExplain</p> <ul> <li>After importing the necessary libraries, let's look at the functions     defined for local training and testing.</li> </ul> <p>Defining the ML models, training, and test functions</p> <ul> <li> <p>You first\u00a0define the models, training, and testing\u00a0functions to be     integrated into the FL system. In\u00a0this code example, we will use     dummy models and training/testing functions, allowing users to be     able to understand the minimal FL procedure without being bothered     by specific ML complications.</p> </li> <li> <p>The following function called\u00a0init_models\u00a0returns the templates of     models (in a dictionary format) to inform the ML model structure.     The models do not need to be trained\u00a0necessarily. In this case, the     models have two\u00a0layers defined by\u00a0model1\u00a0and\u00a0model2, where     some\u00a0random NumPy array is assigned to each layer, as follows:</p> </li> </ul> <p>def init_models() -&gt; Dict[str,np.array]:</p> <p>models = dict()</p> <p>models[\\'model1\\'] = np.array([[1, 2, 3], [4, 5, 6]])</p> <p>models[\\'model2\\'] = np.array([[1, 2], [3, 4]])</p> <p>return models</p> <p>CopyExplain</p> <ul> <li>After initializing the models, you will design the     following\u00a0training\u00a0function that can be a placeholder function for     each ML application:</li> </ul> <p>def training(models: Dict[str,np.array],</p> <p>init_flag: bool = False) -&gt; Dict[str,np.array]:</p> <p># return templates of models to tell the structure</p> <p># This model is not necessarily actually trained</p> <p>if init_flag:</p> <p>return init_models()</p> <p># ML Training. In this example, no actual training.</p> <p>models = dict()</p> <p>models[\\'model1\\'] = np.array([[1, 2, 3], [4, 5, 6]])</p> <p>models[\\'model2\\'] = np.array([[1, 2], [3, 4]])</p> <p>return models</p> <p>CopyExplain</p> <ul> <li> <p>The logic of this function should be in the order of taking models     as input, training them, and returning trained local models. As     input parameters, it takes models with     the\u00a0Dict[str,np.array]\u00a0format and the\u00a0init_flag\u00a0Boolean value,     indicating whether it is the initialization step or not.</p> </li> <li> <p>init_flag\u00a0is\u00a0True\u00a0when you want to call and return the     predefined\u00a0init_models, and it is\u00a0False\u00a0if it's an actual training     step.</p> </li> <li> <p>Eventually, this function returns the trained models that are     decomposed into NumPy arrays, with a dictionary     of\u00a0Dict[str,np.array]\u00a0in this example.</p> </li> <li> <p>In this\u00a0dummy example, we are just giving you\u00a0dummy models that skip     the actual training process.</p> </li> <li> <p>Then, the\u00a0following\u00a0compute_performance\u00a0function is designed to     compute the performance of models given a set of models and a test     dataset:</p> </li> </ul> <p>def compute_performance(models: Dict[str,np.array], \\</p> <p>testdata) -&gt; float:</p> <p># replace with actual performance computation logic</p> <p>accuracy = 0.5</p> <p>return</p> <p>CopyExplain</p> <ul> <li> <p>Again, in this example, just a dummy accuracy value is given,\u00a00.5,     to keep things simple.</p> </li> <li> <p>Then, you may want to define the     following\u00a0judge_termination\u00a0function to decide the criteria to     finish the training process and exit from the FL process:</p> </li> </ul> <p>def judge_termination(training_count: int = 0,</p> <p>global_arrival_count: int = 0) -&gt; bool:</p> <p># Depending on termination criteria, change the return bool value</p> <p># Call a performance tracker to check if the current models satisfy the required performance</p> <p>return True</p> <p>CopyExplain</p> <ul> <li> <p>It is up to you how to design this termination condition. This     function takes parameters such as the number of completed training     processes (training_count), the number of times it received global     models (global_arrival_count), and so on, returning a Boolean value     where the flag is\u00a0True\u00a0if it continues the FL process and\u00a0False\u00a0if     it stops. Here, it just gives a\u00a0True\u00a0Boolean value, meaning the FL     process will not stop unless the agent is forced to stop outside of     this function.</p> </li> <li> <p>If preparing the test data is needed, you can define a function such     as\u00a0prep_test_data:</p> </li> </ul> <p>def prep_test_data():</p> <p>testdata = 0</p> <p>return</p> <p>CopyExplain</p> <ul> <li> <p>In this example, it is just set as\u00a00.</p> </li> <li> <p>Now that\u00a0the necessary functions for testing\u00a0and training are     defined, we will integrate\u00a0client libraries into the local ML engine     to run the FL agent working with the FL server-side components, such     as an aggregator and a database.</p> </li> </ul> <p>Integration of client libraries into your local ML engine</p> <ul> <li> <p>Now, everything is ready to start your very first FL process,     although the models, training, and testing functions are set with     dummy variables.</p> </li> <li> <p>The very\u00a0first thing to do is to create a\u00a0Client\u00a0instance as follows     so that you can call its libraries:</p> </li> </ul> <p># Step1: Create Client instance</p> <p>cl = Client()</p> <p>CopyExplain</p> <p>Second, you create the\u00a0initial_models\u00a0with the training function, as follows:</p> <p># Step2: Create template models (to tell the shapes)</p> <p>initial_models = training(dict(), init_flag=True)</p> <p>CopyExplain</p> <ul> <li>After that, it sends the initial models to the FL aggregator by     calling\u00a0cl.send_initial_model, with\u00a0initial_models\u00a0as a parameter:</li> </ul> <p># Step3: Send initial models</p> <p>cl.send_initial_model(initial_model)</p> <p>CopyExplain</p> <ul> <li>Then, let's just start the client-side FL process by     calling\u00a0cl.start_fl_client(). As explained earlier in the\u00a0Starting     FL client core threads\u00a0section, this function can start three     processes at the same time: registering the agent, waiting for     global models, and the model exchange routine:</li> </ul> <p># Step4: Start the FL client</p> <p>cl.start_fl_client()</p> <p>CopyExplain</p> <ul> <li>Then, we\u00a0design the client-side FL cycle of local training/testing     and sending/receiving models by effectively integrating the several     FL client libraries, as follows:</li> </ul> <p># Step5: Run the local FL loop</p> <p>training_count, gm_arrival_count = 0, 0</p> <p>while judge_termination(training_count, gm_arrival_count):</p> <p>global_models = cl.wait_for_global_model()</p> <p>gm_arrival_count += 1</p> <p>global_model_performance_data = \\</p> <p>compute_performance(global_models, prep_test_data())</p> <p>models = training(global_models)</p> <p>training_count += 1</p> <p>perf_value = compute_performance( \\</p> <p>models, prep_test_data())</p> <p>cl.send_trained_model(models, 1, perf_value)</p> <p>CopyExplain</p> <ul> <li> <p>We use a\u00a0while\u00a0loop and the\u00a0judge_termination\u00a0function to check     whether the system needs to leave the loop. It is up to you to     use\u00a0training_count\u00a0and\u00a0gm_arrival_count\u00a0to judge the termination of     the FL cycle.</p> </li> <li> <p>Then, the agent proceeds to wait for the global models     with\u00a0cl.wait_for_global_model(). Upon the arrival of the global     models from the aggregator, it extracts\u00a0global_models,     increments\u00a0gm_arrival_count, and sets the client state to     the\u00a0training\u00a0state in the\u00a0wait_for_global_model\u00a0function.</p> </li> <li> <p>Next,\u00a0global_model_performance_data\u00a0is calculated with     the\u00a0compute_performance\u00a0function,     taking\u00a0global_models\u00a0and\u00a0prep_test_data\u00a0as input.</p> </li> <li> <p>While executing\u00a0training(global_models)\u00a0in the\u00a0training\u00a0state, the     client might receive new global models from the aggregator. This     scenario happens when the client's local training was too slow, and     the aggregator decided to utilize other local models to create a new     set of global models. If the new global models have already\u00a0arrived     at the agent, the client's state is changed to\u00a0gm_ready\u00a0and the     current ML model being trained will be discarded.</p> </li> <li> <p>After the local training phase has finished with\u00a0models\u00a0generated     by\u00a0training(global_models), an agent increments\u00a0training_count\u00a0and     calculates the performance data,\u00a0perf_value, of the current ML model     with the\u00a0compute_performance\u00a0function.</p> </li> <li> <p>Then, the agent tries to upload the trained local models to the     aggregator via\u00a0cl.send_trained_model, taking the trained models and     the performance value calculated previously as parameters.</p> </li> <li> <p>In the\u00a0send_trained_model\u00a0function, the client state is set     to\u00a0sending. Once the client's\u00a0model_exchange_routine\u00a0observes the     state transition to the\u00a0sending\u00a0state, it sends the trained local     models (stored as a binary file) to the aggregator. After sending     the models, it goes back to the\u00a0waiting_gm\u00a0state in     the\u00a0send_models\u00a0function.</p> </li> <li> <p>After sending the local models, the aggregator stores the uploaded     local models in its buffers and waits for another round of global     model aggregation, until enough local models are uploaded by agents.</p> </li> <li> <p>In the next section, we will briefly talk about how to integrate     image classification ML into the FL system we have discussed.</p> </li> </ul> <p>An example of integrating image classification \u2028into an FL system</p> <ul> <li> <p>We learned\u00a0about how to initiate\u00a0an FL process with a minimal     example. In this section, we will give a brief example of FL     with\u00a0image classification\u00a0(IC) using a CNN.</p> </li> <li> <p>First, the package that contains the image classification example     code is found in the\u00a0examples/image_classification/\u00a0folder in the     GitHub repository     at\u00a0[https://github.com/keshavaspanda/simple-fl]{.underline},     as shown in\u00a0Figure 5.3:</p> </li> </ul> <p>{width=\"4.041666666666667in\" height=\"2.627083333333333in\"}</p> <p>Figure 5.3 -- The image classification package</p> <ul> <li> <p>The main code in charge of integrating the IC algorithms into the FL     systems is found in the\u00a0classification_engine.py\u00a0file.</p> </li> <li> <p>When importing the libraries, we use a couple of extra files that     include CNN models, converter functions, and data managers related     to IC algorithms. The details are provided in the GitHub code     at\u00a0[https://github.com/keshavaspanda/simple-fl]{.underline}.</p> </li> <li> <p>Next, let's import some standard ML libraries as well as client     libraries from the FL code we discussed:</p> </li> </ul> <p>import logging</p> <p>import numpy as np</p> <p>import torch</p> <p>import torch.nn as nn</p> <p>import torch.optim as optim</p> <p>from typing import Dict</p> <p>from .cnn import Net</p> <p>from .conversion import Converter</p> <p>from .ic_training import DataManger, execute_ic_training</p> <p>from fl_main.agent.client import Client</p> <p>CopyExplain</p> <ul> <li>In this case, we define\u00a0TrainingMetaData, which just gives you the     amount of training\u00a0data that will be sent to\u00a0the aggregator and used     when conducting the\u00a0FedAvg\u00a0algorithm. The aggregation algorithm was     discussed in\u00a0section     4,\u00a0Federated     Learning Server Implementation with Python, as well as in\u00a0section     7,\u00a0Model     Aggregation:</li> </ul> <p>class TrainingMetaData:</p> <p># The number of training data used for each round</p> <p># This will be used for the weighted averaging</p> <p># Set to a natural number &gt; 0</p> <p>num_training_data = 8000</p> <p>CopyExplain</p> <ul> <li>The content of the\u00a0init_models\u00a0function is now replaced with a CNN     that is converted into a NumPy array. It returns the template of the     CNN in a dictionary format to inform the structure:</li> </ul> <p>def init_models() -&gt; Dict[str,np.array]:</p> <p>net = Net()</p> <p>return Converter.cvtr().convert_nn_to_dict_nparray(net)</p> <p>CopyExplain</p> <ul> <li> <p>The training function,\u00a0training, is now filled with actual training     algorithms using the CIFAR-10 dataset. It takes the models     and\u00a0init_flag\u00a0as parameters and returns the trained models     as\u00a0Dict[str,np.array]. The\u00a0init_flag\u00a0is a\u00a0bool\u00a0value, where it     is\u00a0True\u00a0if it's at the initial step and\u00a0False\u00a0if it's an actual     training step. When preparing for the training data, we use a     certain threshold for training due to batch size. In this case, the     threshold is\u00a04.</p> </li> <li> <p>Then, we create a CNN-based cluster global model with\u00a0net =     Converter.cvtr().convert_dict_nparray_to_nn(models).</p> </li> <li> <p>We define the loss function and optimizer as the following:</p> </li> </ul> <p>criterion = nn.CrossEntropyLoss()</p> <p>optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</p> <p>CopyExplain</p> <ul> <li> <p>Then, the actual training will be conducted with\u00a0trained_net =     execute_ic_training(DataManger.dm(), net, criterion, optimizer),     where the actual code of the IC training can be found in     the\u00a0ic_training.py\u00a0file.</p> </li> <li> <p>After\u00a0the training, the converted\u00a0models will be returned.</p> </li> <li> <p>The algorithm is summarized as follows:</p> </li> </ul> <p>def training(models: Dict[str,np.array], \\</p> <p>init_flag: bool=False) -&gt; Dict[str,np.array]:</p> <p>if init_flag:</p> <p>DataManger.dm( \\</p> <p>int(TrainingMetaData.num_training_data / 4))</p> <p>return init_models()</p> <p>net = \\</p> <p>Converter.cvtr().convert_dict_nparray_to_nn(models)</p> <p>criterion = nn.CrossEntropyLoss()</p> <p>optimizer = optim.SGD(net.parameters(), \\</p> <p>lr=0.001, momentum=0.9)</p> <p>trained_net = execute_ic_training(DataManger.dm(), \\</p> <p>net, criterion, optimizer)</p> <p>models = Converter.cvtr(). \\</p> <p>convert_nn_to_dict_nparray(trained_net)</p> <p>return models</p> <p>CopyExplain</p> <ul> <li>The following\u00a0compute_performance\u00a0function is filled with an     algorithm to calculate the accuracy, which is simple enough -- just     divide the number of correct outcomes\u00a0by the number of total\u00a0labels.     With a given set of models and a test dataset, it computes the     performance of the models, with\u00a0models\u00a0and\u00a0testdata\u00a0as parameters:</li> </ul> <p>def compute_performance(models: Dict[str,np.array], \\</p> <p>testdata, is_local: bool) -&gt; float:</p> <p># Convert np arrays to a CNN</p> <p>net = \\</p> <p>Converter.cvtr().convert_dict_nparray_to_nn(models)</p> <p>correct, total = 0, 0</p> <p>with torch.no_grad():</p> <p>for data in DataManger.dm().testloader:</p> <p>images, labels = data</p> <p>_, predicted = torch.max(net(images).data, 1)</p> <p>total += labels.size(0)</p> <p>correct += (predicted == labels).sum().item()</p> <p>acc = float(correct) / total</p> <p>return acc</p> <p>CopyExplain</p> <ul> <li>The\u00a0judge_termination\u00a0and\u00a0prep_test_data\u00a0functions are the same as     the functions of the minimal examples.</li> </ul> <p>Integration of client libraries into the IC example</p> <ul> <li> <p>Now, everything\u00a0is ready\u00a0to start the IC algorithm, and all the code     to integrate the preceding functions is the same as that used in the     previous\u00a0Integration of client libraries into your local ML     engine\u00a0section.</p> </li> <li> <p>Please look into the\u00a0classification_engine.py\u00a0file to make sure the     code is the\u00a0same, except for the\u00a0part that shows the actual number     of data samples that we are sending. This way, by just rewriting the     preceding functions, you will be able to easily connect your own     local ML application to the FL system that we have discussed here.</p> </li> <li> <p>Please refer to the\u00a0Running image classification and its     analysis\u00a0section,\u00a0Running the Federated Learning System and     Analyzing the Results, to check the results of running the code     discussed in this section.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>Here, we discussed FL client-side implementation. There are three     basic but important functionalities when participating in the FL     process, receiving global models sent from an aggregator with a push     or polling mechanism, and sending local models to an aggregator     after the local training process.</p> </li> <li> <p>In order to effectively implement the client-side ML engines that     cooperate with the FL aggregator, understanding the client state is     important. The client states include a state waiting for the global     model, a state indicating that local training is happening, a state     showing the readiness to send local models, and a state for having     the updated global models.</p> </li> <li> <p>We also discussed the design philosophy of FL client-side libraries,     where the core functions need to be effectively packaged to provide     user-friendly interfaces for ML developers and engineers.</p> </li> <li> <p>Last but not least, we learned how to actually use the FL client     libraries to integrate a local ML engine into an FL system, where we     used the minimal dummy example and IC example to understand the     integration process itself.</p> </li> <li> <p>In the next section, we will actually run the code that was     introduced in this and previous sections so that we can dig into     what is happening with the models, which are aggregated with a     minimal example as well as an IC example.</p> </li> </ul> <p>Running the Federated Learning System and Analyzing the Results</p> <ul> <li> <p>Here, you will run the\u00a0federated learning\u00a0(FL) system that     has been discussed in previous sections and analyze the system     behaviors and the outcomes of the aggregated models.</p> </li> <li> <p>We will start by explaining the configuration of the FL system     components in order to run the systems properly.</p> </li> <li> <p>Basically, after installing the simple FL system provided by our     GitHub sample, you first need to pick up the server machines or     instances to run the database and aggregator modules.</p> </li> <li> <p>Then, you can run agents to connect to the aggregator that is     already running. The IP address of the aggregator needs to be     correctly set up in each agent-side configuration.</p> </li> <li> <p>Also, there is a simulation mode so that you can run all the     components on the same machine or laptop to just test the     functionality of the FL system. After successfully running all the     modules of the FL system, you will be able to see the data folder     and a database created under the path that you set up in the     database server as well as on the agent side.</p> </li> <li> <p>You will be able to check both the local and global models, trained     and aggregated, so that you can download the recent or     best-performing models from the data folders.</p> </li> <li> <p>In addition, you can also see examples of running the FL system on a     minimal engine and image classification. By reviewing the outcomes     of the generated models and the performance data, you can understand     the aggregation algorithms as well as the actual interaction of the     models between an aggregator and agents.</p> </li> <li> <p>Here, we will cover the following main topics:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Configuring and running the FL system</p> <ul> <li> <p>Understanding what happens when the minimal example runs</p> </li> <li> <p>Running image classification and analyzing the results</p> </li> </ul> <p>Technical requirements</p> <ul> <li>All the code files introduced Here can be found on GitHub     (https://github.com/keshavaspanda/simple-fl).</li> </ul> <p>Configuring and running the FL system</p> <ul> <li>Configuring the FL system\u00a0and installing its\u00a0environment are simple     enough to do. Follow the instructions in the next subsections.</li> </ul> <p>Installing the FL environment</p> <ul> <li>First, to run the FL system\u00a0discussed in the previous section, clone     the following repository to the machines that you want to run FL on     using the following command:</li> </ul> <p>git clone https://github.com/keshavaspanda/simple-fl</p> <p>CopyExplain</p> <ul> <li> <p>Once done with the cloning process, change the directory to     the\u00a0simple-fl\u00a0folder in the command line. The simulation run can be     carried out using just one machine or using multiple systems. In     order to run the FL process on one or multiple machines that include     the FL server (aggregator), FL client (agent), and database server,     you should create a\u00a0conda\u00a0virtual environment and activate it.</p> </li> <li> <p>To create a\u00a0conda\u00a0environment in macOS, you will need to type the     following command:</p> </li> </ul> <p>conda env create -n federatedenv -f ./setups/federatedenv.yaml</p> <p>CopyExplain</p> <ul> <li>If you're using a Linux machine, you can create     the\u00a0conda\u00a0environment by using the following command:</li> </ul> <p>conda env create -n federatedenv -f ./setups/federatedenv_linux.yaml</p> <p>CopyExplain</p> <ul> <li> <p>Then, activate the\u00a0conda\u00a0environment\u00a0federatedenv\u00a0when you run the     code. For your information,     the\u00a0federatedenv.yaml\u00a0and\u00a0federatedenv_linux.yaml\u00a0files can be found     in the\u00a0setups\u00a0folder of the\u00a0simple-fl\u00a0GitHub repository and include     the libraries that are used in the code examples throughout this     book.</p> </li> <li> <p>As noted in the\u00a0README\u00a0file of the GitHub repo, there are mainly     three components to run: the database server, aggregator, and     agent(s). If you want to conduct a simulation within one machine,     you can just install a\u00a0conda\u00a0environment (federatedenv)\u00a0on that     machine.</p> </li> <li> <p>If you want to create a distributed\u00a0environment, you need to install     the\u00a0conda\u00a0environment on all the machines you want to use, such as     the database server on a cloud instance, the aggregator server on a     cloud instance, and the local client machine.</p> </li> <li> <p>Now that the installation process for the entire FL process is     ready, let's move on to configuring the FL system with configuration     files.</p> </li> </ul> <p>Configuring the FL system with JSON files for each component</p> <ul> <li>First, edit the configuration\u00a0JSON files in the\u00a0setups\u00a0folder of the     provided GitHub repository. These JSON files are read by a database     server, aggregator, and agents to configure their initial setups.     Again, the configuration details are explained as follows.</li> </ul> <p>config_db.json</p> <ul> <li>The\u00a0config_db.json\u00a0file deals with configuring a database server.     Use the following information to properly\u00a0operate the server:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   db_ip: The database server's IP address (for example,\u00a0localhost). If     you want to run the database server on a cloud\u00a0instance, such as     an\u00a0Amazon Web Services\u00a0(AWS) EC2 instance, you can specify     the private IP address of the instance.</p> <ul> <li> <p>db_socket: The socket number used between the database and     aggregator (for example,\u00a09017).</p> </li> <li> <p>db_name: The name of the SQLite database (for example,\u00a0sample_data).</p> </li> <li> <p>db_data_path: The path to the SQLite database (for example,\u00a0./db).</p> </li> <li> <p>db_model_path: The\u00a0path to the directory to save all\u00a0Machine     Learning\u00a0(ML) models (for example,\u00a0./db/models).</p> </li> </ul> <p>config_aggregator.json</p> <ul> <li>The\u00a0config_aggregator.json\u00a0file deals with configuring an aggregator     in the FL server. Use the following\u00a0information to properly operate     the aggregator:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   aggr_ip: The aggregator's IP address (for example,\u00a0localhost). If     you want to run the aggregator server on a cloud instance, such as     an AWS EC2 instance, you can specify the private IP address of the     instance.</p> <ul> <li> <p>db_ip: The database server's IP address (for example,\u00a0localhost). If     you want to connect to the database server hosted on a different     cloud instance, you can specify the public IP address of the     database instance. If you host the database server on the same cloud     instance as the aggregator's instance, you can specify the same     private IP address of the instance.</p> </li> <li> <p>reg_socket: The socket number used by agents to connect to an     aggregator for the first time (for example,\u00a08765).</p> </li> <li> <p>recv_socket: The socket number used to upload local models or poll     to an aggregator from an agent. Agents will learn this socket     information by communicating with an aggregator (for example,\u00a07890).</p> </li> <li> <p>exch_socket: The socket number used to send global models back to an     agent from an aggregator when a push method is used. Agents will     learn this socket information by communicating with an aggregator     (for example,\u00a04321).</p> </li> <li> <p>db_socket: The socket number used between the database and an     aggregator (for example,\u00a09017).</p> </li> <li> <p>round_interval: The period of time after which an agent checks     whether there are enough models to start an aggregation step (unit:     seconds; for example,\u00a05).</p> </li> <li> <p>aggregation_threshold: The percentage of collected local models     required to start an aggregation step (for example,\u00a00.85).</p> </li> <li> <p>polling: The flag to\u00a0specify whether to use a polling method or not.     If the flag is\u00a01, use the polling method; if the flag is\u00a00, use a     push method. This value needs to be the same between the aggregator     and agent.</p> </li> </ul> <p>config_agent.json</p> <ul> <li>The\u00a0config_agent.json\u00a0file deals with configuring an agent in the FL     client. Use the following information to properly operate the agent:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   aggr_ip: The aggregator\u00a0server's IP address (for     example,\u00a0localhost). If you want to connect to the aggregator server     hosted on a cloud instance, such as an AWS EC2 instance, you can     specify the public IP address of the aggregator instance.</p> <ul> <li> <p>reg_socket: The socket number used by agents to join an aggregator     for the first time (for example,\u00a08765).</p> </li> <li> <p>model_path: The path to a local director in the agent machine to     save local and global models and some state information (for     example,\u00a0./data/agents).</p> </li> <li> <p>local_model_file_name: The filename to save local models in the     agent machine (for example,\u00a0lms.binaryfile).</p> </li> <li> <p>global_model_file_name: The filename to save local models in the     agent machine (for example,\u00a0gms.binaryfile).</p> </li> <li> <p>state_file_name: The filename to store the agent state in the agent     machine (for example,\u00a0state).</p> </li> <li> <p>init_weights_flag:\u00a01\u00a0if the weights are initialized with certain     values,\u00a00\u00a0otherwise, where weights are initialized with zeros.</p> </li> <li> <p>polling: The flag to specify whether to use a polling method or not.     If the flag is\u00a01, use the polling method; if the flag is\u00a00, use a     push method. This value needs to be\u00a0the same between the aggregator     and agent.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Now, the FL systems can be configured using the configuration files     explained in this section. Next, you will run the database and     aggregator on the FL server side.</p> <p>Running the database and aggregator on the FL server</p> <ul> <li>In this section, you will\u00a0configure the database and aggregator on     the FL server side. Then, you will edit the configuration files in     the\u00a0setups\u00a0folder of the\u00a0simple-fl\u00a0GitHub repo. After that, you will     run\u00a0pseudo_db\u00a0first, and\u00a0then\u00a0server_th, as follows:</li> </ul> <p>python -m fl_main.pseudodb.pseudo_db</p> <p>python -m fl_main.aggregator.server_th</p> <p>CopyExplain</p> <p>Important note</p> <ul> <li> <p>If the database server and aggregator server are running on     different machines, you will need to specify the IP address of the     database server or instance of the aggregator. The IP address of the     database server can be modified in the\u00a0config_aggregator.json\u00a0file     in the\u00a0setups\u00a0folder.</p> </li> <li> <p>Also, if both the database and aggregator instances are running in     public cloud environments, the IP address of the configuration files     of those servers needs to be the private IP address.</p> </li> <li> <p>Agents need to connect to the aggregator using the public IP address     and the connecting socket (port number) needs to be open to accept     inbound messages.</p> </li> <li> <p>After you start the database and aggregator servers, you will see a     message such as the following in the console:</p> </li> </ul> <p># Database-side Console Example</p> <p>INFO:root:--- Pseudo DB Started ---</p> <p>CopyExplain</p> <ul> <li>On the aggregator\u00a0side of the console, you will see something like     the following:</li> </ul> <p># Aggregator-side Console Example</p> <p>INFO:root:--- Aggregator Started ---</p> <p>CopyExplain</p> <ul> <li> <p>Behind this aggregator\u00a0server, the model synthesis module is running     every 5 seconds, where it starts checking whether the number of     collected local models is more than the number that the aggregation     threshold defines.</p> </li> <li> <p>We have now run the database and aggregator modules and are ready to     run a minimal example with the FL client.</p> </li> </ul> <p>Running a minimal example with the FL client</p> <ul> <li> <p>In the previous\u00a0section, we talked about the integration of local ML     engines into the FL system. Here, using a minimal sample that does     not have actual training data, we will try to run the FL systems     that have been discussed. This minimal example can be used as a     template when implementing any locally distributed ML engine.</p> </li> <li> <p>Before running the minimal example, you should check whether the     database and aggregator servers are running already. Then, run the     following command:</p> </li> </ul> <p>python -m examples.minimal.minimal_MLEngine</p> <p>CopyExplain</p> <ul> <li> <p>In this case, only one agent with a minimal ML engine is connected.     Thus, the aggregation happens every time this default agent uploads     the local model.</p> </li> <li> <p>Note that if the aggregator server is running on a different     machine, you will need to specify the public IP address of the     aggregator server or instance. The IP address of the aggregator can     be modified in the\u00a0config_agent.json\u00a0file in the\u00a0setups\u00a0folder. We     also recommend setting the\u00a0polling\u00a0flag to\u00a01\u00a0when running the     aggregator and database in a cloud instance.</p> </li> </ul> <p>Figure 6.1\u00a0shows an example\u00a0of the console screen when running a database server:</p> <p>{width=\"6.268055555555556in\" height=\"4.277777777777778in\"}</p> <p>Figure 6.1 --\u00a0Example of a database-side console</p> <p>Figure 6.2\u00a0shows an example of the console screen when running an aggregator:</p> <p>{width=\"6.268055555555556in\" height=\"4.104861111111111in\"}</p> <p>Figure 6.2 -- Example of an aggregator-side console</p> <p>Figure 6.3\u00a0shows an\u00a0example of the console screen when running an agent:</p> <p>{width=\"6.268055555555556in\" height=\"4.260416666666667in\"}</p> <p>Figure 6.3 -- Example of an agent-side console</p> <ul> <li> <p>Now we know how to\u00a0run all the FL components: a database,     aggregator, and agent.</p> </li> <li> <p>In the next section, we will examine how outputs are generated by     running the FL system.</p> </li> </ul> <p>Data and database folders</p> <ul> <li> <p>After running the FL system, you\u00a0will notice that the database     folder and data folder are created under the locations that you     specified in the config files of the database and agent.</p> </li> <li> <p>For example, the\u00a0db\u00a0folder is created under\u00a0db_data_path, written in     the\u00a0config_db.json\u00a0file. In the database folder, you will find the     SQLite database, such as\u00a0model_data12345.db, where the metadata of     local and cluster global models is stored, as well as     a\u00a0models\u00a0folder that contains all the actual local models uploaded     by the agents and global models created by the aggregator.</p> </li> <li> <p>Figure 6.4\u00a0shows the SQLite database and ML model files in a     binary file format stored in the\u00a0db\u00a0folder created by running the     minimal example code:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"2.6215277777777777in\"}</p> <p>Figure 6.4 -- The SQLite database and ML model files in a binary file format stored in the db folder</p> <ul> <li>The\u00a0data\u00a0folder is created under an agent device at the location of     the\u00a0model_path, a string value\u00a0defined in\u00a0config_agent.json. In the     example run of the minimal example, the following files are created     under the\u00a0data/agents/default-agent\u00a0folder:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   lms.binaryfile: A binary file containing a local model created by     the agent</p> <ul> <li> <p>gms.binaryfile: A binary file containing a global model created by     the aggregator sent back to the agent</p> </li> <li> <p>state: A file that has an integer value that indicates the state of     the client itself</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Figure 6.5\u00a0shows the structure of the agent-side data, which     includes global and local ML models represented with a binary file     format, as well as the file reflecting the FL client state:</p> <p>{width=\"6.268055555555556in\" height=\"1.9270833333333333in\"}</p> <p>Figure 6.5 -- Data of the agents including global and local ML models with a binary file format as well as the client state</p> <ul> <li>Now we understand where the key data, such as global and local     models, is stored. Next, we will take a closer\u00a0look at the database     using SQLite.</li> </ul> <p>Databases with SQLite</p> <ul> <li>The database\u00a0created in the\u00a0db\u00a0folder can be viewed using any tool     to show the SQLite database that can open files with     the\u00a0***.db\u00a0format. The database tables are defined in the     following sections.</li> </ul> <p>Local models in a database</p> <ul> <li>Figure 6.6\u00a0shows sample database\u00a0entries related to uploaded local     models where each entry lists the local model ID, the time that the     model was generated, the ID of the agent that uploaded the local     model, round information, performance metrics, and the number of     data samples:</li> </ul> <p>{width=\"6.268055555555556in\" height=\"1.5618055555555554in\"}</p> <p>Figure 6.6 -- Sample database entries related to uploaded local models</p> <p>Cluster models in a database</p> <ul> <li>Figure 6.7\u00a0shows sample database entries related to uploaded     cluster models where each entry lists the cluster model ID, the time     that the model was created, the ID of the aggregator that created     this cluster model, round information, and the number of data     samples:</li> </ul> <p>{width=\"6.268055555555556in\" height=\"1.7402777777777778in\"}</p> <p>Figure 6.7 -- Sample database entries related to uploaded cluster models</p> <ul> <li>Now we have learned\u00a0how to configure and run the FL system with a     minimal example and how to examine the results. In the next section,     you will learn about the behavior of the FL system and what happens     when the minimal example is run.</li> </ul> <p>Understanding what happens when the minimal example runs</p> <ul> <li>Understanding the behavior of the entire FL system step by step will     help you design applications with FL enabled and further enhance the     FL system itself. Let us first look into what happens when we run     just one agent by printing some procedures of the agent and     aggregator modules.</li> </ul> <p>Running just one minimal agent</p> <ul> <li>Let's run the minimal\u00a0agent after running the database and     aggregator servers and see what happens. When the agent is started     with the minimal ML engine, you will see the following messages in     the agent console:</li> </ul> <p># Agent-side Console Example</p> <p>INFO:root:--- This is a minimal example ---</p> <p>INFO:root:--- Agent initialized ---</p> <p>INFO:root:--- Your IP is xxx.xxx.1.101 ---</p> <p>CopyExplain</p> <ul> <li>When the agent initializes the model to be used for FL, it shows     this message, and if you look at the\u00a0state\u00a0file, it has entered     the\u00a0sending\u00a0state, which will trigger sending models to the     aggregator when the FL client is started:</li> </ul> <p># Agent-side Console Example</p> <p>INFO:root:--- Model template generated ---</p> <p>INFO:root:--- Local (Initial/Trained) Models saved ---</p> <p>INFO:root:--- Client State is now sending ---</p> <p>CopyExplain</p> <ul> <li>Then, after the client is started with the\u00a0start_fl_client\u00a0function,     the participation message is\u00a0sent to the aggregator. Here is the     participation message sent to the aggregator:</li> </ul> <p>[</p> <p>\\&lt;AgentMsgType.participate: 0&gt;, # Agent Message Type</p> <p>\\'A89fd1c2d9*****\\', # Agent ID</p> <p>\\'047b18ddac*****\\',\u00a0\u00a0\u00a0\u00a0# Model ID</p> <p>{</p> <p>\\'model1\\': array([[1, 2, 3], [4, 5, 6]]),</p> <p>\\'model2\\': array([[1, 2], [3, 4]])</p> <p>}, # ML Models</p> <p>True,\u00a0\u00a0\u00a0\u00a0# Init weights flag</p> <p>False, # Simulation flag</p> <p>0, # Exch Port</p> <p>1645141807.846751, # Generated Time of the models</p> <p>{\\'accuracy\\': 0.0, \\'num_samples\\': 1}, # Meta information</p> <p>\\'xxx.xxx.1.101\\' # Agent\\'s IP Address</p> <p>]</p> <p>CopyExplain</p> <ul> <li> <p>The participation message to the aggregator includes the message     type, agent ID, model ID, ML model with NumPy, initialization     weights flag, simulation flag, exchange port number, time the models     were generated, and meta information such as performance metrics and     the agent's IP address.</p> </li> <li> <p>The agent receives the\u00a0welcome message from an aggregator confirming     the connection of this agent, which also includes the following     information:</p> </li> </ul> <p># Agent-side Console Example</p> <p>INFO:root:--- Init Response: [</p> <p>\\&lt;AggMsgType.welcome: 0&gt;, # Message Type</p> <p>\\'4e2da*****\\', # Aggregator ID</p> <p>\\'23487*****\\', # Model ID</p> <p>{\\'model1\\': array([[1, 2, 3], [4, 5, 6]]),</p> <p>\\'model2\\': array([[1, 2], [3, 4]])}, # Global Models</p> <p>1, # FL Round</p> <p>\\'A89fd1c2d9*****\\', # Agent ID</p> <p>\\'7890\\', # exch_socket number</p> <p>\\'4321\\' # recv_socket number</p> <p>] ---</p> <p>CopyExplain</p> <ul> <li>On the aggregator side, after this agent sends a participation     message to the aggregator, the aggregator confirms the participation     and pushes this initial model to the database:</li> </ul> <p># Aggregator-side Console Example</p> <p>INFO:root:--- Participate Message Received ---</p> <p>INFO:root:--- Model Formats initialized, model names: [\\'model1\\', \\'model2\\'] ---</p> <p>INFO:root:--- Models pushed to DB: Response [\\'confirmation\\'] ---</p> <p>INFO:root:---\u00a0\u00a0Global Models Sent to A89fd1c2d9***** ---</p> <p>INFO:root:--- Aggregation Threshold (Number of agents needed for aggregation): 1 ---</p> <p>INFO:root:--- Number of collected local models: 0 ---</p> <p>INFO:root:--- Waiting for more local models to be collected ---</p> <p>CopyExplain</p> <ul> <li>In the database server-side\u00a0console, you can also check that the     local model is sent from the aggregator and the model is saved in     the database:</li> </ul> <p># DB-side Console Example</p> <p>INFO:root:Request Arrived</p> <p>INFO:root:--- Model pushed: ModelType.local ---</p> <p>INFO:root:--- Local Models are saved ---</p> <p>CopyExplain</p> <ul> <li>After the aggregator sends the global model back to the agent, the     agent receives and saves it and changes the client state     from\u00a0waiting_gm\u00a0to\u00a0gm_ready, indicating the global model is ready     for retraining locally:</li> </ul> <p># Agent-side Console Example</p> <p>INFO:root:--- Global Model Received ---</p> <p>INFO:root:--- Global Models Saved ---</p> <p>INFO:root:--- Client State is now gm_ready ---</p> <p>CopyExplain</p> <ul> <li>Here is the message sent to the agent from an aggregator, including     the global model. The contents of the message include the message     type, aggregator ID, cluster model ID, FL round, and ML models with     NumPy:</li> </ul> <p>[</p> <p>\\&lt;AggMsgType.sending_gm_models: 1&gt;, # Message Type</p> <p>\\'8c6c946472*****\\', # Aggregator ID</p> <p>\\'ab633380f6*****\\', # Global Model ID</p> <p>1, # FL Round Info</p> <p>{\u00a0\u00a0\u00a0\u00a0</p> <p>\\'model1\\': array([[1., 2., 3.],[4., 5., 6.]]),</p> <p>\\'model2\\': array([[1., 2.],[3., 4.]])</p> <p>} # ML models</p> <p>]</p> <p>CopyExplain</p> <ul> <li>Then, the agent reads the\u00a0global models to proceed with using them     for local training and changes the client state to\u00a0training:</li> </ul> <p># Agent-side Console Example</p> <p>INFO:root:--- Global Models read by Agent ---</p> <p>INFO:root:--- Client State is now training ---</p> <p>INFO:root:--- Training ---</p> <p>INFO:root:--- Training is happening ---</p> <p>INFO:root:--- Training is happening ---</p> <p>INFO:root:--- Training Done ---</p> <p>INFO:root:--- Local (Initial/Trained) Models saved ---</p> <p>INFO:root:--- Client State is now sending ---</p> <p>INFO:root:--- Local Models Sent ---</p> <p>INFO:root:--- Client State is now waiting_gm ---</p> <p>INFO:root:--- Polling to see if there is any update (shown only when polling) ---</p> <p>INFO:root:--- Global Model Received ---</p> <p>INFO:root:--- The global models saved ---</p> <p>CopyExplain</p> <ul> <li> <p>After the preceding local training process, the agent proceeds     with\u00a0sending\u00a0the trained local models to the aggregator and changes     the client state to\u00a0waiting_gm, which means it waits for the global     model with the polling mechanism.</p> </li> <li> <p>Here is the message\u00a0sent to the aggregator as a trained local model     message. The contents of the message include message type, agent ID,     model ID, ML models, generated time of the models, and metadata such     as performance data:</p> </li> </ul> <p>[</p> <p>\\&lt;AgentMsgType.update: 1&gt;, # Agent\\'s Message Type</p> <p>\\'a1031a737f*****\\', # Agent ID</p> <p>\\'e89ccc5dc9*****\\', # Model ID</p> <p>{</p> <p>\\'model1\\': array([[1, 2, 3],[4, 5, 6]]),</p> <p>\\'model2\\': array([[1, 2],[3, 4]])</p> <p>}, # ML Models</p> <p>1645142806.761495, # Generated Time of the models</p> <p>{\\'accuracy\\': 0.5, \\'num_samples\\': 1} # Meta information</p> <p>]</p> <p>CopyExplain</p> <ul> <li>Then, in the aggregator, after the local model is pushed to the     database, it shows the change in the buffer, that the number of     collected local models is up to 1 from 0, thus indicating that     enough local models are collected to start the aggregation:</li> </ul> <p># Aggregator-side Console Example</p> <p>INFO:root:--- Models pushed to DB: Response [\\'confirmation\\'] ---</p> <p>INFO:root:--- Local Model Received ---</p> <p>INFO:root:--- Aggregation Threshold (Number of agents needed for aggregation): 1 ---</p> <p>INFO:root:--- Number of collected local models: 1 ---</p> <p>INFO:root:--- Enough local models are collected. Aggregation will start. ---</p> <p>CopyExplain</p> <ul> <li>Then, aggregation for round 1 happens and the cluster global models     are formed, pushed to the database, and\u00a0sent to the agent once the     polling message arrives from the agent. The aggregator can also push     the message back to the agent via a push method:</li> </ul> <p># Aggregator-side Console Example</p> <p>INFO:root:Round 1</p> <p>INFO:root:Current agents: [{\\'agent_name\\': \\'default_agent\\', \\'agent_id\\': \\'A89fd1c2d9*****\\', \\'agent_ip\\': \\'xxx.xxx.1.101\\', \\'socket\\': 7890}]</p> <p>INFO:root:--- Cluster models are formed ---</p> <p>INFO:root:--- Models pushed to DB: Response [\\'confirmation\\'] ---</p> <p>INFO:root:--- Global Models Sent to A89fd1c2d9***** ---</p> <p>CopyExplain</p> <ul> <li>On the database server side, the cluster global model is received     and pushed to the database:</li> </ul> <p># DB-side Console Example</p> <p>INFO:root:Request Arrived</p> <p>INFO:root:--- Model pushed: ModelType.cluster ---</p> <p>INFO:root:--- Cluster Models are saved ---</p> <p>CopyExplain</p> <ul> <li> <p>This process in this section is repeated after cluster models are     generated and saved for the upcoming FL round and the round of FL     proceeds with this interaction mechanism.</p> </li> <li> <p>If you look at both the local and cluster global models, they are as     follows:</p> </li> </ul> <p>{</p> <p>\\'model1\\': array([[1, 2, 3],[4, 5, 6]]),</p> <p>\\'model2\\': array([[1, 2],[3, 4]])</p> <p>}</p> <p>CopyExplain</p> <ul> <li> <p>This means only one fixed model is used all the time even if     aggregation happens, so the global model is exactly the same as the     initial one as the dummy training process is used here.</p> </li> <li> <p>We will now look into the\u00a0results when running two minimal agents in     the next section.</p> </li> </ul> <p>Running two minimal agents</p> <ul> <li> <p>With the database and aggregator\u00a0servers running, you can run many     agents using the\u00a0minimal_MLEngine.py\u00a0file in     the\u00a0simple-fl/examples/minimal\u00a0folder.</p> </li> <li> <p>You should run the two individual agents from different local     machines by specifying the IP address of the aggregator to connect     those agents with the minimal ML example.</p> </li> <li> <p>You can also run multiple agents from the same machine for     simulation purposes by specifying the different port numbers for the     individual agents.</p> </li> <li> <p>In the code provided in the\u00a0simple-fl\u00a0repository on GitHub, you can     run the multiple agents by using the following command:</p> </li> </ul> <p>python -m examples.minimal.minimal_MLEngine [simulation_flag] [gm_recv_port] [agent_name]</p> <p>CopyExplain</p> <ul> <li> <p>To conduct the simulation,\u00a0simulation_flag\u00a0should be set     to\u00a01.\u00a0gm_recv_port\u00a0is the port number to receive the global models     from the aggregator. The agent will be notified of the port number     by the aggregator through the response of a participation message.     Also,\u00a0agent_name\u00a0is the name of the local agent and the directory     name storing the state and model files. This needs to be unique for     every agent.</p> </li> <li> <p>For instance, you can run the first and second agents with the     following commands:</p> </li> </ul> <p># First agent</p> <p>python -m examples.minimal.minimal_MLEngine 1 50001 a1</p> <p># Second agent</p> <p>python -m examples.minimal.minimal_MLEngine 1 50002 a2</p> <p>CopyExplain</p> <ul> <li> <p>You can edit the configuration JSON files in the\u00a0setups\u00a0folder if     needed. In this case,\u00a0agg_threshold\u00a0is set to\u00a01.</p> </li> <li> <p>When you run the simulation in the database server running a minimal     example with multiple agents, the console screen will look similar     to that in\u00a0Figure 6.1.</p> </li> <li> <p>Figure 6.8\u00a0shows the console screen of a simulation in the     aggregator server running a minimal example using dummy ML models:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.104861111111111in\"}</p> <p>Figure 6.8 -- Example of an aggregator-side console running a minimal example connecting two agents</p> <ul> <li>Figure 6.9\u00a0shows the\u00a0console screen of a simulation in one of the     agents running a minimal example using dummy ML models:</li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.509027777777778in\"}</p> <p>Figure 6.9 -- Example of agent 1's console running a minimal example using dummy ML models</p> <ul> <li>Figure 6.10\u00a0shows the\u00a0console screen of a simulation in another     agent running a minimal example using dummy ML models:</li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.522222222222222in\"}</p> <p>Figure 6.10 -- Example of agent 2's console running a minimal example using dummy ML models</p> <ul> <li>Now we know how to run the\u00a0minimal example with two agents. In order     to further look into the FL procedure using this example, we will     answer the following questions:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Has aggregation been done correctly for the simple cases?</p> <ul> <li> <p>Has the\u00a0FedAvg\u00a0algorithm been applied correctly?</p> </li> <li> <p>Does aggregation threshold work with connected agents?</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   After running and connecting the two agents, the aggregator will     wait to receive two models from the two connected agents, as     follows:</p> <p># Aggregator-side Console Example</p> <p>INFO:root:--- Aggregation Threshold (Number of agents needed for aggregation): 2 ---</p> <p>INFO:root:--- Number of collected local models: 0 ---</p> <p>INFO:root:--- Waiting for more local models to be collected ---</p> <p>CopyExplain</p> <ul> <li> <p>In this case, the aggregation threshold is set to\u00a01.0\u00a0in     the\u00a0config_aggregator.json\u00a0file in the\u00a0setups\u00a0folder, so the     aggregator needs to collect all the models from connected agents,     meaning it needs to receive local ML models from all the agents that     are connected to the aggregator.</p> </li> <li> <p>Then, it receives one model\u00a0from one of the agents and the number of     collected local models is increased to 1. However, as the aggregator     is still missing one local model, it does not start aggregation yet:</p> </li> </ul> <p># Aggregator-side Console Example</p> <p>INFO:root:--- Local Model Received ---</p> <p>INFO:root:--- Aggregation Threshold (Number of agents needed for aggregation): 2 ---</p> <p>INFO:root:--- Number of collected local models: 1 ---</p> <p>INFO:root:--- Waiting for more local models to be collected ---</p> <p>CopyExplain</p> <ul> <li> <p>On the agent side, after the local models are sent to the     aggregator, it will wait until the cluster global model to be     created in the aggregator and sent back to the agent. In this way,     you can synchronize the FL process at the agent side and automate     the local training procedure when the global model is sent back to     the agent and ready for retraining.</p> </li> <li> <p>After the aggregator receives another local model, enough models are     collected to start the aggregation process:</p> </li> </ul> <p># Aggregator-side Console Example</p> <p>INFO:root:--- Local Model Received ---</p> <p>INFO:root:--- Aggregation Threshold (Number of agents needed for aggregation): 2 ---</p> <p>INFO:root:--- Number of collected local models: 2 ---</p> <p>INFO:root:--- Enough local models are collected. Aggregation will start. ---</p> <p>CopyExplain</p> <ul> <li>It will finally start the aggregation for the first round, as     follows:</li> </ul> <p># Aggregator-side Console Example</p> <p>INFO:root:Round 1</p> <p>INFO:root:Current agents: [{\\'agent_name\\': \\'a1\\', \\'agent_id\\': \\'1f503*****\\', \\'agent_ip\\': \\'xxx.xxx.1.101\\', \\'socket\\': 50001}, {\\'agent_name\\': \\'a2\\', \\'agent_id\\': \\'70de8*****\\', \\'agent_ip\\': \\'xxx.xxx.1.101\\', \\'socket\\': 50002}]</p> <p>INFO:root:--- Cluster models are formed ---</p> <p>INFO:root:--- Models pushed to DB: Response [\\'confirmation\\'] ---</p> <p>INFO:root:--- Global Models Sent to 1f503***** ---</p> <p>INFO:root:--- Global Models Sent to 70de8***** ---</p> <p>CopyExplain</p> <p>Here, let's look at the agent-side\u00a0ML models that are locally trained:</p> <p># Agent 1\\'s Console Example</p> <p>INFO:root:--- Training ---</p> <p>INFO:root:--- Training is happening ---</p> <p>INFO:root:--- Training Done ---</p> <p>Trained models: {\\'model1\\': array([[1, 2, 3],</p> <p>[4, 5, 6]]), \\'model2\\': array([[1, 2],</p> <p>[3, 4]])}</p> <p>INFO:root:--- Local (Initial/Trained) Models saved ---</p> <p>CopyExplain</p> <ul> <li>Also, let's look at another agent's ML models that are locally     trained:</li> </ul> <p># Agent 2\\'s Console Example</p> <p>INFO:root:--- Training ---</p> <p>INFO:root:--- Training is happening ---</p> <p>INFO:root:--- Training Done ---</p> <p>Trained models: {\\'model1\\': array([[3, 4, 5],</p> <p>[6, 7, 8]]), \\'model2\\': array([[3, 4],</p> <p>[5, 6]])}</p> <p>INFO:root:--- Local (Initial/Trained) Models saved ---</p> <p>CopyExplain</p> <ul> <li> <p>As in the models sent to the aggregator from agents 1 and 2,     if\u00a0FedAvg\u00a0is correctly applied, the global model should be the     averaged value of these two models. In this case, the number of data     samples is the\u00a0same for both agents 1 and 2, so the global model     should just be an average of the two models.</p> </li> <li> <p>So, let's look at the global models that are generated in the     aggregator:</p> </li> </ul> <p># Agent 1 and 2\\'s Console Example</p> <p>Global Models: {\\'model1\\': array([[2., 3., 4.],</p> <p>[5., 6., 7.]]), \\'model2\\': array([[2., 3.],</p> <p>[4., 5.]])}</p> <p>CopyExplain</p> <ul> <li> <p>The received model is the average of the two local models and thus     averaging has been correctly conducted.</p> </li> <li> <p>The database and data folders are created in     the\u00a0model_path\u00a0specified in the agent configuration file. You can     look at the database values with an SQLite viewer application and     look for some models based on the model ID.</p> </li> <li> <p>Now that we understand what's happening with minimal example runs,     in the next section, we will run a real ML application using an     image classification model using a\u00a0Convolutional Neural     Network\u00a0(CNN).</p> </li> </ul> <p>Running image classification and analyzing the results</p> <ul> <li> <p>This example demonstrates the\u00a0use of this FL framework for     image\u00a0classification tasks. We will use a famous image dataset,     CIFAR-10     (URL:\u00a0[https://www.cs.toronto.edu/\\~kriz/cifar.html]{.underline}),     to show how an ML model grows through the FL process over time.</p> </li> <li> <p>However, this example is only given for the purposes of using the FL     system we have discussed so far and is not focused on maximizing the     performance of the image classification task.</p> </li> </ul> <p>Preparing the CIFAR-10 dataset</p> <ul> <li>The following is the\u00a0information required related to the dataset     size, the training and test data, the number of classes, and the     image size:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Dataset size: 60,000 images</p> <ul> <li> <p>Training data: 50,000 images</p> </li> <li> <p>Test data: 10,000 images</p> </li> <li> <p>Number of classes: 10     (airplane,\u00a0automobile,\u00a0bird,\u00a0cat,\u00a0deer,\u00a0dog,\u00a0frog,\u00a0horse,\u00a0ship,     and\u00a0truck)</p> </li> <li> <p>Each class has 6,000 images</p> </li> <li> <p>Image size: 32x32 pixels, in color</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Figure 6.11\u00a0shows a collection of sample pictures of 10 different     classes in the dataset with 10 random images for each:</p> <p>{width=\"6.268055555555556in\" height=\"4.816666666666666in\"}</p> <p>Figure 6.11 -- The classes in the dataset as well as 10 random images for each category (the images are adapted from https://www.cs.toronto.edu/\\~kriz/cifar.html)</p> <ul> <li>Now that the dataset is\u00a0prepared, we will look into a CNN model used     for the FL process.</li> </ul> <p>The ML model used for FL with\u00a0image classification</p> <ul> <li>Here is the\u00a0description of the ML model architecture of the CNN     model used in this image classification example. To learn more about     what the CNN is, you can find many useful study resources, such     as\u00a0[https://cs231n.github.io/convolutional-networks/]{.underline}:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Conv2D</p> <ul> <li> <p>MaxPool2D (maximum pooling)</p> </li> <li> <p>Conv2D</p> </li> <li> <p>3 fully-connected layers</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The script to define the CNN model is already designed and can be     found in\u00a0cnn.py\u00a0in\u00a0examples/image_classification\u00a0in     the\u00a0simple-fl\u00a0repository on GitHub.</p> <ul> <li>Next, we will run the image classification application with the FL     system.</li> </ul> <p>How to run\u00a0the image classification example with CNN</p> <ul> <li>As mentioned in the installation\u00a0steps at the beginning of This     section, we first install\u00a0the necessary libraries with\u00a0federatedenv,     and then install\u00a0torch\u00a0and\u00a0torchvision\u00a0after that:</li> </ul> <p>pip install torch</p> <p>pip install torchvision</p> <p>CopyExplain</p> <ul> <li> <p>You can configure many settings through the JSON config files in     the\u00a0setups\u00a0folder of the\u00a0simple-fl\u00a0repo of GitHub. For more details,     you can read the general description of the config files in     our\u00a0setups\u00a0documentation     ([https://github.comkeshavaspandat/simple-fl/tree/master/setups]{.underline}).</p> </li> <li> <p>First, you can run two agents. You can increase the number of agents     running on the same device by specifying the appropriate port     numbers.</p> </li> <li> <p>As you already know, the first thing you can do is run the database     and aggregator:</p> </li> </ul> <p># FL server side</p> <p>python -m fl_main.pseudodb.pseudo_db</p> <p>python -m fl_main.aggregator.server_th</p> <p>CopyExplain</p> <ul> <li>Then, start the first and second agents to run the image     classification example:</li> </ul> <p># First agent</p> <p>python -m examples.image_classification.classification</p> <p>_engine 1 50001 a1</p> <p># Second agent</p> <p>python -m examples.image_classification.classification</p> <p>_engine 1 50002 a2</p> <p>CopyExplain</p> <ul> <li> <p>To simulate the actual FL\u00a0scenarios, the amount of training data     accessible from each agent can be limited to a specific number. This     should be specified with the\u00a0num_training_data\u00a0variable     in\u00a0classification_engine.py. By default, it uses 8,000 images (2,000     batches) for each round.</p> </li> <li> <p>Now that we can run the two agents to test the FL process using CNN     models, let us look further into the results by running the image     classification example.</p> </li> </ul> <p>Evaluation of running the image classification with CNN</p> <ul> <li> <p>The performance\u00a0data (the accuracy of each local model cluster     model) is stored in our database. You can access the     corresponding\u00a0.db\u00a0file to see the performance history.</p> </li> <li> <p>The\u00a0DataManager\u00a0instance (defined in\u00a0ic_training.py) has a function     to return one batch of images and their labels (get_random_images).     You can use this function to show the actual labels and the     predicted labels by the trained CNN on specific images.</p> </li> <li> <p>Figure 6.12\u00a0shows a plot of the learning performance from our     experimental runs on our side; the results may look different when     you run it with your own settings:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.204861111111111in\"}</p> <p>Figure 6.12 -- Plot of the learning performance from the experimental runs for FL using CNN for image classification</p> <ul> <li>Again, as we only\u00a0use two agents here, the results just look     slightly different. However, with the proper hyperparameter     settings, data amount, and the number of agents, you will be able to     carry out an FL evaluation that produces meaningful results, which     we would like you to explore on your own, as the focus here is just     how to connect the actual ML models to this FL environment.</li> </ul> <p>Running five agents</p> <ul> <li> <p>You can easily run five\u00a0agents for the image classification     application by just specifying different port numbers and agent     names in the terminal.</p> </li> <li> <p>The results look similar to what we discussed in the previous     section except the real ML models are connected (in this case, the     ML model being aggregated is CNN).</p> </li> <li> <p>After running the five agents, the data and database folders look     like in\u00a0Figure 6.13:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.6479166666666667in\"}</p> <p>Figure 6.13 -- Results to be stored in each folder with the agent's unique name</p> <ul> <li>Figure 6.14\u00a0shows the uploaded local\u00a0models in the database with     information about the local model ID, the time the models were     generated, the ID of the agent that uploaded the local model,     performance metrics, and round information:</li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.133333333333334in\"}</p> <p>Figure 6.14 -- Information about the local models in the database\u00a0</p> <ul> <li> <p>If you look at the database in\u00a0Figure 6.14, there are five models     collected by the five agents with local performance data.</p> </li> <li> <p>For each round, those five local models are aggregated to produce a     cluster global model, as in the\u00a0cluster_models\u00a0table in the     database, as shown in\u00a0Figure 6.15.</p> </li> <li> <p>The database storing cluster models has information about the     cluster model ID, the time the models were\u00a0generated, the ID of the     aggregator that created the cluster model, and round information:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"1.6520833333333333in\"}</p> <p>Figure 6.15 -- Information about the cluster models in the database</p> <ul> <li>In this way, you can connect as many agents as possible. It is up to     you to optimize the settings of the local ML algorithms to obtain     the best-performing federated models out of the FL system.</li> </ul> <p>Summary</p> <ul> <li> <p>Here, we\u00a0discussed the execution of FL systems in detail and how the     system will behave according to the interactions between the     aggregator and agents.</p> </li> <li> <p>The step-by-step explanation of the FL system behavior based on the     outcomes of the console examples guides you to understand the     aggregation process of the\u00a0FedAvg\u00a0algorithm.</p> </li> <li> <p>Furthermore, the image classification example showed how CNN models     are connected to the FL system and how the FL process increases the     accuracy through aggregation, although this was not optimized to     maximize the training results but simplified to validate the     integration using CNN.</p> </li> <li> <p>With what you have learned Here, you will be able to design your own     FL applications integrating the principles and framework introduced     in this book, and furthermore, will be able to assess the FL     behavior on your own to see whether the whole flow of the FL process     and model aggregation is happening correctly and consistently.</p> </li> <li> <p>In the next section, we will cover a variety of model aggregation     methods and show how FL works well with those aggregation     algorithms.</p> </li> </ul> <p>Model Aggregation</p> <ul> <li> <p>In the\u00a0Model aggregation basics\u00a0section of\u00a0section 3,\u00a0Workings     of the Federated Learning System, we introduced the concept of     aggregation within the\u00a0federated learning\u00a0(FL) process at a     high level.</p> </li> <li> <p>Recall that aggregation is the means by which an FL approach uses     the models trained locally by each agent to produce a model with     strong global performance.</p> </li> <li> <p>It is clear to see that the strength and robustness of the     aggregation method employed are directly correlated to the resulting     performance of the end\u00a0global model.</p> </li> <li> <p>As a result, choosing the appropriate aggregation method based on     the local datasets, agents, and FL system hierarchy is key to     achieving good performance with FL. In fact, the focal point of many     publications in this field is providing mathematically backed     convergence guarantees for these methods in a variety of\u00a0theoretical     scenarios.</p> </li> <li> <p>The goal of This section is to cover some of the research that has     been done on aggregation methods and their convergence in both ideal     and non-ideal cases, tying these methods to their strengths in the     different scenarios that arise in the practical applications of FL.</p> </li> <li> <p>After reading the section, you should be able to understand how     different characterizations of an FL scenario call for different     aggregation methods, and you should have an idea of how these     algorithms can actually\u00a0be implemented.</p> </li> <li> <p>Here, we will cover the\u00a0following topics:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Revisiting aggregation</p> <ul> <li> <p>Understanding FedAvg</p> </li> <li> <p>Modifying aggregation for\u00a0non-ideal cases</p> </li> </ul> <p>Technical requirements</p> <ul> <li> <p>The Python algorithm implementations presented in this work can all     be found in the\u00a0ch7\u00a0folder, which is     located\u00a0at\u00a0https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7.</p> </li> <li> <p>For the pure aggregation algorithms, auxiliary code is included to     display example output from preset local parameters. The aggregation     methods that modify the local training process require an FL system     in order to operate -- for these, full implementations using STADLE     are included.</p> </li> <li> <p>Also, the pure aggregation algorithms can be directly tested with     STADLE by configuring the aggregation method. Information on how to     run the examples can be found in the associated\u00a0README\u00a0files.</p> </li> <li> <p>The installation of the\u00a0stadle-client\u00a0package through\u00a0pip\u00a0is     necessary to run the full FL process examples. The following command     can be used to perform\u00a0this installation:</p> </li> </ul> <p>pip install stadle-client</p> <p>CopyExplain</p> <ul> <li>Using a virtual environment is recommended to isolate the specific     package versions installed with\u00a0stadle-client\u00a0from other     installations on\u00a0the system.</li> </ul> <p>Revisiting aggregation</p> <ul> <li>To solidly\u00a0contextualize aggregation within FL, first, we describe     the components of a system that are necessary for FL to\u00a0be applied:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   A set of computational agents that perform the local training     portion\u00a0of FL.</p> <ul> <li> <p>Each agent possesses a local dataset (static or dynamic), of which     no portion can be communicated to another agent under the     strictest\u00a0FL scenario.</p> </li> <li> <p>Each agent possesses a parameterized model that can be trained on     the local dataset, a process that produces the local optima     parameter set for\u00a0the model.</p> </li> <li> <p>A parameter server, or aggregator, which receives the locally     trained models at each iteration from the agents and sends back the     resulting model produced by the aggregation method chosen to\u00a0be     used.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Every FL communication round can then be broken down into the     following\u00a0two phases:</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The\u00a0local training phase, where agents train their local models on     their local datasets for some number\u00a0of iterations</p> <ul> <li>The\u00a0aggregation phase, where the agents send the resulting trained     local models from the previous phase to the aggregator and receive     the aggregated model for use as the starting model in the local     training phase of the\u00a0next round.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   So, what exactly does it mean for an agent to send a locally trained     model during the aggregation phase?</p> <ul> <li> <p>The general approach is to use the\u00a0parameter sets\u00a0that define the     local models, allowing for some degree of generalization across all     models that can be parameterized in such a way. However, a second     approach focuses on sending the\u00a0local gradients\u00a0accumulated during     the local training when using a gradient-based optimization approach     to the aggregator, with the agents updating their models using the     received aggregate gradient at the end of the round.</p> </li> <li> <p>While this approach restricts usage to models with gradient-based     local training methods, the prevalence of such methods when\u00a0training     deep learning models has led to a subset of aggregation methods     based on gradient aggregation.</p> </li> <li> <p>Here, we choose to frame model aggregation through the lens of     the\u00a0FedAvg algorithm.</p> </li> </ul> <p>Understanding FedAvg</p> <ul> <li> <p>Earlier, the aggregation algorithm\u00a0known as FedAvg was introduced to     help clarify the general structure and represent the more abstract     concepts discussed earlier with a specific example.</p> </li> <li> <p>FedAvg was used for two reasons: simplicity in the underlying     algorithm, and generalizability across more model types than     gradient-based approaches. It also benefits from extensive     references by researchers, with performance analysis in different     theoretical scenarios using FedAvg as a baseline when proposing new     aggregation methods.</p> </li> <li> <p>This focus in the research community can most likely be attributed     to the fact that the original FedAvg paper was published by the team     working at Google that first brought exposure to the concept and     benefits of FL. For further reading, this paper can be     found\u00a0at\u00a0https://arxiv.org/abs/1602.05629?context=cs.</p> </li> <li> <p>FedAvg is predated by an aggregation approach known as\u00a0Federated     Stochastic Gradient Descent\u00a0(FedSGD). FedSGD can be viewed as     the gradient aggregation\u00a0analog of the model parameter averaged     performed by FedAvg.</p> </li> <li> <p>In addition, the concept of averaging model parameters was examined     prior to FedAvg for parallelized SGD approaches, outside of the     context of FL. Essentially, the analysis of these parallelized SGD     approaches mirrors the\u00a0Independently and Identically     Distributed\u00a0(IID) case of FedAvg -- this concept will be     discussed\u00a0later in the section.</p> </li> <li> <p>Regardless, the simplicity, generalizability, and popularity of     FedAvg make it a good base to delve deeper into, contextualizing the     need for the numerous aggregation approaches discussed in later     sections that have built upon or, otherwise, improved\u00a0on FedAvg.</p> </li> <li> <p>Previously, FedAvg was only presented as an algorithm that takes     models\u00a0{width=\"1.4427088801399826in\"     height=\"0.23811679790026247in\"}with a respective local dataset size     of\u00a0{width=\"1.3452384076990376in\"     height=\"0.20833333333333334in\"}, where the sum equals\u00a0N\u00a0and     returns:</p> </li> </ul> <p>{width=\"2.026042213473316in\" height=\"0.9540912073490814in\"}</p> <p>As shown in the\u00a0Aggregating local models\u00a0section of\u00a0section 4,\u00a0Federated Learning Server Implementation with Python,\u00a0simple-fl\u00a0uses the\u00a0following function to compute a weighted average of the buffered models (models sent from clients during the current round) based on the amount of data used to locally train\u00a0each model:</p> <p>def _average_aggregate(self,</p> <p>buffer: List[np.array],</p> <p>num_samples: List[int]) -&gt; np.array:</p> <p>\\\"\\\"\\\"</p> <p>Given a list of models, compute the average model (FedAvg).</p> <p>This function provides a primitive mathematical operation.</p> <p>:param buffer: List[np.array] - A list of models to be aggregated</p> <p>:return: np.array - The aggregated models</p> <p>\\\"\\\"\\\"</p> <p>denominator = sum(num_samples)</p> <p># weighted average</p> <p>model = float(num_samples[0]) / denominator * buffer[0]</p> <p>for i in range(1, len(buffer)):</p> <p>model += float(num_samples[i]) / denominator * buffer[i]</p> <p>return model</p> <p>CopyExplain</p> <ul> <li> <p>The original algorithm does not differ too greatly from this     portrayal. The high-level steps of the algorithm are\u00a0as follows:</p> </li> <li> <p>The server randomly samples\u00a0K * C\u00a0clients, where\u00a0K\u00a0is the total     number of clients and\u00a0C\u00a0is a parameter between 0\u00a0and 1.</p> </li> <li> <p>The selected\u00a0K * C\u00a0clients receive the most recent aggregate     model and begin to train the model on their\u00a0local data.</p> </li> <li> <p>Each client\u00a0sends its locally trained model back to the server after     some desired amount of training\u00a0is completed.</p> </li> <li> <p>The server computes the parameter-wise arithmetic mean of the     received models to compute the newest\u00a0aggregate model.</p> </li> <li> <p>Parallels can be immediately drawn between this formal     representation and our presentation of the FL process,     with\u00a0ClientUpdate\u00a0performing local training for an agent and the     server performing aggregation using the same weighted averaging     algorithm.</p> </li> <li> <p>One important point is the sampling of a subset of clients to     perform the local training and model transmission during each round,     allowing for client subsampling parameterized by C.</p> </li> <li> <p>This parameter is included to experimentally determine the     convergence rates of various client set sizes -- in an ideal     scenario, this will be set\u00a0to\u00a01.</p> </li> <li> <p>As previously mentioned, FedAvg is the ideal FL scenario that     essentially mirrors an approach to parallelized stochastic\u00a0gradient     descent. In\u00a0parallelized SGD\u00a0(pSGD), the goal is to leverage     hardware parallelization (for example, running on multiple cores in     parallel) in order to speed up SGD convergence on a specific machine     learning task.</p> </li> <li> <p>One approach for this task is for each core to train a base model on     some subset of the data in parallel for some number of iterations,     then aggregate the partially trained models and use the aggregated     models as the next base for training.</p> </li> <li> <p>In this case, if the cores are considered to be agents in an FL     scenario, the parallelized SGD approach is the same as FedAvg in an     ideal scenario.</p> </li> <li> <p>This means all of the convergence guarantees and respective analyses     that were done for pSGD can be directly applied to FedAvg, assuming     the ideal FL scenario. From this prior work, it has, therefore, been     shown that FedAvg demonstrates strong\u00a0convergence rates.</p> </li> <li> <p>After all this praise for FedAvg, it is only natural to question why     more complex aggregation methods are even necessary.</p> </li> <li> <p>Recall that the phrase \"ideal FL scenario\" was used several times     when discussing FedAvg convergence. The unfortunate reality is that     most practical FL\u00a0applications will fail to meet one or more of the     conditions stipulated by\u00a0that phrase.</p> </li> <li> <p>The ideal FL scenario can be broken down into three\u00a0main conditions:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The local datasets used for training are IID (the datasets are     independently drawn from the same\u00a0data distribution).</p> <ul> <li> <p>The computational agents are relatively homogeneous in\u00a0computational     power.</p> </li> <li> <p>All agents can be assumed to\u00a0be non-adversarial.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   At a high level, it is clear why these qualities would be desirable     in an FL scenario. To understand, in greater detail, why these three     conditions are necessary, the performance of\u00a0FedAvg in the absence     of each condition will be examined in the\u00a0upcoming subsections.</p> <p>Dataset distributions</p> <ul> <li> <p>To examine\u00a0FedAvg in the non-IID case, first, it is important     to\u00a0define what exactly is being referred to by the distribution of a     dataset. In classification problems, the data distribution often     refers to the distribution of the true classes associated with each     data point. For example, consider the MNIST dataset, where each     image is a handwritten digit from 0 to 9.</p> </li> <li> <p>If a uniform random sample of 1,000 images was to be taken from the     dataset, the expected number of images from each class would be the     same -- this could be considered a uniform data distribution.</p> </li> <li> <p>Alternatively, a sample with 910 images of the digit 0 along with 10     images of the other digits would be a heavily skewed\u00a0data     distribution.</p> </li> <li> <p>To generalize outside of classification tasks, this definition can     be extended to refer to the distribution of\u00a0features\u00a0present     across the dataset.</p> </li> <li> <p>These features could be manually crafted and provided to the model     (such as linear regression), or they could be extracted from the raw     data as part of the model pipeline (such as deep CNN models).</p> </li> <li> <p>For classification problems, the class distribution is generally     contained within the feature distribution, due to the implicit     belief that the features are sufficient for correctly predicting the     class.</p> </li> <li> <p>The benefit of looking at feature distributions is the data-centric     focus on features (versus the task-centric focus on classes),     allowing for generalization across machine\u00a0learning tasks.</p> </li> <li> <p>However, in the context of experimental analysis, the ability to     easily construct non-IID samples from a dataset makes classification     tasks ideal for testing the robustness of FedAvg and different     aggregation methods within an FL context.</p> </li> <li> <p>To examine FedAvg in this section, consider\u00a0a toy FL scenario where     each agent trains a CNN on\u00a0data samples taken from the MNIST dataset     described earlier. There are two main cases, which are\u00a0detailed     next.</p> </li> </ul> <p>IID case</p> <ul> <li> <p>The convergence\u00a0of the models can be represented\u00a0through the use of     the model parameter space. The parameter space of a model     with\u00a0n\u00a0parameters can be thought of as an\u00a0n-dimensional     Euclidean space, where each parameter corresponds to one dimension     in the space.</p> </li> <li> <p>Consider an initialized model; the initial parameters of this model     can then be represented as a\u00a0point\u00a0in the parameter space.</p> </li> <li> <p>As local training and aggregation occur, this representative point     will move in the parameter space, with the end goal being     convergence to a point in the space corresponding to a local optimum     of the loss or error function\u00a0being minimized.</p> </li> <li> <p>One key point of these functions is the dependence on the data used     during the local training process -- when the datasets across the     agents are IID, there is a general tendency for the optima of the     respective loss/error functions to be relatively close in the     parameter space.</p> </li> <li> <p>Consider a trivial case where the datasets are IID and all models     are initialized with the same parameters. As shown in the\u00a0Model     aggregation basics\u00a0section of\u00a0section 3,\u00a0</p> </li> <li> <p>Workings of the Federated Learning System, a simplified version of     the parameter space can\u00a0be depicted:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.855555555555555in\"}</p> <p>Figure 7.1 -- Models with the same initialization and IID datasets</p> <ul> <li> <p>Observe how both\u00a0models start at the same point (purple x) and\u00a0move     toward the same optima (purple dot), resulting in aggregate models     close to the optima shared by\u00a0both models.</p> </li> <li> <p>Due to the resulting similarity of the error/loss functions across     the agents, the models tend to converge toward the same or similar     optima in the space during training.</p> </li> <li> <p>This means that the change in the models after each aggregation step     is relatively small, resulting in the convergence rates mirroring     the single local model case.</p> </li> <li> <p>If the underlying data distribution is representative of the true     data distribution (for example, uniform across the 10 different     digits for MNIST), the resulting aggregated model will     demonstrate\u00a0strong performance.</p> </li> <li> <p>Next, consider the generalized IID case where each model     is\u00a0initialized separately:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.747222222222222in\"}</p> <p>Figure 7.2 -- Models with different initializations and IID datasets</p> <ul> <li> <p>In this scenario, observe\u00a0how both models start at different\u00a0points     (bold/dotted\u00a0x) and initially move toward different optima,     producing a poor first model. However, after the first aggregation,     both models start at the same point and move toward the same optima,     resulting in similar convergence to the\u00a0first case.</p> </li> <li> <p>It should be clear that this reduces to the previous case after the     first aggregation step since each model starts the second round with     the resulting aggregated parameters. As a result, the convergence     properties previously stated can be extended to the general case\u00a0of     FedAvg\u00a0with IID\u00a0local datasets.</p> </li> </ul> <p>Non-IID Case</p> <ul> <li> <p>The key property\u00a0in the IID local dataset case that\u00a0allows for     convergence speeds mirroring the single model case is the similarity     of the local optima of the loss/error functions, due to their     construction from similar data distributions. In the non-IID case,     similarity in the optima is generally no\u00a0longer observed.</p> </li> <li> <p>Using the MNIST example, let's consider an FL scenario with two     agents such that the first agent only has images with digits 0 to 4     and the second agent only has images with digits 5 to 9; that is,     the datasets are not IID. These datasets would essentially lead to     two completely different five-class classification tasks at the     local training level, as opposed to the original 10-class     classification problem -- this will result in completely different     parameter space optima between the first agent and the second agent.</p> </li> <li> <p>Consider the simplified representation of this parameter space as     follows, with both models having the\u00a0same initialization:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.8597222222222225in\"}</p> <p>Figure 7.3 -- Models with different initializations and non-IID datasets</p> <ul> <li> <p>Now that optima are no longer shared (triangles/squares representing     optima for bold/dotted\u00a0model, respectively), even     repeated\u00a0aggregations cannot create an aggregate model close to     optima of either model.</p> </li> <li> <p>The models diverge, or drift, during each local training phase due     to the different target optima in\u00a0each round.</p> </li> <li> <p>Only a small subset of the optima will be shared between the     loss/error functions of both agents.</p> </li> <li> <p>As a result, there is a high probability that each model will move     toward optima that are not shared during local training, leading the     models to drift apart in the parameter space. Each aggregation step     will then pull the models toward the wrong optima, reverting the     progress made during local training and hampering convergence.</p> </li> <li> <p>Note that just taking the average of optima from different agents is     very unlikely to be near optima from any of the agents in the     parameter space, so in this case, the result of continued     aggregation is generally a model that performs poorly across the     whole dataset.</p> </li> <li> <p>Convergence to a shared optimum might eventually occur due to     stochasticity observed during local training, inducing movement of     the aggregate model in the parameter space, but this does not have     theoretical guarantees and will be far slower than convergence in     the IID case when it\u00a0does occur.</p> </li> </ul> <p>Important note</p> <ul> <li> <p>This MNIST example is a theoretical extreme of non-IID datasets. In     practice, non-IID datasets might refer to different skews in the     data distributions across agents (for example, twice as many images     with digits 0--4 versus 5--9, and vice versa).</p> </li> <li> <p>The severity of the difference is correlated to the performance of     FedAvg, so adequate performance can still be reached in less severe     cases.</p> </li> <li> <p>However, in these cases, the performance of FedAvg will generally     always be inferior to the analogous centralized training task where     a single model is trained on all of the local datasets at once --     the theoretically optimal model achievable\u00a0by FL.</p> </li> <li> <p>While this section\u00a0focused on the statistical basis for\u00a0the issues     that arise from non-IID datasets, the next section examines a far     more direct problem that can arise -- especially when deploying     at\u00a0larger scales.</p> </li> </ul> <p>Computational power distributions</p> <ul> <li> <p>An unstated\u00a0assumption of the agents\u00a0participating in FL is that     each agent is capable of performing local training if given infinite     time.</p> </li> <li> <p>Agents with limited computational power (memory and speed) might     take significantly more time than other agents to finish local     training, or they might require techniques such as quantization to     support the model and training process.</p> </li> <li> <p>However, agents that cannot complete local training during some     rounds will trivially prevent convergence by stalling the\u00a0FL     process.</p> </li> <li> <p>Generally, convergence bounds and experimental results focus on the     number of communication rounds required to reach some level of     performance.</p> </li> <li> <p>Under this metric and the aforementioned assumption, convergence is     completely independent of the computational power afforded to each     agent, since computational power only affects the actual time     necessary to complete one round.</p> </li> <li> <p>However, convergence speed in practical applications is measured by     the actual time taken, not the number of completed communication     rounds -- this means that the time to complete each round is as     important as the number of rounds.</p> </li> <li> <p>This metric of the total time taken is where na\u00efve FedAvg     demonstrates poor performance when heterogeneous computation power     is observed in the agents participating\u00a0in FL</p> </li> <li> <p>Specifically, the time to complete each round is bottlenecked by the     local training time of the slowest agent participating in the round;     this is because aggregation is trivially fast compared to training     in most cases and must wait for all agents to complete local     training.</p> </li> <li> <p>When all agents are participating in the round, this bottleneck     becomes the slowest overall agent. In the homogeneous computational     power case, the difference in local training time between the     fastest agent and the slowest agent will be relatively     insignificant. In the heterogeneous case, a single straggler will     greatly reduce the convergence time of FedAvg and lead to     significant idle time in the faster agents waiting to receive     the\u00a0aggregated model.</p> </li> <li> <p>Two modifications\u00a0to FedAvg with full agent participation\u00a0might     initially seem to address this problem; however, both have drawbacks     that lead to\u00a0suboptimal performance:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   One approach is to rely on agent subsampling in each round, leading     to the probability of the straggler effect occurring in each round     depending on the number of agents and the sample size taken in each     round. This can be sufficient in cases with only a few straggling     agents, but it becomes proportionately worse as this number     increases and does not completely eliminate the problem from     occurring. In addition, small sample sizes lose out on the     robustness benefits from aggregation over\u00a0more agents.</p> <ul> <li>A second approach is to allow all agents to begin local training at     the beginning of each round and prematurely begin aggregation after     some number of models have been received. This method has the     benefit of being able to completely eliminate the straggler effect     without greatly restricting the number of agents participating in     aggregation during each round. However, it results in the slowest     agents never participating in aggregation over all rounds,     essentially reducing the number of active agents and potentially     limiting the variety of data used during training. In addition, the     agents that are too slow to participate in aggregation will have     done computational work for\u00a0no benefit.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   It is clear that some local adjustment based on available     computational power is necessary for aggregation to be performed     efficiently, regardless of the end aggregation method applied to the     received models at the end of\u00a0each round.</p> <ul> <li>Both the non-IID case and the heterogeneous computation power case     focus on properties of an FL system that are generally easy to     observe and under some level of administrative\u00a0control. The next     case we present deviates\u00a0from this by challenging a key assumption     when considering practical\u00a0FL systems.</li> </ul> <p>Protecting against adversarial agents</p> <ul> <li> <p>So far, it has been\u00a0assumed that every agent\u00a0participating in an FL     scenario always acts in the desired way; that is, actively and     correctly training the received model locally and participating in     model transmission to/from the aggregator.</p> </li> <li> <p>This is easily achieved in a research setting, where the federated     setting is simulated and the agents are singularly controlled;     however, this assumption of agents behaving correctly does not     always hold\u00a0in practice.</p> </li> <li> <p>One example that does not involve targeted malicious intent is an     error in the model weights being transmitted by an agent to the     aggregator.</p> </li> <li> <p>This can happen when the dataset used by an agent is flawed or the     training algorithm is incorrectly implemented (the corruption of the     parameter data during transmission is also possible). In the worst     case, this can essentially lead to the parameters of one or many     models being statistically equivalent to random noise.</p> </li> <li> <p>When the L2 norm (the extension of vector magnitude     for\u00a0n-dimensional tensors) of the random noise is not     significantly greater than that of the valid models,</p> </li> <li> <p>FedAvg will suffer performance loss that is proportional to the     ratio of faulty agents to all agents -- which is relatively     acceptable when this ratio is small. However, even a single faulty     agent can induce a near-random aggregate model if the norm of the     agent's noise is significantly high.</p> </li> <li> <p>This is due to the nature of the arithmetic mean being performed     internally during the\u00a0FedAvg aggregation.</p> </li> <li> <p>The problem becomes worse when agents can be controlled by malicious     adversaries.</p> </li> <li> <p>A single malicious agent with sufficient information is capable of     producing any desired model after aggregation through large     modifications to the parameters of the model it submits.</p> </li> <li> <p>Even without direct knowledge of the model parameters and associated     weights of the other agents, a malicious agent can leverage     relatively small changes between the local models and the aggregate     model in later rounds to use the previous aggregate model as an     estimate of the expected local\u00a0model parameters.</p> </li> <li> <p>Therefore, FedAvg offers little to no robustness against both random     and controlled adversarial agents in an FL setting. While one     potential means of mitigation would be to separately monitor the     agents and prevent adversarial agents from transmitting models,     significant damage to the convergence of the final model might have     already occurred in the time necessary to identify\u00a0such agents.</p> </li> <li> <p>It should now be clear that FedAvg trades robustness in these     non-ideal cases for simplicity in the calculation. Unfortunately,     this robustness is a key consideration for practical\u00a0applications of     FL due to the lack of\u00a0control compared to the research setting.</p> </li> <li> <p>The next section focuses on methods of achieving robustness against     the three non-ideal cases presented in\u00a0this section.</p> </li> </ul> <p>Modifying aggregation for non-ideal cases</p> <ul> <li> <p>In practical FL\u00a0applications, at least one of the aforementioned     assumptions that constitute an ideal FL scenario generally does not     hold; therefore, the usage of alternative aggregation methods might     be necessary to best perform FL.</p> </li> <li> <p>The goal of this section is to cover examples of aggregation methods     that target heterogeneous computational power, adversarial agents,     and non-IID datasets, in order\u00a0of difficulty.</p> </li> </ul> <p>Handling heterogeneous computational power</p> <ul> <li> <p>As mentioned\u00a0earlier, the\u00a0ideal aggregation approach, in this case,     consistently avoids the straggler effect while maximizing the number     of agents participating in FL and allowing all agents to contribute     to some extent, regardless of computational power differences.</p> </li> <li> <p>Agents become stragglers during a round when their local training     takes significantly more time than the majority of the agents.</p> </li> <li> <p>Therefore, effectively addressing this problem actually requires     some level of adaptability at the agent level in the local     training\u00a0process, based\u00a0on the computational power available to\u00a0each     agent.</p> </li> </ul> <p>Manual adjustment</p> <ul> <li> <p>One straightforward\u00a0way of accomplishing this is to change the     number of local training iterations based on the time necessary for     each iteration. In other words, the local training time is fixed and     each agent performs as many iterations as possible within this time,     as opposed to performing a fixed number of iterations.</p> </li> <li> <p>This trivially eliminates the straggler problem but might result in     poor performance if a large amount of local training time must be     allocated for the slow agents to meaningfully contribute due to the     model drift from faster agents potentially performing too many local     training iterations.</p> </li> <li> <p>This can be mitigated by setting a maximum number of local training     iterations. However, a careful balance in the allocated local     training must be found to have enough time for slow agents to     produce adequate models while preventing faster agents from sitting     idle after reaching the maximum number of iterations.</p> </li> <li> <p>It is also unclear how such a threshold could be preemptively     determined to achieve optimal performance instead of relying on     experimental results to search for the\u00a0best configuration.</p> </li> </ul> <p>Automatic adjustment -- FedProx</p> <ul> <li> <p>An aggregation\u00a0method known\u00a0as FedProx follows this same methodology     of dynamically adjusting the local training processes for each agent     based on computational power, while also revising the termination     condition for local training to aid in the theoretical analysis of     convergence.</p> </li> <li> <p>Specifically, the fixed number of local training iterations is     replaced by a termination condition for the training loop that     accommodates agents with varying levels of\u00a0computational power.</p> </li> <li> <p>The underlying concept for this termination condition is the     \u03b3-inexact solution, which is satisfied when the magnitude of the     gradient at the \u03b3-inexact optima is less than \u03b3 times the magnitude     of the gradient at the beginning of local training. Intuitively, \u03b3     is a value between 0 and 1, with values closer to 0 leading to more     local training iterations due to the stricter termination condition.</p> </li> <li> <p>Therefore, \u03b3 allows for the parameterization of an     agent's\u00a0computational power.</p> </li> <li> <p>One potential problem with the termination condition approach is the     divergence of the locally trained model from the aggregate model     after many iterations of local training resulting from a strict     condition.</p> </li> <li> <p>To combat this, FedProx adds a proximal term to the objective     function being minimized equal to\u00a0the following:</p> </li> </ul> <p>{width=\"1.1302088801399826in\" height=\"0.40687554680664917in\"}</p> <p>Here,\u00a0{width=\"0.5311986001749781in\" height=\"0.26833770778652666in\"}\u00a0represents the received aggregate\u00a0model weights.</p> <ul> <li> <p>The proximal term penalizes differences between the current weights     and the aggregated model weights, restricting the aforementioned     local model divergence with the strength parameterized by \u03bc.</p> </li> <li> <p>From these two concepts, FedProx allows for a variable number of     iterations proportional to the computational power to be performed     by each agent without requiring manually tuned iteration counts for     each agent or a set amount of allocated training time.</p> </li> <li> <p>Because of the addition of the proximal term, FedProx requires     gradient-based optimization methods to be employed in order to work     -- more information\u00a0on the underlying\u00a0theory and comparison to     FedAvg can be found in the original paper (which     is\u00a0at\u00a0https://arxiv.org/abs/1812.06127).</p> </li> </ul> <p>Implementing FedProx</p> <ul> <li> <p>Because the\u00a0modifications\u00a0made by FedProx to FedAvg are all on the     client side, the actual implementation of FedProx consists entirely     of modifications to the local training framework.</p> </li> <li> <p>Specifically, FedProx involves a new termination condition for local     training and the addition of a constraining term to the local loss     function. Therefore, it is helpful to use an example of the local     training code to frame exactly how FedProx can\u00a0be integrated.</p> </li> <li> <p>Let's consider the following generic training code\u00a0using PyTorch:</p> </li> </ul> <p>agg_model = ... # Get aggregate model -- abstracted out of example</p> <p>model.load_state_dict(agg_model.state_dict())</p> <p>for epoch in range(num_epochs):</p> <p>for batch_idx, (inputs, targets) in enumerate(trainloader):</p> <p>inputs, targets = inputs.to(device), targets.to(device)</p> <p>optimizer.zero_grad()</p> <p>outputs = model(inputs)</p> <p>loss = criterion(outputs, targets)</p> <p>loss.backward()</p> <p>optimizer.step()</p> <p>CopyExplain</p> <ul> <li> <p>Let this be the code that performs\u00a0num_epochs\u00a0epochs of training on     the local dataset using the received aggregate model for each round.</p> </li> <li> <p>The first necessary modification for FedProx is to\u00a0replace the     fixed\u00a0number of epochs with a dynamic termination condition,     checking whether a \u03b3-inexact solution has been found with the     aggregated model as the initial model. To do this, the total     gradient over the entire training dataset for the aggregate model     and the current local model must be stored -- this can be     performed\u00a0as follows:</p> </li> </ul> <p>agg_model = ... # Get aggregated model from aggregator</p> <p>model.load_state_dict(agg_model.state_dict())</p> <p>agg_grad = None</p> <p>curr_grad = None</p> <p>gamma = 0.9</p> <p>mu = 0.001</p> <p>CopyExplain</p> <ul> <li> <p>Values for the two FedProx parameters,\u00a0gamma\u00a0and\u00a0mu, are set, and     variables to store the gradients of both the aggregate model and the     latest local model\u00a0are defined.</p> </li> <li> <p>We then define the \u03b3-inexact new termination condition for local     training using these\u00a0gradient variables:</p> </li> </ul> <p>def gamma_inexact_solution_found(curr_grad, agg_grad, gamma):</p> <p>if (curr_grad is None):</p> <p>return False</p> <p>return curr_grad.norm(p=2) \\&lt; gamma * agg_grad.norm(p=2)</p> <p>CopyExplain</p> <ul> <li>This condition is now checked before each training loop iteration to     determine when to stop local training. The\u00a0total_grad\u00a0variable is     created to store the cumulative gradients that\u00a0were created\u00a0from     each minibatch\u00a0during backpropagation:</li> </ul> <p>model.train()</p> <p>while (not gamma_inexact_solution_found(curr_grad, agg_grad, gamma)):</p> <p>total_grad = torch.cat([torch.zeros_like(param.data.flatten()) for param in model.parameters()])</p> <p>for batch_idx, (inputs, targets) in enumerate(trainloader):</p> <p>inputs, targets = inputs.to(device), targets.to(device)</p> <p>optimizer.zero_grad()</p> <p>outputs = model(inputs)</p> <p>loss = criterion(outputs, targets)</p> <p>CopyExplain</p> <ul> <li>To compute the proximal term, the weights of both the aggregate     model and the latest local model are computed. From these weights,     the proximal term is computed and added to the\u00a0loss term:</li> </ul> <p>curr_weights = torch.cat([param.data.flatten() for param in model.parameters()])</p> <p>agg_weights = torch.cat([param.data.flatten() for param in agg_model.parameters()])</p> <p>prox_term = mu * torch.norm(curr_weights - agg_weights, p=2)**2</p> <p>loss += prox_term</p> <p>CopyExplain</p> <ul> <li>The gradients are computed and added to the cumulative sum     stored\u00a0in\u00a0total_grad:</li> </ul> <p>loss.backward()</p> <p>grad = torch.cat([param.grad.flatten() for param in model.parameters()])</p> <p>total_grad += grad</p> <p>optimizer.step()</p> <p>CopyExplain</p> <ul> <li>Finally, we\u00a0update\u00a0agg_grad\u00a0(if the\u00a0gradients were computed with the     aggregate weights) and\u00a0curr_grad\u00a0after the current local training     iteration\u00a0is completed:</li> </ul> <p>if (agg_grad == None):</p> <p>agg_grad = total_grad</p> <p>curr_grad = total_grad</p> <p>CopyExplain</p> <ul> <li> <p>These modifications allow for FedProx to be implemented on top of     FedAvg. The full FL example using FedProx can be     found\u00a0at\u00a0https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7/agg_fl_examples/cifar_fedprox_example.</p> </li> <li> <p>An auxiliary approach to handle the heterogeneous computational     power scenario that helps with computational efficiency when only     mild heterogeneity is observed is the idea of\u00a0compensation in     aggregation.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Consider the case where aggregation occurs once the number of     received models surpasses some threshold (generally, this is less     than the number of participating agents). Using this threshold     allows the straggler effect to be mitigated; however, the work done     by slower agents ends up being discarded each round, leading     to\u00a0training inefficiency.</p> <ul> <li> <p>The core idea of compensation is to allow for the local training     that is done by a slower agent in one round to instead be included     in the model aggregation of a subsequent round. The age of the model     is compensated for in this subsequent round by multiplying the     weight used for the weighted average and a penalizing term during     aggregation. By doing so, slower agents can be given two or three     times as much training time as that used by the faster agents while     avoiding the straggler effect.</p> </li> <li> <p>Mild heterogeneity is required in order to prevent cases where     slower agents require too much extra time for training. This is due     to the associated penalty given to the model after many rounds have     passed; it will be severe enough to effectively lead to no     contribution and reduce aggregation without compensation -- this is     necessary to prevent models that are too old from hampering the     convergence of the\u00a0aggregate model.</p> </li> <li> <p>Finally, we examine methods that help to address the third non-ideal     property, where some subset of\u00a0agents are\u00a0controlled by an adversary     or are, otherwise, behaving in an\u00a0undesirable way.</p> </li> </ul> <p>Adversarial agents</p> <ul> <li> <p>In the previous\u00a0section, it was shown that the\u00a0core problem with     FedAvg in the presence of adversarial agents was the lack of     robustness to outliers in the underlying arithmetic mean used during     aggregation.</p> </li> <li> <p>This naturally raises the question of whether this mean can be     estimated in such a manner that does offer such robustness. The     answer is the class of robust mean estimators.</p> </li> <li> <p>There are many such estimators that offer varying trade-offs between     robustness, distance from the true arithmetic mean,     and\u00a0computational efficiency.</p> </li> <li> <p>For use as a base for the implementation of the following     aggregation methods, consider the following general\u00a0aggregation     function:</p> </li> </ul> <p>def aggregate(parameter_vectors):</p> <p># Perform some form of aggregation</p> <p>return aggregated_parameter_vector</p> <p>CopyExplain</p> <ul> <li> <p>This function takes a list of parameter vectors and returns the     resulting aggregated\u00a0parameter vector.</p> </li> <li> <p>Now we will examine three example implementations of robust\u00a0mean     estimators.</p> </li> </ul> <p>Aggregation using the geometric median</p> <ul> <li> <p>The geometric\u00a0median of a sample\u00a0is the point minimizing the sum of     L1 distances\u00a0between itself and the sample. This is conceptually     similar to the arithmetic mean, which is the point minimizing the     sum of L2 distances between itself and the sample.</p> </li> <li> <p>The use of L1 distances allows for greater robustness to outliers;     in fact, an arbitrary point can only be induced in the geometric     median if at least half of the points are from adversarial agents.</p> </li> <li> <p>However, the geometric median cannot be directly computed, instead     relying on numerical approximations or iterative algorithms\u00a0to     compute.</p> </li> <li> <p>To compute the geometric mean iteratively, Weiszfeld's algorithm can     be used\u00a0as follows:</p> </li> </ul> <p>def geometric_median_aggregate(parameter_vectors, epsilon):</p> <p>vector_shape = parameter_vectors[0].shape</p> <p>vector_buffer = list(v.flatten() for v in parameter_vectors)</p> <p>prev_median = np.zeros(vector_buffer[0].shape)</p> <p>delta = np.inf</p> <p>vector_matrix = np.vstack(vector_buffer)</p> <p>while (delta &gt; epsilon):</p> <p>dists = np.sqrt(np.sum((vector_matrix - prev_median[np.newaxis, :])**2, axis=1))</p> <p>curr_median = np.sum(vector_matrix / dists[:, np.newaxis], axis=0) / np.sum(1 / dists)</p> <p>delta = np.linalg.norm(curr_median - prev_median)</p> <p>prev_median = curr_median</p> <p>return prev_median.reshape(vector_shape)</p> <p>CopyExplain</p> <ul> <li>This algorithm uses the fact that the geometric median of a set of     points is the point that minimizes the sum of Euclidean distances     over the set, performing a form of weighted least\u00a0squares     with\u00a0weights\u00a0inversely proportional to the Euclidean distance     between the point and the current median estimate at\u00a0each iteration.</li> </ul> <p>Aggregation using the coordinate-wise median</p> <ul> <li> <p>The\u00a0coordinate-wise median is\u00a0constructed by taking the\u00a0median of     each coordinate across the sample, as the name suggests.</p> </li> <li> <p>This median can be directly computed, unlike the geometric median,     and intuitively offers similar robustness to outliers due to the     properties of the median in univariate statistics.</p> </li> <li> <p>However, it is unclear whether the resulting model displays any     theoretical similarities to the arithmetic mean in regard to     performance on the dataset\u00a0and convergence.</p> </li> <li> <p>NumPy makes the implementation of this function quite simple,\u00a0as     follows:</p> </li> </ul> <p>def coordinate_median_aggregate(parameter_vectors):</p> <p>**\u00a0\u00a0\u00a0\u00a0return np.median(parameter_vectors, axis=0)**</p> <p>CopyExplain</p> <ul> <li>It is clear that the coordinate-wise median is far more     computationally efficient to compute than the geometric median,     trading off theoretical guarantees\u00a0for speed.</li> </ul> <p>Aggregation using the Krum algorithm</p> <ul> <li> <p>An alternative\u00a0approach is to isolate\u00a0outlier\u00a0points from     adversarial agents prior to aggregation. The most well-known example     of this approach is\u00a0the\u00a0Krum algorithm, where distance-based     scoring is performed prior to aggregation as a means of     locating\u00a0outlier points.</p> </li> <li> <p>Specifically, the Krum algorithm first computes the pairwise L2     distance between each point -- these distances are then used to     compute a score for each point equal to the sum of     the\u00a0n-f-2\u00a0smallest L2 distances (f\u00a0is a parameter that is set).</p> </li> <li> <p>Then, Krum outputs the received point with the lowest score,     effectively returning the point with a minimal total L2 distance     with\u00a0f\u00a0outlier points that are ignored. Alternatively, the scoring     approach used by Krum can be used to trim outlier points prior to     the computation of the arithmetic mean.</p> </li> <li> <p>In both cases, for sufficiently large\u00a0n\u00a0and\u00a02f+2 \\&lt; n,     convergence rates similar to those of FedAvg in the non-adversarial     case are achieved. More information on the Krum algorithm can be     found in the original paper, which is     located\u00a0at\u00a0https://papers.nips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html.</p> </li> <li> <p>The Krum\u00a0algorithm can be used to\u00a0perform\u00a0aggregation\u00a0as follows:</p> </li> </ul> <p>def krum_aggregate(parameter_vectors, f, use_mean=False):</p> <p>num_vectors = len(parameter_vectors)</p> <p>filtered_size = max(1, num_vectors-f-2)</p> <p>scores = np.zeros(num_vectors)</p> <p>for i in range(num_vectors):</p> <p>distances = np.zeros(num_vectors)</p> <p>for j in range(num_vectors):</p> <p>distances[j] = np.linalg.norm(parameter_vectors[i] - parameter_vectors[j])</p> <p>scores[i] = np.sum(np.sort(distances)[:filtered_size])</p> <p>if (use_mean):</p> <p>idx = np.argsort(scores)[:filtered_size]</p> <p>return np.mean(np.stack(parameter_vectors)[idx], axis=0)</p> <p>else:</p> <p>idx = np.argmin(scores)</p> <p>return parameter_vectors[idx]</p> <p>CopyExplain</p> <ul> <li>Note that a flag has been included to determine which of the two     Krum aggregation approaches (single selection versus trimmed mean)     should be used. Vectorizing the distance computation is possible,     but the iterative approach was preferred due to the expectation\u00a0of     large parameter\u00a0vectors\u00a0and smaller\u00a0agent counts.</li> </ul> <p>Non-IID datasets</p> <ul> <li> <p>The\u00a0theoretical\u00a0underpinning granted to FL by working with IID     datasets plays a significant role in allowing performant aggregate     models to be achieved through FL. At a high level, this can be     explained by the discrepancy between the learning done by models in     different datasets.</p> </li> <li> <p>No theoretical guarantees can be made for the convergence of such     models when dataset-agnostic aggregation methods are applied --     unless constraints on the non-IID nature of the datasets are     applied.</p> </li> <li> <p>The key hindering factor is the high probability of local models     moving toward non-shared optima in the parameter space, leading to     consistent drift between the local models and the aggregate model     after each local\u00a0training phase.</p> </li> <li> <p>There are methods that attempt to restrict the modifications made to     the aggregate model based on the local machine learning task,     relying on the overparameterization of deep learning models to find     relatively disjointed parameter subsets to optimize the aggregate     model of each task.</p> </li> <li> <p>One such aggregation\u00a0approach is\u00a0FedCurv, which uses the Fisher     information matrix of the previous aggregate model to act as a     regulator for auxiliary parameter modifications during local     training.</p> </li> <li> <p>However, the robustness of this approach for extreme non-IID cases     in practical applications likely needs to be tested further     to\u00a0ensure\u00a0acceptable performance.</p> </li> </ul> <p>Implementing FedCurv</p> <ul> <li> <p>The implementation\u00a0of FedCurv involves two key\u00a0modifications to the     standard FedAvg approach.</p> </li> <li> <p>First, the local loss function must be modified to include the     regularization term incorporating the aggregated Fisher information     from the previous round.</p> </li> <li> <p>Second, the Fisher information matrix of the parameters must be     calculated and aggregated correctly for use in the\u00a0next round.</p> </li> <li> <p>The local training example code, as shown in the\u00a0Implementing     FedProx\u00a0section, will be used again to demonstrate an     implementation of FedCurv.</p> </li> <li> <p>Earlier, we saw that a model conversion layer allows for     framework-agnostic model representations to be operated on by the     aggregator.</p> </li> <li> <p>Previously, these representations only contained the respective     parameters from the original models; however, this agnostic     representation actually allows for any desired parameter to be     aggregated, even those only loosely tied to the true model     parameters.</p> </li> <li> <p>This means that the secondary parameters can be bundled and sent     with the local model, aggregated, and then separated from the     aggregate model in the\u00a0next round.</p> </li> <li> <p>In FedCurv, there are two sets of parameters that must be computed     locally and aggregated for use in the next round; therefore, it can     be assumed that these parameters are sent with the local model after     training and separated from the aggregate model before training, for     the sake of brevity in the example code (the implementation of this     functionality is straightforward).</p> </li> <li> <p>As a result, the two key modifications for FedCurv, as mentioned     earlier, can be simplified down into computing the Fisher     information parameters after locally training the model and     computing the regularization term with the received aggregate     Fisher\u00a0information parameters.</p> </li> <li> <p>The Fisher information matrix refers to the covariance of the     gradient of the log-likelihood function of a model with respect to     its parameters, often empirically evaluated over the data present.</p> </li> <li> <p>FedCurv only utilizes the diagonal entries of this matrix, the     variances between the gradient parameters, and their expected     values\u00a0of zero.</p> </li> <li> <p>At a high level, this variance term can be considered an estimate of     how influential the parameter is in changing the performance of the     model on the data.</p> </li> <li> <p>This information is essential for preventing the modification of     parameters key to good performance on one dataset during the local     training of other agents -- the underlying idea\u00a0behind FedCurv.</p> </li> <li> <p>Relaxing the measure of model performance from the gradient of the     log-likelihood to the gradient of any objective function allows for     the direct use of the gradient terms computed during backpropagation     when computing the variance terms for models using gradient-based     optimization methods, such as deep learning models.</p> </li> <li> <p>Specifically, the variance term of a parameter is equal to the     square of its respective gradient term, allowing for the terms to be     directly computed from the net gradients calculated during\u00a0local     training.</p> </li> <li> <p>First, we create two variables to store the agent's most recent     Fisher information parameters and the received aggregate Fisher     information parameters, which are used to determine the Fisher     information from the other agents. The value of the lambda parameter     of FedCurv is fixed, and\u00a0total_grad\u00a0is initialized as a container     for the cumulative\u00a0gradient\u00a0from each\u00a0training loop:</p> </li> </ul> <p>agg_model = ... # Get aggregated model from aggregator</p> <p>model.load_state_dict(agg_model.state_dict())</p> <p>fisher_info_params = ... # Initialize at start, then maintain to store past round parameters</p> <p>agg_fisher_info_params = ... # Separate aggregate Fisher information parameters from aggregate model parameters</p> <p># Only consider other agents, and convert to PyTorch tensor</p> <p>agg_fisher_info_params = {k:torch.tensor(agg_fisher_info_params[k] - fisher_info_params[k]) for k in fisher_info_params.keys()}</p> <p># Scaling parameter for FedCurv regularization term</p> <p>fedcurv_lambda = 1.0</p> <p>total_grad = {i:torch.zeros_like(param.data) for i,param in enumerate(model.parameters())}</p> <p>CopyExplain</p> <ul> <li>Then, we compute the FedCurv regularization term from the model     weights and the aggregate Fisher information parameters. This term     is weighted by lambda and added to the loss term before     computing\u00a0the gradients:</li> </ul> <p>model.train()</p> <p>for epoch in range(num_epochs):</p> <p>for batch_idx, (inputs, targets) in enumerate(trainloader):</p> <p>inputs, targets = inputs.to(device), targets.to(device)</p> <p>optimizer.zero_grad()</p> <p>outputs = model(inputs)</p> <p>loss = criterion(outputs, targets)</p> <p>for i,param in enumerate(model.parameters()):</p> <p># Factor out regularization term to use saved fisher info parameters</p> <p>reg_term = (param.data ** 2) * agg_fisher_info_params[f\\'fedcurv_u_{i}\\']</p> <p>reg_term += 2 * param.data * agg_fisher_info_params[f\\'fedcurv_v_{i}\\']</p> <p>reg_term += (agg_fisher_info_params[f\\'fedcurv_v_{i}\\'] ** 2) / agg_fisher_info_params[f\\'fedcurv_u_{i}\\']</p> <p>loss += fedcurv_lambda * reg_term.sum()</p> <p>CopyExplain</p> <ul> <li>The\u00a0gradients are\u00a0then computed and stored in\u00a0total_grad\u00a0before     updating the\u00a0model weights:</li> </ul> <p>loss.backward()</p> <p>for i,param in enumerate(model.parameters()):</p> <p>total_grad[i] += param.grad</p> <p>optimizer.step()</p> <p>CopyExplain</p> <ul> <li>Finally, we compute and store the agent's most recent Fisher     information parameters for use in the\u00a0next round:</li> </ul> <p>for i,param in enumerate(model.parameters()):</p> <p>fisher_info_params[f\\'fedcurv_u_{i}\\'] = (total_grad[i] ** 2).numpy()</p> <p>fisher_info_params[f\\'fedcurv_v_{i}\\'] = ((total_grad[i] ** 2) * param.data).numpy()</p> <p>CopyExplain</p> <ul> <li>Therefore, framework-agnostic\u00a0aggregation can\u00a0be used to implement     FedCurv on top of FedAvg. The full FL example using FedCurv can be     found\u00a0at\u00a0https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7/agg_fl_examples/cifar_fedcurv_example.</li> </ul> <p>Data-sharing approach</p> <ul> <li> <p>To make further\u00a0progress, changes to external aspects of\u00a0the FL     scenario are necessary. For example, let's assume that the data     privacy restriction is loosened, such that small subsets of the     local datasets from each agent can be shared with the other agents.</p> </li> <li> <p>This data-sharing approach allows for homogeneity in the local data     distributions proportional to the amount of shared data to be     achieved, at the expense of the key stationary data property of FL     that makes it desirable in many privacy-oriented applications.</p> </li> <li> <p>Thus, data-sharing approaches are generally unsuitable for the     majority\u00a0of applications.</p> </li> </ul> <p>Personalization through fine-tuning</p> <ul> <li> <p>It is clear that\u00a0producing a single model that demonstrates strong     performance across the local datasets is not easy when the datasets     are IID. However, what would happen if the single model restriction     was removed from the FL process?</p> </li> <li> <p>If the goal is to produce local models that perform well on the same     edge devices where training is conducted, removing the single model     restriction allows for the use of different local models that have     been trained on the exact data distributions where inference     is\u00a0being applied.</p> </li> <li> <p>This concept is\u00a0called\u00a0personalization, in which agents use     versions of the aggregate model tuned for the local data     distribution to achieve strong performance. The key point of this     approach is to balance the local performance of the locally trained     model with the global performance and the resulting robustness of     the aggregate model received in each round.</p> </li> <li> <p>One method of accomplishing this is for each agent to maintain their     local models across the rounds, updating the local model with the     weighted average of the previous local model and the received     aggregate model during\u00a0each round.</p> </li> <li> <p>Alternatively, consider a relaxation that allows for multiple     aggregate models to be produced in each round. In cases where the     local data distributions can be clustered into just a few separated     groups, distribution-aware aggregation would allow for the selective     application of aggregation methods to groups of models belonging to     the same\u00a0distribution cluster.</p> </li> <li> <p>One example of this approach is the\u00a0Performance-Based Neighbor     Selection\u00a0(PENS) algorithm, where\u00a0agents receive locally     trained models from other agents and test them on their\u00a0own local     dataset during the first phase. Using the assumption that models     trained on similar datasets will perform better than models trained     on different datasets, the agents then determine the set of other     agents with similar data distributions, allowing for aggregation to     only be performed with similar agents in the\u00a0second phase.</p> </li> <li> <p>A second approach\u00a0is to\u00a0add an intermediate aggregation\u00a0step     between the local models and the global aggregate model called a     cluster model. By leveraging knowledge about the agent data     distributions or through a dynamic allocation method, agents with     similar data distributions can be assigned to a cluster aggregator,     which is then known to produce a strong model due to its agents     having\u00a0IID datasets.</p> </li> <li> <p>Balancing the performance of the cluster models with the robustness     of global aggregation leads to the concept of the semi-global model,     in which subsamples of the cluster models can be selected     (potentially based on data distribution) to create a smaller set of     partially global aggregate models that maintain performance and     robustness.</p> </li> <li> <p>Therefore, the cluster and semi-global model approach is beneficial     for both aggregation and achieving a fully distributed\u00a0FL system.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>The goal of This section was to provide a conceptual overview of the     current knowledge of aggregation, the key theoretical step in FL     that allows for the disjoint training done by each agent to be     pooled together with minimal transmission required.</p> </li> <li> <p>FedAvg is a simple, yet surprisingly powerful aggregation algorithm     that performs well in an ideal FL scenario. This scenario is     achieved when training is done across IID datasets using machines     with similar levels of computational power and no adversarial or     otherwise incorrectly\u00a0performing agents.</p> </li> <li> <p>Unfortunately, these conditions are often not met when deploying an     FL system in the real world.</p> </li> <li> <p>To address these cases, we introduced and implemented modified     aggregation approaches: FedProx, FedCurv, and three different robust     mean estimators.</p> </li> <li> <p>After reading This section, you should have a solid understanding of     the considerations that must be taken into account for practical FL     applications, and you should be able to integrate the aforementioned     algorithms into\u00a0these applications.</p> </li> <li> <p>In the next section, we will do a deep dive into some of the     existing FL frameworks with several toy examples to demonstrate the     functionalities provided\u00a0by each.</p> </li> </ul> <p>Introducing Existing Federated Learning Frameworks</p> <ul> <li> <p>The objective of This section is to introduce\u00a0existing\u00a0federated     learning\u00a0(FL) frameworks and platforms, applying each to     federated learning scenarios\u00a0involving toy\u00a0machine     learning\u00a0(ML) problems.</p> </li> <li> <p>The platforms focused on Here are Flower, TensorFlow Federated,     OpenFL, IBM FL, and STADLE -- the idea behind this selection was to     help you by covering a breadth of existing FL platforms.</p> </li> <li> <p>By the end of This section, you should have a basic understanding of     how to use each platform for FL, and you should be able to choose a     platform based on its associated strengths and weaknesses for an\u00a0FL     application.</p> </li> <li> <p>Here, we will cover the\u00a0following topics:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Introduction to existing\u00a0FL frameworks</p> <ul> <li> <p>Implementations of an example NLP FL task on movie review dataset,     using\u00a0existing frameworks</p> </li> <li> <p>Implementations of example computer vision FL task with non-IID     datasets, using\u00a0existing frameworks</p> </li> </ul> <p>Technical requirements</p> <ul> <li> <p>You can find the supplemental code files for This section in the     book's\u00a0GitHub repository:</p> </li> <li> <p>https://github.com/PacktPublishing/Federated-Learning-with-Python</p> </li> <li> <p>Each implementation example Here was run on an x64 machine     running\u00a0Ubuntu 20.04.</p> </li> <li> <p>The implementation of the training code for the NLP example requires     the following libraries\u00a0to run:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Python 3 (version \u2265 3.8)</p> <ul> <li> <p>NumPy</p> </li> <li> <p>TensorFlow (version \u2265\u00a02.9.1)</p> </li> <li> <p>TensorFlow Hub (pip\u00a0install tensorflow-hub)</p> </li> <li> <p>TensorFlow Datasets (pip\u00a0install tensorflow-datasets)</p> </li> <li> <p>TensorFlow Text (pip\u00a0install tensorflow-text)</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Using a GPU with the appropriate TensorFlow installation is     recommended to save training time for the NLP example, due to the     size of\u00a0the model.</p> <ul> <li>The implementation of the training code for     the\u00a0non-IID\u00a0(non-independent and identical distribution)     computer vision example requires the following libraries\u00a0to run:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Python 3 (version \u2265 3.8)</p> <ul> <li> <p>NumPy</p> </li> <li> <p>PyTorch (version \u2265\u00a01.9)</p> </li> <li> <p>Torchvision (version \u2265 0.10.0, tied to\u00a0PyTorch version)</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The installation instructions for each FL framework are listed in     the\u00a0following subsections.</p> <p>TensorFlow Federated</p> <ul> <li>You can\u00a0install the following libraries to\u00a0use TFF:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   tensorflow_federated\u00a0(using the\u00a0pip     install\u00a0tensorflow_federated\u00a0command)</p> <ul> <li>nest_asyncio\u00a0(using the\u00a0pip install\u00a0nest_asyncio\u00a0command)</li> </ul> <p>OpenFL</p> <ul> <li> <p>You can install\u00a0OpenFL using\u00a0pip\u00a0install openfl.</p> </li> <li> <p>Alternatively, you can build from source with the\u00a0following     commands:</p> </li> </ul> <p>git clone https://github.com/intel/openfl.git</p> <p>cd openfl</p> <p>pip install .</p> <p>CopyExplain</p> <p>IBM FL</p> <ul> <li>Installing the\u00a0locally hosted version of IBM FL requires the wheel     installation file located in the code repository. To perform this     installation, run the\u00a0following commands:</li> </ul> <p>git clone https://github.com/IBM/federated-learning-lib.git</p> <p>cd federated-learning-lib</p> <p>pip install federated_learning_lib-*-py3-none-any.whl</p> <p>CopyExplain</p> <p>Flower</p> <p>= You can install\u00a0Flower using the\u00a0pip install\u00a0flwr\u00a0command.</p> <p>STADLE</p> <ul> <li>You can install\u00a0the STADLE client-side library using the\u00a0pip     install\u00a0stadle-client\u00a0command.</li> </ul> <p>Introduction to FL frameworks</p> <ul> <li>First, we\u00a0introduce the FL frameworks and platforms to be used in     the subsequent implementation-focused sections.</li> </ul> <p>Flower</p> <ul> <li> <p>Flower ([https://flower.dev/]{.underline})     is\u00a0an open source and ML framework-agnostic\u00a0FL framework\u00a0that aims     to be accessible to users. Flower follows a standard client-server     architecture, in which the clients are set up to receive the model     parameters from the server, train on local data, and send the new     local model parameters back to\u00a0the server.</p> </li> <li> <p>The high-level orchestration of the federated learning process is     dictated by what Flower calls strategies, used by the server for     aspects such as client selection and\u00a0parameter aggregation.</p> </li> <li> <p>Flower\u00a0uses\u00a0Remote Procedure Calls\u00a0(RPCs) in order to     perform said orchestration through client-side execution from     messages sent by the server. The extensibility of the framework     allows researchers to experiment with novel approaches such as new     aggregation algorithms and communication methods (such as\u00a0model     compression).</p> </li> </ul> <p>TensorFlow Federated (TFF)</p> <ul> <li> <p>TFF     ([https://www.tensorflow.org/federated]{.underline})     is\u00a0an open source FL/computation\u00a0framework\u00a0built on top of     TensorFlow that aims to allow researchers to easily simulate     federated learning with existing TensorFlow/Keras models and     training pipelines.</p> </li> <li> <p>It consists of the Federated Core layer, which allows for the     implementation of general federated computations, and the Federated     Learning layer, which is built on top and provides interfaces     for\u00a0FL-specific processes.</p> </li> <li> <p>TFF focuses on single-machine local simulations of FL, using     wrappers to create TFF-specific datasets, models, and federated     computations (core client and server computation performed during     the FL process) from the standard TensorFlow equivalents.</p> </li> <li> <p>The focus on building everything from general federated computations     allows researchers to implement each step as desired, allowing     experimentation to\u00a0be supported.</p> </li> </ul> <p>OpenFL</p> <ul> <li> <p>OpenFL     ([https://github.com/intel/openfl]{.underline})     is\u00a0an open source FL\u00a0framework\u00a0developed by Intel, focused on     allowing cross-silo privacy-preserving ML to be performed.</p> </li> <li> <p>OpenFL allows for two different workflows depending on the desired     lifespan of the federation (where federation refers to the entire\u00a0FL     system).</p> </li> <li> <p>In the aggregator-based workflow, a single experiment and associated     federated learning plan are sent from the aggregator to the     participating\u00a0collaborators\u00a0(agents) to be run as the local     training step of the FL process---the federation is stopped after     the experiment is complete. In the director-based workflow,     long-lived components are instead used to allow for experiments to     be run on demand.</p> </li> <li> <p>The following diagram depicts the\u00a0architecture and users for the     director-based workflow:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.473611111111111in\"}</p> <p>Figure 8.1 -- Architecture of director-based workflow (adapted from https://openfl.readthedocs.io/en/latest/source/openfl/components.html)</p> <ul> <li> <p>Director Manager\u00a0oversees the running of experiments, working     with long-lived\u00a0Envoy\u00a0components residing on the collaborator     nodes to manage the short-lived components (collaborators +     aggregator) for each experiment.</p> </li> <li> <p>In targeting the cross-silo data scenario, OpenFL applies a unique     focus on managing data shards, including cases where data     representations differ\u00a0across silos.</p> </li> </ul> <p>IBM FL</p> <ul> <li> <p>IBM FL is a\u00a0framework that also focuses on enterprise FL. It follows     a\u00a0straightforward aggregator-party design, where some number of     parties with local data collaborate with other parties by sending     incremental model training results to the aggregator and working     with the produced aggregate models (following standard client-server     FL architecture).</p> </li> <li> <p>IBM FL has official support for a number of fusion (aggregation)     algorithms and certain fairness techniques aimed at combating     bias---the details of these algorithms can be found at the     repository located at https://github.com/IBM/federated-learning-lib.</p> </li> <li> <p>One specific goal of IBM FL is to be highly extensible,     allowing\u00a0users to easily make necessary modifications if specific     features are desired. It also supports a Jupyter-Notebook-based     dashboard to aid in orchestrating\u00a0FL experiments.</p> </li> </ul> <p>STADLE</p> <ul> <li> <p>Unlike the\u00a0previous\u00a0frameworks, STADLE     ([https://stadle.ai/]{.underline}) is     an\u00a0ML-framework-agnostic FL and distributed learning SaaS platform     that aims to allow for the seamless integration of FL into     production-ready applications and ML pipelines.</p> </li> <li> <p>The goal of STADLE is to minimize the amount of FL-specific code     necessary for integration, making FL accessible to newcomers while     still providing flexibility to those looking to experiment.</p> </li> <li> <p>With the STADLE SaaS platform, users of varying technical abilities     can collaborate on FL projects at all scales.</p> </li> <li> <p>Performance tracking and model management functionalities allow     users to produce validated federated models with strong performance,     while an intuitive configuration panel allows for detailed control     over the federated learning process.</p> </li> <li> <p>STADLE uses a two-level component hierarchy that allows for multiple     aggregators to operate in parallel, scaling to match demand. The     following figure depicts the\u00a0high-level architecture:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.026388888888889in\"}</p> <p>Figure 8.2 -- STADLE multi-aggregator architecture</p> <ul> <li>Development of STADLE clients is streamlined with\u00a0pip\u00a0installation     and an easy-to-understand\u00a0configuration file, with several examples     made publicly\u00a0available for use as a reference on the different ways     STADLE can be integrated into existing\u00a0ML code.</li> </ul> <p>PySyft</p> <ul> <li> <p>While\u00a0PySyft     ([https://github.com/OpenMined/PySyft]{.underline})     implementations\u00a0are not included in this\u00a0section due to ongoing     changes in the codebase, it is still a major player in the     privacy-preserving deep learning space.</p> </li> <li> <p>The core principle behind PySyft is to allow for the ability to     perform computations over data stored on a machine without direct     access to said data ever being given.</p> </li> <li> <p>This is accomplished by adding an intermediate layer between the     user and the data location that sends computation requests to     participating worker machines, returning the computed result to the     user while maintaining the privacy of the data stored and used by     each worker to perform\u00a0the computation.</p> </li> <li> <p>This general capability directly extends itself to FL, reworking     each step of a normal deep learning training flow to be a     computation over the model parameters and data stored at each worker     (agent) participating in FL.</p> </li> <li> <p>To accomplish this, PySyft utilizes hooks that encapsulate the     standard PyTorch/TensorFlow libraries, modifying the requisite     internal functions in order to allow model training and testing to     be supported as PySyft\u00a0privacy-preserving computations.</p> </li> <li> <p>Now that the high-level ideas behind the FL frameworks have been     explained, we move to the implementation-level details for their     practical usage in two example scenarios. First, we\u00a0look at how to     modify the existing centralized training code for\u00a0an NLP model     to\u00a0use FL.</p> </li> </ul> <p>Example -- the federated training of an NLP model</p> <ul> <li> <p>The first ML\u00a0problem that will be converted into an FL scenario     through each of the aforementioned FL frameworks will be a     classification problem within the domain of NLP.</p> </li> <li> <p>At a high level, NLP refers to the intersection of computational     linguistics and ML with an overarching goal of allowing computers to     achieve some level of\u00a0understanding\u00a0from human language -- the     details of this understanding vary widely based on the specific     problem being targeted.</p> </li> <li> <p>For this example, we will be performing sentiment analysis on movie     reviews, classifying them as positive or negative. The dataset we     will be using is the SST-2 dataset     (https://nlp.stanford.edu/sentiment/), containing movie reviews in a     string format and the associated binary labels 0/1 representing     negative and positive\u00a0sentiment, respectively.</p> </li> <li> <p>The model we will use to perform binary classification is a     pretrained BERT model with a custom classification head.</p> </li> <li> <p>The BERT model allows us to encode a sentence into a     high-dimensional numerical vector, which can then be passed to the     classification head to output the binary label prediction; more     information on the BERT model can be found at     https://huggingface.co/blog/bert-101.</p> </li> <li> <p>We choose to use a pretrained model that has already learned how to     produce general encodings for sentences after a significant amount     of training, as opposed to performing said training from scratch.     This allows us to focus training on the classification head to     fine-tune the model on the SST-2 dataset, saving time     while\u00a0maintaining performance.</p> </li> <li> <p>We will now go through the local (centralized) training code that     will be used as a base when showing how to use each of the FL     frameworks, starting with the Keras model definition\u00a0and\u00a0dataset     loader.</p> </li> </ul> <p>Defining the sentiment analysis model</p> <ul> <li> <p>The\u00a0SSTModel\u00a0object\u00a0defined\u00a0in the\u00a0sst_model.py\u00a0file is the Keras     model we will be using for this example.</p> </li> <li> <p>First, we import the\u00a0requisite libraries:</p> </li> </ul> <p>import tensorflow as tf</p> <p>from tensorflow import keras</p> <p>from keras import layers</p> <p>import tensorflow_text</p> <p>import tensorflow_hub as hub</p> <p>import tensorflow_datasets as tfds</p> <p>CopyExplain</p> <ul> <li> <p>TensorFlow Hub is used to easily download the pretrained BERT     weights into a Keras layer. TensorFlow Text is used when loading in     the BERT weights from TensorFlow Hub. TensorFlow Datasets will allow     us to download and cache the\u00a0SST-2 dataset.</p> </li> <li> <p>Next, we define the model and initialize the model\u00a0layer objects:</p> </li> </ul> <p>class SSTModel(keras.Model):</p> <p>def __init__(self):</p> <p>super(SSTModel, self).__init__()</p> <p>self.preprocessor = hub.KerasLayer(\\\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\\\")</p> <p>self.small_bert = hub.KerasLayer(\\\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\\\")</p> <p>self.small_bert.trainable = False</p> <p>self.fc1 = layers.Dense(512, activation=\\'relu\\')</p> <p>self.fc2 = layers.Dense(64, activation=\\'relu\\')</p> <p>self.fc3 = layers.Dense(1, activation=\\'sigmoid\\')</p> <p>CopyExplain</p> <ul> <li> <p>The\u00a0preprocessor\u00a0object takes the raw sentence input batches and     converts them into the format used by the BERT model.</p> </li> <li> <p>We load the preprocessor and BERT layers from TensorFlow Hub, then     initialize the dense layers that make up the classification head. We     use the sigmoid activation function at the end to squash the output     into the interval (0,1), allowing for comparison with the\u00a0true     labels.</p> </li> <li> <p>We can then\u00a0define the forward pass of\u00a0the model:</p> </li> </ul> <p>def call(self, inputs):</p> <p>input_dict = self.preprocessor(inputs)</p> <p>bert_output = self.small_bert(input_dict)[\\'pooled_output\\']</p> <p>output = self.fc1(keras.activations.relu(bert_output, alpha=0.2))</p> <p>scores = self.fc3(self.fc2(output))</p> <p>return scores</p> <p>CopyExplain</p> <ul> <li>We apply leaky ReLU to the BERT output to add non-linearity before     passing the output to the\u00a0classification\u00a0head layers.</li> </ul> <p>Creating the data loader</p> <ul> <li>We also\u00a0implement a function\u00a0to load in the SST-2 dataset using the     TensorFlow Datasets library. First, the training data is loaded and     converted into a NumPy array for use\u00a0during training:</li> </ul> <p>def load_sst_data(client_idx=None, num_clients=1):</p> <p>x_train = []</p> <p>y_train = []</p> <p>for d in tfds.load(name=\\\"glue/sst2\\\", split=\\\"train\\\"):</p> <p>x_train.append(d[\\'sentence\\'].numpy())</p> <p>y_train.append(d[\\'label\\'].numpy())</p> <p>x_train = np.array(x_train)</p> <p>y_train = np.array(y_train)</p> <p>CopyExplain</p> <ul> <li>We load the test data in a\u00a0similar manner:</li> </ul> <p>x_test = []</p> <p>y_test = []</p> <p>for d in tfds.load(name=\\\"glue/sst2\\\", split=\\\"validation\\\"):</p> <p>x_test.append(d[\\'sentence\\'].numpy())</p> <p>y_test.append(d[\\'label\\'].numpy())</p> <p>x_test = np.array(x_test)</p> <p>y_test = np.array(y_test)</p> <p>CopyExplain</p> <ul> <li>If\u00a0client_idx\u00a0and\u00a0num_clients\u00a0are specified, we return the     respective partition of the training\u00a0dataset -- this will be\u00a0used     for\u00a0performing FL:</li> </ul> <p>if (client_idx is not None):</p> <p>shard_size = int(x_train.size / num_clients)</p> <p>x_train = x_train[client_idx*shard_size:(client_idx+1)*shard_size]</p> <p>y_train = x_train[client_idx*shard_size:(client_idx+1)*shard_size]</p> <p>return (x_train, y_train), (x_test, y_test)</p> <p>CopyExplain</p> <ul> <li>Next, we examine the code to perform local training,     located\u00a0in\u00a0local_training.py.</li> </ul> <p>Training the model</p> <ul> <li>We first\u00a0import\u00a0the\u00a0requisite libraries:</li> </ul> <p>import tensorflow as tf</p> <p>from tensorflow import keras</p> <p>from sst_model import SSTModel, load_sst_data</p> <p>CopyExplain</p> <ul> <li>We can then use the previously defined dataset loader (without     splitting) to load in the train and\u00a0test splits:</li> </ul> <p>(x_train,y_train), (x_test,y_test) = load_sst_data()</p> <p>CopyExplain</p> <ul> <li>We can now compile the model and\u00a0begin training:</li> </ul> <p>model.compile(</p> <p>optimizer = keras.optimizers.Adam(learning_rate=0.0005, amsgrad=False),</p> <p>loss = keras.losses.BinaryCrossentropy(),</p> <p>metrics = [keras.metrics.BinaryAccuracy()]</p> <p>)</p> <p>model.fit(x_train, y_train, batch_size=64, epochs=3)</p> <p>CopyExplain</p> <ul> <li>Finally, we evaluate the model on the\u00a0test split:</li> </ul> <p>_, acc = model.evaluate(x_test, y_test, batch_size=64)</p> <p>print(f\\\"Accuracy of model on test set: {(100*acc):.2f}%\\\")</p> <p>CopyExplain</p> <ul> <li> <p>The model should reach around 82% test accuracy after three     epochs\u00a0of training.</p> </li> <li> <p>Now that we have gone through the local training code, we can     examine how the code can be modified to use FL with each of the     aforementioned\u00a0FL frameworks.</p> </li> </ul> <p>Adopting an FL training approach</p> <ul> <li> <p>To demonstrate\u00a0how FL can be\u00a0applied to the SST model training     scenario, we have to first split the original SST-2 dataset into     disjoint subsets representing the local datasets in an FL     application. To keep things simple, we will examine the case of     three agents each training on separate thirds of the dataset.</p> </li> <li> <p>For now, these subsets are randomly sampled without replacement from     the dataset -- in the next section,\u00a0Federated training of an image     classification model on non-IID data, we examine the case where the     local datasets are created from a biased sampling of the original     dataset. Instead of locally training for three epochs, we will     perform three rounds of FL with each local training phase training     for one epoch on the local data.</p> </li> <li> <p>FedAvg will be used to aggregate the locally trained models at the     end of each round. After these three rounds, the aforementioned     validation metrics will be computed using the final aggregate model,     allowing for comparisons to be drawn between the local training     cases and the\u00a0FL case.</p> </li> </ul> <p>Integrating TensorFlow Federated for SST-2</p> <ul> <li> <p>As previously\u00a0mentioned, the\u00a0TensorFlow Federated\u00a0(TFF)     framework was built on top of the TensorFlow\u00a0and Keras deep     learning\u00a0libraries. The model implementation was done using Keras;     as a result, the integration of TFF into the local training code     is\u00a0relatively straightforward.</p> </li> <li> <p>The first step is to add the TFF-specific imports and FL-specific     parameters prior to loading\u00a0the dataset:</p> </li> </ul> <p>import nest_asyncio</p> <p>nest_asyncio.apply()</p> <p>import tensorflow_federated as tff</p> <p>NUM_CLIENTS = 3</p> <p>NUM_ROUNDS = 3</p> <p>CopyExplain</p> <ul> <li>TFF allows us to simulate some number of agents by passing the     appropriate number of datasets (local datasets) to the FL process.     To split the SST-2 dataset into thirds after preprocessing, we can     use the\u00a0following code:</li> </ul> <p>client_datasets = [load_sst_data(idx, NUM_CLIENTS)[0] for idx in range(NUM_CLIENTS)]</p> <p>CopyExplain</p> <ul> <li>Next, we have to wrap the Keras model using a TFF API function to     easily create the respective\u00a0tff.learning.Model\u00a0object. We create a     function that initializes the SST model and passes it along with the     input spec (information on the size of each data element) to this     API function, returning the result -- TFF will use this function     internally to create\u00a0the\u00a0model\u00a0during the\u00a0FL process:</li> </ul> <p>def sst_model_fn():</p> <p>sst_model = SSTModel()</p> <p>sst_model.build(input_shape=(None,64))</p> <p>return tff.learning.from_keras_model(</p> <p>sst_model,</p> <p>input_spec=tf.TensorSpec(shape=(None), dtype=tf.string),</p> <p>loss=keras.metrics.BinaryCrossentropy()</p> <p>)</p> <p>CopyExplain</p> <ul> <li>The TFF FedAvg process can then be created, using     the\u00a0sst_model_fn\u00a0function along with the optimizers used to update     the local models and the aggregate model. Using a learning rate of     1.0 for the server optimizer function allows for the new aggregate     model to replace the old one at the end of each round (as opposed to     computing a weighted average of the old and\u00a0new models):</li> </ul> <p>fed_avg_process = tff.learning.algorithms.build_unweighted_fed_avg(</p> <p>model_fn = sst_model_fn,</p> <p>client_optimizer_fn = lambda: keras.optimizers.Adam(learning_rate=0.001),</p> <p>server_optimizer_fn = lambda: keras.optimizers.SGD(learning_rate=1.0)</p> <p>)</p> <p>CopyExplain</p> <ul> <li>Finally, we initialize and run the federated learning process for 10     rounds. Each\u00a0fed_avg_process.next()\u00a0call simulates one round by     performing local training with three models on the client datasets     followed by aggregation using FedAvg. The resulting state after the     first round is passed to the next call as the starting FL state     for\u00a0the round:</li> </ul> <p>state = fed_avg_process.initialize()</p> <p>for round in range(NUM_ROUNDS):</p> <p>state = fed_avg_process.next(state, client_datasets).state</p> <p>CopyExplain</p> <ul> <li>After the FL process is completed, we convert the final     aggregate\u00a0tff.learning.Model\u00a0object back into the original Keras     model format in order to compute\u00a0the\u00a0validation\u00a0metrics:</li> </ul> <p>fed_weights = fed_avg_process.get_model_weights(state)</p> <p>fed_sst_model = SSTModel()</p> <p>fed_sst_model.build(input_shape=(None, 64))</p> <p>fed_sst_model.compile(</p> <p>optimizer = keras.optimizers.Adam(learning_rate=0.005, amsgrad=False),</p> <p>loss = keras.losses.BinaryCrossentropy(),</p> <p>metrics = [keras.metrics.BinaryAccuracy()]</p> <p>)</p> <p>fed_weights.assign_weights_to(fed_sst_model)</p> <p>_, (x_test, y_test) = load_sst_data()</p> <p>_, acc = fed_sst_model.evaluate(x_test, y_test, batch_size=64)</p> <p>print(f\\\"Accuracy of federated model on test set: {(100*acc):.2f}%\\\")</p> <p>CopyExplain</p> <ul> <li> <p>The final accuracy of the aggregate model should be\u00a0around 82%.</p> </li> <li> <p>From this, it\u00a0should be clear that the TFF FedAvg results are nearly     identical to those of the local\u00a0training scenario.</p> </li> </ul> <p>Integrating OpenFL for SST-2</p> <ul> <li> <p>Recall that\u00a0OpenFL supports two different\u00a0workflows: the     aggregator-based workflow and the director-based workflow. This     example will use the director-based workflow, involving long-living     components that can conduct FL task requests as they come in.\u00a0This     was chosen due to the desirability of having a persistent FL setup     for deploying multiple projects; however, both workflows conduct the     same core FL process and thus demonstrate\u00a0similar performance.</p> </li> <li> <p>To help with model serialization in this case, we only aggregate the     classification head weights, reconstructing the full model at     runtime during training and validation (TensorFlow Hub caches the     downloaded layers, so the download process only occurs once).\u00a0We     include the following functions in\u00a0sst_model.py\u00a0to aid with\u00a0this     modification:</p> </li> </ul> <p>def get_sst_full(preprocessor, bert, classification_head):</p> <p>sst_input = keras.Input(shape=(), batch_size=64, dtype=tf.string)</p> <p>scores = classification_head(bert(preprocessor(sst_input))[\\'pooled_output\\'])</p> <p>return keras.Model(inputs=sst_input, outputs=scores, name=\\'sst_model\\')</p> <p>def get_classification_head():</p> <p>classification_head = keras.Sequential([</p> <p>layers.Dense(512, activation=\\'relu\\', input_shape=(768,)),</p> <p>layers.Dense(64, activation=\\'relu\\', input_shape=(512,)),</p> <p>layers.Dense(1, activation=\\'sigmoid\\', input_shape=(64,))</p> <p>])</p> <p>return classification_head</p> <p>CopyExplain</p> <ul> <li> <p>Because OpenFL focuses on addressing the data silo case, the     creation of the local datasets from the SST-2 data is slightly more     involved than the TFF case. The objects needed to create the dataset     will be implemented in a separate file\u00a0named\u00a0sst_fl_dataset.py.</p> </li> <li> <p>First, we include the necessary imports. The two OpenFL-specific     objects we import are the\u00a0ShardDescriptor\u00a0object, which handles the     dataset loading and sharding, and the\u00a0DataInterface\u00a0object,     which\u00a0handles\u00a0access to\u00a0the datasets:</p> </li> </ul> <p>from openfl.interface.interactive_api.shard_descriptor import ShardDescriptor</p> <p>from openfl.interface.interactive_api.experiment import DataInterface</p> <p>import tensorflow as tf</p> <p>from sst_model import load_sst_data</p> <p>CopyExplain</p> <p>Implementing ShardDescriptor</p> <ul> <li>We first\u00a0implement the\u00a0SSTShardDescriptor\u00a0class. When\u00a0this shard     descriptor is created, we save the\u00a0rank\u00a0(client number)     and\u00a0worldsize\u00a0(total number of clients) values, then load the     training and\u00a0validation datasets:</li> </ul> <p>class SSTShardDescriptor(ShardDescriptor):</p> <p>def __init__(</p> <p>self,</p> <p>rank_worldsize: str = \\'1, 1\\',</p> <p>**kwargs</p> <p>):</p> <p>self.rank, self.worldsize = tuple(int(num) for num in rank_worldsize.split(\\',\\'))</p> <p>(x_train,y_train), (x_test,y_test) = load_sst_data(self.rank-1, self.worldsize)</p> <p>self.data_by_type = {</p> <p>\\'train\\': tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(64),</p> <p>\\'val\\': tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)</p> <p>}</p> <p>CopyExplain</p> <ul> <li>We implement the\u00a0ShardDescriptor\u00a0class functions to get the     available dataset types (training\u00a0and validation in this case) and     the respective dataset/shard based on the rank of\u00a0the client:</li> </ul> <p>def get_shard_dataset_types(self):</p> <p>return list(self.data_by_type)</p> <p>def get_dataset(self, dataset_type=\\'train\\'):</p> <p>if dataset_type not in self.data_by_type:</p> <p>raise Exception(f\\'Wrong dataset type: {dataset_type}\\')</p> <p>return self.data_by_type[dataset_type]</p> <p>CopyExplain</p> <ul> <li>We also specify the properties of the specific dataset being used.     Note that the sample shape is set to\u00a01. The preprocessor layer of     the\u00a0SSTModel\u00a0allows us to pass in strings as input, which are     treated as input vectors of type\u00a0tf.string\u00a0and\u00a0length\u00a01:</li> </ul> <p>@property</p> <p>def sample_shape(self):</p> <p>return [\\\"1\\\"]</p> <p>@property</p> <p>def target_shape(self):</p> <p>return [\\\"1\\\"]</p> <p>@property</p> <p>def dataset_description(self) -&gt; str:</p> <p>return (f\\'SST dataset, shard number {self.rank}\\'</p> <p>f\\' out of {self.worldsize}\\')</p> <p>CopyExplain</p> <ul> <li>With\u00a0this, the\u00a0SSTShardDescriptor\u00a0implementation\u00a0is completed.</li> </ul> <p>Implementing DataInterface</p> <ul> <li>Next, we\u00a0implement the\u00a0SSTFedDataset\u00a0class\u00a0as a subclass     of\u00a0DataInterface. This is done by implementing the shard descriptor     getter and setter methods, with the setter method preparing the data     to be provided to the training/validation\u00a0FL tasks:</li> </ul> <p>class SSTFedDataset(DataInterface):</p> <p>def __init__(self, **kwargs):</p> <p>super().__init__(**kwargs)</p> <p>@property</p> <p>def shard_descriptor(self):</p> <p>return self._shard_descriptor</p> <p>@shard_descriptor.setter</p> <p>def shard_descriptor(self, shard_descriptor):</p> <p>self._shard_descriptor = shard_descriptor</p> <p>self.train_set = shard_descriptor.get_dataset(\\'train\\')</p> <p>self.valid_set = shard_descriptor.get_dataset(\\'val\\')</p> <p>CopyExplain</p> <ul> <li>We also implement\u00a0the API functions to grant dataset access and     dataset size information (used\u00a0during aggregation):</li> </ul> <p>def get_train_loader(self):</p> <p>return self.train_set</p> <p>def get_valid_loader(self):</p> <p>return self.valid_set</p> <p>def get_train_data_size(self):</p> <p>return len(self.train_set) * 64</p> <p>def get_valid_data_size(self):</p> <p>return len(self.valid_set) * 64</p> <p>CopyExplain</p> <ul> <li>With this, the\u00a0local SST-2 datasets can be constructed\u00a0and used.</li> </ul> <p>Creating FLExperiment</p> <ul> <li>We now focus on\u00a0the actual implementation\u00a0of the FL process within a     new file,\u00a0fl_sim.py. First, we import the necessary libraries --     from OpenFL, we import\u00a0the following:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   TaskInterface: Allows us to define our FL training and validation     tasks for the model; the registered tasks are what the director     instructs each envoy\u00a0to conduct</p> <ul> <li> <p>ModelInterface: Allows us to convert our Keras model into the format     used by OpenFL in the\u00a0registered tasks</p> </li> <li> <p>Federation: Manages information relating to the connection with\u00a0the     director</p> </li> <li> <p>FLExperiment: Uses the\u00a0TaskInterface,\u00a0ModelInterface,     and\u00a0Federation\u00a0objects to conduct the\u00a0FL process</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The requisite\u00a0imports are done\u00a0as follows:</p> <p>import tensorflow as tf</p> <p>from tensorflow import keras</p> <p>import tensorflow_hub as hub</p> <p>from openfl.interface.interactive_api.experiment import TaskInterface</p> <p>from openfl.interface.interactive_api.experiment import ModelInterface</p> <p>from openfl.interface.interactive_api.experiment import FLExperiment</p> <p>from openfl.interface.interactive_api.federation import Federation</p> <p>from sst_model import get_classification_head, get_sst_full</p> <p>from sst_fl_dataset import SSTFedDataset</p> <p>CopyExplain</p> <ul> <li>Next, we create the\u00a0Federation\u00a0object using the     default\u00a0director\u00a0connection information:</li> </ul> <p>client_id = \\'api\\'</p> <p>director_node_fqdn = \\'localhost\\'</p> <p>director_port = 50051</p> <p>federation = Federation(</p> <p>client_id=client_id,</p> <p>director_node_fqdn=director_node_fqdn,</p> <p>director_port=director_port,</p> <p>tls=False</p> <p>)</p> <p>CopyExplain</p> <ul> <li>We then initialize\u00a0the model with the associated optimizer and loss     function -- these objects are used by the OpenFL\u00a0KerasAdapter\u00a0to     create the\u00a0ModelInterface\u00a0object. We call the model on a dummy Keras     input in order to initialize all of the weights before passing the     model\u00a0to\u00a0ModelInterface:</li> </ul> <p>classification_head = get_classification_head()</p> <p>optimizer = keras.optimizers.Adam(learning_rate=0.005, amsgrad=False)</p> <p>loss = keras.losses.BinaryCrossentropy()</p> <p>framework_adapter = \\'openfl.plugins.frameworks_adapters.keras_adapter.FrameworkAdapterPlugin\\'</p> <p>MI = ModelInterface(model=classification_head, optimizer=optimizer, framework_plugin=framework_adapter)</p> <p>CopyExplain</p> <ul> <li>Next, we\u00a0create a\u00a0TaskInterface\u00a0object and use it to register the     training task. Note that including the optimizer in the decorator     function of a task will result in the training dataset being passed     to the task; otherwise, the validation dataset will be passed to\u00a0the     task:</li> </ul> <p>TI = TaskInterface()</p> <p>\\@TI.register_fl_task(model=\\'model\\', data_loader=\\'train_data\\', device=\\'device\\', optimizer=\\'optimizer\\')</p> <p>def train(model, train_data, optimizer, device):</p> <p>preprocessor = hub.KerasLayer(\\\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\\\")</p> <p>small_bert = hub.KerasLayer(\\\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\\\")</p> <p>small_bert.trainable = False</p> <p>full_model = get_sst_full(preprocessor, small_bert, model)</p> <p>full_model.compile(loss=loss, optimizer=optimizer)</p> <p>history = full_model.fit(train_data, epochs=1)</p> <p>return {\\'train_loss\\':history.history[\\'loss\\'][0]}</p> <p>CopyExplain</p> <ul> <li>Similarly, we register the validation task using     the\u00a0TaskInterface\u00a0object. Note that we can collect the metrics     generated\u00a0by\u00a0the\u00a0evaluate\u00a0function and return the values as a means     of\u00a0tracking performance:</li> </ul> <p>\\@TI.register_fl_task(model=\\'model\\', data_loader=\\'val_data\\', device=\\'device\\')</p> <p>def validate(model, val_data, device):</p> <p>preprocessor = hub.KerasLayer(\\\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\\\")</p> <p>small_bert = hub.KerasLayer(\\\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\\\")</p> <p>small_bert.trainable = False</p> <p>full_model = get_sst_full(preprocessor, small_bert, model)</p> <p>full_model.compile(loss=loss, optimizer=optimizer)</p> <p>loss, acc = full_model.evaluate(val_data, batch_size=64)</p> <p>return {\\'val_acc\\':acc, \\'val_loss\\':loss,}</p> <p>CopyExplain</p> <ul> <li>We can now load in the dataset using the\u00a0SSTFedDataset\u00a0class     implemented earlier and create and start\u00a0a new\u00a0FLExperiment\u00a0using     the created\u00a0ModelInterface,\u00a0TaskInterface,     and\u00a0SSTFedDatasets\u00a0objects:</li> </ul> <p>fed_dataset = SSTFedDataset()</p> <p>fl_experiment = FLExperiment(federation=federation, experiment_name=\\'sst_experiment\\')</p> <p>fl_experiment.start(</p> <p>model_provider=MI,</p> <p>task_keeper=TI,</p> <p>data_loader=fed_dataset,</p> <p>rounds_to_train=3,</p> <p>opt_treatment=\\'CONTINUE_LOCAL\\'</p> <p>)</p> <p>CopyExplain</p> <p>Defining the configuration files</p> <ul> <li>The last step is to create the configuration files used     by\u00a0director\u00a0and\u00a0envoys\u00a0in order to actually load the data and start     the FL process. First, we create\u00a0director_config\u00a0containing     the\u00a0following information:</li> </ul> <p>settings:</p> <p>listen_host: localhost</p> <p>listen_port: 50051</p> <p>sample_shape: [\\\"1\\\"]</p> <p>target_shape: [\\\"1\\\"]</p> <p>CopyExplain</p> <ul> <li> <p>This is saved\u00a0in\u00a0director/director_config.yaml.</p> </li> <li> <p>We then create the three\u00a0envoy\u00a0configuration files. The first file     (envoy_config_1.yaml) contains\u00a0the following:</p> </li> </ul> <p>params:</p> <p>cuda_devices: []</p> <p>optional_plugin_components: {}</p> <p>shard_descriptor:</p> <p>template: sst_fl_dataset.SSTShardDescriptor</p> <p>params:</p> <p>rank_worldsize: 1, 3</p> <p>CopyExplain</p> <ul> <li>The second and third\u00a0envoy\u00a0config files are the same, except with     the values\u00a0rank_worldsize: 2, 3\u00a0and\u00a0rank_worldsize: 3, 3,     respectively. These config files, alongside all of the code files,     are stored in the experiment directory. The directory structure     should look like\u00a0the following:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   -director</p> <pre><code>-   director_config.yaml\n</code></pre> <ul> <li> <p>experiment</p> <ul> <li> <p>envoy_config_1.yaml</p> </li> <li> <p>envoy_config_2.yaml</p> </li> <li> <p>envoy_config_3.yaml</p> </li> <li> <p>sst_fl_dataset.py</p> </li> <li> <p>sst_model.py</p> </li> <li> <p>fl_sim.py (file with\u00a0FLExperiment creation)</p> </li> </ul> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   With everything set up, we can now perform FL\u00a0with OpenFL.</p> <ul> <li> <p>Running the OpenFL example</p> </li> <li> <p>First, start the\u00a0director by running the following command from     within the\u00a0director\u00a0folder (make sure OpenFL is installed in     the\u00a0working environment):</p> </li> </ul> <p>fx director start --disable-tls -c director_config.yaml</p> <p>CopyExplain</p> <ul> <li>Next, run the following commands in separate terminals from     the\u00a0experiment directory:</li> </ul> <p>fx envoy start -n envoy_1 ---disable-tls --envoy-config-path envoy_config_1.yaml -dh localhost -dp 50051</p> <p>fx envoy start -n envoy_2 ---disable-tls --envoy-config-path envoy_config_2.yaml -dh localhost -dp 50051</p> <p>fx envoy start -n envoy_3 ---disable-tls --envoy-config-path envoy_config_3.yaml -dh localhost -dp 50051</p> <p>CopyExplain</p> <ul> <li>Finally, start\u00a0FLExperiment\u00a0by running the\u00a0fl_sim.py\u00a0script. After     the three rounds are\u00a0completed, the aggregate model should achieve a     validation accuracy of around 82%. Once again, the performance is     nearly identical to the local\u00a0training scenario.</li> </ul> <p>Integrating IBM FL for SST-2</p> <ul> <li>IBM FL uses a\u00a0saved version\u00a0of the model when performing FL. The     following code (create_saved_model.py) initializes a model (calling     the model on a dummy input to initialize the parameters) and then     saves the model in the Keras\u00a0SavedModel\u00a0format for IBM FL\u00a0to use:</li> </ul> <p>import tensorflow as tf</p> <p>from tensorflow import keras</p> <p>from sst_model import SSTModel</p> <p>sst_model = SSTModel()</p> <p>optimizer = keras.optimizers.Adam(learning_rate=0.005, amsgrad=False)</p> <p>loss = keras.losses.BinaryCrossentropy(),</p> <p>sst_model.compile(loss=loss, optimizer=optimizer)</p> <p>sst_input = keras.Input(shape=(), dtype=tf.string)</p> <p>sst_model(sst_input)</p> <p>sst_model.save(\\'sst_model_save_dir\\')</p> <p>CopyExplain</p> <ul> <li>Run this once to save the model into the folder     named\u00a0sst_model_save_dir\u00a0-- we will point\u00a0IBM FL\u00a0to load in the     model saved in\u00a0this directory.</li> </ul> <p>Creating DataHandler</p> <ul> <li>Next, we create a\u00a0subclass of the IBM FL\u00a0DataHandler\u00a0class\u00a0in charge     of providing the training and validation data to the model -- this     subclass will load, preprocess, and store the SST datasets as class     attributes. We first import the\u00a0necessary libraries:</li> </ul> <p>from ibmfl.data.data_handler import DataHandler</p> <p>import tensorflow as tf</p> <p>from sst_model import load_sst_data</p> <p>CopyExplain</p> <ul> <li>The\u00a0init\u00a0function of this class loads the data info parameters,     which are then used to load the correct SST-2\u00a0data partition:</li> </ul> <p>class SSTDataHandler(DataHandler):</p> <p>def __init__(self, data_config=None):</p> <p>super().__init__()</p> <p>if (data_config is not None):</p> <p>if (\\'client_id\\' in data_config):</p> <p>self.client_id = int(data_config[\\'client_id\\'])</p> <p>if (\\'num_clients\\' in data_config):</p> <p>self.num_clients = int(data_config[\\'num_clients\\'])</p> <p>train_data, val_data = load_sst_data(self.client_id-1, self.num_clients)</p> <p>self.train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(64)</p> <p>self.val_dataset = tf.data.Dataset.from_tensor_slices(val_data).batch(64)</p> <p>CopyExplain</p> <ul> <li>We also\u00a0implement the API function that returns the\u00a0loaded datasets     for use\u00a0during training/validation:</li> </ul> <p>def get_data(self):</p> <p>return self.train_dataset, self.val_dataset</p> <p>CopyExplain</p> <p>Defining the configuration files</p> <ul> <li>The next step\u00a0is to create the configuration JSON files used when     starting the aggregator and initializing the parties.\u00a0The     aggregation config first specifies the connection information it     will use to communicate with\u00a0the parties:</li> </ul> <p>{</p> <p>\\\"connection\\\": {</p> <p>\\\"info\\\": {</p> <p>\\\"ip\\\": \\\"127.0.0.1\\\",</p> <p>\\\"port\\\": 5000,</p> <p>\\\"tls_config\\\": {</p> <p>\\\"enable\\\": \\\"false\\\"</p> <p>}</p> <p>},</p> <p>\\\"name\\\": \\\"FlaskConnection\\\",</p> <p>\\\"path\\\": \\\"ibmfl.connection.flask_connection\\\",</p> <p>\\\"sync\\\": \\\"False\\\"</p> <p>},</p> <p>CopyExplain</p> <ul> <li>Next, we specify the\u00a0fusion handler used\u00a0for aggregation:</li> </ul> <p>\\\"fusion\\\": {</p> <p>\\\"name\\\": \\\"IterAvgFusionHandler\\\",</p> <p>\\\"path\\\": \\\"ibmfl.aggregator.fusion.iter_avg_fusion_handler\\\"</p> <p>},</p> <p>CopyExplain</p> <ul> <li>We also specify the hyperparameters related to both local training     and aggregation.\u00a0perc_quorum\u00a0refers to the percentage of parties     that must participate before aggregation\u00a0can begin:</li> </ul> <p>\\\"hyperparams\\\": {</p> <p>\\\"global\\\": {</p> <p>\\\"max_timeout\\\": 10800,</p> <p>\\\"num_parties\\\": 1,</p> <p>\\\"perc_quorum\\\": 1,</p> <p>\\\"rounds\\\": 3</p> <p>},</p> <p>\\\"local\\\": {</p> <p>\\\"optimizer\\\": {</p> <p>\\\"lr\\\": 0.0005</p> <p>},</p> <p>\\\"training\\\": {</p> <p>\\\"epochs\\\": 1</p> <p>}</p> <p>}</p> <p>},</p> <p>CopyExplain</p> <ul> <li>Finally, we specify the IBM FL protocol handler\u00a0to use:</li> </ul> <p>\\\"protocol_handler\\\": {</p> <p>\\\"name\\\": \\\"ProtoHandler\\\",</p> <p>\\\"path\\\": \\\"ibmfl.aggregator.protohandler.proto_handler\\\"</p> <p>}</p> <p>}</p> <p>CopyExplain</p> <ul> <li>This configuration is saved\u00a0in\u00a0agg_config.json.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   We also create\u00a0the base party configuration file used to conduct FL     with the local data. We first specify the connection information of     the aggregator and\u00a0the party:</p> <p>{</p> <p>\\\"aggregator\\\":</p> <p>{</p> <p>\\\"ip\\\": \\\"127.0.0.1\\\",</p> <p>\\\"port\\\": 5000</p> <p>},</p> <p>\\\"connection\\\": {</p> <p>\\\"info\\\": {</p> <p>\\\"ip\\\": \\\"127.0.0.1\\\",</p> <p>\\\"port\\\": 8085,</p> <p>\\\"id\\\": \\\"party\\\",</p> <p>\\\"tls_config\\\": {</p> <p>\\\"enable\\\": \\\"false\\\"</p> <p>}</p> <p>},</p> <p>\\\"name\\\": \\\"FlaskConnection\\\",</p> <p>\\\"path\\\": \\\"ibmfl.connection.flask_connection\\\",</p> <p>\\\"sync\\\": \\\"false\\\"</p> <p>},</p> <p>CopyExplain</p> <ul> <li>We then specify the data handler and the local training handler to     use -- this component trains the SST model\u00a0using the model     information and the\u00a0local data:</li> </ul> <p>\\\"data\\\": {</p> <p>\\\"info\\\": {</p> <p>\\\"client_id\\\": 0,</p> <p>\\\"num_clients\\\": 3</p> <p>},</p> <p>\\\"name\\\": \\\"SSTDataHandler\\\",</p> <p>\\\"path\\\": \\\"sst_data_handler\\\"</p> <p>},</p> <p>\\\"local_training\\\": {</p> <p>\\\"name\\\": \\\"LocalTrainingHandler\\\",</p> <p>\\\"path\\\": \\\"ibmfl.party.training.local_training_handler\\\"</p> <p>},</p> <p>CopyExplain</p> <ul> <li>The model format and information is then specified -- this is where     we point to the saved model\u00a0created earlier:</li> </ul> <p>\\\"model\\\": {</p> <p>\\\"name\\\": \\\"TensorFlowFLModel\\\",</p> <p>\\\"path\\\": \\\"ibmfl.model.tensorflow_fl_model\\\",</p> <p>\\\"spec\\\": {</p> <p>\\\"model-name\\\": \\\"sst_model\\\",</p> <p>\\\"model_definition\\\": \\\"sst_model_save_dir\\\"</p> <p>}</p> <p>},</p> <p>CopyExplain</p> <ul> <li>Finally, we specify\u00a0the\u00a0protocol handler:</li> </ul> <p>\\\"protocol_handler\\\": {</p> <p>\\\"name\\\": \\\"PartyProtocolHandler\\\",</p> <p>\\\"path\\\": \\\"ibmfl.party.party_protocol_handler\\\"</p> <p>}</p> <p>}</p> <p>CopyExplain</p> <p>Creating IBM FL party</p> <ul> <li>With this, all that\u00a0is left is the code that starts each party,     saved in\u00a0fl_sim.py. We first import the\u00a0necessary libraries:</li> </ul> <p>import argparse</p> <p>import json</p> <p>from ibmfl.party.party import Party</p> <p>CopyExplain</p> <ul> <li>We include an\u00a0argparse\u00a0argument that allows for the party number to     be specified -- this is used to modify the base party configuration     file in order to allow for distinct parties to be started from     the\u00a0same file:</li> </ul> <p>parser = argparse.ArgumentParser()</p> <p>parser.add_argument(\\\"party_id\\\", type=int)</p> <p>args = parser.parse_args()</p> <p>party_id = args.party_id</p> <p>with open(\\'party_config.json\\') as cfg_file:</p> <p>party_config = json.load(cfg_file)</p> <p>party_config[\\'connection\\'][\\'info\\'][\\'port\\'] += party_id</p> <p>party_config[\\'connection\\'][\\'info\\'][\\'id\\'] += f\\'_{party_id}\\'</p> <p>party_config[\\'data\\'][\\'info\\'][\\'client_id\\'] = party_id</p> <p>CopyExplain</p> <ul> <li>Finally, we create and start a new\u00a0Party\u00a0object with the     modified\u00a0configuration information:</li> </ul> <p>party = Party(config_dict=party_config)</p> <p>party.start()</p> <p>party.register_party()</p> <p>CopyExplain</p> <ul> <li>With this, we\u00a0can now begin performing FL using\u00a0IBM FL.</li> </ul> <p>Running the IBM FL example</p> <ul> <li>First, start\u00a0aggregator\u00a0by\u00a0running the\u00a0following command:</li> </ul> <p>python -m ibmfl.aggregator.aggregator agg_config.json</p> <p>CopyExplain</p> <ul> <li>After the aggregator is finished setting up, type\u00a0START\u00a0and     press\u00a0Enter\u00a0key to open the aggregator to receive incoming     connections. You can then start three parties using the following     commands in\u00a0separate terminals:</li> </ul> <p>python fl_sim.py 1</p> <p>python fl_sim.py 2</p> <p>python fl_sim.py 3</p> <p>CopyExplain</p> <ul> <li>Finally, type\u00a0TRAIN\u00a0into the aggregator window and press\u00a0Enter\u00a0key     to begin the FL process. When\u00a0three rounds are completed, you can     type\u00a0SAVE\u00a0into the same window to save the latest\u00a0aggregate model.</li> </ul> <p>Integrating Flower for SST-2</p> <ul> <li> <p>The two main\u00a0Flower\u00a0components that must be incorporated on top of     the existing local training code are the client and strategy     subclass implementations.</p> </li> <li> <p>The client subclass implementation allows us to interface with     Flower, with API functions that allow for model parameters to be     passed between the clients and the server.</p> </li> <li> <p>The strategy subclass implementation allows us to specify the     details of the aggregation approach performed by\u00a0the server.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   We begin by writing the code to implement and start a client (stored     in\u00a0fl_sim.py). First, the necessary libraries\u00a0are imported:</p> <p>import argparse</p> <p>import tensorflow as tf</p> <p>from tensorflow import keras</p> <p>from sst_model import SSTModel, load_sst_data</p> <p>import flwr as fl</p> <p>CopyExplain</p> <ul> <li>We add a command-line argument specifying the client ID in order to     allow for the same client script to be reused for all\u00a0three agents:</li> </ul> <p>parser = argparse.ArgumentParser()</p> <p>parser.add_argument(\\\"client_id\\\", type=int)</p> <p>args = parser.parse_args()</p> <p>client_id = args.client_id</p> <p>NUM_CLIENTS = 3</p> <p>CopyExplain</p> <p>We then load in the\u00a0SST-2 datasets:</p> <p>(x_train,y_train), (x_test,y_test) = load_sst_data(client_id-1, NUM_CLIENTS)</p> <p>CopyExplain</p> <ul> <li>Note that we use the client ID to get the respective shard from     the\u00a0training dataset.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Next, we create the model and the associated optimizer and loss     objects, making sure to call the model\u00a0on a dummy\u00a0input to     initialize\u00a0the weights:</p> <p>sst_model = SSTModel()</p> <p>sst_model.compile(</p> <p>optimizer = keras.optimizers.Adam(learning_rate=0.005, amsgrad=False),</p> <p>loss = keras.losses.BinaryCrossentropy(),</p> <p>metrics = [keras.metrics.BinaryAccuracy()]</p> <p>)</p> <p>sst_input = keras.Input(shape=(), dtype=tf.string)</p> <p>sst_model(sst_input)</p> <p>CopyExplain</p> <p>Implementing the Flower client</p> <ul> <li>We can now\u00a0implement the Flower client object that will pass model     parameters to and from the server. To implement a client subclass,     we have to define\u00a0three functions:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   get_parameters(self, config): Returns the model\u00a0parameter values</p> <ul> <li> <p>fit(self, parameters, config): Sets the weights of the local model     to the received parameters, performs local training, and returns the     new model parameters alongside the dataset size and\u00a0training metrics</p> </li> <li> <p>evaluate(self, parameters, config): Sets the weights of the local     model to the received parameters, then evaluates the model on     validation/test data and returns the\u00a0performance metrics</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Using\u00a0fl.client.NumPyClient\u00a0as the superclass allows us to take     advantage of the Keras model\u00a0get_weights\u00a0and\u00a0set_weights\u00a0functions     that convert the model parameters into\u00a0lists of\u00a0NumPy arrays:</p> <p>class SSTClient(fl.client.NumPyClient):</p> <p>def get_parameters(self, config):</p> <p>return sst_model.get_weights()</p> <p>def fit(self, parameters, config):</p> <p>sst_model.set_weights(parameters)</p> <p>history = sst_model.fit(x_train, y_train, epochs=1)</p> <p>return sst_model.get_weights(), len(x_train),</p> <p>CopyExplain</p> <ul> <li>The\u00a0evaluate\u00a0function is\u00a0also defined:</li> </ul> <p>def evaluate(self, parameters, config):</p> <p>sst_model.set_weights(parameters)</p> <p>loss, acc = sst_model.evaluate(x_test, y_test, batch_size=64)</p> <p>return loss, len(x_train), {\\'val_acc\\':acc, \\'val_loss\\':loss}</p> <p>CopyExplain</p> <ul> <li>With this client implementation, we can finally start the client     using the default connection information with the\u00a0following line:</li> </ul> <p>fl.client.start_numpy_client(server_address=\\\"[::]:8080\\\", client=SSTClient())</p> <p>CopyExplain</p> <ul> <li>Creating the Flower server</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The final step\u00a0before running Flower is to create the script     (server.py) that will start the Flower server. We begin with the     necessary imports and the\u00a0MAX_ROUNDS\u00a0parameter:</p> <p>import flwr as fl</p> <p>import tensorflow as tf</p> <p>from tensorflow import keras</p> <p>from sst_model import SSTModel</p> <p>MAX_ROUNDS = 3</p> <p>CopyExplain</p> <ul> <li>Because we want to save the model after performing federated     learning, we create a subclass of the flower FedAvg strategy and add     a final step that saves the model at the last round during     the\u00a0aggregation phase:</li> </ul> <p>class SaveKerasModelStrategy(fl.server.strategy.FedAvg):</p> <p>def aggregate_fit(self, server_round, results, failures):</p> <p>agg_weights = super().aggregate_fit(server_round, results, failures)</p> <p>if (server_round == MAX_ROUNDS):</p> <p>sst_model = SSTModel()</p> <p>sst_input = keras.Input(shape=(), dtype=tf.string)</p> <p>sst_model(sst_input)</p> <p>sst_model.set_weights(fl.common.parameters_to_ndarrays(agg_weights[0]))</p> <p>sst_model.save(\\'final_agg_sst_model\\')</p> <p>return agg_weights</p> <p>CopyExplain</p> <ul> <li>With this strategy, we can run the following line to start the     server (passing the\u00a0MAX_ROUNDS\u00a0parameter through     the\u00a0config\u00a0argument):</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   fl.server.start_server(strategy=SaveKerasModelStrategy(),     config=fl.server.ServerConfig(num_rounds=MAX_ROUNDS))</p> <p>CopyExplain</p> <ul> <li>We can now\u00a0start the server and clients, allowing for FL to be     performed\u00a0using Flower.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Running the Flower example</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   To start the\u00a0server, first run the\u00a0server.py\u00a0script.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Each of the three clients can then be started by running the     following commands in separate\u00a0terminal windows:</p> <p>python fl_sim.py 1</p> <p>python fl_sim.py 2</p> <p>python fl_sim.py 3</p> <p>CopyExplain</p> <ul> <li>The final aggregate model after FL will be saved in     the\u00a0final_agg_sst_model\u00a0directory as a\u00a0SavedModel\u00a0object.</li> </ul> <p>Integrating STADLE for SST-2</p> <ul> <li> <p>STADLE differs\u00a0from the\u00a0previously examined FL frameworks by     providing a cloud-based platform (STADLE Ops) to handle the     deployment of aggregators and management of the FL process.</p> </li> <li> <p>Because the deployment of the server side can be done through the     platform, the client-side implementation is all that needs to be     implemented for performing FL with STADLE.</p> </li> <li> <p>This integration is done by creating a client object that     occasionally sends the local model and returns the aggregate model     from the previous round. To do this, we need to create the agent     configuration file and modify the local training code to     interface\u00a0with STADLE.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   First, we create the configuration file for the agent\u00a0as follows:</p> <p>{</p> <p>\\\"model_path\\\": \\\"./data/agent\\\",</p> <p>\\\"aggr_ip\\\": \\\"localhost\\\",</p> <p>\\\"reg_port\\\": \\\"8765\\\",</p> <p>\\\"token\\\": \\\"stadle12345\\\",</p> <p>\\\"base_model\\\": {</p> <p>\\\"model_fn\\\": \\\"SSTModel\\\",</p> <p>\\\"model_fn_src\\\": \\\"sst_model\\\",</p> <p>\\\"model_format\\\": \\\"Keras\\\",</p> <p>\\\"model_name\\\": \\\"Keras-SST-Model\\\"</p> <p>}</p> <p>}</p> <p>CopyExplain</p> <ul> <li>Information on these parameters can be found at     https://stadle-documentation.readthedocs.io/en/latest/documentation.html#configuration-of-agent.     Note that the aggregator IP and registration port values listed here     are placeholders and will be modified when connecting to the     STADLE\u00a0Ops platform.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Next, we\u00a0modify the local training\u00a0code to work with STADLE. We     first import the\u00a0requisite libraries:</p> <p>import argparse</p> <p>import tensorflow as tf</p> <p>from tensorflow import keras</p> <p>from sst_model import SSTModel, load_sst_data</p> <p>from stadle import BasicClient</p> <p>CopyExplain</p> <ul> <li>Once again, we add a command-line argument to specify which     partition of the training data the agent\u00a0should receive:</li> </ul> <p>parser = argparse.ArgumentParser()</p> <p>parser.add_argument(\\\"client_id\\\", type=int)</p> <p>args = parser.parse_args()</p> <p>client_id = args.client_id</p> <p>NUM_CLIENTS = 3</p> <p>(x_train,y_train), (x_test,y_test) = load_sst_data(client_id-1, NUM_CLIENTS)</p> <p>CopyExplain</p> <ul> <li>Next, we instantiate a\u00a0BasicClient\u00a0object -- this is the STADLE     client component that handles communication between the local     training process and the aggregators on the server side. We use the     configuration file defined earlier to create\u00a0this client:</li> </ul> <p>stadle_client = BasicClient(config_file=\\\"config_agent.json\\\", agent_name=f\\\"sst_agent_{client_id}\\\")</p> <p>CopyExplain</p> <ul> <li>Finally, we implement the FL training loop. In each round, the     client gets the aggregate model from the previous round (starting     with the base model) and trains it further on the local data before     sending it back to the aggregator through\u00a0the client:</li> </ul> <p>for round in range(3):</p> <p>sst_model = stadle_client.wait_for_sg_model()</p> <p>history = sst_model.fit(x_train, y_train, epochs=1)</p> <p>loss = history.history[\\'loss\\'][0]</p> <p>stadle_client.send_trained_model(sst_model, {\\'loss_training\\': loss})</p> <p>stadle_client.disconnect()</p> <p>CopyExplain</p> <ul> <li>The\u00a0wait_for_sg_model\u00a0function returns the latest aggregate model     from the server, and the\u00a0send_trained_model\u00a0function sends the     locally trained model with the desired performance metrics to the     server. More information on these integration steps can be     found\u00a0at\u00a0https://stadle-documentation.readthedocs.io/en/latest/usage.html#client-side-stadle-integration.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Now that\u00a0the client\u00a0side has been implemented, we can use the STADLE     Ops platform to start an aggregator and start an\u00a0FL process.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Creating a STADLE Ops project</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   First, go to stadle.ai\u00a0and create a new account. Once you are logged     in, you should be directed to the project information page in\u00a0STADLE     Ops:</p> <p>{width=\"6.268055555555556in\" height=\"2.0034722222222223in\"}</p> <p>Figure 8.3 -- Project information page in STADLE Ops</p> <ul> <li>Click on\u00a0Create New Project, then fill in the project     information and click\u00a0Create Project. The project information     page should have changed to show\u00a0the following:</li> </ul> <p>{width=\"6.268055555555556in\" height=\"1.8298611111111112in\"}</p> <p>Figure 8.4 -- New project added to the project information page</p> <ul> <li>Click on the plus icon under\u00a0Initiate Aggregator\u00a0to start a new     aggregator for the project, then click\u00a0OK\u00a0on the confirmation     prompt. You can now navigate to the\u00a0Dashboard\u00a0page on the left     side, resulting in a page that looks like\u00a0the following:</li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.50625in\"}</p> <p>Figure 8.5 -- Dashboard page of STADLE Ops</p> <ul> <li>Replace the\u00a0aggr_ip\u00a0and\u00a0reg_port\u00a0placeholder parameter values in     the\u00a0config_agent.json\u00a0file with\u00a0the values under\u00a0IP Address to     Connect\u00a0and\u00a0Port to\u00a0Connect, respectively.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   With this, we are now ready to begin the FL\u00a0training process.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Running the STADLE example</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The first\u00a0step is to send the base model object to the server,     allowing it to in turn distribute the model to the training     agents.\u00a0This is done with the\u00a0following command:</p> <p>stadle upload_model --config_path config_agent.json</p> <p>CopyExplain</p> <ul> <li>Once the command successfully runs, the\u00a0Base Model Info\u00a0section     on the STADLE Ops dashboard should update to show the model     information. We can now start the three agents by running     the\u00a0following commands:</li> </ul> <p>python fl_sim.py 1</p> <p>python fl_sim.py 2</p> <p>python fl_sim.py 3</p> <p>CopyExplain</p> <ul> <li>After three rounds, the agents will terminate and the final     aggregate model will be displayed in the project dashboard,     available for download in the Keras SavedModel format.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The user guide located     at\u00a0[https://stadle.ai/user_guide/guide]{.underline}\u00a0is     recommended for more information on the various functionalities of     the STADLE\u00a0Ops platform.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Evaluating the resulting aggregate models produced by each FL     framework results in the same conclusion---the performance of the     aggregate model essentially matches that of the centralized training     model.</p> <ul> <li>As explained in the\u00a0Dataset distributions\u00a0section of\u00a0section     7,\u00a0Model Aggregation, this is generally the expected result. The     natural question to\u00a0ask is how the performance is affected when the     local datasets are not IID---this is the focal point of the\u00a0next     section.</li> </ul> <p>Example -- the federated training of an image classification model on non-IID data</p> <ul> <li>In the previous\u00a0example, we\u00a0examined how a centralized deep learning     problem could be converted into an FL analog by training multiple     clients on disjoint subsets of the original training dataset     (the\u00a0local datasets) in an FL process. One key point of this local     dataset creation was that the subsets were created by random     sampling, leading to local datasets that were all IID under the same     distribution as the original dataset.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   As a result, the similar performance of FedAvg compared to the local     training scenario was expected -- each client's model essentially     had the same set of local minima to move toward during training,     making all local training beneficial for the\u00a0global objective.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Recall that in\u00a0section     7,\u00a0Model     Aggregation, we explored how FedAvg was susceptible to the     divergence in training objectives induced by severely non-IID local     datasets.</p> <ul> <li> <p>To explore the performance of FedAvg on varying non-IID severities,     this example trains the VGG-16 model (a simple deep-learning-based     image classification model) on constructed non-IID local datasets     sampled from the CIFAR-10 dataset (located     at\u00a0[https://www.cs.toronto.edu/\\~kriz/cifar.html]{.underline}).</p> </li> <li> <p>CIFAR-10 is a well-known simple image classification dataset     containing 60,000 images separated into 10 different classes; the     goal of models trained on CIFAR-10 is to correctly predict the class     associated with an input image.</p> </li> <li> <p>The relatively low complexity and ubiquity as a benchmark dataset     make CIFAR-10 ideal for exploring the response of FedAvg to\u00a0non-IID     data.</p> </li> </ul> <p>Important note</p> <ul> <li> <p>To avoid including redundant code samples, this section focuses on     the key lines of code that allow FL to be performed on PyTorch     models using non-IID local datasets.</p> </li> <li> <p>It is recommended that you go through the examples within     the\u00a0Example -- the federated training of an NLP model\u00a0section Here     prior to reading this section in order to understand the core     components needed for each FL framework.</p> </li> <li> <p>The implementations for this example can be found in full at this     book's GitHub repository     ([https://github.com/PacktPublishing/Federated-Learning-with-Python]{.underline}\u00a0tree/main/ch8/cv_code),     for use as\u00a0a reference.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The key point of this example is determining how the non-IID     datasets should be constructed.\u00a0We will change the class label     distributions of each local dataset by changing the number of images     of each class included in the training dataset.\u00a0</p> <ul> <li>For example, a dataset skewed toward cars and birds might have 5,000     images of cars, 5,000 images of birds, and 500 images for every     other class. By creating three disjointed subsets of\u00a0the 10 classes     and\u00a0constructing local datasets skewed toward these classes, we     produce three local datasets with non-IID severity proportional to     the number of images included from the classes\u00a0not selected.</li> </ul> <p>Skewing the CIFAR-10 dataset</p> <ul> <li>We first map\u00a0the three class subsets to client IDs, and set the     proportion of images to be taken from the original dataset for     selected classes (sel_count) and the other\u00a0classes (del_count):</li> </ul> <p>classes = (\\'airplane\\', \\'automobile\\', \\'bird\\', \\'cat\\', \\'deer\\',</p> <p>\\'dog\\', \\'frog\\', \\'horse\\', \\'ship\\', \\'truck\\')</p> <p>class_id_map = {</p> <p>1: classes[:3],</p> <p>2: classes[3:6],</p> <p>3: classes[6:]</p> <p>}</p> <p>sel_count = 1.0, def_count = 0.2</p> <p>CopyExplain</p> <ul> <li>We then sample the appropriate number of images from the original     dataset, using the indices of the images in the dataset to construct     the skewed\u00a0CIFAR-10 subset:</li> </ul> <p>class_counts = int(def_count * 5000) * np.ones(len(classes))</p> <p>for c in classes:</p> <p>if c in class_rank_map[self.rank]:</p> <p>class_counts[trainset.class_to_idx[c]] = int(sel_count * 5000)</p> <p>class_counts_ref = np.copy(class_counts)</p> <p>imbalanced_idx = []</p> <p>for i,img in enumerate(trainset):</p> <p>c = img[1]</p> <p>if (class_counts[c] &gt; 0):</p> <p>imbalanced_idx.append(i)</p> <p>class_counts[c] -= 1</p> <p>trainset = torch.utils.data.Subset(trainset, imbalanced_idx)</p> <p>CopyExplain</p> <ul> <li>The skewed\u00a0trainset is then used to create the     skewed\u00a0trainloader\u00a0for local training. When we refer to biasing the     training data going forward, this is the code that\u00a0is run.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   We will now demonstrate how to use different FL frameworks to run     this non-IID FL process. Please refer to the installation     instructions and framework-specific implementations in the previous     section,\u00a0Example -- the federated training of an NLP model, for     the\u00a0explanations of the basics omitted in\u00a0this section.</p> <p>Integrating OpenFL for CIFAR-10</p> <ul> <li>Similar to\u00a0the Keras\u00a0NLP example, we first create     the\u00a0ShardDescriptor\u00a0and\u00a0DataInterface\u00a0subclasses for the non-IID     CIFAR-10 datasets in\u00a0cifar_fl_dataset.py. Only a few changes need to     be made in order to accommodate the\u00a0new dataset.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   First, we modify the\u00a0self.data_by_type\u00a0dictionary to instead store     the modified\u00a0CIFAR datasets:</p> <p>train_dataset, val_dataset = self.load_cifar_data()</p> <p>self.data_by_type = {</p> <p>\\'train\\': train_dataset,</p> <p>\\'val\\': val_dataset</p> <p>}</p> <p>CopyExplain</p> <ul> <li>The\u00a0load_cifar_data\u00a0function loads in the training and test data     using\u00a0torchvision, then biases the training data based on the rank     passed to\u00a0the object.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Because the dimensions of a data element are now known (the size of     CIFAR-10 image), we also modify the shape properties with\u00a0fixed     values:</p> <p>@property</p> <p>def sample_shape(self):</p> <p>return [\\\"32\\\", \\\"32\\\"]</p> <p>@property</p> <p>def target_shape(self):</p> <p>return [\\\"10\\\"]</p> <p>CopyExplain</p> <ul> <li>We then implement the\u00a0CifarFedDataset\u00a0subclass of     the\u00a0DataInterface\u00a0class. No significant modifications are needed for     this implementation; thus, we can now use the biased CIFAR-10     dataset\u00a0with OpenFL.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   We now move to the actual FL process implementation (fl_sim.py). One     key difference is the framework adapter that must be used to create     the\u00a0ModelInterface\u00a0object from a\u00a0PyTorch model:</p> <p>model = vgg16()</p> <p>optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)</p> <p>criterion = nn.CrossEntropyLoss()</p> <p>framework_adapter = \\'openfl.plugins.frameworks_adapters.pytorch_adapter.FrameworkAdapterPlugin\\'</p> <p>MI = ModelInterface(model=model, optimizer=optimizer, framework_plugin=framework_adapter)</p> <p>CopyExplain</p> <ul> <li>The only other major change is modifying the train and validation     functions passed to the\u00a0TaskInterface\u00a0object to mirror the PyTorch     implementations of these functions from the local\u00a0training code.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The\u00a0last\u00a0step is to create the configuration files used by the     director and envoys. The only necessary change in the director     config is the updated\u00a0sample_shape\u00a0and\u00a0target_shape\u00a0for the\u00a0CIFAR-10     data:</p> <p>settings:</p> <p>listen_host: localhost</p> <p>listen_port: 50051</p> <p>sample_shape: [\\\"32\\\",\\\"32\\\"]</p> <p>target_shape: [\\\"10\\\"]</p> <p>CopyExplain</p> <ul> <li>This is saved\u00a0in\u00a0director/director_config.yaml.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The envoy configuration files require no changes outside of updating     the object and filenames -- the directory structure should look     like\u00a0the following:</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   director</p> <pre><code>-   director_config.yaml\n</code></pre> <ul> <li> <p>experiment</p> <ul> <li> <p>envoy_config_1.yaml</p> </li> <li> <p>envoy_config_2.yaml</p> </li> <li> <p>envoy_config_3.yaml</p> </li> <li> <p>cifar_fl_dataset.py</p> </li> <li> <p>fl_sim.py</p> </li> </ul> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   You can\u00a0refer\u00a0to\u00a0Running the OpenFL example\u00a0in the\u00a0Integrating     OpenFL for SST-2\u00a0section to run\u00a0this example.</p> <p>Integrating IBM FL for CIFAR-10</p> <ul> <li>Recall\u00a0that IBM FL\u00a0requires a saved version of the model used during     training. We first run the following code     in\u00a0create_saved_model.py\u00a0to create the saved VGG-16\u00a0PyTorch model:</li> </ul> <p>import torch</p> <p>from torchvision.models import vgg16</p> <p>model = vgg16()</p> <p>torch.save(model, \\'saved_vgg_model.pt\\')</p> <p>CopyExplain</p> <ul> <li>Next, we create the\u00a0DataHandler\u00a0subclass for the skewed CIFAR-10     datasets. The only core change is the modification of     the\u00a0load_and_preprocess_data\u00a0function to instead load in the     CIFAR-10 data and bias the\u00a0training set.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The next step is to create the configuration JSON files used when     starting the aggregator and initializing the parties. No significant     changes to the aggregator config (agg_config.json) are necessary,     and the only core change in the party config is the modification of     the model information to work\u00a0with PyTorch:</p> <p>\\\"model\\\": {</p> <p>\\\"name\\\": \\\"PytorchFLModel\\\",</p> <p>\\\"path\\\": \\\"ibmfl.model.pytorch_fl_model\\\",</p> <p>\\\"spec\\\": {</p> <p>\\\"model-name\\\": \\\"vgg_model\\\",</p> <p>\\\"model_definition\\\": \\\"saved_vgg_model.pt\\\",</p> <p>\\\"optimizer\\\": \\\"optim.SGD\\\",</p> <p>\\\"criterion\\\": \\\"nn.CrossEntropyLoss\\\"</p> <p>}</p> <p>},</p> <p>CopyExplain</p> <ul> <li>The code in\u00a0fl_sim.py\u00a0responsible for starting up the parties can     essentially remain unmodified due to the extensive use of     the\u00a0configuration files.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   You\u00a0can\u00a0refer to\u00a0Running the IBM FL example\u00a0in the\u00a0Integrating     IBM FL for SST-2\u00a0section to run\u00a0this example.</p> <p>Integrating Flower for CIFAR-10</p> <ul> <li>After\u00a0loading in the CIFAR-10 data and biasing the training data,     the\u00a0core change needed for the Flower implementation is     the\u00a0NumPyClient\u00a0subclass. Unlike the Keras example,     the\u00a0get_parameters\u00a0and\u00a0set_parameters\u00a0methods rely on the PyTorch     model state dictionaries and are a bit\u00a0more involved:</li> </ul> <p>class CifarClient(fl.client.NumPyClient):</p> <p>def get_parameters(self, config):</p> <p>return [val.numpy() for _, val in model.state_dict().items()]</p> <p>def set_parameters(self, parameters):</p> <p>params_dict = zip(model.state_dict().keys(), parameters)</p> <p>state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})</p> <p>model.load_state_dict(state_dict)</p> <p>CopyExplain</p> <ul> <li>We modify the\u00a0fit\u00a0function to mirror the training code in the local     training example and modify the evaluate function to similarly     mirror the local training evaluation code. Note that we     call\u00a0self.set_parameters(parameters)\u00a0in order to update the local     model instance with the most\u00a0recent weights.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   We also set the\u00a0grpc_max_message_length\u00a0parameter to 1 GB when     starting the Flower client and server to accommodate the larger     VGG16 model size. The client initialization function is now\u00a0the     following:</p> <p>fl.client.start_numpy_client(</p> <p>server_address=\\\"[::]:8080\\\",</p> <p>client=CifarClient(),</p> <p>grpc_max_message_length=1024**3</p> <p>)</p> <p>CopyExplain</p> <ul> <li>Finally, we\u00a0modify\u00a0the aggregator code in\u00a0server.py\u00a0-- the custom     strategy we used previously to save the aggregate model at the end     of the last round needs to be modified to work with\u00a0PyTorch models:</li> </ul> <p>if (server_round == MAX_ROUNDS):</p> <p>vgg_model = vgg16()</p> <p>np_weights = fl.common.parameters_to_ndarrays(agg_weights[0])</p> <p>params_dict = zip(vgg_model.state_dict().keys(), np_weights)</p> <p>state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})</p> <p>torch.save(state_dict, \\\"final_agg_vgg_model.pt\\\")</p> <p>CopyExplain</p> <ul> <li>With this strategy, we can run the following line to start the     server (adding the\u00a0grpc_max_message_length\u00a0parameter here\u00a0as well):</li> </ul> <p>fl.server.start_server(</p> <p>strategy=SavePyTorchModelStrategy(),</p> <p>config=fl.server.ServerConfig(num_rounds=MAX_ROUNDS),</p> <p>grpc_max_message_length=1024**3</p> <p>)</p> <p>CopyExplain</p> <ul> <li>Refer to\u00a0Running the Flower example\u00a0in the\u00a0Integrating Flower for     SST-2\u00a0section to run\u00a0this\u00a0example.</li> </ul> <p>Integrating STADLE for CIFAR-10</p> <ul> <li>We first\u00a0modify\u00a0the\u00a0config_agent.json\u00a0config file to use the VGG16     model from the\u00a0torchvision\u00a0library:</li> </ul> <p>{</p> <p>\\\"model_path\\\": \\\"./data/agent\\\",</p> <p>\\\"aggr_ip\\\": \\\"localhost\\\",</p> <p>\\\"reg_port\\\": \\\"8765\\\",</p> <p>\\\"token\\\": \\\"stadle12345\\\",</p> <p>\\\"base_model\\\": {</p> <p>\\\"model_fn\\\": \\\"vgg16\\\",</p> <p>\\\"model_fn_src\\\": \\\"torchvision.models\\\",</p> <p>\\\"model_format\\\": \\\"PyTorch\\\",</p> <p>\\\"model_name\\\": \\\"PyTorch-VGG-Model\\\"</p> <p>}</p> <p>}</p> <p>CopyExplain</p> <ul> <li>To integrate STADLE into the local training code, we initialize     the\u00a0BasicClient\u00a0object and modify the training loop to send the     local model every two local training epochs and wait for the     new\u00a0aggregate model:</li> </ul> <p>stadle_client = BasicClient(config_file=\\\"config_agent.json\\\")</p> <p>for epoch in range(num_epochs):</p> <p>state_dict = stadle_client.wait_for_sg_model().state_dict()</p> <p>model.load_state_dict(state_dict)</p> <p># Normal training code...</p> <p>if (epoch % 2 == 0):</p> <p>stadle_client.send_trained_model(model)</p> <p>CopyExplain</p> <p>Note</p> <ul> <li> <p>The code located     at\u00a0[https://github.com/PacktPublishing/Federated-Learning-with-Python]{.underline}\u00a0contains     the full implementation of this integration example for reference.</p> </li> <li> <p>To start an aggregator and perform FL with the CIFAR-10 STADLE     example, please refer to\u00a0Creating a STADLE Ops     project\u00a0and\u00a0Running the STADLE example\u00a0in the\u00a0Integrating STADLE     for\u00a0SST-2\u00a0subsection.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Testing\u00a0different\u00a0levels of bias in the constructed local datasets     should lead to the same conclusion stated in the\u00a0Dataset     distributions\u00a0section of\u00a0section     7,\u00a0Model     Aggregation\u00a0for non-IID cases---as the non-IID severity increases,     the convergence speed and model performance decrease.</p> <ul> <li> <p>The goal of this section was to build off of the understanding of     each FL framework from the SST-2 example, highlighting the key     changes necessary to work with a PyTorch model on a modified     dataset.</p> </li> <li> <p>Using this section alongside the code examples     in\u00a0[https://github.com/PacktPublishing/Federated-Learning-with-Python]{.underline}\u00a0should     help in understanding this\u00a0example integration.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>Here, we covered several FL frameworks through the context of two     different examples.</p> </li> <li> <p>From the first example, you learned how a traditional centralized ML     problem can be converted into the analogous FL scenario by     separating the data into disjointed subsets.</p> </li> <li> <p>It is now clear that random sampling leads to local datasets that     are IID, allowing FedAvg to reach the same level of performance as     the centralized equivalent with any of the FL frameworks.</p> </li> <li> <p>In the second example, you learned one of the many ways a group of     datasets can be non-IID (different class label distributions) and     observed how different severities of non-IID datasets affect the     performance of FedAvg. We encourage you to explore how alternative     aggregation methods can improve on FedAvg in\u00a0these cases.</p> </li> <li> <p>Both examples also should have given you a solid understanding of     the general trends when working with different FL frameworks; while     the specific implementation-level details may change (due to the     rapidly changing field), the core concepts and implementation     details will remain fundamentals.</p> </li> <li> <p>In the next section, we continue our transition to the business     application side of FL by taking a look at several case studies     involving the application of FL to\u00a0specific domains.</p> </li> </ul> <p>Case\u00a0Studies with Key Use Cases of Federated Learning Applications</p> <ul> <li> <p>Federated learning\u00a0(FL) has\u00a0met with a variety of AI     applications so far in various contexts and integration has been     explored with trials and errors in those fields. One of the most     popular areas has been in the medical and healthcare fields where     the concept of privacy-preserving AI naturally fits with the current     needs and challenges of healthcare AI.</p> </li> <li> <p>FL has also been applied to the financial services industry, edge     computing devices, and the\u00a0Internet of Things\u00a0(IoT),     through\u00a0which FL has been shown to have significant benefits in     quite a few applications, which will resolve many important\u00a0social     problems.</p> </li> <li> <p>Here, we will be discussing some of the major use cases of FL in     different fields. It is our hope that by the end of This section,     you'll be familiar with some of the applications of FL in different     industries.</p> </li> <li> <p>We\\'ll start by exploring the use of FL in the healthcare and     financial industries before making the transition to the edge     computing and IoT sectors. Finally, we will conclude the section by     discussing the intersection of FL and distributed learning for\u00a0big     data.</p> </li> <li> <p>Here, we will cover the\u00a0following topics:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Applying FL to the\u00a0healthcare sector</p> <ul> <li> <p>Applying FL to the\u00a0financial sector</p> </li> <li> <p>FL meets\u00a0edge computing</p> </li> <li> <p>Moving towards the Internet\u00a0of Intelligence</p> </li> <li> <p>Applying FL to distributed learning for\u00a0big data</p> </li> </ul> <p>Applying FL to the healthcare sector</p> <ul> <li> <p>FL\u00a0used in healthcare is a topic\u00a0that gained quite a lot of     attention in the last couple of years. Healthcare advances can have     an enormous effect on our lives.</p> </li> <li> <p>However, several challenges make these advances perhaps more     difficult than in other domains. Let's begin by discussing some of     the common challenges that exist and are preventing the further     development of AI\u00a0in healthcare.</p> </li> </ul> <p>Challenges in healthcare</p> <ul> <li> <p>One of the\u00a0primary challenges is data accessibility.\u00a0</p> </li> <li> <p>Data accessibility\u00a0is not an issue unique to healthcare. It is a     huge problem across the AI industry and will only become a greater     challenge as time goes on.</p> </li> <li> <p>It is a core problem in the development of AI in healthcare and we     will touch briefly on some of the reasons why it is an issue here.     We will also continue to revisit this major hurdle, addressing     problems and solutions from many different angles and applications.</p> </li> <li> <p>This strategy will allow you to understand the many different     aspects, complexity, and drivers of\u00a0the problem.</p> </li> <li> <p>The data accessibility problem has\u00a0many components:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Privacy regulations: The\u00a0main bottleneck of the data     accessibility issue is centered around the privacy regulations put     in place to protect personal data; these regulations are absolutely     necessary and should be in place. Rather than circumventing these     regulations, we'll be discussing how we can work in parallel with     them, keeping these valuable regulations in place, and at the same     time, making use of the intelligence that this data can provide. You     can think of this as a best-of-both-worlds scenario. Several     important privacy regulations are discussed in the\u00a0Data privacy as     a bottleneck\u00a0section of\u00a0section     1,\u00a0Challenges     in Big Data and\u00a0Traditional AI.</p> <ul> <li> <p>Lack of data/need for real data: Few areas hold as much promise     for providing a positive societal impact as healthcare does. Yet,     the healthcare industry has fallen far behind in capitalizing on all     of the many benefits that AI has to offer. One reason for this is     that for AI and ML models to learn effectively, they need large     amounts of data. We'll discuss more on the need for large amounts of     data throughout This section. This is the limiting factor for AI. In     healthcare, there are many regulations in place that prevent these     models from utilizing the data in any way, and\u00a0rightfully so.</p> </li> <li> <p>Many data types from many places: As we'll discuss further,     there are many different data types from many different places. Data     can be in the form of text, video, images, or speech, which is     stored in many different places. Aside from the ability to access     data from many different locations, which is a\u00a0major challenge on     its own, these\u00a0institutions store data in various formats\u00a0as well.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   As mentioned, some of the earliest work in the application of FL has     been in the healthcare space. Within this large field, there are     several ways FL can help solve problems. Let's take a look at just a     few of the areas where FL has great potential to transform\u00a0the     healthcare system. Some of these areas include medical imaging, drug     discovery, and\u00a0Electronic Health\u00a0Records\u00a0(EHRs).</p> <ul> <li>Let's start by taking a closer look at how FL is being used     in\u00a0medical\u00a0imaging.</li> </ul> <p>Medical imaging</p> <ul> <li> <p>There is\u00a0a lot of optimism surrounding FL in the medical imaging     space as discussed in\u00a0A Comprehensive Analysis of Recent Deep and     Federated-Learning-Based Methodologies for Brain Tumor Diagnosis,     which is listed in the\u00a0Further reading\u00a0section Here.</p> </li> <li> <p>These high expectations are, in part, due to some of the challenges     that need to be addressed and the capability of FL to overcome these     hurdles. One of these challenges is needing large amounts\u00a0of data.</p> </li> <li> <p>Large amounts of medical imaging data are created every day as the     medical imaging industry continues to develop better equipment,     procedures, and facilities. The exponential growth of this data is a     huge opportunity for healthcare providers to develop better ML     models and increase the quality\u00a0of healthcare.</p> </li> <li> <p>Another reason\u00a0for the optimism around FL having a positive impact     on medical imaging is the already\u00a0proven success of\u00a0machine     learning\u00a0(ML) -- more     specifically,\u00a0deep\u00a0learning\u00a0(DL).</p> </li> <li> <p>Let's take a brief look at DL to understand better why it is so     important when dealing with large amounts of data. DL is a subspace     of ML encompassed by the AI umbrella.</p> </li> <li> <p>DL is different from ML in that it uses several layers of what are     known as neural networks. Several books have been written on DL and     neural networks as a singular subject, so we won't attempt to     explain these in greater detail in this book. For more in-depth     coverage of DL and neural networks,\u00a0Advanced Deep Learning with     Python is a great book to read.</p> </li> <li> <p>For our general discussion, we'll provide a very\u00a0basic explanation.</p> </li> <li> <p>The following\u00a0figure,\u00a0Figure 9.1, shows a simple example of a     neural network being used to help classify types of brain tumors     using\u00a0medical imaging:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.765277777777778in\"}</p> <p>Figure 9.1 -- Tumor classification with a neural network</p> <ul> <li> <p>On the left-hand side of the figure, we have an image of a brain     that has one of two types of tumors. The image is broken down into     numbers that represent the image pixels.</p> </li> <li> <p>These numbers are added to the neural network. The hidden layers     utilize different weights for these numbers and produce different     outputs through activation functions. Finally, we can see the two     output layers. In this case, there are two possible outputs. One     is\u00a0Tumor is Type 1\u00a0and the other is\u00a0Tumor is\u00a0Type 2.</p> </li> </ul> <p>Single location data is limited</p> <ul> <li>As you can see, DL models require a lot of data to train. Generally,     a single data repository has only a small amount of data, limiting     the ability of any model to\u00a0generalize well.</li> </ul> <p>Possible solutions to data accessibility challenges</p> <ul> <li> <p>One\u00a0solution is to utilize privacy-preserving FL, which can make use     of all the data available in multiple centers while keeping     sensitive\u00a0data private.</p> </li> <li> <p>FL enables the deployment of large-scale ML models trained in     different data centers without sharing\u00a0sensitive data.</p> </li> <li> <p>In FL, rather than moving the data to the model to be trained, we     move the model to the data and only bring back the intelligence     gathered from the data, referred to as\u00a0Intelligence from     Data\u00a0(IfD), discussed\u00a0later in the\u00a0Potential of IfD\u00a0section     in\u00a0This section.</p> </li> </ul> <p>Example use case -- ML in hospitals</p> <ul> <li> <p>Let's\u00a0walk through an example of how FL could be applied to medical     imaging data. This example is actually what was done in an     international challenge focused on brain tumors. The goal here is to     segment the tumor using\u00a0MRI scans.</p> </li> <li> <p>For this example, we're going to use three hospitals. We will label     them Hospital A, Hospital B, and Hospital C. Each hospital has     anonymized private medical imaging data that is stored locally. Each     hospital begins with a learning model, which you can see in     the\u00a0following diagram:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"5.235416666666667in\"}</p> <p>Figure 9.2 -- Hospitals sharing ML models for FL</p> <ul> <li> <p>Each hospital runs the model locally; this creates what is referred     to as a local model. It's important to note that each one of the     hospital's local models will be different at this point. Each of     them has only trained on the data that resides at\u00a0their hospital.</p> </li> <li> <p>The\u00a0intelligence from training these three local models is sent to a     centralized server in the form of model parameters. The server     gathers the local models and combines them to create a global model.     This global model, a combination of the intelligence from all three     local models, is then sent back to each hospital and is again     trained locally only on that\u00a0hospital's data.</p> </li> <li> <p>Again, only the intelligence from these models is sent back to the     server for aggregation. This process is repeated until the model has     learned all it can (known\u00a0as convergence).</p> </li> <li> <p>Utilizing\u00a0FL, you can train models that perform as if all the data     came from a single location even when data resides at different     locations. As you can see, the implementation of privacy-preserving     methods such as this one has the power to revolutionize the field\u00a0of     medicine.</p> </li> <li> <p>Let's now take a look at how FL can improve the drug\u00a0discovery     space.</p> </li> </ul> <p>Drug discovery</p> <ul> <li>Data has\u00a0become somewhat of a new currency in our modern world. For     pharmaceutical companies, especially, the use of this data to     provide\u00a0personalized medicine\u00a0has become a major focus. In the     years ahead, companies that can make use of more data will be far     more competitive. This will be one of the defining strategies for     the future success of\u00a0any organization.</li> </ul> <p>Precision medicine</p> <ul> <li>Personalized medicine, also\u00a0known as\u00a0precision medicine, relies     heavily on large amounts of\u00a0real-world data\u00a0to make this possible.     In addition, ML algorithms are needed to process and analyze this     data in order to extract meaningful insights. As we will discuss,     accessing significant amounts of real data is currently very     difficult, if\u00a0not impossible.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   In\u00a0Figure 9.3, we show some of the areas where precision medicine     will have an\u00a0immediate impact:</p> <p>{width=\"6.268055555555556in\" height=\"6.09375in\"}</p> <p>Figure 9.3 -- Precision medicine impacting many areas</p> <ul> <li>As you can see from\u00a0Figure 9.3, precision\u00a0medicine covers a vast     area of different fields and disciplines, such as oncology,     wellness, diagnostics, research, and\u00a0health monitoring.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Currently, many AI solutions that are deployed in the healthcare     systems fail due to being created with small datasets that only     represent a fraction of the patient population. Researchers and     developers can validate and improve models using data from many     sources\u00a0without ownership.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   To access this data so that it can be processed and analyzed, a new     approach is needed. This is where FL comes in. As we've covered     throughout this book, FL provides access to the intelligence needed,     and not the\u00a0data itself.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Precision medicine\u00a0is a model that proposes that rather than     utilizing a\u00a0one-size-fits-all\u00a0approach, it should customize     healthcare to individuals for better results in terms of drug     effectiveness and cancer treatment outcomes, among other things. To     do this, precision medicine relies heavily on large amounts of     real-world data. Accessing large amounts of real-world data is the     first hurdle that must be overcome to realize precision medicine\u00a0at     scale.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Let's look at the current approach to see how FL can provide     the\u00a0needed answer.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Shown in\u00a0Figure 9.4\u00a0is the current common approach to ML system     implementation. Here, the data is transmitted to a central server,     where all of the data is gathered together, and algorithms are then     trained on this aggregated data. This is known as the\u00a0bringing the     data to the\u00a0model\u00a0approach:</p> <p>{width=\"5.467361111111111in\" height=\"4.206944444444445in\"}</p> <p>Figure 9.4 -- Precision medicine now</p> <ul> <li>It's easy to imagine the immense cost of moving data from many     hospitals into one centralized place in this fashion. Processing     data in this way also compromises data security and makes regulatory     compliance difficult, if\u00a0not impossible.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   As shown in\u00a0Figure 9.5, the FL approach is quite different.     Instead of\u00a0bringing the data to the model, the\u00a0model is moved to     the data. This means that the ML model trains on local data and     only sends the intelligence to the FL server for aggregation, thus     allowing the various local models to benefit from updates to     the\u00a0global model:</p> <p>{width=\"5.177777777777778in\" height=\"3.7395833333333335in\"}</p> <p>Figure 9.5 -- Precision medicine with FL</p> <ul> <li>The FL\u00a0approach allows for efficient model transfers and data     security while\u00a0being compliance-friendly.</li> </ul> <p>Potential of IfD</p> <ul> <li> <p>Utilizing\u00a0FL to gain access to real-world data has huge potential to     improve all of the clinical research stages. Accessing this kind of     data allows us to utilize the intelligence gathered and IfD can     dramatically accelerate the processes and steps in drug discovery.</p> </li> <li> <p>One important idea to keep in mind when discussing how FL works is     that the training data never leaves\u00a0the device.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Figure 9.6\u00a0describes the process of extracting the ML model from     training data through the\u00a0training process:</p> <p>{width=\"6.268055555555556in\" height=\"1.8090277777777777in\"}</p> <p>Figure 9.6 -- An IfD diagram</p> <ul> <li>As you\u00a0can see in\u00a0Figure 9.6, the data is used locally to train     the ML model. In FL, the model is located on the device itself,     where it is trained, and only the model weights are sent for     aggregation -- so only the intelligence from the data, not the\u00a0data     itself.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The ability to gather IfD collected from multiple sources, as well     as a wide range of data types, including video, text, speech, and     other sensory data, can help improve enrollment processes, both in     terms of the speed of doing so and finding the right match for the     research and development\u00a0of treatments.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   This is especially important for rare diseases and cancers. When     identifications are made in this way, subjects can be notified     of\u00a0trial opportunities.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Federated access to data and the collection of IfD can open up     access to a substantial amount of data worldwide. This allows for     aggregated data repositories to mine a sufficient number of patients     fulfilling the protocol criteria. Potentially, this could allow all     participants in a trial to receive the actual drug and not\u00a0the     placebo.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Ultimately, precision medicine at scale is not possible without     robust AI and robust AI can only be trained with large amounts of     real-world data. Using FL could allow for improved outcome     measurements. In the coming years, a federated approach has the     potential to drive advancements and innovation in the discovery of     new medical treatments in new ways that have not been\u00a0possible     before.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   In the following figure,\u00a0Figure 9.7, we show a generalized view of     how FL\u00a0collects IfD:</p> <p>{width=\"6.268055555555556in\" height=\"4.642361111111111in\"}</p> <p>Figure 9.7 -- A generalized view of how FL collects IfD</p> <ul> <li> <p>As\u00a0shown in\u00a0Figure 9.7\u00a0here, all the data remains isolated within     each organization and is not transferred to the\u00a0federated server.</p> </li> <li> <p>Let's now move forward and discuss an FL application\u00a0with EHRs.</p> </li> </ul> <p>EHRs</p> <ul> <li>An EHR is a\u00a0collection of health information that is systematically     and digitally stored. These records are designed to be shared     with\u00a0healthcare providers\u00a0(HCPs) when\u00a0appropriate. According     to\u00a0HealthIT.gov\u00a0statistics, as of 2017, 86% of office-based     physicians have adopted EHRs in the\u00a0United States.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Figure 9.8\u00a0depicts the\u00a0EHRs\u00a0collected from various sources, such     as hospitals and insurers. In\u00a0this figure, we use the     terms\u00a0Electronic Medical Records\u00a0(EMRs) and\u00a0Personal     Health\u00a0Records\u00a0(PHRs):</p> <p>{width=\"6.268055555555556in\" height=\"4.810416666666667in\"}</p> <p>Figure 9.8 -- EHRs</p> <ul> <li>This adoption of EHRs has laid the groundwork for beneficial     collaboration between healthcare organizations. As we've discussed     throughout the book, the ability to access more real-world data     allows AI models trained on this data to be much more robust\u00a0and     effective.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Although this foundation has been put in place, there are still many     challenges to the traditional ML approach in terms of sharing EHR     data across multiple institutions.</p> <ul> <li> <p>These include privacy concerns and regulations as well as data     standardization.</p> </li> <li> <p>One of the\u00a0major problems is the storage of this data in\u00a0Central     Data Repositories\u00a0(CDRs), as shown in\u00a0Figure 9.9, where     various forms of local data are stored to be trained to create an\u00a0ML     model.</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"2.834722222222222in\"}</p> <p>Figure 9.9 -- A centralized data mining method</p> <ul> <li>This CDR approach is not ideal because of the data isolation     problem, which is discussed in the\u00a0following section.</li> </ul> <p>The data isolation problem</p> <ul> <li>The use of CDRs for\u00a0data storage brings many problems. Some examples     include things such as\u00a0data leakage,\u00a0hefty regulations, and     a\u00a0high cost\u00a0to set up\u00a0and maintain.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The effect that data storage methods have on the quality and     useability of the data itself is just as important. ML models     trained on a single center data usually cannot generalize well when     compared to data that has been gathered from\u00a0multiple locations.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   FL allows model training collaboration across multiple     organizations, resulting in the production of superior model     performance without violating data\u00a0privacy regulations.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Let's take a look at an example of an FL application\u00a0with EHRs.</p> <p>Representation learning in EHRs</p> <ul> <li>Researchers have\u00a0applied FL to representation learning in EHRs as     mentioned in the\u00a0Further reading\u00a0list such as\u00a0Two-stage federated     phenotyping and patient representation learning.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   They used FL with\u00a0natural language processing\u00a0(NLP)     for\u00a0phenotyping and representation learning with patient data. In     the first stage, a representation of patient data is created based     on some medical records from several hospitals without sharing the     raw data itself.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The representation that has been learned is not limited to any     specific medical task. In the second stage, an ML model for specific     phenotyping work is trained in a federated manner using the related     features derived from the\u00a0learned representations.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   FL has been showcased as an effective alternative to current     methodologies in the advancement of AI developments in healthcare,     whether in drug discovery, medical imaging, or the\u00a0analysis\u00a0of EHRs.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Let's move on to the financial sector as another promising use case     related to FL in the\u00a0following section.</p> <p>Applying FL to the financial sector</p> <ul> <li> <p>In the US alone, financial\u00a0services firms spend billions of dollars     every year on compliance to combat laundering, yet the current     system is so ineffective that less than 1% of money laundering     activities are thwarted. In fact, it's estimated that firms spend     roughly 100 times more money than they are able to recover from this     criminal activity.</p> </li> <li> <p>Only a small percentage of transactions\u00a0are caught by\u00a0anti-money     laundering\u00a0(AML) systems, and an even smaller\u00a0percentage of     those alerts are eventually reported in\u00a0suspicious activity     reports\u00a0(SARs), as required\u00a0by the\u00a0Bank Secrecy     Act\u00a0(BSA)\u00a0of 1970.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Conservatively speaking, the value of information coming from a     network of banks is many times higher than the information any one     bank has. This is because you can see not just where the money came     from, but also where\u00a0it went.</p> <p>Anti-Money Laundering (AML)</p> <ul> <li>The current AML\u00a0system needs major improvements, with several     challenges that need to be overcome. Many privacy regulations are in     place to protect personal financial data. These regulations vary     from institution to institution and region\u00a0to region.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   One solution is a collaboration between financial institutions. This     collaboration would allow these institutions to share intelligence     gathered from their own data in a mutually beneficial way. Rather     than moving data between collaborators, the ML models themselves     would be deployed locally at each institution.</p> <ul> <li>This would allow only the IfD to be shared and benefit each     collaborator that could utilize the intelligence gathered. As we've     discussed, FL has\u00a0this capability.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   In FL, the models themselves move from institution to institution.     While doing so, the models adjust parameters and become more     intelligent as they learn from more data. This is different from     these two alternatives\u00a0to FL:</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   One option is to collect the data from the collaborating financial     institutions into one central repository. However, this approach     isn't possible due to customer\u00a0privacy regulations.</p> <ul> <li>Another approach is to share some kind of identifier as necessary.     However, again due to privacy laws and regulations, this is not     possible and could only be used as part of an\u00a0investigation process.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   To be able\u00a0to use FL, we need to be able to ensure that the privacy     and security of data remain intact. To do so, we must ensure that     the models are being trained in a financial institution without     privacy violations. Lastly, we need to ensure that all communication     related to model parameter updates sent to the federated server is     safe\u00a0and secure.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   FL can improve the effectiveness, efficiency, and fairness of the     global BSA and AML regimes. In the following section, we will look     at using FL in the context of transaction monitoring implementation     across\u00a0AML disciplines.</p> <p>Proposed solutions to the existing AML approach</p> <ul> <li>The\u00a0development of FL approaches across AML disciplines includes the     essential topic of customer onboarding and it may help to use     non-traditional information to verify the identities of\u00a0potential     customers.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   A new approach is needed that can leverage sophisticated technology     to increase the awareness and efficiency of risk\u00a0detection systems.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   This approach should enable\u00a0the following:</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Enable firms and regulators to learn from each other without sharing     sensitive or\u00a0protected data</p> <ul> <li> <p>Enhance the ability of firms to identify accurately real risks and     reduce unfounded\u00a0risk reporting</p> </li> <li> <p>Improve the risk-reward calculi of firms when making decisions about     whether to serve\u00a0specific markets</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The AML framework serves a vital national interest in preventing any     intent of harm, whether through terrorism, money laundering, fraud,     or human trafficking, using the global financial system for     those\u00a0illegal purposes.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Although there has been enormous investment in and attention to     money laundering risk detection systems, the system is broken. Firms     invest considerable resources to satisfy AML requirements but get     little feedback on the quality of their\u00a0risk reporting.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Two key\u00a0factors have driven firms out of operating in\u00a0specific     markets:</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The first is the costs of risk rating activity that occurs within     the money\u00a0services industry</p> <ul> <li>The second key factor is the regulatory risks and reputational     impact for financial firms connected to illicit\u00a0financial activities</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   As a result, globally, correspondent banking relationships have     fallen by 25%. Since 2009, 25% of banks in emerging markets have     reported correspondent\u00a0banking losses.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   In\u00a0Figure 9.10, transaction reporting is depicted and each     institution's database of risk patterns reflects its experience     with\u00a0illicit activities:</p> <p>{width=\"6.268055555555556in\" height=\"4.373611111111111in\"}</p> <p>Figure 9.10 -- Institutional reporting of suspected illicit activity</p> <ul> <li>Institutions won't necessarily know about patterns their competitors     are picking up or what the government knows about which transactions     flagged are suspicious or genuine. Firms get little timely feedback     on the accuracy of the reports they submit. The result is that firms     lack the most vital information for improving their risk detection     capabilities: timely information about\u00a0confirmed problems.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   So, imagine\u00a0transaction monitoring that is more effective,     efficient, and fair in this scenario. The bank acts as a     utility-like hub.</p> <ul> <li> <p>Powerful computers combined with smart algorithms could be deployed     to evaluate data at different institutions.</p> </li> <li> <p>The ML model that has learned the risk patterns would then move     between the participating firms to pick up the patterns and learn     from the risk at each institution. All of this could be done without     sharing sensitive or protected data.</p> </li> <li> <p>This is depicted in\u00a0Figure 9.11:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.429861111111111in\"}</p> <p>Figure 9.11 -- The data and intelligence flow within a banking ecosystem</p> <ul> <li> <p>In the\u00a0FL approach, the bank creates a classification algorithm that     trains on each participating firm's data.</p> </li> <li> <p>The bank develops a key model and model parameters that reflect     insights from all participating firms and the data in the     government's possession.</p> </li> <li> <p>The bank distributes the key model and model parameters to the     participating firms while the data stays in each institution. These     distributed models adopt the risk patterns in those firms by     learning from their local data and then sending them back to\u00a0the     bank.</p> </li> </ul> <p>Demo of FL in the AML space</p> <ul> <li> <p>The\u00a0researchers at TieSet, Inc. have\u00a0conducted an experiment of     applying FL to the AML space over STADLE, using some synthetic     transaction data generated by the PaySim mobile money simulator     ([https://www.kaggle.com/ealaxi/paysim1]{.underline}).</p> </li> <li> <p>They have used supervised learning with logistic regression where     model features include time, amount, and the new and old balance of     the original account and the destination account.</p> </li> <li> <p>The dataset has 636,2620 transactions (8,213 fraud transactions and     635,4407 valid transactions), which are split into 10 separate\u00a0local     agents.</p> </li> <li> <p>Figure 9.12\u00a0is the\u00a0outcome of applying FL to AML\u00a0where the     precision score and F1 score are plotted at each round of training.     In the figure, the thicker line is the performance of the aggregated     model, and the thin lines are the results of individual agents     training separately only using\u00a0local data:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.0034722222222223in\"}</p> <p>Figure 9.12 -- Outcome of applying FL to AML (thicker line: aggregated model, thin lines: individual agent training separately)</p> <ul> <li> <p>As in\u00a0Figure 9.12, the aggregated model performs in a quite stable     manner, constantly achieving more than 90% in terms of precision and     F1 score. FL could reduce the fraud transactions down from the total     fraud transactions of $1,241,770,000 to $65,780,000, meaning only     5.3% of fraud transactions\u00a0are missing.</p> </li> <li> <p>Let's conclude this section by looking at a list of benefits that FL     provides for\u00a0risk detection.</p> </li> </ul> <p>Benefits of FL for risk detection systems</p> <ul> <li>There are\u00a0several benefits in financial sectors for applying FL to     risk detection systems\u00a0as follows:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   FL can create a larger dataset of risk detection algorithms to\u00a0train     on</p> <ul> <li> <p>Improved accuracy of illicit\u00a0activity detection</p> </li> <li> <p>Provides a way for organizations\u00a0to collaborate</p> </li> <li> <p>Firms can enter\u00a0new markets</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The ability to identify money laundering tactics within your own     financial institution is limited by the data you have access to.     Data sharing restrictions make collaboration difficult, if not     impossible.</p> <ul> <li> <p>The solutions and advantages that FL brings to the finance industry     are numerous. Additional advantages include better operational     efficiency and better allocation of human capital.</p> </li> <li> <p>There is no limit to the application of FL, especially in the     financial space, due to the ability to extract intelligence     from\u00a0client data.</p> </li> <li> <p>We'll now be switching gears a bit as we move on to discussing the     use of FL in several emerging technologies in the\u00a0following section.</p> </li> </ul> <p>FL meets edge computing</p> <ul> <li> <p>The section in this\u00a0section is a\u00a0mixture of different areas, some of     which are emergent technologies.</p> </li> <li> <p>These areas are all very interconnected, as we will cover. Many of     these technologies depend on one another to overcome their own     challenges and limitations.</p> </li> <li> <p>Combining these technologies alongside FL is an especially potent     combination of technology that is sure to be key to advancements and     innovation in the\u00a0coming years.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   In the decade ahead, we will see several changes brought about by     the expansion of edge computing capabilities, IoT, and 5G     connectivity.</p> <ul> <li> <p>We will see an exponential increase in both the amount of data and     the speed at which it is transmitted.</p> </li> <li> <p>We will continue to see more privacy regulations put in place to     protect private user data and an explosion in the automation     and\u00a0analytics areas.</p> </li> </ul> <p>Edge computing with IoT over 5G</p> <ul> <li> <p>The foundation for\u00a0realizing the full potential of smart devices is     only possible if these devices are able to connect with a     much-improved network, such as 5G.</p> </li> <li> <p>In fact, by the end of 2023, it is expected that there will be 1.3     billion subscribers to 5G services worldwide. Alongside edge     computing, 5G networks are essential for IoT connectivity. Combining     these technologies will help pave the way for\u00a0smart devices.</p> </li> <li> <p>Figure 9.13\u00a0depicts a variety of things, with edge computing     capability connected to the cloud and data centers within an\u00a0IoT     framework:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.565277777777778in\"}</p> <p>Figure 9.13 -- Edge computing and the Internet</p> <ul> <li> <p>Many of these\u00a0IoT devices, however, lack adequate security     capabilities. In\u00a0addition to laws and regulations such as     the\u00a0General Data Protection Regulation\u00a0(GDPR), we can also     expect additional policies to be implemented to protect user data.     Essentially\u00a0the need for a solution that extracts IfD will continue     to build as more\u00a0time passes.</p> </li> <li> <p>Let's take a look at an example of FL applied to\u00a0edge computing.</p> </li> </ul> <p>Edge FL example -- object detection</p> <ul> <li>Edge computing\u00a0is an architecture that uses distributed computing to     bring computation and data storage as close to the sources of data     as possible. Ideally, this should reduce latency and save bandwidth.     Here's an example of how FL can be utilized with different types     of\u00a0edge devices.</li> </ul> <p>Technical settings</p> <ul> <li> <p>In this example, three\u00a0devices were used to demonstrate object     detection with FL using edge devices.</p> </li> <li> <p>One was an EIS200 edge microserver that ran on Nvidia's Jetson with     an Ubuntu OS.</p> </li> <li> <p>The second device was a Raspberry Pi, using Raspberry Pi OS, and the     third device was simply a regular PC, whose OS was Ubuntu as well.     These machines individually trained an object detection model     with\u00a0unique datasets.</p> </li> </ul> <p>How to do it</p> <ul> <li> <p>The EIS200 trained on pictures of fish, meat, and tomatoes with the     labels\u00a0fish,\u00a0meat, and\u00a0vegetable. The Raspberry Pi trained on     pictures of fish, meat, and eggplants. Evidently, here, tomatoes     were replaced with eggplants.</p> </li> <li> <p>The labels, however, remained the same -- fish, meat, and vegetable.     Likewise, the regular PC trained on pictures of fish, meat, and     leeks, still with the labels fish, meat,\u00a0and vegetable.</p> </li> <li> <p>As you would expect, each environment had biased data containing     different vegetables -- as in, tomatoes, eggplants, and leeks --     with an identical label, vegetable, for all\u00a0of them.</p> </li> </ul> <p>How it works</p> <ul> <li> <p>First, the model was\u00a0trained with pictures of tomatoes by EIS200. As     you would expect, only tomatoes were correctly labeled as     vegetables, whereas eggplants and leeks\u00a0were mislabeled.</p> </li> <li> <p>In the same manner, the Raspberry Pi's model trained with pictures     of eggplants only identified eggplants correctly. One of the two     leeks was labeled as a vegetable as well, but the other one was     identified as fish. As expected, the regular PC's model only     identified leeks\u00a0as vegetables.</p> </li> <li> <p>None of the\u00a0three agents could label all three vegetables correctly,     as we would have anticipated. Next, they were connected to an FL     platform called STADLE, developed by\u00a0TieSet, Inc:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.2736111111111112in\"}</p> <p>Figure 9.14 -- Demo of detecting tomatoes, eggplants, and leeks using FL where the distribution of the datasets is different on each machine</p> <ul> <li> <p>The STADLE aggregator was run as an instance in AWS. Here, again,     each environment had a uniquely biased dataset containing only one     type of vegetable.</p> </li> <li> <p>Connected with the STADLE platform, each agent trained with local     data. After several training epochs, the weights of the models were     sent from the agents to the aggregator.</p> </li> <li> <p>Those weights were then aggregated and sent back to the agents to     continue training. The\u00a0repetition of this aggregation cycle     generated\u00a0unbiased weights.</p> </li> </ul> <p>Examining the results</p> <ul> <li>The FL model\u00a0was able to detect and label all three types of     vegetables correctly as in\u00a0Figure 9.15. This is a straightforward     example of the power of FL in terms of\u00a0bias elimination:</li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.8965277777777776in\"}</p> <p>Figure 9.15 -- Results of the demo using three edge devices</p> <ul> <li> <p>As mentioned previously, all of the model training took place at the     local storage of the edge device itself.</p> </li> <li> <p>The model trained on the local data and then only sent the parameter     weights to the federated server for aggregation.</p> </li> <li> <p>The federated server averaged the model. If you recall from earlier     sections, this is called FedAvg.</p> </li> <li> <p>The federated server then sent back the improved and updated model     to the edge device. So, again, only IfD is collected, not the\u00a0data     itself.</p> </li> <li> <p>Now, let's look at another edge example in\u00a0the automotive sector in     the\u00a0following section.</p> </li> </ul> <p>Making autonomous driving happen with FL</p> <ul> <li> <p>Edge computing\u00a0with ML gains significant interest in AI industries     at scale, especially in the automotive field. Use cases such as     autonomous driving require low latency and real-time responses to     operate correctly. Therefore, FL becomes one of the best solutions     for the automotive field in terms of distributed data processing\u00a0and     training.</p> </li> <li> <p>Offloading computation and storage to edge IoT devices makes the     cloud systems for managing autonomous driving applications much     smaller and cheaper. That's the most powerful benefit of moving on     to the FL paradigm from central\u00a0cloud-based ML.</p> </li> <li> <p>Modern cars already have edge devices with complex computing     capabilities.\u00a0Advanced Driver Assistance Systems\u00a0(ADASs) are     the essential functions for autonomous cars where\u00a0calculations     happen onboard. They also require significant\u00a0computation power.</p> </li> <li> <p>The model is trained and prepared using regular, costly training     systems within on-premises servers or in the cloud even if the     prediction happens in the\u00a0autonomous vehicle.</p> </li> <li> <p>The training process will become more computationally expensive and     slower if the data becomes bigger and will require significant     storage\u00a0as well.</p> </li> <li> <p>FL needs to be used to avoid those issues because updated ML models     are passed between the\u00a0vehicles and the server where the car stores     the user driving patterns and streaming images from the onboard     camera. FL, again, can work in accordance with user consent and     adherence to privacy and\u00a0regional regulations.</p> </li> <li> <p>Figure 9.16\u00a0is about decentralized FL with multiple aggregators to     improve ADASs for safe driving, conducted as a real use case by     TieSet, Inc. with its\u00a0technological partner:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.0777777777777775in\"}</p> <p>Figure 9.16 -- Decentralized FL with multiple aggregators to improve ADASs for safe driving</p> <ul> <li> <p>The\u00a0ADAS integrated into the STADLE of TieSet, Inc., tailored to     provide comfort and personal safety measures, especially to senior     citizens, delivered optimized steering controlling assistance for     car products.</p> </li> <li> <p>Via advanced computer vision and\u00a0Reinforcement Learning\u00a0(RL)     technologies, they achieved a\u00a0design that provides prompt danger     situation awareness and intelligently learns about the     best-personalized\u00a0driving strategies.</p> </li> <li> <p>While personalization is a principal focus of the design,     substantial privacy issues come with personal data usage. The FL     framework enabled by the STADLE platform provided a realistic     solution to overcome this barrier.</p> </li> <li> <p>The architecture, presenting a collaborative form of ML training     distributed among edge users via AI intelligence exchanges, avoids     data transferal and ensures data privacy.</p> </li> <li> <p>Furthermore, the aggregated models can cope with various risky and     unpredictable situations beyond the personal experience\u00a0of drivers.</p> </li> <li> <p>During the\u00a0proof of concept using real cars, they successfully     demonstrated that the designed RL model could efficiently generate     the desirable steering strategy customized for the drivers using     STADLE's\u00a0aggregation framework.</p> </li> <li> <p>In the following section, we will talk about how FL could be applied     to the\u00a0robotics domain.</p> </li> </ul> <p>Applying FL to robotics</p> <ul> <li> <p>In\u00a0robotics systems and applications, ML\u00a0has already become an     integral and essential part of completing necessary tasks. Computer     vision has evolved to make robotics systems perform very well for     many tasks, such as image segmentation and object detection and     classification as well as NLP and signal processing tasks.</p> </li> <li> <p>ML can handle many robotics tasks, including perception, path     planning, sensor fusion, and grasping detected objects     in\u00a0manufacturing settings.</p> </li> <li> <p>However, ML in robotics also has many challenges. The first is the     training time. Even when the amount of data is enough to train the     ML models to solve the aforementioned problems, it takes weeks or     months to train an authentic robotics system.</p> </li> <li> <p>Equally, if the data is not sufficient, it can restrict the ML model     performance significantly. Often, data privacy and accessibility     become an issue for collecting enough data to train the ML models     for\u00a0the robots.</p> </li> <li> <p>That is why the FL framework is considered an essential solution to     the domain of robotics.</p> </li> <li> <p>Researchers at TieSet, Inc. developed a system and methods that     allow robotic manipulators and tools to share their manipulation     skills (including reaching, pick-and-place, holding, and grasping)     for objects of various types and shapes with other robots, as well     as use the skills of other robots to improve and expand\u00a0their own.</p> </li> <li> <p>This system covers the methods to create a general manipulation     model for robots that continuously improves by crowdsourcing skills     from various robotic agents while keeping the data private.</p> </li> <li> <p>They propose a new architecture where multiple AI-powered robotic     agents collaboratively train a global manipulation model by     submitting their models to an aggregator. This communication enables     each agent to utilize the training results of the agents by     receiving an optimally updated\u00a0global model.</p> </li> <li> <p>Figure 9.17\u00a0is the architecture showing how the federated     crowdsourced global manipulation framework for\u00a0robotics works:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"5.684027777777778in\"}</p> <p>Figure 9.17 -- Architecture of the federated crowdsourced global manipulation framework for robotics</p> <ul> <li> <p>Based on the\u00a0architecture of the preceding figure, in\u00a0the simulation     settings, they prepare five robotic arms for the individual tasks of     grabbing boxes, balls, ducks, and teddies.</p> </li> <li> <p>Using the STADLE platform by TieSet, Inc., which can conduct     asynchronous FL, the ML models from those arms are aggregated     continuously. In the end, the federated robotics ML model can grab     all these objects, whether boxes, balls, ducks, or teddies, with a     higher performance (an 80% success rate) in grabbing those objects,     as seen in\u00a0Figure 9.18:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.0875in\"}</p> <p>Figure 9.18 -- Arm robots can cross-train with different tasks for accuracy and efficiency</p> <ul> <li> <p>The FL based on\u00a0STADLE could significantly reduce\u00a0the time taken to     train robots and ML for production lines using computer vision.     Federated performance is much better than training individual robots     and the learning time is much faster than when training\u00a0individual     robots.</p> </li> <li> <p>In the following section, we will talk about AI at scale, where     learning should constantly happen even with numerous devices with     connected environments and the IoT should evolve into the     Internet\u00a0of Intelligence.</p> </li> </ul> <p>Moving toward the Internet of Intelligence</p> <ul> <li> <p>In this section, we will\u00a0talk about why FL is quite important in the     context of the latest development of scalable technologies, such as     the IoT and 5G.</p> </li> <li> <p>As in the previous section, the areas in which AI needs to keep     learning at scale include autonomous driving, retail systems, energy     management, robotics, and manufacturing, all of which generate a     huge amount of data on the edge side, and most of the data needs to     be fully learned to generate performant\u00a0ML models.</p> </li> <li> <p>Following this trend, let us look into the world of the Internet of     Intelligence, in which learning can happen on the edge side to cope     with dynamic environments and numerous devices connected to\u00a0the     Internet.</p> </li> </ul> <p>Introducing the IoFT</p> <ul> <li> <p>The IoT involves intelligent\u00a0and connected systems. They are     intelligent because the information is shared and intelligence is     extracted and used for some purpose -- for example, prediction or     control of a device. They are often connected to the cloud and are     able to collect data from\u00a0many endpoints.</p> </li> <li> <p>Figure 9.19\u00a0shows the current IoT system with more and more     data\u00a0over time:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.9027777777777777in\"}</p> <p>Figure 9.19 -- A current IoT system</p> <ul> <li> <p>As shown in\u00a0Figure 9.19, in the current IoT flow, large amounts of     data must be uploaded and stored in\u00a0the cloud.</p> </li> <li> <p>The models train for specific purposes, such as predictive     maintenance and text prediction. Finally, the trained models are     sent back to the\u00a0edge devices.</p> </li> <li> <p>As you can see, there are several issues with the\u00a0current approach:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   A large amount of storage space\u00a0is needed</p> <ul> <li> <p>Latency is affected due to the amount\u00a0of data</p> </li> <li> <p>Privacy issues due to the movement\u00a0of data</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   To overcome these issues, FL plays an\u00a0important role.</p> <ul> <li> <p>The\u00a0Internet of Federated Things\u00a0(IoFT) is an\u00a0idea     originally presented by researchers at the University of Michigan,     whose paper\u00a0The Internet of Federated Things\u00a0(IoFT) is listed in     the\u00a0Further reading\u00a0section of This section. The IoFT is an     extended framework combining IoT with the concept of FL.</p> </li> <li> <p>As the computational power on the edge side has improved     significantly, AI chips have been penetrating the market rapidly.     Even smartphones have a really strong computing capability these     days and small but powerful computers are often attached to     most\u00a0edge devices.</p> </li> <li> <p>Therefore, the ML model training process is brought down to the edge     due to the increased computational capability of edge devices, and     the IoT's functionality of sending data to the server can be used to     transmit ML models to the cloud. This is also a very effective     approach to protecting private data on edge devices, such as\u00a0mobile     phones.</p> </li> <li> <p>Let's take a look at an example of the IoFT shown in\u00a0Figure 9.20.</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"5.33125in\"}</p> <p>Figure 9.20 -- An example of the IoFT</p> <ul> <li>Potential applications of the IoFT include distributed     manufacturing, traffic intersection control, and energy control, to     name\u00a0a few.</li> </ul> <p>Understanding the role of FL in Web 3.0</p> <ul> <li> <p>FL can be\u00a0integrated into Web 3.0 technologies to accelerate the     adoption of the Internet of Intelligence.</p> </li> <li> <p>The intelligence represented by ML models could be the property of     particular individuals or industries. At the same time, it could be     considered a public asset if it is something that could contribute     to the entire learning process of that ML model for people     worldwide.</p> </li> <li> <p>Whether private intellectual property or public assets, by utilizing     Web 3.0 technology, intelligence can be managed and evolved in a     decentralized manner.</p> </li> <li> <p>Therefore, more and more people will receive the benefits of     intelligence that people have collaboratively trained, which leads     to the true innovation of our entire society in various domains and     with\u00a0various applications.</p> </li> </ul> <p>Applying FL to distributed learning for big data</p> <ul> <li> <p>In this section, we will discuss how FL can be applied to     distributed learning in the context of\u00a0big data.</p> </li> <li> <p>FL for\u00a0big data may not be related to privacy-related issues so much     because the data needed for intelligence purposes is already     possessed. Therefore, it may be more applicable to efficient     learning for big data and improving training time significantly, as     well as reducing the costs of using huge servers, computation,\u00a0and     storage.</p> </li> <li> <p>There are several ways to conduct distributed learning on big data,     such as building a specific end-to-end ML stack applied to different     types of servers, such as parameter servers, or utilizing certain ML     schemes on top of big data platforms such as Hadoop and Spark.</p> </li> <li> <p>There are also some other platforms, such as GraphLab and Pregel.     You can use any libraries, and methods such as stochastic proximal     descent and coordinate descent with low-level utilities\u00a0for ML.</p> </li> <li> <p>These frameworks can support the parallel training of ML models     computationally, but will not be able to assign the data source to     different machines to train them locally in a distributed way,     especially when the training environments are dispersed over the     Internet.</p> </li> <li> <p>With FL, you can simply aggregate what different distributed     machines learn just by synchronizing the federation of the models,     but you do need to develop a well-designed platform to coordinate     the continuous operation of distributed learning, with proper model     repository and versioning approaches\u00a0as well.</p> </li> <li> <p>An example of conducting distributed learning on big data is     depicted in\u00a0Figure 9.21.</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.6145833333333335in\"}</p> <p>Figure 9.21 -- Distributed learning integration into big data</p> <ul> <li> <p>In the\u00a0example in\u00a0Figure 9.21, the data source, which is typically     very large, is sharded into multiple data sources to be dispersed     even into different machines or instances that are available for     training.</p> </li> <li> <p>Within an FL framework, trained models from distributed environments     are all aggregated. The trained and aggregated model then goes to     the processes of\u00a0ML Operations\u00a0(ML Ops) for\u00a0performance     validation and\u00a0continuous monitoring with\u00a0Model     Operations\u00a0(Model Ops).</p> </li> <li> <p>Another layer on top of the preceding scenario can be to combine the     insights from the other data sources. In this case, the FL can     elegantly combine the insights from the other data sources and     nicely coordinate the integration of the other forms of intelligence     directly created in the distributed environments.</p> </li> <li> <p>This way, you can also create the hybrid model of centralized ML and     distributed ML\u00a0as well.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>Here, we discussed many of the challenges facing different     industries in terms of AI advancements. The majority of the     challenges are related in some way to data accessibility. Issues     such as data privacy regulations, lack of real data, and data     transmission costs are all unique and challenging problems that we     expect to see FL continue to\u00a0help solve.</p> </li> <li> <p>Here, you learned about the use cases of the areas in which the FL     is playing a more and more important role, such as healthcare,     financial, edge, and IoT domains. The adherence to privacy that FL     offers is particularly important for the healthcare and financial     sectors, while FL can add significant value in terms of scalability     and learning efficiency to lots of edge AI and IoT scenarios. You     also learned how to apply FL to distributed learning for big data to     reduce training time\u00a0and costs.</p> </li> <li> <p>In the next and final section, we will wrap up this work by     discussing the very exciting future trends and developments in which     FL is expected to play a key role in the\u00a0coming decade.</p> </li> </ul> <p>Future Trends and Developments</p> <ul> <li> <p>Intelligence will drive the next generation of technologies, not big     data. Big data systems have some issues, as discussed in\u00a0section     1,\u00a0Challenges     in Big Data and Traditional AI, and the world is gradually     transitioning from the data-centric era to the intelligence-centric     generation.\u00a0Federated learning\u00a0(FL) will play a core role in     wisdom-driven technologies. Thus, the time is now to welcome the     world of collective intelligence.</p> </li> <li> <p>Here, we will talk about the direction of future AI technologies     that are driven by the paradigm shift happening with FL. For many AI     fields, such as privacy-sensitive areas and fields requiring     scalability in\u00a0machine learning\u00a0(ML), the benefits and     potential of FL are already significant, mainly because of the     privacy-preserving and distributed learning aspects that FL     naturally supports with its design. You will then learn about the     different types of FL as well as the latest development efforts in     that area, as seen in the split and swarm learning techniques, which     can be considered as evolutional frameworks enhancing FL.</p> </li> <li> <p>In addition, FL creates a new concept of an\u00a0Internet of     Intelligence, where people and computers exchange their wisdom     instead of just data themselves. The Internet of Intelligence for     everyone is further accelerated by blockchain technologies as well.     This Internet of Intelligence can then form a newly defined concept     of\u00a0collective intelligence\u00a0that drives another innovation,     from\u00a0data-centric\u00a0approaches     to\u00a0intelligence-centric\u00a0or\u00a0model-centric\u00a0approaches.</p> </li> <li> <p>Finally, we will share a collective vision in which FL plays a key     role in collaboratively creating intelligence learned by many people     and machines around the world.</p> </li> <li> <p>Here, we will cover the following topics:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Looking at future AI trends</p> <ul> <li> <p>Ongoing research and developments in FL</p> </li> <li> <p>Journeying on to collective intelligence</p> </li> </ul> <p>Looking at future AI trends</p> <ul> <li>The majority of industry leaders are now aware of the limitations of     centralized ML as discussed in the next section.</li> </ul> <p>The limitation of centralized ML</p> <ul> <li> <p>When looking\u00a0at the future of AI, it is important to first know the     fact that many companies today are struggling to extract     intelligence and obtain insight from the data they possess.</p> </li> <li> <p>More than half of the data that organizations and companies have     collected is usually not used. Traditional approaches to machine     learning and data science need data to be organized and consolidated     into data lakes and stores in advance of analyzing and training ML     models.</p> </li> <li> <p>You need to duplicate and move the data, which will result in delays     in realizing and delivering the value of the intelligence extracted     from the data, together with certain operational risks and     complexities.</p> </li> <li> <p>In addition, most of the data generated by enterprise companies will     be created and processed outside a traditional centralized data     center or cloud. It is becoming increasingly unrealistic and     inefficient to process data for generating insight in a centralized     manner.</p> </li> <li> <p>Furthermore, based on some market reports out there, most of the     largest global organizations and companies will explore FL at least     once to create much more accurate, secure, and sustainable models     environmentally.</p> </li> <li> <p>That being said, quite a few industries and markets are gradually     becoming aware of the importance of a distributed and FL paradigm,     because they are facing the unavoidable issues and limitations of     the current centralized AI training with big data, as discussed     in\u00a0section     1,\u00a0Challenges     in Big Data and Traditional AI.</p> </li> <li> <p>FL brings the model to the data where the training process resides     instead of bringing the data to the model. Thus, FL is considered to     be the future of data science and ML.</p> </li> <li> <p>In the next section, let's summarize the points of why FL is     beneficial to those companies, especially enterprises that have been     facing the aforementioned issues.</p> </li> </ul> <p>Revisiting the benefits of FL</p> <ul> <li>In this section, we will summarize the benefits of FL that have been     introduced throughout this book.</li> </ul> <p>Increased model accuracy and generalizability</p> <ul> <li>FL realizes collaborative\u00a0and distributed learning that can improve     the performance of ML models, by training on dispersed datasets     locally to continuously incorporate the learning into a global     model. This way, more accurate and generalized ML models can be     produced.</li> </ul> <p>Further privacy and security</p> <ul> <li>FL provides privacy and security advantages because it won't require     private and raw data by its design and security mechanisms, as we     discussed previously in\u00a0section     2,\u00a0What     Is Federated Learning?\u00a0and\u00a0section     9,\u00a0Case     Studies with Key Use Cases of Federated Learning Applications.     Thus, FL\u00a0reduces the potential risk\u00a0of data misuse, leakage, or     exposure\u00a0to sensitive information. FL is also compliant with many     privacy regulations, such as\u00a0General Data Protection     Regulation\u00a0(GDPR),\u00a0California Consumer Privacy     Act\u00a0(CCPA), and\u00a0Health Insurance Portability and     Accountability Act\u00a0(HIPAA).</li> </ul> <p>Improved speed and efficiency</p> <ul> <li> <p>FL is also known to realize high computation efficiency, which can     accelerate the deployment and testing of ML models as well as     decrease communication and computational latency.</p> </li> <li> <p>Due to the decentralized nature of FL, the delay for model delivery     and update is minimized, which leads to a prediction by the global     model in near real time. Real-time delivery and updates of     intelligence are really valuable for time-sensitive ML applications.</p> </li> <li> <p>FL also helps reduce bandwidth and energy consumption by overcoming     system heterogeneity and unbalanced data distribution, which leads     to minimizing data storage and transfer costs that can also     significantly contribute to reducing the environmental impact.</p> </li> </ul> <p>Toward distributed learning for further privacy and training efficiency</p> <ul> <li> <p>Currently, AI is trained\u00a0on huge computational servers,     usually\u00a0happening on big machines in big data companies.</p> </li> <li> <p>As seen in the era of the supercomputer, which can process a huge     amount of data and tasks within one machine or one cluster of     machines, the evolutionary process in technology starts from a     central location and gradually transitions to distributed     environments.</p> </li> <li> <p>The same thing is exactly about to happen in AI. Now, the data lake     concept is popular to organize and train ML models in one place, but     ML already requires distributed learning frameworks.</p> </li> <li> <p>FL is a great way to distribute a training process over multiple     nodes. As shown in many research reports, most data is not fully     used to extract insights into ML models.</p> </li> <li> <p>There are some companies and projects that are trying to use FL as a     powerful distributed learning\u00a0technique, such as the\u00a0platforms     provided by Devron ([devron.ai]{.underline}),     FedML ([fedml.ai]{.underline}), and STADLE     ([stadle.ai]{.underline}).</p> </li> <li> <p>These platforms are already\u00a0resolving the issues discussed in\u00a0The     limitation of centralized AI\u00a0section and have shown a drastic     improvement in the ML process in various use cases, as stated in     the\u00a0Revisiting the benefits of FL\u00a0section.</p> </li> <li> <p>Based on the AI trends that we have\u00a0discussed, let's look into the     ongoing research\u00a0and developments related to FL that cutting-edge     companies are conducting now in the next section.</p> </li> </ul> <p>Ongoing research and developments in FL</p> <ul> <li>We now talk about the ongoing research\u00a0and development projects\u00a0that     are being taken place both in academia and industries around the     world. Let's start with the different types and approaches of FL,     and move on to ongoing efforts to further enhance the FL framework.</li> </ul> <p>Exploring various FL types and approaches</p> <ul> <li>In this work, we have visited\u00a0the most basic algorithms and design     concepts of an FL system. In the real world, we need to dig a bit     deeper into what types of FL frameworks are available to extract the     best performance out of those algorithms. Depending on the data     scenario and use cases, we have several approaches in FL, as     follows:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Horizontal FL and vertical FL</p> <ul> <li> <p>Centralized FL and decentralized FL</p> </li> <li> <p>Cross-silo FL and cross-device FL</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Now, let's look at each type of FL in the following sections.</p> <p>Horizontal FL and vertical FL</p> <ul> <li> <p>Horizontal FL\u00a0uses datasets with the same feature\u00a0space or     schema across all distributed devices     ([https://www.arxiv-vanity.com/papers/1902.04885/]{.underline}).     This actually means that datasets\u00a0share the same columns with     different rows. Most existing FL projects are based on horizontal     FL. Datasets and training processes with horizontal FL are     straightforward because the datasets are formed identically, with     different data\u00a0distributions and inputs\u00a0to be learned. Horizontal FL     is also called homogeneous or sample-based FL.</p> </li> <li> <p>Vertical FL\u00a0is applied to the cases where different\u00a0datasets     share the same sample ID space\u00a0but differ in feature space. You can     check out this paper (https://arxiv.org/pdf/2202.04309) for further     information about vertical FL. Relating these different databases     through FL can be challenging, especially if the unique ID for the     data is different. The key idea of vertical FL is to improve an ML     model by using distributed datasets with a diverse set of     attributes. Therefore, vertical FL\u00a0can handle the partitioned     data\u00a0vertically with different attributes in the same sample space.     Vertical FL is also called heterogeneous or feature-based FL.</p> </li> </ul> <p>Centralized FL and decentralized FL</p> <ul> <li>Centralized FL\u00a0is currently the most common approach\u00a0and most of     the platforms employ this framework. It uses a centralized server to     collect and aggregate the different ML models, with distributed     training across all local data sources. In this book, we focused on     a centralized FL approach, with a scenario where local training     agents communicate the learning results to a centralized FL server     to create a global model.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Decentralized FL, on the other hand, does not use a     centralized\u00a0server to aggregate ML models. It requires individual ML     models trained over local data sources to be communicated among     themselves without a master node. In this case, model weights are     transferred from each individual dataset to the others for further     training. It could potentially be susceptible to model poisoning if     an untrusted party could access the intelligence, and this is a     common problem derived from peer-to-peer frameworks as well.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Cross-silo FL and cross-device FL</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Cross-silo FL\u00a0is the case where ML models are trained\u00a0on data     distributed across any functional, organizational, and regulatory     barriers. In this case, big data is usually stored in a larger size     of storage, with training computing capabilities such as cloud     virtual machines. In the cross-silo FL case, the number of     silos/training environments is relatively small, so not so many     agents are needed in the FL process.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Cross-device FL\u00a0is the case where models need to be trained\u00a0at     scale, often within edge devices, such as mobile phones,\u00a0Internet     of Things\u00a0(IoT) devices, Raspberry Pi-type\u00a0environments, and     so on. In this case, a huge number of devices are connected for the     aggregation of ML models. In the cross-device FL case, the     limitation basically lies in the low computing power of those edge     devices. The framework also needs to handle a number of disconnected     and inactive devices to conduct a consistent and continuous FL     process. The training process and its data volume should be limited     too.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   That concludes the different types of FL that can be applied to a     variety of scenarios in ML applications. There are new techniques     that try to enhance the FL framework to evolve into the next     generation of AI technologies with FL. Let's look into several     advanced approaches in the next section.</p> <p>Understanding enhanced distributed learning frameworks with FL</p> <ul> <li>There are ongoing efforts\u00a0to further enhance FL or distributed     learning frameworks.</li> </ul> <p>Split learning</p> <ul> <li> <p>Split learning, developed in the MIT Media Lab, is an     emerging\u00a0distributed learning technique that enables partitioning ML     models into multiple sections, trains those partitioned ML models at     distributed clients, and aggregates them at the end. Split learning     does not have to share the data either, so it is considered a     privacy-preserving AI as well.</p> </li> <li> <p>The overall framework is similar to the FL. However, there is a     difference in that the neural network is partitioned into multiple     sections that will be trained on distributed clients.</p> </li> <li> <p>The trained weights of the section of the neural network are then     transferred to the server and clients.</p> </li> <li> <p>The weights of those multiple sections are continuously trained in     the next training sessions.</p> </li> <li> <p>Therefore, no raw and private data is shared among the distributed     clients, and only the weights of each section are sent to the next     client.</p> </li> <li> <p>Especially,\u00a0SplitFed\u00a0([https://arxiv.org/abs/2004.12088]{.underline})     is another advanced technique\u00a0that combines split learning\u00a0and FL.     SplitFed splits the deep neural network architecture between the FL     clients and servers to realize a higher level of privacy than FL. It     offers better efficiency than split learning based on the parallel     learning paradigm of FL.</p> </li> </ul> <p>Swarm learning</p> <ul> <li> <p>Swarm learning\u00a0is a decentralized ML solution built\u00a0on     blockchain technology, particularly designed to enable enterprise     industries to take advantage of the power of distributed data, which     results in protecting data privacy and security.</p> </li> <li> <p>This can be achieved by individual nodes sharing parameters of ML     models derived from the local data.</p> </li> <li> <p>Parameters shared from the distributed clients are merged into a     global model. The difference from the normal FL is that the merge     process is not performed by a central server.</p> </li> <li> <p>The distributed nodes and clients choose a temporary leader to     perform the merge. That is why swarm learning is truly     decentralized, also providing greater fault tolerance and     resiliency.</p> </li> <li> <p>The distributed agents have the collective intelligence of a network     without sharing local data into one node.</p> </li> <li> <p>Swarm learning builds\u00a0on top of blockchain. Blockchain provides the     decentralized control, scalability, and fault-tolerance aspects to     work beyond the restrictions of a single enterprise.</p> </li> <li> <p>At the same time, blockchain introduces a tamperproof cryptocurrency     framework, and the participants can use the framework to monetize     their contributions.</p> </li> </ul> <p>BAFFLE</p> <ul> <li> <p>In addition, there is a framework called\u00a0BAFFLE\u00a0that stands     for\u00a0Blockchain Based Aggregator Free Federated     Learning\u00a0(https://arxiv.org/abs/1909.07452). BAFFLE is also     an\u00a0aggregator-free, blockchain-driven FL framework\u00a0that is     inherently decentralized.</p> </li> <li> <p>BAFFLE utilizes\u00a0Smart Contracts\u00a0(SCs) from the blockchain     framework to coordinate\u00a0round management, as well as model     aggregation and updating tasks of FL. Using BAFFLE boosts     computational performance.</p> </li> <li> <p>The global model is also decomposed into many sets of chunks,     directly handled by the SC.</p> </li> <li> <p>Now that we have learned about the latest research and developments     in the FL field, in the next section, let's look at a more visionary     aspect of the AI, science, and technologies of collective     intelligence.</p> </li> </ul> <p>Journeying on to collective intelligence</p> <ul> <li> <p>Big data has been a game changer\u00a0for the AI movement. While the     amount of data generated at the edge and by people will increase     exponentially, intelligence derived from that data benefits society.     Therefore, the big data era will gradually pass the baton to the     collective intelligence era, empowered by FL, in which people will     collaboratively create a wisdom-driven world.</p> </li> <li> <p>Let's start by defining an intelligence-centric era where the     concept of collective intelligence is realized based on FL.</p> </li> </ul> <p>Intelligence-centric era with collective intelligence</p> <ul> <li> <p>Collective Intelligence\u00a0(CI) is the concept of a large group     of single entities acting together in ways\u00a0that seem intelligent. CI     is an emergent phenomenon where groups of people process information     to achieve insights that are not understandable by just individual     members alone.</p> </li> <li> <p>Recently, Thomas Malone, the head of the MIT Center for Collective     Intelligence, and the person\u00a0who initially coined the     phrase\u00a0collective intelligence, broadened the definition of     CI:\u00a0\"CI is something that can emerge from a group that includes     people and computers. CI is a very general property, and superminds     can arise in many kinds of systems, although the systems I've mostly     talked about are those that involve people and     computers\"\u00a0(Reference:\u00a0[https://www2.deloitte.com/xe/en/insights/focus/technology-and-the-future-of-work/human-and-machine-collaboration.html]{.underline}).</p> </li> <li> <p>We are now welcoming the new perspective of CI in technologies     empowered by FL.</p> </li> <li> <p>Data, in the current world of technology, is a great source to     extract intelligence. Dispersed datasets around the world can be     converted into a collection of intelligence represented by AI     technologies. The current trend, as mentioned, is big data, so big     data companies are leading not only the technology industries but     also the entire economy of the world as well. The future is moving     in a CI direction.</p> </li> <li> <p>The vision of CI is even clearer with the emergence of sophisticated     ML algorithms, including deep learning, as the intelligence     represented by ML models can extract intelligence from people,     computers, or any devices that generate meaningful data.</p> </li> <li> <p>Why does FL promote the idea of CI? The nature of FL is to collect a     set of distributed intelligence to be enhanced by an aggregating     mechanism as discussed in this book. This itself enables a data-less     platform that does not require collecting data from people or     devices directly.</p> </li> <li> <p>With the big data issues discussed throughout the book, we have     steered clear of focusing on data-centric platforms. However, it is     also true that learning big data is very much critical and     inevitable to really create systems and applications that are truly     valuable and deliver real value in many domains of the world. That     is why the big data field is still the most prosperous industry,     even if it is facing significant challenges represented by privacy     regulations, security, data silos, and so on.</p> </li> <li> <p>Now is the time to further develop and disseminate the technologies     such as FL that can accelerate the era of CI by fundamentally     resolving the issues of big data. This way, we can realize a new era     of technologies, truly driven by CI that has been backed up by an     authentic mathematical basis.</p> </li> <li> <p>As mentioned,\u00a0data-centric\u00a0platforms are the current trend. So     many\u00a0data and auto ML vendors can support and automate the processes     of creating ML-based intelligence by organizing data and learning     procedures to do so.     An\u00a0intelligence-centric\u00a0or\u00a0model-centric\u00a0platform should be the     next wave of technology\u00a0in which people can share and enhance     intelligence\u00a0that they generate on their own.</p> </li> <li> <p>With FL, we can even realize crowd-sourced learning, where people     can collaboratively and continuously enhance the quality and     performance of ML models.</p> </li> <li> <p>Thus, FL is a critical and essential part of the     intelligence-centric platform to truly achieve a     wisdom-driven\u00a0world.</p> </li> </ul> <p>Internet of Intelligence</p> <ul> <li> <p>The IoT evolved into the\u00a0Internet of Everything. However,     what\u00a0is the essential information\u00a0that people\u00a0want? Is it just\u00a0big     data? Or intelligence derived from data? With 5G technologies, a lot     of data can be transferred over the Internet at a much higher speed,     partially resolving the latency issues in many AI applications. FL     can exchange less information than raw data but still needs to     transfer ML models over the Internet.</p> </li> <li> <p>While lots of research projects are minimizing communications     latency in FL, in the future, information related to intelligence     will be another entity often exchanged over the web. There\u00a0will be a     model repository such as\u00a0Model Zoo\u00a0everywhere, and crowdsourced     learning empowered by FL will be more common to create better     intelligence over the Internet with people worldwide     collaboratively.</p> </li> <li> <p>This paradigm shift is not just in the AI field itself but also in     the wide range of information technologies. As we'll discuss in the     next sections, this\u00a0Internet of Intelligence\u00a0movement will be     the basis of crowdsourced learning and CI, and will help make     intelligence\u00a0available\u00a0to as many people as possible in the coming     years.</p> </li> </ul> <p>Crowdsourced learning with FL</p> <ul> <li> <p>The\u00a0collection of intelligence\u00a0performed by FL naturally\u00a0makes it     a strong fit\u00a0for moving toward CI. The same thing is applied to a     scenario where people can collectively contribute a training process     to global ML models.</p> </li> <li> <p>High-performing ML models in areas such as computer vision and     natural language processing have been trained by certain big data     companies, often spending a huge amount of money, including hundreds     of millions of dollars.</p> </li> <li> <p>Is there any way to collectively train an ML model that will     probably be beneficial for a wide range of people in general? With     the advanced framework of FL, that is possible.</p> </li> <li> <p>FL provides an authentic way to manage the aggregation of multiple     trained models from various distributed agents. In this case, the     distributed agents themselves may be people worldwide, where each     individual user and trainer of the ML model has their own unique     datasets that are not available to anybody else because of data     privacy, silos, and so on.</p> </li> <li> <p>This way of utilizing CI is often called\u00a0crowdsourced learning.     However, traditional crowdsourced learning\u00a0is conducted in a much     more limited way, just based on facilitating and recruiting data     annotators at a large scale.</p> </li> <li> <p>With this new paradigm with FL, users on the CI platform can access     and download ML models that they are interested in and retrain them     if necessary to absorb learning in their own environments. Then,     with the framework to share the trained ML models by those users, an     advanced aggregation framework of FL could pick up the appropriate     models to be federated and make the global model perform better,     adopting diverse data that can be only accessible to the users.</p> </li> <li> <p>This way, intelligence by ML is becoming more available to many     individuals in general, not just to specific companies that have a     significant amount of data and budgets to train an authentic ML     model. In other words, without an FL framework, collaborative     learning is difficult and tricky and almost impossible to even     automate. This openness of the ML models will move the entire     technological world to the next level, and a lot more     applications\u00a0will become feasible, with truly powerful intelligence     that is trained\u00a0by enthusiasts to make the world better.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>In this final section of the book, we discussed fascinating future     trends and developments in which FL is expected to play a crucial     role in the coming decade. In the future, FL is     a\u00a0must-to-have\u00a0technology from a\u00a0nice-to-have\u00a0framework for most     enterprises and application providers, because of the inevitable     privacy regulations and technology trends requiring scalability with     so many users.</p> </li> <li> <p>As we discussed, future technologies will be empowered by the     concept of the Internet of Intelligence, by which people and     computers mainly exchange their wisdom altogether to create a more     intelligent society and world. Finally, the data-centric     technologies will gradually evolve into intelligence-centric     technologies because of the current collaborative learning trend     with CI, which makes people pay significant attention to FL-related     technologies, whose foundations are discussed throughout this book.</p> </li> <li> <p>This book was written at the dawn of a new age in advancements made     possible by AI. There are many uncertainties and many more     challenges ahead. We have made great strides in utilizing the big     data playbook in the last couple of decades, and we have now     outgrown those methods and must adopt new ways of doing things, new     technologies, and new ideas to forge ahead. As long as we capture     the current moment and invest in new technologies such as FL, we     will have a bright future ahead of us.</p> </li> </ul>"}]}