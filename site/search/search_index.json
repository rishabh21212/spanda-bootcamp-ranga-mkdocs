{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Spanda%20Bootcamp%20Day%201/","title":"Spanda Bootcamp Day 1","text":"<p>Testing some changes AGAIN AND AGAIN AND AGAIN</p> <p>Spanda Bootcamp Day One</p> <p>Section 0(Pre-requisites)____</p> <p>Installing Docker on your system</p> <ul> <li>Docker is a platform designed to help developers build, share, and run container applications.</li> <li>Go to https://www.docker.com/ </li> <li>Download and install the version per your operating system</li> <li>During installation, we get a configuration option:  \\ WSL-2 vs Hyper-V</li> <li>Docker Desktop for Windows provides a development environment for building, shipping, and running dockerized apps. </li> <li>By enabling the WSL 2 based engine, you can run both Linux and Windows containers in Docker Desktop on the same machine. Docker Desktop is free for personal use and small businesses, for info on Pro, Team, or Business pricing, see the Docker site FAQs.</li> <li>Following this, installation will proceed and finish.</li> </ul> <p>Section 1________</p> <p>Let\u2019s start by running an LLM on your Laptop</p> <ul> <li>LM Studio is a free, desktop software tool that makes installing and using open-source LLM models extremely easy.  It is not open source. More on that later.</li> <li> <p>Let\u2019s download, install and use it:</p> </li> <li> <p>Go to https://lmstudio.ai/ </p> </li> <li> <p>Download and install the version for your operating system:</p> </li> </ul> <p></p> <ol> <li> <p>Open LM Studio using the newly created desktop icon:</p> </li> <li> <p>Select an LLM to install. </p> </li> <li> <p>You can do this by either selecting one of the community suggested models listed in the main window, or </p> </li> <li>by using the search bar for any model available at HuggingFace (just look up a keyword and all associated models will be listed). </li> <li>Note that there are currently 371,692 (?) models listed at HuggingFace.co</li> </ol> <p></p> <p>selecting LLMs</p> <ul> <li>Whether you elect to download from the community suggested models, or search for one on your own, you can see the size of the install/download file. </li> <li>So be sure you are okay with the size of the download.</li> </ul> <p></p> <p>specific model information</p> <ul> <li>You will note that at the top of the left half of the screen over the release date column, is \u201ccompatibility guess\u201d. </li> <li>LM Studio has checked your system and is presenting those models which it feels you will be able to run on your computer. </li> <li>To see All Models, click on \u201ccompatibility guess\u201d (#1). </li> <li>Clicking on a model on the left, will present the available versions on the right and display those models which should work given your computer\u2019s specs (#2). </li> </ul> <p></p> <p>Compatibility and Should Work indicators</p> <ul> <li>Note that depending on the capabilities/speed of your computer, larger models will be more accurate but slower. </li> <li>You will also find that most of these models are quantized.</li> <li>Quantization refers to using lower precision numbers like 8-bit integers rather than 32-bit floating point values to represent the weights and activations in the model. </li> <li>This reduces memory usage and speeds up inference on your computer\u2019s hardware. </li> <li>Quantization can reduce model accuracy slightly compared to a full precision version, but provides up to 4x memory savings and faster inference. </li> <li>Think of it like how MP-3\u2019s are compressed music files or .jpgs are compressed image files. </li> <li>Although these are of less quality, you often won\u2019t see a significant difference. </li> <li>In the case of LLM\u2019s, the \u201cQ\u201d number you see in the listing of the LLM, represents the amount of quantization. </li> <li>Lower is more and higher is less quantization.</li> <li>Also, in the model listing, you will see references to GGML and GGUF. </li> <li>These are two quantization strategies; \u201cMixed Logits\u201d vs \u201cUniformly Quantized Fully Connected\u201d. </li> <li>GGML provides a more flexible mixed-precision quantization framework while GGUF is specifically optimized for uniformly quantizing all layers of Transformer models. </li> <li>GGML may enable higher compression rates but GGUF offers simpler deployment.</li> <li>Once the model has finished its download,</li> <li>select the model from the drop-down menu at the top of the window; </li> <li>select the chat bubble in the left side column; (3) open up the following sections on the right, \u201cContext Overflow Policy\u201d and \u201cChat Appearance\u201d.</li> </ul> <p></p> <p>ready the model</p> <ol> <li>Make sure \u201cMaintain a rolling window and truncate past messages\u201d is selected under \u201cContent Overflow Policy\u201d and \u201cPlaintext\u201d is selected under \u201cChat Appearance\u201d.</li> </ol> <p></p> <ol> <li>Now close those two areas and open up \u201cModel Configuration\u201d and then open \u201cPrompt Format\u201d and scroll down to \u201cPre-prompt / System prompt\u201d and select the \u201c&gt;\u201d symbol to open that. </li> <li>Here you can enter the system \u201crole\u201d. Meaning, you can set up how you want the bot to act and what \u201cskills\u201d or other specific qualities should be provided in its answers. </li> <li>You can modify what is there to suit your needs. If you have a ChatGPT Plus account, this is the same as \u201cCustom instructions\u201d.</li> </ol> <p></p> <p></p> <p>adding system role / custom instructions</p> <ol> <li>Continue to scroll down in this column until you come to \u201cHardware Settings\u201d. </li> <li>Open this area if you wish to offload some processing to your GPU. </li> <li>The default is to allow your computer\u2019s CPU to do all the work, but if you have a GPU installed, you will see it listed here. </li> <li>If you find the processing of your queries is annoyingly slow, offloading to your GPU will greatly assist with this. </li> <li>Play around with how many layers you want it to handle (start with 10\u201320). This really depends on the model and your GPU. </li> <li>Leaving it all to be handled by the CPU is fine but the model might run a bit slow (again\u2026 depending on the model and its size). </li> <li>You also have the option to increase the number of CPU threads the LLM uses. </li> <li>The default is 4 but you can increase the number, or just leave it where it is if you don\u2019t feel comfortable experimenting and don\u2019t know how many threads your CPU has to play with.</li> </ol> <p></p> <p>optional hardware settings</p> <ol> <li>After these changes, you are now ready to use your local LLM. </li> <li>Simply enter your query in the \u201cUSER\u201d field and the LLM will respond as \u201cAI\u201d.</li> </ol> <p></p> <p>chat dialogue</p> <ul> <li>Let\u2019s download the Zephyr 7B \u03b2 model, adapted by _TheBloke _for llama.cpp's GGUF format.</li> </ul> <p></p> <ul> <li>Activating and loading the model into LM Studio is straightforward.</li> </ul> <p></p> <ul> <li>You can then immediately start using the model from the Chat panel, no Internet connection required.</li> </ul> <p></p> <ul> <li>The right panel displays and allows modification of default presets for the model. </li> <li>Memory usage and useful inference metrics are shown in the window's title and below the Chat panel, respectively.</li> <li>Other models, like codellama Instruct 7B, are also available for download and use.</li> </ul> <p></p> <ul> <li>LM Studio also highlights new models and versions from Hugging Face, making it an invaluable tool for discovering and testing the latest releases.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#accessing-models-with-apis","title":"Accessing Models with APIs","text":"<ul> <li>A key feature of LM Studio is the ability to create Local Inference Servers with just a click.</li> </ul> <ul> <li>The Automatic Prompt Formatting option simplifies prompt construction to match the model's expected format. The exposed API aligns with the OpenAI format.</li> <li>Here's an example of calling the endpoint with CURL: \\ (Does not work)</li> </ul> <pre><code>curl http://localhost:1234/v1/chat/completions\n-H \"Content-Type: application/json\"\n-d '{\n \"messages\": [\n   { \"role\": \"system\", \"content\": \"You are an AI assistant answering Tech questions\" },\n   { \"role\": \"user\", \"content\": \"What is Java?\" }\n ],\n \"temperature\": 0.7,\n \"max_tokens\": -1,\n \"stream\": false\n}'\n\n\n\n![alt_text](images/image16.png \"image_tooltip\")\n\n\n\nThe curl command below works:\n$url = \"http://localhost:1234/v1/chat/completions\"\n$headers = @{\n    \"Content-Type\" = \"application/json\"\n}\n\n$data = @{\n    messages = @(\n        @{\n            role = \"system\"\n            content = \"You are an AI assistant answering Tech questions, but answer only in rhymes\"\n        },\n        @{\n            role = \"user\"\n            content = \"What is Java?\"\n        }\n    )\n    temperature = 0.7\n    max_tokens = -1\n    stream = $false\n}\n\nInvoke-RestMethod -Uri $url -Headers $headers -Method Post -Body ($data | ConvertTo-Json) -UseBasicParsing\n</code></pre> <p>The response provides the requested information:</p> <pre><code>{\n   \"id\": \"chatcmpl-iyvpdtqs1qzlv6jqkmdt9\",\n   \"object\": \"chat.completion\",\n   \"created\": 1699806651,\n   \"model\": \"~/.cache/lm-studio/models/TheBloke/zephyr-7B-beta-GGUF/zephyr-7b-beta.Q4_K_S.gguf\",\n   \"choices\": [\n       {\n           \"index\": 0,\n           \"message\": {\n               \"role\": \"assistant\",\n               \"content\": \"Java is a high-level, object-oriented\n                           programming language that was first released by Sun\n                           Microsystems in 1995. It is now owned by Oracle Corporation.\n                           Java is designed to be platform independent, meaning that it\n                           can run on any operating system that has a Java Virtual\n                           Machine (JVM) installed. Java's primary applications are in\n                           the development of desktop applications, web applications,\n                           and mobile apps using frameworks such as Android Studio,\n                           Spring Boot, and Apache Struts. Its syntax is similar to\n                           C++, but with added features for object-oriented programming\n                           and memory management that make it easier to learn and use\n                           than C++. Java's popularity is due in part to its extensive\n                           library of pre-written code (known as the Java Class\n                           Library) which makes development faster and more efficient.\"\n           },\n           \"finish_reason\": \"stop\"\n       }\n   ],\n   \"usage\": {\n       \"prompt_tokens\": 0,\n       \"completion_tokens\": 166,\n       \"total_tokens\": 166\n   }\n}\n</code></pre> <p>This feature greatly aids in testing integrations with frontends like chatbots or workflow solutions like Flowise.</p> <p>Gpt4all is open source and is my preference as its hackable: https://github.com/keshavaspanda/gpt4all</p> <p>Section 2__________</p> <p>Let\u2019s explore some relevant sections in the SEAI course material:**  **</p> <p>Navigate to: https://ckaestne.github.io/seai/</p> <p>https://github.com/ckaestne/seai</p> <p>Section 3__________</p> <p>The ML Process and DevOps vs MLOps vs AIOps</p> <p></p>"},{"location":"Spanda%20Bootcamp%20Day%201/#devops-the-confluence-of-development-operations-and-quality-assurance","title":"DevOps: The Confluence of Development, Operations, and Quality Assurance","text":"<ul> <li>DevOps brings together development, quality assurance, and operations  involving people, processes, and technology to streamline software development and release throughput using a cycle of Continuous Integration(CI) and Continuous Deployment(CD).</li> </ul> <ul> <li>In DevOps, <ul> <li>developers merge their code changes to a central repository like GitHub. </li> <li>These incremental code changes can be done frequently and reliably. </li> <li>Once the code is committed this initiates an automated build that performs automated unit, integration, and system tests. </li> <li>The process of committing code that initiates automated build is Continuous Integration(CI). </li> <li>CI makes it easier for developers to develop and commit the code. </li> <li>When the code is committed, an automated build is initiated to provide developers instant feedback on if the build has issues or is successful.</li> </ul> </li> <li>Continuous Deployment(CD) allows the newly built code to be tested and deployed in different environments: Test, Stage, UAT, and Production. </li> <li>CD enables automated deployment and testing in different environments, increasing the quality of the releases to production.</li> <li> <p>How does Dev Ops help? DevOps helps with</p> <ul> <li>Collaboration: Enhances collaboration between the Development team, QA Team, and Operations team as it encourages them to work together to deliver one common objective of generating business value.</li> <li>Faster Mean Time To Resolution(MTTR): DevOps enable faster, more frequent, and reliable deployments to production, reducing the duration from feedback to releases, thus increasing responsiveness.</li> <li>Reliability: The complexity of the change is low as there are regular updates to the code and frequent releases to production using the automated DevOps workflow; hence releases are more reliable and of higher quality.</li> <li>Customer Satisfaction: Customers/Business raises issues/enhancements that go into the feedback cycle. Faster resolution of the issues/enhancements leads to greater customer satisfaction.</li> </ul> <p>MLOps</p> </li> <li> <p>Now, these applications are available, are running reliably and are generating voluminous amounts of data. </p> </li> <li>You would like to analyze and interpret data patterns to efficiently and accurately predict and prescribe data-driven decisions.</li> <li>This is where Machine Learning Algorithms come into play</li> <li></li> </ul> <p></p> <ul> <li>Machine learning algorithms take the data and the results as an input to identify the patterns using machine learning algorithms to build analytical models.</li> <li>For example, Financial institutions use their customer\u2019s transactional data and machine learning algorithms like clustering to identify patterns of fraud or legitimate transactions.</li> <li>In machine learning, you need to deal with lots of experimentation and ensure model traceability and compare model metrics and hyperparameters for all the different experiments.</li> <li>What if you could automate and monitor all steps of an ML system?</li> <li>MLOps is an ML engineering culture and practice to unify ML system development (Dev) and ML system operation (Ops) where Data scientists, data Engineers, and Operations teams collaborate.</li> <li></li> </ul> <p></p> <ul> <li>ML Ops build the ML pipeline to encompass all stages of Machine Learning:<ul> <li>Data extraction</li> <li>Data exploration and validation</li> <li>Data curation or data preprocessing</li> <li>Feature analysis</li> <li>Model training and evaluation</li> <li>Model validation</li> <li>Model deployment or model serving </li> <li>Model monitoring for data drift and concept drift</li> </ul> </li> </ul> <p></p> <ul> <li> <p>How does ML Ops help?</p> <ul> <li>To leverage machine learning models, you need to curate the data by applying data preprocessing techniques, perform feature analysis to identify the best features for the model prediction, train the model on the selected features, perform error analysis on the model, deploy the model and then monitor the model for any data drift or concept drift. If the model degrades performance, retrain the model again by repeating the steps from data curation to deployment and monitoring.</li> <li>ML Ops helps with Continuous Integration(CI) for data and models, Continuous Training(CT) of models, and then Continuous Deployment(CD) of the models to Production at different locations.</li> <li>ML Ops helps to<ul> <li>Effectively manage the full ML lifecycle.</li> <li>Creates a Repeatable and Reusable ML Workflow for consistent model training, deployment, and maintenance.</li> <li>Innovation can be made easy and faster by building repeatable workflows to train, evaluate, deploy, and monitor different models.</li> <li>Track different versions of model and data to enable auditing</li> <li>Easy Deployment to production with high precision</li> </ul> </li> </ul> <p>AIOps</p> <ul> <li>AIOps is understood in general to be defined as  Artificial Intelligence for IT Operations (it should be AI4ITOps)</li> <li>The term originally was much broader than that</li> <li>Data from different systems are digitized, and organizations are going through digital  transformation and striving to have a data-driven culture. </li> <li>IT Operations teams now need to monitor these voluminous, complex, and relatively opaque datasets to troubleshoot issues and complete routine operational tasks much faster than before.</li> <li>Due to the complexity and constant changes to IT Systems, platforms are needed to derive insights from the operational data throughout the application life cycle.</li> <li>AIOps applies analytics and machine learning capabilities to IT operations data <ul> <li>to separate significant events from noise in the operation data </li> <li>to identify root causes  </li> <li>to prescribe resolutions</li> </ul> </li> <li>Per Gartner<ul> <li>AI Ops platform ingest, index and normalize events or telemetry data from multiple domains, vendors, and sources, including infrastructure, networks, apps, the cloud, or existing monitoring tools.</li> </ul> </li> <li>AI Ops platforms enable data analytics using machine learning methods, including real-time analysis at the point of ingestion and historical analysis of stored operational data like system logs, metrics, network data, incident-related data, etc.</li> <li>How does AIOps help? Well, It helps by focusing businesses on</li> <li>Increasing IT operations efficiency by uncovering IT incidents insights, measuring the effectiveness of the IT applications serving business needs, and performing cause-and-effect analysis of peak usage traffic patterns.</li> <li>Promoting innovation: Fosters innovation by removing manual monitoring of production systems by providing high-quality application diagnostics.</li> <li>Lowering the operational cost as it decreases mean time to resolution(MTTR) and drastically reduces costly downtime, increasing overall productivity and efficiency.</li> <li>Accelerating the return on investment by enabling teams to collaborate towards a faster resolution</li> </ul> </li> </ul> <p></p> <ul> <li>Per Gartner<ul> <li>There is no future of IT operations that does not include AIOps. This is due to the rapid growth in data volumes and pace of change exemplified by rate of application delivery and event-driven business models that cannot wait on humans to derive insights.</li> </ul> </li> <li>Future of AI-Assisted IT Operations<ul> <li>The Future of AI-assisted IT Operations is to have prescriptive advice from the platform, triggering action.</li> </ul> </li> </ul> <p></p> <ul> <li>So in summary,<ul> <li>DevOps co-opts development, quality assurance, and operations  involving people, processes, and technology to streamline the software development lifecycle and reduced mean time to resolution</li> <li>MLOps is a discipline that combines Machine Learning, Data Engineering, and Dev Ops to build automated ML pipelines for Continuous Training and CI/CD to manage the full ML lifecycle effectively</li> <li>AIOps is a platform to monitor and automate the data and information flowing from IT applications that utilizes big data, machine learning, and other advanced analytics technologies</li> </ul> </li> </ul> <p>Section 4__________</p> <p>Back to the Basics: Data Collection and Data Management</p> <p></p> <ul> <li>MLOps requires highly disciplined data collection and management. </li> <li>It is particularly needed when the outcomes could affect people\u2019s careers, students' lives and educational organization\u2019s reputations.</li> <li>Towards that,  let us take a look at framework used as the basis to make some very important decisions:</li> </ul> <p>https://github.com/keshavaspanda/openneuro</p> <p>The Data Submission Process</p> <p></p> <p>Let\u2019s consider our context and ask ourselves the following questions:</p> <ul> <li>Could Modalities be equivalent to Disciplines? </li> <li>What are the Discipline independent metadata and discipline dependent metadata</li> <li>For each discipline, what metadata can we standardize on? </li> <li>What could be our equivalent to the BIDS validator? </li> <li>Are there existing standards that we can leverage to this data? </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#understanding-responsible-and-fair-data-collection-in-this-context","title":"Understanding Responsible and FAIR Data Collection in this context","text":"<ul> <li>There is growing recognition of the importance of data sharing for scientific progress </li> <li>However, not all shared data is equally useful. </li> <li>The FAIR principles have formalized the notion that in order for shared data to be maximally useful, they need to be findable, accessible, interoperable, and reusable. </li> <li>An essential necessity for achieving these goals is that the data and associated metadata follow a common standard for organization, so that data users can easily understand and reuse the shared data. </li> <li>The AI4Edu data archive will enable FAIR-compliant data sharing for a growing range of education data types through the use of a common community standard, the Educational Data Structure (EDS)  .</li> <li>Data sharing has become well established in education and Datasets collected about education and educational processes have provided immense value to the field and have strongly demonstrated the utility of shared data. </li> <li>However, their scientific scope is necessarily limited, given that each dataset includes only a limited number of tasks and measurement types. </li> <li>Beyond these large focused data sharing projects, there is a \u2018long tail\u2019 of smaller datasets that have been collected in service of specific research questions in education. </li> <li>Making these available is essential to ensure reproducibility as well as to allow aggregation across many different types of measurements in service of novel scientific questions. </li> <li>The AI4Edu archive will address this challenge by providing researchers with the ability to easily share a broad range of education data types in a way that adheres to the FAIR principles.</li> </ul> <p>Core principles of the Data Archive</p> <p>Sharing only upon gaining permissions</p> <ul> <li>There is a range of restrictiveness across data archives with regard to their data use agreements. </li> <li>At one end of the spectrum are highly restricted databases which require researchers to submit their scientific question for review and requires the consortium to be included as a corporate author on any publications. </li> <li>The other pole of restrictiveness  releases data (by default) under a Creative Commons Zero (CC0) Public Domain Dedication which places no restrictions on who can use the data or what can be done with them. </li> <li>While not legally required, researchers using the data are expected to abide by community norms and cite the data following the guidelines included within each dataset. </li> <li>The primary motivation for this policy is that it makes the data maximally accessible to the largest possible number of researchers and citizen-scientists.</li> <li>In the AI4Edu data archive effort we will strike a balance and provide ABAC over the data. </li> <li>Subsequently we will create DIDs and ensure that personal data is always in the control of the individual who owns that data. </li> </ul> <p>Standards-focused data sharing</p> <ul> <li>To ensure the utility of shared data for the purposes of efficient discovery, reuse, and reproducibility, standards are required for data and metadata organization. </li> <li>These standards make the structure of the data clear to users and thus reduce the need for support by data owners and curation by repository owners, as well as enabling automated QA, preprocessing, and analytics. </li> <li>Unfortunately, most prior data sharing projects in this space have relied upon custom organizational schemes, which can lead to misunderstanding and can also require substantial reorganization to adapt to common analysis workflows. </li> <li>The need for a clearly defined standard for data emerged from experiences in the other projects where the repository had developed a custom scheme for data organization and file naming, this scheme was ad hoc and limited in its coverage, and datasets often required substantial manual curation (involving laborious interaction with data owners). </li> <li>In addition, there was no built in mechanism to directly validate whether a particular dataset met the standard.</li> <li>For these reasons, we focus at the outset of the AI4Edu project on developing a robust data organization standard that could be implemented in an automated validator. </li> <li>We will engage representatives from the education community to establish a standard as a community standard for a broad and growing range of education data types. </li> <li>EDS will define a set of schemas for file and folder organization and naming, along with a schema for metadata organization. </li> <li>The framework was inspired by the existing data organization frameworks used in many organizations, so that transitioning to the standard is relatively easy for most researchers. </li> <li>One of the important features of EDS is its extensibility; using a scheme inspired by open-source software projects, community members can propose extensions to EDS that encompass new data types. </li> <li>All data uploaded to OpenEdu must first pass an EDS validation step, such that all data in OpenEdu are compliant with the EDS specifications at upload time. </li> <li>Conversely, the Edu4AI  team will make substantial contributions to the EDS standard and validator. </li> <li>As a consequence, this model maximizes compatibility with processing and analysis tools but more importantly, it effectively minimizes the potential for data misinterpretation (e.g., when owner and reuser have slightly different definitions of a critical acquisition parameter). Through the adoption of EDS, OpenEdu can move away from project- or database-specific data structures designed by the owner or the distributor (as used in earlier projects) and toward a uniform and unambiguous representation model agreed upon by the research community prior to sharing and reuse.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#fair-sharing","title":"FAIR sharing","text":"<ul> <li>The FAIR principles have provided an important framework to guide the development and assessment of open data resources. </li> <li>AI4Edu will implement these principles.</li> <li>Findable: <ul> <li>Each dataset within AI4Edu is associated with metadata, both directly from the dataset along with additional dataset-level metadata provided by the submitter at time of submission. </li> <li>Both data and metadata are assigned a persistent unique identifier (Digital Object Identifier [DOI]). </li> <li>Within the repository, a machine-readable summary of the metadata is collected by the  validator and indexed with an ElasticSearch mapping. </li> <li>In addition, dataset-level metadata are exposed according to the schema.org standard, which allows indexing by external resources such as Google Dataset Search.</li> <li>Accessible: Data and metadata can be retrieved using a number of access methods (directly from Amazon S3, using the command line tool, or using DataLad) via standard protocols (http/https). </li> <li>Metadata are also accessible programmatically via a web API. Metadata remains available even in the case that data must be removed (e.g., in cases of human subjects concerns). </li> <li>Authentication is necessary to access the data.</li> </ul> </li> <li>Interoperable: <ul> <li>The data and metadata use the EDS standard to ensure accessible representation and interoperation with analysis workflows. </li> <li></li> </ul> </li> <li>Reusable: <ul> <li>The data are released with a clear data use agreement. </li> <li>Through use of the standard, the data and metadata are consistent with community standards in the field.</li> </ul> </li> <li>Data versioning and preservation<ul> <li>AI4Edu will keep track of all changes in stored datasets and allows researchers to unambiguously report the exact version of the data used for any analysis. </li> <li>AI4Edu will preserve all versions of the data through the creation of \u2018snapshots\u2019 that unequivocally point to one specific point in the lifetime of a dataset. </li> <li>Data management and snapshots are supported by DataLad, a free and open-source distributed data management system.</li> </ul> </li> <li>Protecting privacy and confidentiality of data<ul> <li>There is a direct relationship in data sharing between the openness of the data and their reuse potential; all else being equal, data that are more easily or openly available will be more easily and readily reused. </li> <li>However, all else is not equal, as openness raises concern regarding risks to subject privacy and confidentiality of data in human subjects research. </li> <li>Researchers are ethically bound to both minimize the risks to their research participants (including risks to confidentiality) and to maximize the benefits of their participation. </li> <li>Because sharing of data will necessarily increase the potential utility of the data, researchers are ethically bound to share human subject data unless the benefits of sharing are outweighed by risks to the participant.</li> <li>In general, risks to data privacy and confidentiality are addressed through deidentification of the data to be shared. </li> <li>De-identification can be achieved through the removal of any of 18 personal identifiers, unless the researcher has knowledge that the remaining data could be re-identified (known as the \u2018safe harbor\u2019 method). </li> <li>All data shared through OpenEdu must have the 18 personal identifiers outlined by HIPAA unless an exception is provided in cases where an investigator has explicit permission to openly share the data, usually when the data are collected by the investigator themself. </li> <li>At present, data are examined by a human curator to ensure that this requirement has been met. </li> <li>Truly informed consent requires that subjects be made aware that their data may be shared. </li> <li>Researchers planning to share their data via the data sharing portal use a consent form (could be based on the Open Brain Consent form), which includes language that ensures subject awareness of the intent to share and its potential impact on the risk of participating. </li> </ul> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#open-source","title":"Open source","text":"<ul> <li>The entirety of the code for AI4Edu will be available under a permissive open-source software license (MIT License) at github.</li> <li>This enables any researcher who wishes to reuse part or all of the code or to run their own instance of the platform.</li> </ul> <p>Section 5__________</p> <p>**Data Analysis, Data Visualization, Data Lake Houses and Analytics Dashboards **</p> <p></p> <ul> <li>Once data is \u201cFAIR\u201d ly collected and placed in an archive, it needs to be checked for quality, analyzed, visualized profiled and then models need to be selected, trained (on the curated data), tested and served up for use.</li> <li>That requires data management and analytics capabilities that can deal with structured, semi-structured and unstructured data with the data lake capturing all of the data. </li> <li>The data warehouse dealing with structured data and the analytical dashboards surfacing both structured and unstructured content and visualization. </li> <li>This is where the data lake house architecture has become popular.  </li> <li>As the name suggests, a data lake house architecture combines a data lake and a data warehouse. </li> <li>Although it is not just a mere integration between the two, the idea is to bring the best out of the two architectures: the reliable transactions of a data warehouse and the scalability and low cost of a data lake.</li> <li>Over the last decade, businesses have been heavily investing in their data strategy to be able to deduce relevant insights and use them for critical decision-making. </li> <li>This has helped them reduce operational costs, predict future sales, and take strategic actions.</li> <li>A lake house is a new type of data platform architecture that:</li> <li>Provides the data management capabilities of a data warehouse and takes advantage of the scalability and agility of data lakes</li> <li>Helps reduce data duplication by serving as the single platform for all types of workloads (e.g., BI, ML)</li> <li>Is cost-efficient</li> <li>Prevents vendor lock-in and lock-out by leveraging open standards</li> </ul> <p></p>"},{"location":"Spanda%20Bootcamp%20Day%201/#evolution-of-the-data-lakehouse","title":"Evolution of the Data Lakehouse","text":"<ul> <li>Data Lake House is a relatively new term in big data architecture and has evolved rapidly in recent years. It combines the best of both worlds: the scalability and flexibility of data lakes, and the reliability and performance of data warehouses. </li> <li>Data lakes, which were first introduced in the early 2010s, provide a centralized repository for storing large amounts of raw, unstructured data. </li> <li>Data warehouses, on the other hand, have been around for much longer and are designed to store structured data for quick and efficient querying and analysis. </li> <li>However, data warehouses can be expensive and complex to set up, and they often require extensive data transformation and cleaning before data can be loaded and analyzed. </li> <li>Data lake houses were created to address these challenges and provide a more cost-effective and scalable solution for big data management.</li> <li>With the increasing amount of data generated by businesses and the need for fast and efficient data processing, the demand for a data lake house has grown considerably. As a result, many companies have adopted this new approach, which has evolved into a central repository for all types of data in an organization.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#what-does-a-data-lake-house-do","title":"What Does a Data Lake House Do?","text":"<p>There are four key problems in the world of data architecture that data lake houses address: </p> <ul> <li>Solves the issues related to data silos by providing a centralized repository for storing and managing large amounts of structured and unstructured data. </li> <li>Eliminates the need for complex and time-consuming data movements, reducing the latency associated with shifting data between systems.</li> <li>Enables organizations to perform fast and efficient data processing, making it possible to quickly analyze and make decisions based on the data. </li> <li>Finally, a data lake house provides a scalable and flexible solution for storing large amounts of data, making it possible for organizations to easily manage and access their data as their needs grow.</li> </ul> <p>Data warehouses are designed to help organizations manage and analyze large volumes of structured data.</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#how-does-a-data-lake-house-work","title":"How Does a Data Lake House Work?","text":"<ul> <li>A data lakehouse operates by utilizing a multi-layer architecture that integrates the benefits of data lakes and data warehouses. </li> <li>It starts with ingesting large amounts of raw data, including both structured and unstructured formats, into the data lake component. </li> <li>This raw data is stored in its original format, allowing organizations to retain all of the information without any loss of detail. </li> <li>From there, advanced data processing and transformation can occur using tools such as Apache Spark and Apache Hive. </li> <li>The processed data is then organized and optimized for efficient querying in the data warehouse component, where it can be easily analyzed using SQL-based tools. </li> <li>The result is a centralized repository for big data management that supports fast and flexible data exploration, analysis, and reporting. </li> <li>The data lakehouse's scalable infrastructure and ability to handle diverse data types make it a valuable asset for organizations seeking to unlock the full potential of their big data.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#elements-of-a-data-lakehouse","title":"Elements of a Data Lakehouse","text":"<p>Data lake houses have a range of elements to support organizations\u2019 data management and analysis needs. </p> <ul> <li>A key element is the ability to store and process a variety of data types including structured, semi-structured, and unstructured data. </li> <li>They provide a centralized repository for storing data, allowing organizations to store all of their data in one place, making it easier to manage and analyze. </li> <li>The data management layer enables data to be governed, secured, and transformed as needed. </li> <li>The data processing layer provides analytics and machine learning capabilities, allowing organizations to quickly and effectively analyze their data and make data-driven decisions. </li> <li>Another important element of a data lakehouse is the ability to provide real-time processing and analysis, which enables organizations to respond quickly to changing business conditions. </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#cloud-data-lake","title":"Cloud Data Lake","text":"<ul> <li>Data lake houses are often spoken in tandem with cloud data lakes and cloud data warehouses. With the increasing adoption of cloud-based solutions, many organizations have turned to cloud data lakes to build their data platforms. </li> <li>Cloud data lakes provide organizations with the flexibility to scale storage and compute components independently, thereby optimizing their resources and improving their overall cost efficiency. </li> <li>By separating storage and computing, organizations can store any amount of data in open file formats like Apache Parquet and then use a computing engine to process the data. </li> <li>Additionally, the elastic nature of cloud data lakes enables workloads \u2013 like machine learning \u2013 to run directly on the data without needing to move data out of the data lake.</li> </ul> <p>Despite the many benefits of cloud data lakes, there are also some potential drawbacks: </p> <ul> <li>One challenge is ensuring the quality and governance of data in the lake, particularly as the volume and diversity of data stored in the lake increases.</li> <li>Another challenge is the need to move data from the data lake to downstream applications \u2013 such as business intelligence tools \u2013 which often require additional data copies and can lead to job failures and other downstream issues. </li> <li>Additionally, because data is stored in raw formats and written by many different tools and jobs, files may not always be optimized for query engines and low-latency analytical applications.+</li> </ul> <p>**Lets start up a Dremio DataLakehouse with MinIO and Apache Superset Dashboards **</p> <p>First create a Dremio cloud account:</p> <p>https://www.dremio.com/resources/tutorials/from-signup-to-subsecond-dashboards-in-minutes-with-dremio-cloud/</p> <p> Then let\u2019s try out this: https://github.com/developer-advocacy-dremio/quick-guides-from-dremio/blob/main/guides/superset-dremio.md</p> <p></p>"},{"location":"Spanda%20Bootcamp%20Day%201/#cloud-data-warehouse","title":"Cloud Data Warehouse","text":"<ul> <li>The first generation of on-premises data warehouses provide businesses with the ability to derive historical insights from multiple data sources. </li> <li>However, this solution required significant investments in terms of both cost and infrastructure management. In response to these challenges, the next generation of data warehouses leveraged cloud-based solutions to address these limitations.</li> <li>One of the primary advantages of cloud data warehouses is the ability to separate storage and computing, allowing each component to scale independently. This feature helps to optimize resources and reduce costs associated with on-premises physical servers. </li> <li>However, there are also some potential drawbacks to using cloud data warehouses: </li> <li>While they do reduce some costs, they can still be relatively expensive.</li> <li>Additionally, running any workload where performance matters often requires copying data into the data warehouse before processing, which can lead to additional costs and complexity. </li> <li>Moreover, data in cloud data warehouses is often stored in a vendor-specific format, leading to lock-in/lock-out issues, although some cloud data warehouses do offer the option to store data in external storage. </li> <li>Finally, support for multiple analytical workloads, particularly those related to unstructured data like machine learning, is still unavailable in some cloud data warehouses.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#future-of-the-data-lakehouse","title":"Future of the Data Lakehouse","text":"<ul> <li>Upon discussion of data lake houses, their elements, and what they do, it\u2019s only natural to look at the implications of this technology moving forward. </li> <li>The future looks very promising, as more and more organizations are embracing big data and the need for flexible, scalable, and cost-effective solutions for managing it continues to grow. </li> <li>In the coming years, expect to see increased adoption of data lake houses, with organizations of all sizes and across all industries recognizing their value in providing a unified platform for managing and analyzing big data. </li> <li>Additionally, expect to see continued innovation and advancements in data lakehouse technology, such as improved data processing and transformation capabilities, enhanced security and governance features, and expanded integration with other data management tools and technologies.</li> <li>The rise of machine learning and artificial intelligence will drive the need for flexible and scalable big data platforms that can support the development and deployment of these advanced analytics models. </li> <li>The future of data lake houses will also be influenced by the increasing importance of data privacy and security, and we can expect to see data lake houses evolving to meet these new requirements, including better data masking and data encryption capabilities. </li> <li>Overall, the future of data lake houses looks bright, and they are likely to play an increasingly critical role in helping organizations extract value from their big data.</li> </ul> <p>Section 6__________</p> <p>Model Training, Model Serving and ML Ops</p> <p> MLOps vs LLMOps</p> <ul> <li>While LLMOps borrows heavily from MLOps, the differences are notable.</li> <li>The model training approach in LLMs leans more towards fine-tuning or prompt engineering rather than the frequent retraining typical of traditional Machine Learning (ML).</li> <li>In LLMOps, human feedback becomes a pivotal data source that needs to be incorporated from development to production, often requiring a constant human feedback loop in contrast to traditional automated monitoring.</li> <li>Automated quality testing faces challenges and may often require human evaluation, particularly during the continuous deployment stage. Incremental rollouts for new models or LLM pipelines have become the norm.</li> <li>This transition might also necessitate changes in production tooling, with the need to shift serving from CPUs to GPUs, and the introduction of a new object like a vector and graph databases  into the data layer.</li> <li> <p>Lastly, managing cost, latency, and performance trade-offs becomes a delicate balancing act, especially when comparing self-tuned models versus paid third-party LLM APIs.</p> <p>Continuities With Traditional MLOps</p> </li> <li> <p>Despite these differences, certain foundational principles remain intact.</p> </li> <li>The dev-staging-production separation, enforcement of access controls, usage of Git and model registries for shipping pipelines and models, and the Data Lake architecture for managing data continue to hold ground. </li> <li>Also, the Continuous Integration (CI) infrastructure can be reused, and the modular structure of MLOps, focusing on the development of modular data pipelines and services, remains valid.</li> <li>Exploring LLMOps Changes</li> <li>As we delve deeper into the changes brought by LLMOps, we will explore the operational aspects of Language Learning Models (LLMs), creating and deploying LLM pipelines, fine-tuning models, and managing cost-performance trade-offs.</li> <li>Differentiating between ML and Ops becomes crucial, and tools like MLflow, LangChain, LlamaIndex, and others play key roles in tracking, templating, and automation. </li> <li>Packaging models or pipelines for deployment, scaling out for larger data and models, managing cost-performance trade-offs, and gathering human feedback become critical factors for assessing model performance. </li> <li>Moreover, the choice between deploying models versus deploying code, and considering service architecture, become essential considerations, especially when deploying multiple pipelines or fine-tuning multiple models.</li> </ul> <p></p> <p>A Minimal LLMOps pipeline</p> <p>https://github.com/keshavaspanda/BigBertha** **</p> <p></p>"},{"location":"Spanda%20Bootcamp%20Day%201/#_1","title":"Spanda Bootcamp Day 1","text":"<p>**LLMOps Capabilities **</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#_2","title":"Spanda Bootcamp Day 1","text":"<p>1. LLM Monitoring</p> <p>The framework utilizes Prometheus to monitor LLM (Large Language Model) serving modules. For demo purposes, a Streamlit app is used to serve the LLM, and Prometheus scrapes metrics from it. Alerts are set up to detect performance degradation.</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#_3","title":"Spanda Bootcamp Day 1","text":"<p>2. Auto-triggering LLM Retraining/Fine-tuning</p> <p>Prometheus triggers alerts when the model performance degrades. These alerts are managed by AlertManager, which uses Argo Events to trigger a retraining pipeline to fine-tune the model.</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#_4","title":"Spanda Bootcamp Day 1","text":"<p>3. Training, Evaluating, and Logging the Retrained LLM</p> <p>The retraining pipeline is orchestrated using Argo Workflows. This pipeline can be tailored to perform LLM-specific retraining, fine-tuning, and metrics tracking. MLflow is used for logging the retrained LLM.</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#_5","title":"Spanda Bootcamp Day 1","text":"<p>4. Triggering the Generation of New Vectors for Fresh Data</p> <p>MinIO is used for unstructured data storage. Argo Events is set up to listen for upload events on MinIO, triggering a vector ingestion workflow when new data is uploaded.</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#_6","title":"Spanda Bootcamp Day 1","text":"<p>5. Ingesting New Vectors into the Knowledge Base</p> <p>Argo Workflows is used to run a vector ingestion pipeline that utilizes LlamaIndex for generating and ingesting vectors. These vectors are stored in Milvus, which serves as the knowledge base for retrieval-augmented generation.</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#_7","title":"Spanda Bootcamp Day 1","text":"<p>Stack Overview</p> <p>This stack relies on several key components:</p> <ul> <li>ArgoCD: A Kubernetes-native continuous delivery tool that manages all components in the BigBertha stack.</li> <li>Argo Workflows: A Kubernetes-native workflow engine used for running vector ingestion and model retraining pipelines.</li> <li>Argo Events: A Kubernetes-native event-based dependency manager that connects various applications and components, triggering workflows based on events.</li> <li>Prometheus + AlertManager: Used for monitoring and alerting related to model performance.</li> <li>LlamaIndex: A framework for connecting LLMs and data sources, used for data ingestion and indexing.</li> <li>Milvus: A Kubernetes-native vector database for storing and querying vectors.</li> <li>MinIO: An open-source object storage system used for storing unstructured data.</li> <li>MLflow: An open-source platform for managing the machine learning lifecycle, including experiment tracking and model management.</li> <li>Kubernetes: The container orchestration platform that automates the deployment, scaling, and management of containerized applications.</li> <li>Docker Containers: Docker containers are used for packaging and running applications in a consistent and reproducible manner.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_8","title":"Spanda Bootcamp Day 1","text":"<p>Demo Chatbot</p> <p>As a demonstration, the framework includes a Streamlit-based chatbot that serves a Llama2 7B quantized chatbot model. </p> <p>A simple Flask app is used to expose metrics, and Redis acts as an intermediary between Streamlit and Flask processes.</p> <p>Section 7__________</p> <p>** Pipeline Debt, Data Testing, Model Testing in MLOps **</p> <ul> <li>You are now part of a data science team at your organization </li> <li>Your team has a number of machine learning models in place</li> <li>Their outputs guide critical business decisions, as well as dashboards displaying important financial KPIs </li> <li>These KPIs are closely watched by your executives day/night </li> <li>Early AM, as you are navigating traffic to your office, you suddenly start receiving multiple messages, calls and emails (simultaneously) </li> <li>These are from your manager as well as other teams and from senior management </li> <li>They are all complaining about the same thing: The high visibility business metrics dashboard that you and your team had built the pipelines &amp; dashboards for and deployed as well as many of the dashboards  that many other teams were using (which you did not know until today) were displaying what seemed to be random numbers (except every full hour, when the KPIs look okay for a short time) </li> <li>The financial models that are part of the pipelines are predicting the company\u2019s insolvency looming fast.</li> <li>Once you get in and hurriedly try to put in some quick fixes in the pipeline (excluding predictions beyond thresholds etc.) you find out that every fix results in your data engineering and research teams reporting new broken services and models in the pipelines.</li> <li>This is the Debt Collection Day scenario we are trying to avoid desperately here. </li> <li>Of all debts in data engineering the most vengeful unpaid debt is Pipeline Debt. </li> </ul> <p>The roots of  Pipeline Debt (aka The road to hell is paved with good intentions)</p> <ul> <li>A few months ago You were just about to start that new exciting machine learning project. </li> <li>You had located useful data scattered around your company\u2019s databases, feature stores, documents, videos, audios and spreadsheets belonging to employees which they (reluctantly) gave you access to (cross silo collaboration is hard). </li> <li>To make the data usable, you constructed a data pipeline: a set of jobs and Python functions that ingest, process, clean and combine all these data. </li> <li>The pipeline feeds the data into a machine learning model. </li> <li>The entire process is depicted schematically below.</li> </ul> <p> Simple manageable data pipelines </p> <p>The first data pipelines worked well, consistently populating the downstream machine learning model with data, which turned it into accurate predictions. </p> <p>However, the model deployed as a service in the cloud was not very actionable.To make it more useful, you built a set of dashboards for presenting the model\u2019s output as well as important KPIs to the business stakeholders, The pipeline deepened</p> <p> Extended pipelines</p> <ul> <li>You were telling a colleague from the research team about your project over lunch who decided to do something similar with their data, making the company\u2019s data pipeline wider and cross-team-border.</li> </ul> <p></p> <pre><code>More pipelines, more complexity\n</code></pre> <ul> <li>A few weeks later the two of you who were informally collaborating on these dashboards got together and talked about each of your pipelines and dashboards. </li> <li>As you\u2019ve learned more about what the research team was up to, both of you noticed how useful and valuable it would be if your two teams used each other\u2019s data for powering your respective models and analyses. </li> <li>Upon implementing this idea, the company\u2019s data pipeline was looking like this.</li> </ul> <p> If multiple pipelines exist, they will inevitably blend</p> <ul> <li>This diagram should have made you flinch \u2013 what they show is accumulating pipeline debt which is technical debt in data pipelines </li> <li>It arises when your data pipelines are triple-U: Undocumented, Untested, Unstable</li> <li>It comes in many flavors but all share some characteristics. </li> <li>The system is entangled; so, a change in one place can derail a different process elsewhere. </li> <li>This makes code refactoring and debugging exceptionally hard. </li> <li>For a software engineer, this will sound like a solved problem </li> <li>The solution is called automated testing. </li> <li>However, testing software is very different from testing data in two major ways:<ul> <li>First, while you have full control over your code and can change it when it doesn\u2019t work, you can\u2019t always change your data; in many cases, you are merely an observer watching data as it comes, generated by some real-world process.</li> <li>Second, software code is always right or wrong: either it does what it is designed to do, or it doesn\u2019t. Data is never right or wrong. It can only be suitable or not for a particular purpose. </li> </ul> </li> <li>This is why automated testing needs a special approach when data is involved.</li> </ul> <p>Testing machine learning models</p> <ul> <li>Fundamentally when testing ML Models, we are asking the question: \u201cDo we know if the model actually works?\u201d </li> <li>We want to be sure that the learned model will behave consistently and produce the results expected of it per expectation. </li> </ul> <p></p> <p>A typical workflow for software development.</p> <ul> <li>In traditional software development, when we run our testing suite against the code, we'll get a report of the specific behaviors that we've written tests around and verify that our code changes don't affect the expected behavior of the system. </li> <li>If a test fails, we'll know which specific behavior is no longer aligned with our expected output. </li> <li>We can also look at this testing report to get an understanding of how extensive our tests are by looking at metrics such as code coverage.</li> </ul> <p></p> <ul> <li>Unlike traditional software applications, it is not as straightforward to establish a standard for testing ML applications </li> <li>This is because the tests do not just depend on the software, they also rely on<ul> <li>the business context</li> <li>problem domain</li> <li>the dataset used</li> <li>the model selected. </li> </ul> </li> <li>Most teams are comfortable using model evaluation metrics to quantify a model\u2019s performance before deploying it, but these metrics are just not enough to ensure ML models are ready for production deployment and use. </li> <li>Contrast a typical software development workflow with one for developing machine learning systems. <ul> <li>After training a new model, we'll typically produce an evaluation report including:</li> <li>performance of an established metric on a validation dataset,</li> <li>plots such as precision-recall curves,</li> <li>operational statistics such as inference speed,</li> <li>examples where the model was most confidently incorrect,</li> </ul> </li> </ul> <p>We will rigorously follow practices such as:</p> <ul> <li>Save all of the hyper-parameters used to train the model along with the model,</li> <li>Only promote models which offer an improvement over the existing model (or baseline) when evaluated on the same dataset.</li> </ul> <p> A typical workflow for model development.</p> <ul> <li>When reviewing a new machine learning model, we'll inspect metrics and plots which summarize model performance over a validation dataset. </li> <li>We're able to compare performance between multiple models and make relative judgements, but we're not immediately able to characterize specific model behaviors. </li> <li>For example, figuring out where the model is failing usually requires additional investigative work </li> <li>One common practice here is to look through a list of the top most egregious model errors on the validation dataset and manually categorize these failure modes.</li> <li>Assuming we write behavioral tests for our models (discussed below), there's also the question of whether or not we have enough tests! </li> <li>While traditional software tests have metrics such as the lines of code covered when running tests, this becomes harder to quantify when you shift your application logic from lines of code to parameters of a machine learning model. </li> <li>Do we want to quantify our test coverage with respect to the input data distribution? Or perhaps the possible activations inside the model?</li> <li>_Odena et al. introduce one possible metric for coverage where we track the model logits for all of the test examples and quantify the area covered by radial neighborhoods around these activation vectors. _</li> <li>_As an industry we don't have well-established standards here _</li> <li>_Testing for machine learning systems is in early days _</li> <li>The question of ML Model Test coverage isn't really being asked by most people (certainly not in industry).</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#difference-between-model-testing-and-model-evaluation","title":"Difference between model testing and model evaluation","text":"<ul> <li>While reporting evaluation metrics is certainly a good practice for quality assurance during model development, it is insufficient. </li> <li>Without a granular report of specific behaviors, we won't be able to immediately understand the nuances of how behavior may change if we switch over to the new model. </li> <li>Additionally, we won't be able to track (and prevent) behavioral regressions for specific failure modes that had been previously addressed.</li> <li>This can be especially dangerous for machine learning systems since oftentimes failures happen silently. </li> <li>For example, <ul> <li>you might improve the overall evaluation metric but introduce a regression on a critical subset of data. </li> <li>Or you could unknowingly add a gender bias to the model through the inclusion of a new dataset during training. </li> </ul> </li> <li>We need more nuanced reports of model behavior to identify such cases, which is exactly where model testing can help.</li> <li>For machine learning systems, we should be running model evaluation and model tests in parallel.</li> <li>Model evaluation covers metrics and plots which summarize performance on a validation or test dataset.</li> <li>Model testing involves explicit checks for behaviors that we expect our model to follow.</li> <li>Both of these perspectives are instrumental in building high-quality models.</li> <li>In practice, most people are doing a combination of the two where evaluation metrics are calculated automatically and some level of model \"testing\" is done manually through error analysis (i.e. classifying failure modes). </li> <li>Developing model tests for machine learning systems can offer a systematic approach towards error analysis.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#how-do-you-write-model-tests","title":"How do you write model tests?","text":"<p>There are two types of model tests needed.</p> <ul> <li>Pre-train tests allow us to identify some bugs early on and short-circuit a training job.</li> <li>Post-train tests use the trained model artifact to inspect behaviors for a variety of important scenarios that we define.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#pre-train-tests","title":"Pre-train tests","text":"<p>There are some tests that we can run without needing trained parameters. These tests include:</p> <ul> <li>check the shape of your model output and ensure it aligns with the labels in your dataset</li> <li>check the output ranges and ensure it aligns with our expectations (eg. the output of a classification model should be a distribution with class probabilities that sum to 1)</li> <li>make sure a single gradient step on a batch of data yields a decrease in your loss</li> <li>make assertions about your datasets</li> <li>check for label leakage between your training and validation datasets</li> </ul> <p>The main goal here is to identify some errors early so we can avoid a wasted training job.</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#post-train-tests","title":"Post-train tests","text":"<ul> <li>However, in order for us to be able to understand model behaviors we'll need to test against trained model artifacts. </li> <li>These tests aim to interrogate the logic learned during training and provide us with a behavioral report of model performance.</li> </ul> <p>Reference: Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</p> <ul> <li>Three different types of model tests can be used to understand behavioral attributes.</li> <li>Invariance Tests</li> <li>Invariance tests allow us to describe a set of perturbations we should be able to make to the input without affecting the model's output. </li> <li>We can use these perturbations to produce pairs of input examples (original and perturbed) and check for consistency in the model predictions. </li> <li>This is closely related to the concept of data augmentation, where we apply perturbations to inputs during training and preserve the original label.</li> <li>For example, imagine running a sentiment analysis model on the following two sentences:<ul> <li>Mark was a great instructor.</li> <li>Samantha was a great instructor.</li> </ul> </li> <li>We would expect that simply changing the name of the subject doesn't affect the model predictions.</li> <li>Directional Expectation Tests</li> <li>Directional expectation tests, on the other hand, allow us to define a set of perturbations to the input which should have a predictable effect on the model output.</li> <li>For example, if we had a housing price prediction model we might assert:</li> <li>Increasing the number of bathrooms (holding all other features constant) should not cause a drop in price.</li> <li>Lowering the square footage of the house (holding all other features constant) should not cause an increase in price.</li> <li>Let's consider a scenario where a model fails the second test - taking a random row from our validation dataset and decreasing the feature <code>house_sq_ft</code> yields a higher predicted price than the original label. <ul> <li>This is surprising as it doesn't match our intuition, so we decide to look further into it </li> <li>We realize that,_ without having a feature for the house's neighborhood/location, our model has learned that smaller units tend to be more expensive; this is due to the fact that smaller units from our dataset are more prevalent in cities where prices are generally higher. _</li> </ul> </li> <li>In this case, the selection of our dataset has influenced the model's logic in unintended ways - this isn't something we would have been able to identify simply by examining performance on a validation dataset.</li> <li>Minimum Functionality Tests (aka data unit tests)</li> <li>Just as software unit tests aim to isolate and test atomic components in your codebase, data unit tests allow us to quantify model performance for specific cases found in your data.</li> <li>This allows you to identify critical scenarios where prediction errors lead to high consequences. </li> <li>You may also decide to write data unit tests for failure modes that you uncover during error analysis; this allows you to \"automate\" searching for such errors in future models.</li> <li>Take a look at Snorkel (https://www.snorkel.org/) who have introduced a very similar approach through their concept of _slicing functions. _</li> <li>These are programmatic functions which allow us to identify subsets of a dataset which meet certain criteria. </li> <li>For example, you might write a slicing function to identify sentences less than 5 words to evaluate how the model performs on short pieces of text.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#organizing-tests","title":"Organizing tests","text":"<ul> <li>In traditional software tests, we typically organize our tests to mirror the structure of the code repository. </li> <li>However, this approach doesn't translate well to machine learning models since our logic is structured by the parameters of the model.</li> <li>The authors of the CheckList paper linked above recommend structuring your tests around the \"skills\" we expect the model to acquire while learning to perform a given task.</li> </ul> <p>For example, a sentiment analysis model might be expected to gain some understanding of:</p> <ul> <li>vocabulary and parts of speech,</li> <li>robustness to noise,</li> <li>identifying named entities,</li> <li>temporal relationships,</li> <li>and negation of words.</li> </ul> <p>For an image recognition model, we might expect the model to learn concepts such as:</p> <ul> <li>object rotation,</li> <li>partial occlusion,</li> <li>perspective shift,</li> <li>lighting conditions,</li> <li>weather artifacts (rain, snow, fog),</li> <li>and camera artifacts (ISO noise, motion blur).</li> <li></li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_9","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Model development pipeline**\n</code></pre> <ul> <li>We also need to perform thorough testing of the models to ensure they are robust enough for real-world use.</li> <li>Let's go through some of the ways we can perform testing for different scenarios.ML testing is problem-dependent. </li> <li>This is not a template approach but rather a guide to what types of test suites you might want to establish for your application based on your use case.</li> </ul> <p>**Developing, testing, and deploying machine learning models **</p> <p>Combining automated tests and manual validation for effective model testing</p> <ul> <li>To perform ML testing in their projects, this approach involves having a few levels of tests suites, as well as validation:</li> <li>Automated tests for model verification</li> <li>Manual model evaluation and validation.</li> <li>To implement automated tests in their workflow, GitOps can be used. </li> <li>Jenkins runs code quality checks and smoke tests using production-like runs in the test environment. </li> <li>A single pipeline for model code is created where every pull request goes through code reviews and automated unit tests.</li> <li>The pull requests also go through automated smoke tests. </li> <li>The automated test suites\u2019 goal was to make sure tests flagged erroneous code early in the development process.</li> <li>After the automation tests are run and passed by the model pipeline, a domain expert manually reviewed the evaluation metrics to make sure that they made sense, validated them, and marked them ready for deployment.</li> </ul> <p>Automated tests for model verification</p> <p>The workflow for the automated tests will be that whenever someone on the team made a commit, </p> <ul> <li>the smoke test would run to ensure the code worked, </li> <li>then the unit tests would run, making sure that the assertions in the code and data were met. </li> <li>Finally, the integration tests would run to ensure the model works well with other components in the pipeline.</li> </ul> <p>Automated smoke test</p> <ul> <li>Every pull request goes through automated smoke tests where the team trained models and made predictions, running the entire end-to-end pipeline on some small chunk of actual data to ensure the pipeline worked as expected and nothing broke. </li> <li>The right kind of testing for the smoke suite can give any team a chance to understand the quality of their pipeline before deploying it. </li> <li>Running the smoke test suite does not mean the entire pipeline is guaranteed to be fully working because the code passed. </li> <li>The team has to consider the unit test suite to test data and model assumptions.</li> </ul> <p>Automated unit and integration tests</p> <ul> <li>The unit and integration tests the team run will check assertions about the dataset to prevent low-quality data from entering the training pipeline and prevent problems with the data preprocessing code. </li> <li>You could think of these assertions as assumptions the team made about the data. </li> <li>For example, they would expect to see some kind of correlation in the data or see that the model\u2019s prediction bounds are non-negative.</li> <li>Unit testing machine learning code is more challenging than typical software code. </li> <li>Unit testing several aspects of the model code is very difficult for a team. </li> <li>For example, to test accurately, teams would have to train the model, and even with a modest data set, a unit test could take a long time.</li> <li>Furthermore, some of the tests could be erratic and flaky (failed at random). </li> <li>One of the challenges of running the unit tests to assert the data quality is that running these tests on sample datasets was more complex and took way less time than running them on the entire dataset. </li> <li>It was difficult to fix for the team but to address the issues. </li> <li>Some teams opt to eliminate part of the unit tests in favor of smoke tests. </li> <li>The team defines acceptance criteria and their test suite continuously evolves as they experiment by adding new tests, and removing others, gaining more knowledge on what works and what doesn't.</li> <li>The model is trained in a production-like environment on a complete dataset for each new pull request, except that they would adjust the hyperparameters at values that resulted in quick results. Finally, they would monitor the pipeline\u2019s health for any issues and catch them early.</li> </ul> <p> The MLOps toolstack including testing tools</p> <p>Manual model evaluation and validation</p> <ul> <li>Have a human-in-the-loop framework where after training the model, reports are  created with different plots showing results based on the dataset, so the domain experts could review them before the model could be shipped.</li> <li>After training the model, a domain expert generated and reviewed a model quality report. </li> <li>The expert would approve (or deny) the model through a manual auditing process before it could eventually be shipped to production by the team after getting validation and passing all previous tests.</li> </ul> <p>Stress Tests and A/B Tests</p> <ul> <li>Once the pipeline generates the build (a container image), the models are stress-tested in a production-like environment through the release pipelines. </li> <li>Depending on the use case, the team also carried out A/B tests to understand how their models performed in varying conditions before they deployed them, rather than relying purely on offline evaluation metrics. </li> <li>With what they learned from the A/B tests, they knew whether a new model improved a current model and tuned their model to optimize the business metrics better.</li> </ul> <p>Stress testing machine learning models</p> <ul> <li>Testing the model\u2019s performance under extreme workloads is crucial for business applications that typically expect high traffic from users. </li> <li>Therefore, the team performed stress tests to see how responsive and stable the model would be under an increased number of prediction requests at a given time scale. </li> <li>This way, they benchmarked the model\u2019s scalability under load and identified the breaking point of the model. In addition, the test helped them determine if the model\u2019s prediction service meets the required service-level objective (SLO) with uptime or response time metrics.</li> <li>It is worth noting that the point of stress testing the model isn\u2019t so much to see how many inference requests the model could handle as to see what would happen when users exceed such traffic. </li> <li>This way, you can understand the model\u2019s performance problems, including the load time, response time, and other bottlenecks.</li> </ul> <p>Testing model quality after deployment</p> <ul> <li>The goal of the testing production models is to ensure that the deployment of the model is successful and the model works correctly in production together with other services. For this team, testing the inference performance of the model in production was a crucial process for continuously providing business value. </li> <li>In addition, the team tested for data and model drift to make sure models could be monitored and perhaps retrained when such drift was detected. On another note, testing production models can enable teams to perform error analysis on their mission-critical models through manual inspection from domain experts.</li> </ul> <p> - An example of a dashboard showing information on data drift for a machine learning project in Azure ML Studio | Source</p> <p>Drift MonitoringExample:</p> <p>https://github.com/keshavaspanda/drift-monitoring</p> <p>** Behavioral tests for ML (Natural language processing (NLP) and classification tasks)**</p> <ul> <li>Business use case: The transaction metadata product at MonoHQ uses machine learning to classify transaction statements that are helpful for a variety of corporate customer applications such as credit application, asset planning/management, BNPL (buy now pay later), and payment. Based on the narration, the product classifies transactions for thousands of customers into different categories.</li> <li>Before deploying the model, the team conducts a behavioral test. This test consists of 3 elements:</li> <li>Prediction distribution,</li> <li>Failure rate,</li> <li>Latency.</li> <li>If the model passes the three tests, the team lists it for deployment. If the model does not pass the tests, they would have to re-work it until it passes the test. They always ensure that they set a performance threshold as a metric for these tests.</li> <li>They also perform A/B tests on their models to learn what version is better to put into the production environment.</li> </ul> <p>Behavioral tests to check for prediction quality</p> <ul> <li>This test shows how the model responds to inference data, especially NLP models. </li> <li>First, the team runs an invariance test, introducing perturbability to the input data.</li> <li>Next, they check if the slight change in the input affects the model response\u2014its ability to correctly classify the narration for a customer transaction. </li> <li>Essentially, they are trying to answer here: does a minor tweak in the dataset with a similar context produce consistent output?</li> </ul> <p>Performance testing for machine learning models</p> <ul> <li>To test the response time of the model under load, the team configures a testing environment where they would send a lot of traffic to the model service. Here\u2019s their process:</li> <li>They take a large amount of transaction dataset,</li> <li>Create a table, </li> <li>Stream the data to the model service,</li> <li>Record the inference latency,</li> <li>And finally, calculate the average response time for the entire transaction data.</li> <li>If the response time passes a specified latency threshold, it is up for deployment. If it doesn\u2019t, the team would have to rework it to improve it or devise another strategy to deploy the model to reduce the latency. </li> </ul> <p>A/B testing machine learning models</p> <ul> <li>For this test, the team containerizes two models to deploy to the production system for upstream services to consume to the production system. </li> <li>They deploy one of the models to serve traffic from a random sample of users and another to a different sample of users so they can measure the real impact of the model\u2019s results on their users. </li> <li>In addition, they can tune their models using their real customers and measure how they react to the model predictions. </li> <li>This test also helps the team avoid introducing complexity from newly trained that are difficult to maintain and add no value to their users.</li> </ul> <p>** Performing  model engineering and statistical tests for machine learning applications**</p> <ul> <li>This team performed two types of tests on their machine learning projects:</li> <li>Engineering-based tests (unit and integration tests),</li> <li>Statistical-based tests (model validation and evaluation metrics). </li> <li>The engineering team ran the unit tests and checked whether the model threw errors. </li> <li>Then, the data team would hand off (to the engineering team) a mock model with the same input-output relationship as the model they were building. </li> <li>Also, the engineering team would test this model to ensure it does not break the production system and then serve it until the correct model from the data team is ready.</li> <li>Once the data team and stakeholders evaluate and validate that the model is ready for deployment, the engineering team will run an integration test with the original model. </li> <li>Finally, they will swap the mock model with the original model in production if it works.</li> </ul> <p>Engineering-based test for machine learning models</p> <ul> <li>Unit and integration tests</li> <li>To run an initial test to check if the model will integrate well with other services in production, the data team will send a mock (or dummy) model to the engineering team. </li> <li>The mock model has the same structure as the real model, but it only returns the random output. </li> <li>The engineering team will write the service for the mock model and prepare it for testing.</li> <li>The data team will provide data and input structures to the engineering team to test whether the input-output relationships match with what they expect, if they are coming in the correct format, and do not throw any errors. </li> <li>The engineering team does not check whether that model is the correct model; they only check if it works from an engineering perspective. </li> <li>They do this to ensure that when the model goes into production, it will not break the product pipeline.</li> <li>When the data team trains and evaluates the correct model and stakeholders validate it, the data team will package it and hand it off to the engineering team. </li> <li>The engineering team will swap the mock model with the correct model and then run integration tests to ensure that it works as expected and does not throw any errors.</li> </ul> <p>Statistical-based test for machine learning models</p> <ul> <li>The data team would train, test, and validate their model on real-world data and statistical evaluation metrics. </li> <li>The head of data science audits the results and approves (or denies) the model. If there is evidence that the model is the correct model, the head of data science will report the results to the necessary stakeholders. </li> <li>He will explain the results and inner workings of the model, the risks of the model, and the errors it makes, and confirm if they are comfortable with the results or the model still needs to be re-worked. </li> <li>If the model is approved, the engineering team swaps the mock model with the original model, reruns an integration test to confirm that it does not throw any error, and then deploy it.</li> <li>Model evaluation metrics are not enough to ensure your models are ready for production. </li> <li>You also need to perform thorough testing of your models to ensure they are robust enough for real-world encounters.</li> <li>Developing tests for ML models can help teams systematically analyze model errors and detect failure modes, so resolution plans are made and implemented before deploying the models to production.</li> </ul> <p>**Automated testing with Great Expectations **</p> <p></p> <p>Automated testing tailored for data pipelines is the premise of Great Expectations, a widely used open-source Python package for data validation.</p> <p>https://medium.com/dataroots/great-expectations-tutorial-by-paolo-l%C3%A9onard-95e689d73702</p> <ul> <li>The package is built around the concept of an expectation. </li> <li>The expectation can be thought of as a unit test for data. It is a declarative statement that describes the property of a dataset and does so in a simple, human-readable language.</li> <li>For example, to assert that the values of the column \u201cnum_complaints\u201d in some table in between one and five, you can write:</li> </ul> <p>expect_column_values_to_be_between(</p> <pre><code>column=\"num_complaints\",\n\nmin_value=1,\n\nmax_value=5,\n</code></pre> <p>)</p> <ul> <li>This statement will validate your data and return a success or a failure result. </li> <li>As we have already mentioned, you do not always control your data but rather passively observe it flowing. It is often the case that an atypical value pops up in your data from time to time without necessarily being a reason for distress. Great - Expectations accommodate this via the \u201cmostly\u201d keyword which allows for describing how often should the expectation be matched.</li> </ul> <p>expect_column_values_to_be_between(</p> <pre><code>column=\"num_complaints\",\n\nmin_value=1,\n\nmax_value=5,\n\nmostly=0.95,\n</code></pre> <p>)</p> <ul> <li>The above statement will return success if at least 95% of \u201cnum_complaints\u201d values are between one and five.</li> <li>In order to understand the data well, it is crucial to have some context about why we expect certain properties from it. </li> <li>We can simply add it by passing the \u201cmeta\u201d  parameter to the expectation with any relevant information about how it came to be. Our colleagues or even our future selves will thank us for it.</li> </ul> <p>expect_column_values_to_be_between(</p> <pre><code>column=\"num_complaints\",\n\nmin_value=1,\n\nmax_value=5,\n\nmostly=0.95,\n\nmeta={\n\n    \u201ccreated_by\u201d: \u201cMichal\u201d,\n\n    \u201ccraeted_on\u201d: \u201c28.03.2022\u201d,\n\n    \u201cnotes\u201d: \u201cnumber of client complaints; more than 5 is unusual\u201d\n\n             \u201cand likely means something broke\u201d,\n\n}\n</code></pre> <p>)</p> <ul> <li>These metadata notes will also form a basis for the data documentation which Great Expectations can just generate out of thin air \u2013 but more on this later!</li> <li>The package contains several dozen expectations to use out of the box, all of them with wordy, human-readable names such as \u201cexpect_column_distinct_values_to_be_in_set\u201d, \u201cexpect_column_sum_to_be_between\u201d, or \u201cexpect_column_kl_divergence_to_be_less_than\u201d. This syntax allows one to clearly state what is expected of the data and why. </li> <li>Some expectations are applicable to column values, others to their aggregate functions or entire density distributions. Naturally, the package also makes it possible to easily create custom expectations for when a tailored solution is needed.</li> <li>Great Expectations works with many different backends. </li> <li>You can evaluate your expectations locally on a Pandas data frame just as easily as on a SQL database (via SQLAlchemy) or on an Apache Spark cluster.</li> <li>So, how do the expectations help to reduce pipeline debt? The answer to this is multifold. </li> <li> <ol> <li>The process of crafting the expectations forces us to sit and ponder about our data: its nature, sources, and what can go wrong with it. This creates a deeper understanding and improves data-related communication within the team.</li> </ol> </li> <li> <ol> <li>By clearly stating what we expect from the data, we can detect any unusual situations such as system outages early on.</li> </ol> </li> <li> <ol> <li>By validating new data against a set of pre-existing expectations we can be sure we don\u2019t feed our machine learning models garbage.</li> </ol> </li> <li> <ol> <li>Having the expectations defined brings us very close to having well-maintained data documentation in place. The list goes on and on.</li> </ol> </li> </ul> <p>A few specific use cases in which investing time in GE pays back a great deal are:</p> <p>Detecting data drift</p> <ul> <li>A notorious danger to machine learning models deployed in production is data drift. Data drift is a situation when the distribution of model inputs changes. This can happen for a multitude of reasons: data-collecting devices tend to break or have their software updated, which impacts the way data is being recorded. If the data is produced by humans, it is even more volatile as fashions and demographics evolve quickly.</li> <li>Data drift constitutes a serious problem for machine learning models. It can make the decision boundaries learned by the algorithm invalid for the new-regime data, which has a detrimental impact on the model\u2019s performance.</li> <li>Data drift may impact the model\u2019s performance</li> <li>You have collected and cleaned your data, experimented with various machine learning models and data preprocessing variants and fine-tuned your model\u2019s hyperparameters to finally come up with a solution good enough for your problem. </li> <li>Then, you\u2019ve built a robust, automatic data pipeline, wrote an API for the model, put it in a container, and deployed it to the production environment. </li> <li>You even made sure to check that the model runs smoothly and correctly in production. Finally, you're done! Or are you? </li> <li>Not even close. In fact, this is just the beginning of the journey.</li> <li>There are so many things that could go wrong with a machine learning system after it has been deployed to production! </li> <li>Broadly speaking, we can divide all these potential concerns into two buckets: statistical issues and infrastructure issues. </li> <li>The latter comprise things like computing resources and memory (are there enough?), latency (is the model responding quickly enough?), throughput (can we answer all the incoming requests?), and so on. </li> <li>Here, we\u2019ll focus on the former: the statistical issues, which come in two main flavors: data drift and concept drift.</li> <li>Enters data validation. </li> <li>In situations where data drift could be of concern, just create expectations about the model input features that validate their long-term trend, average values, or historic range and volatility. </li> <li>As soon as the world changes and your incoming data starts to look differently, GE will alert you by spitting out an array of failed tests!</li> </ul> <p>Preventing outliers from distorting model outputs</p> <ul> <li>Another threat to models deployed in production, slightly similar to the data drift, are outliers. </li> <li>What happens to a model\u2019s output when it gets an unusual value as input, typically very high or very low? </li> <li>If the model has not seen such an extreme value during training, an honest answer for it would be to say: I don\u2019t know what the prediction should be!</li> <li>Unfortunately, machine learning models are not this honest. Much to the contrary: the model will likely produce some output that will be highly unreliable without any warning.</li> <li>Fortunately, one can easily prevent it with a proper expectations suite! Just set allowed ranges for the model\u2019s input features based on what it has seen in training to make sure you are not making predictions based on outliers.</li> </ul> <p>Preventing pipeline failures from spilling over</p> <ul> <li>Data pipelines do fail sometimes. You might have missed a corner case. Or the power might have gone off for a moment in your server room. </li> <li>Whatever the reason, it happens that a data processing job expecting new files to appear somewhere suddenly finds none.</li> <li>If this makes the code fail, that\u2019s not necessarily bad.  </li> <li>But often it doesn\u2019t: the job succeeds, announcing happily to the downstream systems that your website had 0 visits on the previous day. </li> <li>These data points are then shown on KPI dashboards or even worse, are fed into models that automatically retrain. </li> <li>Q: How do we prevent such a scenario? Expect recent data \u2013 for instance, with a fresh enough timestamp \u2013 to be there.</li> </ul> <p>Detecting harmful biases</p> <ul> <li>Bias in machine learning models is a topic that has seen increasing awareness and interest recently. </li> <li>This is crucial, considering how profoundly the models can impact people\u2019s lives. The open question is how to detect and prevent these biases from doing charm.</li> <li>While by no means do they provide an ultimate answer, Great Expectations can at least help us in detecting dangerous biases. </li> </ul> <p>Fairness</p> <ul> <li>Fairness in machine learning is a vast and complex topic, so let us focus on two small parts of the big picture: the training data that goes into the model, and the predictions produced by it for different test inputs.</li> <li>When it comes to the training data, we want it to be fair and unbiased, whatever that means in our particular case. </li> <li>If the data is about users, for instance, you might want to include users from various geographies in appropriate proportions, matching their global population. Whether or not this is the case can be checked by validating each batch of training data against an appropriate expectations suite before the data is allowed to be used for training.</li> <li>As for the model\u2019s output, we might want it, for instance, to produce the same predictions for both women and men if their remaining characteristics are the same. To ensure this, just test the model on a hold-out test set and run the results against a pre-crafted suite of expectations.</li> </ul> <p>Improving team communication and data understanding.</p> <ul> <li>Finally, we could start off by creating an empty expectations suite, that is: list all the columns, but don\u2019t impose any checks on their values yet. </li> <li>Then, get together people who own the data or the business processes involved and ask them:</li> <li>What is the maximal monthly churn rate that is worrisome? </li> <li>How low does the website stickiness have to fall to trigger an alert? Such conversations can improve the data-related communication between the teams and the understanding of the data themselves in the company</li> </ul> <p>Resources</p> <ul> <li>Great Expectations official documentation. </li> </ul> <p>Testing LLMs</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#testing-large-language-models-with-wb-and-giskard","title":"Testing Large Language Models with W&amp;B and Giskard","text":"<ul> <li>Combining W&amp;B with Giskard to deeply understand LLM behavior and avoid common pitfalls like hallucinations and injection attacks</li> <li>According to the Open Worldwide Application Security Project, some of the most critical vulnerabilities that affect LLMs are prompt injection (when LLMs are manipulated to behave as the attacker wishes), sensitive information disclosure (when LLMs inadvertently leak confidential information), and hallucination (when LLMs generate inaccurate or inappropriate content).</li> <li>Giskard's scan feature ensures the identification of these vulnerabilities\u2014and many others. </li> <li>The library generates a comprehensive report which quantifies these into interpretable metrics. The Giskard/W&amp;B integration allows the logging of both the report and metrics into W&amp;B, which in conjunction with the tracing, creates the ideal combination for building and debugging LLM apps.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#giskards-vulnerability-scanning-for-llms","title":"Giskard's vulnerability scanning for LLMs","text":"<ul> <li>Giskard is an open-source testing framework dedicated to ML models, covering any Python model, from tabular to LLMs.</li> <li>Testing machine learning applications can be tedious: Where to start testing? Which tests to implement? What issues to cover? How do we implement the tests?</li> <li>With Giskard, data scientists can scan their model to find dozens of hidden vulnerabilities, instantaneously generate domain-specific tests, and leverage the Quality Assurance best practices of the open-source community.</li> <li>For more information, you can check Giskard's documentation following this link.</li> <li>Watch: https://www.youtube.com/watch?v=KeY6qPAvyq0</li> <li>The better developer version: https://www.youtube.com/watch?v=rkjFFx_nXhU</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#wb-traces-for-debugging-llms","title":"**W&amp;B Traces for Debugging LLMs: **","text":"<ul> <li>Weights &amp; Biases, often referred to as wandb or even simply W&amp;B, is an MLOps platform that helps AI developers streamline their ML workflow from end to end.</li> <li>With W&amp;B, developers can monitor the progress of training their models in real-time, log key metrics and hyperparameters, and visualize results through interactive dashboards. It simplifies collaboration by enabling team members to share experiments and compare model performance. For more information, you can check W&amp;B's documentation following this link.</li> <li>In the context of LLMs, earlier this year, W&amp;B introduced a new debugging tool \u201cW&amp;B Traces\u201d designed to support ML practitioners working on prompt engineering for LLMs. </li> <li>It lets users visualize and drill down into every component and activity throughout the trace of the LLM pipeline execution. In addition, it enables the review of past results, identification and debugging of errors, gathering insights about the LLM\u2019s behavior, and sharing insights.</li> <li>Tracing is invaluable, but how do we measure the quality of the outputs throughout the pipeline? </li> <li>Could there be hidden vulnerabilities that our carefully-crafted prompts may have inadvertently failed to counter? Is there a way to detect such vulnerabilities automatically? Would it be possible to log these issues into W&amp;B to complement the tracing?</li> <li>In a nutshell, the answer to all these questions is \"yes.\" That's precisely the capability that Giskard brings to the table.</li> <li>Combining Weights &amp; Biases and Giskard, makes it possible to overcome this very challenge with this example available as a Google Colab notebook!</li> </ul> <p>Using LLMs to perform Data Quality:</p> <ul> <li>BirdiDQ https://github.com/keshavaspanda/BirdiDQ  is a simple, intuitive and user-friendly data quality application that allows you to run data quality checks on top of python based great expectation open source library using natural language queries. </li> <li>The idea is to type in your requests, and BirdiDQ will generate the appropriate GE method, run the quality control and return the results along with data docs you need. Demo</li> </ul> <p>Using Large Language Models for Efficient Data Annotation and Model Fine-Tuning with Iterative Active Learning</p> <p></p> <p>https://github.com/keshavaspanda/llm-data-annotation</p> <p>Additional Resource for us to try out:</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#httpsgithubcomkeshavaspandallama-in-a-container","title":"https://github.com/keshavaspanda/llama-in-a-container","text":"<p>Section 8__________</p>"},{"location":"Spanda%20Bootcamp%20Day%201/#training-tuning-and-serving-up-llms-in-production","title":"Training, Tuning and Serving up LLMs in production:","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#serving-up-llms-in-production","title":"Serving up LLMs in production:","text":"<ul> <li>OpenLLM is an open-source platform designed to facilitate the deployment and operation of large language models (LLMs) in real-world applications. </li> <li>With OpenLLM, you can run inference on any open-source LLM, deploy them on the cloud or on-premises, and build powerful AI applications.</li> <li>https://github.com/bentoml/OpenLLM?tab=readme-ov-file</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#trainingfine-tuning-large-language-models-llms-the-first-pass","title":"Training/Fine-Tuning Large Language Models (LLMs) - The First Pass","text":"<pre><code>Comparison of the number of parameters of models. Just look at how big GPT-3 is. And nobody knows about GPT-4\u2026\n</code></pre> <pre><code>LLMs capabilities\n</code></pre> <ul> <li>Creating a local large language model (LLM) is a significant undertaking.</li> <li>It requires substantial computational resources and expertise in machine learning. </li> <li>It was not feasible to run local LLMs on your own local system because of the computational costs involved. </li> <li>However, with the advent of new software, GPT4All and LM-Studio can be used to create complete software packages that work locally. </li> <li>But let\u2019s start with a HuggingFace Transformers source code example that shows you how to use the HuggingFace Libraries and PyTorch for LLMs (cloud-based, not local in this case):</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#consider-the-huggingface-transformers-example","title":"Consider the HuggingFace Transformers Example","text":"<ul> <li>This is a complete program that uses the GPT-2 model, GPT-2 tokenizer, and is fine-tuned on the AG NEWS dataset (a small dataset used for utility purposes) is given below and explained in code snippets. </li> <li>We can leverage the power of pre-trained models and fine-tune them on specific tasks.</li> <li> <p>Importing necessary libraries and modules: </p> <ul> <li>The script starts by importing the necessary libraries and modules. AG_NEWS is a news classification dataset from the \u201ctorchtext.datasets\u201d package. AutoModelWithLMHead and AdamW are imported from the transformers library. </li> <li>AutoModelWithLMHead is a class that provides automatic access to pre-trained models with a language modeling head, and AdamW is a class that implements the AdamW optimizer, a variant of the Adam optimizer with weight decay.</li> </ul> <p>```</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#from-torchtextdatasets-import-ag_news","title":"from torchtext.datasets import AG_NEWS","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#from-transformers-import-automodelwithlmhead-adamw","title":"from transformers import AutoModelWithLMHead, AdamW","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#from-transformers-import-autotokenizer","title":"from transformers import AutoTokenizer","text":"<pre><code>```\n\n\n* **Setting up the tokenizer:** The script uses the AutoTokenizer class from the transformers library to load the tokenizer associated with the \u201cgpt2\u201d model. The tokenizer is responsible for converting input text into a format that the model can understand. This includes splitting the text into tokens (words, subwords, or characters), mapping the tokens to their corresponding IDs in the model\u2019s vocabulary, and creating the necessary inputs for the model (like attention masks).\n* tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n* **Setting the number of epochs:** The script sets the number of epochs for training to 50. An epoch is one complete pass through the entire training dataset. The number of epochs is a hyperparameter that you can tune. Training for more epochs can lead to better results, but it also increases the risk of overfitting and requires more computational resources.\n\n```\n</code></pre>"},{"location":"Spanda%20Bootcamp%20Day%201/#epochs-50","title":"EPOCHS = 50","text":"<pre><code>```\n\n\n* **Preprocessing the data: **The preprocess_data function is defined to preprocess the data. It takes an iterator over the data and encodes the text in each item using the tokenizer. The AG_NEWS dataset is then loaded and preprocessed. The dataset is split into \u2018train\u2019 and the text from each item is encoded. Encoding the text involves splitting it into tokens, mapping the tokens to their IDs in the model\u2019s vocabulary, and creating the necessary inputs for the model.\n\n```\n</code></pre>"},{"location":"Spanda%20Bootcamp%20Day%201/#def-preprocess_datadata_iter","title":"def preprocess_data(data_iter):","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#data-tokenizerencodetext-for-_-text-in-data_iter","title":"data = [tokenizer.encode(text) for _, text in data_iter]","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#return-data","title":"return data","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#train_iter-ag_newssplittrain","title":"train_iter = AG_NEWS(split='train')","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#train_data-preprocess_datatrain_iter","title":"train_data = preprocess_data(train_iter)","text":"<pre><code>```\n</code></pre> <ul> <li> <p>Setting up the model and optimizer: </p> <ul> <li>The script loads the pre-trained \u201cgpt2\u201d model using the AutoModelWithLMHead class and sets up the AdamW optimizer with the model\u2019s parameters. The model is a transformer-based model with a language modeling head, which means it\u2019s designed to generate text. The AdamW optimizer is a variant of the Adam optimizer with weight decay, which can help prevent overfitting.</li> </ul> <p>```</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#model-automodelwithlmheadfrom_pretrainedgpt2","title":"model = AutoModelWithLMHead.from_pretrained(\"gpt2\")","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#optimizer-adamwmodelparameters","title":"optimizer = AdamW(model.parameters())","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#modeltrain","title":"model.train()","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#for-epoch-in-rangeepochs","title":"for epoch in range(EPOCHS):","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#for-batch-in-train_data","title":"for batch in train_data:","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#outputs-modelbatch","title":"outputs = model(batch)","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#loss-outputsloss","title":"loss = outputs.loss","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#lossbackward","title":"loss.backward()","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#optimizerstep","title":"optimizer.step()","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#optimizerzero_grad","title":"optimizer.zero_grad()","text":"<pre><code>```\n</code></pre> <ul> <li>Training the model: The script trains the model for the specified number of epochs. In each epoch, it iterates over the batches of training data, feeds each batch to the model, computes the loss, performs backpropagation with loss.backward(), and updates the model\u2019s parameters with optimizer.step(). It also resets the gradients with optimizer.zero_grad(). This is a standard training loop for PyTorch models.</li> <li> <p>Generating text: After training, the script uses the model to generate text. It starts by encoding a prompt using the tokenizer, then feeds this encoded prompt to the model\u2019s generate method. The output of the generate() method is a sequence of token IDs, which is then decoded back into text using the tokenizer.</p> <p>```</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#prompt-tokenizerencodewrite-a-summary-of-the-new-features-in-the-latest-release-of-the-julia-programming-language-return_tensorspt","title":"prompt = tokenizer.encode(\"Write a summary of the new features in the latest release of the Julia Programming Language\", return_tensors=\"pt\")","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#generated-modelgenerateprompt","title":"generated = model.generate(prompt)","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#generated_text-tokenizerdecodegenerated0","title":"generated_text = tokenizer.decode(generated[0])","text":"<pre><code>```\n</code></pre> <ul> <li> <p>Saving the generated text: Finally, the script saves the generated text to a file named \u201cgenerated.txt\u201d. This is done using Python\u2019s built-in file handling functions.</p> <p>```</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#with-opengeneratedtxt-w-as-f","title":"with open(\"generated.txt\", \"w\") as f:","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#fwritegenerated_text","title":"f.write(generated_text)","text":"<pre><code>```\n</code></pre> <ul> <li>This script is a good example of how to fine-tune a pre-trained language model on a specific task. </li> <li>Fine-tuning a large model like GPT-2 can be computationally intensive and may require a powerful machine or cloud-based resources. </li> <li>This script doesn\u2019t include some important steps like splitting the data into training and validation sets, shuffling the data, and batching the data. _These steps are crucial for training a robust model. _</li> <li> <p>The entire program is given below:</p> <p>```</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#from-torchtextdatasets-import-ag_news_1","title":"from torchtext.datasets import AG_NEWS","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#from-transformers-import-automodelwithlmhead-adamw_1","title":"from transformers import AutoModelWithLMHead, AdamW","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#from-transformers-import-autotokenizer_1","title":"from transformers import AutoTokenizer","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#tokenizer-autotokenizerfrom_pretrainedgpt2","title":"tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#epochs-50_1","title":"EPOCHS = 50","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#def-preprocess_datadata_iter_1","title":"def preprocess_data(data_iter):","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#data-tokenizerencodetext-for-_-text-in-data_iter_1","title":"data = [tokenizer.encode(text) for _, text in data_iter]","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#return-data_1","title":"return data","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#train_iter-ag_newssplittrain_1","title":"train_iter = AG_NEWS(split='train')","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#train_data-preprocess_datatrain_iter_1","title":"train_data = preprocess_data(train_iter)","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#model-automodelwithlmheadfrom_pretrainedgpt2_1","title":"model = AutoModelWithLMHead.from_pretrained(\"gpt2\")","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#optimizer-adamwmodelparameters_1","title":"optimizer = AdamW(model.parameters())","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#modeltrain_1","title":"model.train()","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#for-epoch-in-rangeepochs_1","title":"for epoch in range(EPOCHS):","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#for-batch-in-train_data_1","title":"for batch in train_data:","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#outputs-modelbatch_1","title":"outputs = model(batch)","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#loss-outputsloss_1","title":"loss = outputs.loss","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#lossbackward_1","title":"loss.backward()","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#optimizerstep_1","title":"optimizer.step()","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#optimizerzero_grad_1","title":"optimizer.zero_grad()","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#prompt-tokenizerencodewrite-a-summary-of-the-new-features-in-the-latest-release-of-the-julia-programming-language-return_tensorspt_1","title":"prompt = tokenizer.encode(\"Write a summary of the new features in the latest release of the Julia Programming Language\", return_tensors=\"pt\")","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#generated-modelgenerateprompt_1","title":"generated = model.generate(prompt)","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#generated_text-tokenizerdecodegenerated0_1","title":"generated_text = tokenizer.decode(generated[0])","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#with-opengeneratedtxt-w-as-f_1","title":"with open(\"generated.txt\", \"w\") as f:","text":""},{"location":"Spanda%20Bootcamp%20Day%201/#fwritegenerated_text_1","title":"f.write(generated_text)","text":"<pre><code>```\n</code></pre>"},{"location":"Spanda%20Bootcamp%20Day%201/#there-are-two-packaged-solutions-for-local-llms-and-many-more-popping-up-everyday-two-of-them-are-the-best-one-is-lm-studio-the-other-is-httpsgpt4allioindexhtml","title":"There are two packaged solutions for Local LLMs (and many more popping up, everyday). Two of them are the best. One is LM-Studio. The other is https://gpt4all.io/index.html","text":"<ul> <li>This is the best for those if you want a completely open-source on-premises system. But you need to have at least 32 GB of local RAM, 16 GB GPU RAM, a 3+ Ghz multicore(the more, the better) processor, and a local SSD.  LLMs are computationally, extremely expensive!</li> <li>There\u2019s a lot more to LLM models than just chat</li> <li>Given the expensive;y daunting computational requirements for fine-tuning musical and pictures and audio for LLMs we are not going to run them. </li> <li>Some popular, already built and ready-to-go solutions as well as some interesting source material are:</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#audio-llms","title":"Audio LLMs","text":"<ul> <li>https://www.assemblyai.com/docs/guides/processing-audio-with-llms-using-lemur</li> <li>AudioGPT Research Paper \u2014 https://arxiv.org/abs/2304.12995</li> <li>Tango https://tango-web.github.io/</li> <li>https://blog.google/technology/ai/musiclm-google-ai-test-kitchen/</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#image-llms","title":"Image LLMs","text":"<ul> <li>https://www.linkedin.com/pulse/generating-images-large-language-model-gill-arun-krishnan</li> <li>Stable Diffusion</li> <li>DALL E-1,2,3</li> <li>MidJourney</li> <li>Bing Image Creator</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#multimodal-llms","title":"Multimodal LLMs","text":"<ul> <li>https://arxiv.org/abs/2306.09093 Macaw-LLM research paper. </li> <li>https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models</li> <li>https://openai.com/research/gpt-4 </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#general-llm-resources","title":"General LLM Resources","text":"<ul> <li>https://beebom.com/best-large-language-models-llms/</li> <li>https://roadmap.sh/guides/free-resources-to-learn-llms</li> <li>https://github.com/Hannibal046/Awesome-LLM</li> <li>https://medium.com/@abonia/best-llm-and-llmops-resources-for-2023-75e96ac37feb</li> <li>https://learn.deeplearning.ai/ </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_10","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Fine-Tuning Your LLM - A Revisit**\n</code></pre> <ul> <li>Again. fine-tuning is the process of continuing the training of a pre-trained LLM on a specific dataset. </li> <li>You might ask why we need to train the model further if we can already add data using RAG. </li> <li> <p>The simple answer is that only fine-tuning can tailor your model to understand a specific domain or define its \u201cstyle\u201d. </p> <p>:</p> </li> </ul> <p></p> <pre><code>Classical approach of fine-tuning on domain specific data (all icons from [flaticon](http://flaticon.com/))\n</code></pre> <ol> <li>Take a trained LLM, sometimes called Base LLM. You can download them from HuggingFace.</li> <li>Prepare your training data. You only need to compile instructions and responses. Here\u2019s an example of such a dataset. You can also generate synthetic data using GPT-4.</li> <li>Choose a suitable fine-tuning method. LoRA and QLoRA are currently popular.</li> <li>Fine-tune the model on new data.</li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%201/#_11","title":"Spanda Bootcamp Day 1","text":"<pre><code>**When to Use**\n</code></pre> <ul> <li>Niche Applications: When the application deals with specialized or unconventional topics. For example, legal document applications that need to understand and handle legal jargon.</li> <li>Custom Language Styles: For applications requiring a specific tone or style. For example, creating an AI character whether it\u2019s a celebrity or a character from a book.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_12","title":"Spanda Bootcamp Day 1","text":"<pre><code>**When NOT to Use**\n</code></pre> <ul> <li>Broad Applications: Where the scope of the application is general and doesn\u2019t require specialized knowledge.</li> <li>Limited Data: Fine-tuning requires a significant amount of relevant data. However, you can always generate them with another LLM. For example, the Alpaca dataset of 52k LLM-generated instruction-response pairs was used to create the first finetuning Llama v1 model earlier this year.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_13","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Fine-tuning LLM**\n\n\nLet us look at a high-level library, [Lit-GPT](https://github.com/Lightning-AI/lit-gpt), which hides all complexities, hence doesn\u2019t allow for much customization of the training process, but one can quickly conduct experiments and get initial results.\n\n\nYou\u2019ll need just a few lines of code:\n\n\n```\n# 1. Download the model:\npython scripts/download.py --repo_id meta-llama/Llama-2-7b\n\n# 2. Convert the checkpoint to the lit-gpt format:\npython scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/llama\n\n# 3. Generate an instruction tuning dataset:\npython scripts/prepare_alpaca.py  # it should be your dataset\n\n# 4. Run the finetuning script\npython finetune/lora.py \\\n   --checkpoint_dir checkpoints/llama/\n   --data_dir your_data_folder/\n   --out_dir my_finetuned_model/\n```\n\n\n\nAnd that\u2019s it! Your training process will start:\n</code></pre> <pre><code>_This  takes approximately **10 hours** and **30 GB** memory to fine-tune Falcon-7B on a single A100 GPU._\n</code></pre> <ul> <li>The fine-tuning process is quite complex and to get better results, you\u2019ll need to understand various adapters, their parameters, and much more. </li> <li>However, even after such a simple iteration, you will have a new model that follows your instructions.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_14","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Some References to chase down:**\n</code></pre> <ul> <li>Create a Clone of Yourself With a Fine-tuned LLM \u2014 an article about collecting datasets, using parameters, and  useful tips on fine-tuning.</li> <li>Understanding Parameter-Efficient Fine-tuning of Large Language Models \u2014 an excellent tutorial to get into the details of the concept of fine-tuning and popular parameter-efficient alternatives.</li> <li>Fine-tuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments \u2014 one of my favorite articles for understanding the capabilities of LoRA.</li> <li>OpenAI Fine-tuning \u2014 if you want to fine-tune GPT-3.5 with minimal effort.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_15","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Deploying Your LLM Application in Production**\n</code></pre> <ul> <li>There are a huge number of frameworks that specialize in deploying large language models with</li> <li>Lots of pre-built wrappers and integrations.</li> <li>A vast selection of available models.</li> <li>A multitude of internal optimizations.</li> <li>Rapid prototyping.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_16","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Choosing the Right Framework**\n</code></pre> <ul> <li>The choice of framework for deploying an LLM application depends on various factors, including the size of the model, the scalability requirements of the application, and the deployment environment. </li> <li>Heres a  cheat sheet:</li> </ul> <pre><code>You can get a more detailed overview of the existing solutions here [7 Frameworks for Serving LLMs](https://medium.com/better-programming/frameworks-for-serving-llms-60b7f7b23407)\n</code></pre> <pre><code>    Comparison of frameworks for LLMs inference\n</code></pre>"},{"location":"Spanda%20Bootcamp%20Day%201/#_17","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Example Code for Deployment**\n</code></pre> <ul> <li>Let\u2019s move from theory to practice and try to deploy LLaMA-2 using Text Generation Inference. </li> <li> <p>And, as you might have guessed, you\u2019ll need just a few lines of code:</p> <pre><code># 1. Create a folder where your model will be stored:\nmkdir data\n\n# 2. Run Docker container (launch RestAPI service):\ndocker run --gpus all --shm-size 1g -p 8080:80 \\\n   -v $volume:/data \\\n   ghcr.io/huggingface/text-generation-inference:1.1.0\n   --model-id meta-llama/Llama-2-7b\n\n# 3. And now you can make requests:\ncurl 127.0.0.1:8080/generate \\\n   -X POST \\\n   -d '{\"inputs\":\"Tell me a joke!\",\"parameters\":{\"max_new_tokens\":20}}' \\\n   -H 'Content-Type: application/json'\n</code></pre> <ul> <li>That\u2019s it! You\u2019ve set up a RestAPI service with built-in logging, Prometheus endpoint for monitoring, token streaming, and your model is fully optimized. </li> </ul> </li> </ul> <p></p> <pre><code>API Documentation\n</code></pre>"},{"location":"Spanda%20Bootcamp%20Day%201/#_18","title":"Spanda Bootcamp Day 1","text":"<pre><code>**References:**\n</code></pre> <ul> <li>7 Frameworks for Serving LLMs \u2014 comprehensive guide into LLMs inference and serving with detailed comparison.</li> <li>Inference Endpoints \u2014 a product from HuggingFace that will allow you to deploy any LLMs in a few clicks. A good choice when you need rapid prototyping.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_19","title":"Spanda Bootcamp Day 1","text":"<pre><code>**To get in a little deeper**\n</code></pre> <ul> <li>We\u2019ve covered the basic concepts needed for developing LLM-based applications, there are still some aspects you\u2019ll likely encounter in the future. Here are  a few useful reference:</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_20","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Optimization**\n</code></pre> <ul> <li>When you launch your first model, you inevitably find it\u2019s not as fast as you\u2019d like and consumes a lot of resources and you\u2019ll need to understand how it can be optimized.</li> <li>7 Ways To Speed Up Inference of Your Hosted LLMs \u2014 techniques to speed up inference of LLMs to increase token generation speed and reduce memory consumption.</li> <li>Optimizing Memory Usage for Training LLMs in PyTorch \u2014 article provides a series of techniques that can reduce memory consumption in PyTorch by approximately 20x without sacrificing modeling performance and prediction accuracy.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_21","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Evaluating**\n</code></pre> <ul> <li>Suppose you have a fine-tuned model you need to be sure that its quality has improved.  What metrics should we use to check quality?</li> <li>All about evaluating Large language models \u2014 a good overview article about benchmarks and metrics.</li> <li>evals \u2014 the most popular framework for evaluating LLMs and LLM systems.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_22","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Vector Databases**\n</code></pre> <ul> <li>If you work with RAG, at some point, you\u2019ll move from storing vectors in memory to a database. </li> <li>For this, it\u2019s important to understand what\u2019s currently on the market and its limitations.</li> <li>All You Need to Know about Vector Databases \u2014 a step-by-step guide by  \\ Dominik Polzer \\  to discover and harness the power of vector databases.</li> <li>Picking a vector database: a comparison and guide for 2023 \u2014 comparison of Pinecone, Weviate, Milvus, Qdrant, Chroma, Elasticsearch and PGvector databases.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_23","title":"Spanda Bootcamp Day 1","text":"<pre><code>**LLM Agents**\n</code></pre> <ul> <li>One of  the most promising developments in LLMs are LLM Agents i f you want multiple models to work together. </li> <li>The following links are worth going through</li> <li>A Survey on LLM-based Autonomous Agents \u2014 this is probably the most comprehensive overview of LLM based agents.</li> <li>autogen \u2014 is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks.</li> <li>OpenAgents \u2014 an open platform for using and hosting language agents in the wild.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%201/#_24","title":"Spanda Bootcamp Day 1","text":"<pre><code>**Reinforcement Learning from Human Feedback (RLHF)**\n</code></pre> <ul> <li>As soon as you allow users access to your model, you start taking responsibility. </li> <li>What if it responds rudely? Or reveals bomb-making ingredients? To avoid this, check out these articles:</li> <li>Illustrating Reinforcement Learning from Human Feedback (RLHF) \u2014 an overview article that details the RLHF technology.</li> <li>RL4LMs \u2014 a modular RL library to fine-tune language models to human preferences.</li> <li>TRL \u2014 a set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step.</li> </ul> <p>Summary:</p> <p></p> <ul> <li>The material covered today is broad but is the technology of the future. </li> <li>Junior Programmers, Artists, ML Engineers, Data Processing Analysts, Beginner Data Scientists, and practically every other digital job should be learning this technology. There is a lot of scope and opportunity. </li> <li>Generative AI is the future of the Digital Media World. Artists are feeling the impact today. A similar situation is looming for junior-level software engineers. </li> <li>But the solution is simple: Skill up! Help someone else Skill up! Regain Control!</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/","title":"Spanda Bootcamp Day 2","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#spanda-boot-camp-day-2","title":"Spanda Boot Camp Day 2","text":"<p>[Part 1____________________________________________________________________]{.underline}</p> <p>We will cover:</p> <ul> <li> <p>How to handle the growing scale in large-scale machine learning     applications</p> </li> <li> <p>We will identify patterns needed to build scalable and reliable     distributed systems</p> </li> <li> <p>We will outline the use of these patterns in distributed systems and     building reusable patterns</p> </li> </ul> <p>Part 1 Outcome:</p> <ul> <li> <p>You will be able to choose and apply the correct patterns for     building and deploying distributed machine learning systems; use     common tooling viz.,</p> <ul> <li> <p>TensorFlow     (https://www.tensorflow.org),</p> </li> <li> <p>Kubernetes (https://kubernetes.io),</p> </li> <li> <p>Kubeflow     (https://www.kubeflow.org),</p> </li> <li> <p>Argo Workflows</p> </li> </ul> </li> <li> <p>appropriately within a machine learning workflow; and gain practical     experience in managing and automating machine learning tasks in     Kubernetes.</p> </li> <li> <p>The project we will explore later will help us learn to build a     real-life distributed machine learning system using the patterns we     study.</p> </li> </ul> <p>ML Scale</p> <ul> <li> <p>The scale of machine learning applications has become     unprecedentedly large. Case in point: LLMs</p> </li> <li> <p>Users are demanding faster responses to meet real-life requirements,     and machine learning pipelines and model architectures are getting     more complex.</p> </li> <li> <p>Due to the growing demand and complexity, machine learning systems     have to be built with the ability to handle the growing scale,     including the increasing volume of historical data; frequent batches     of incoming data; complex machine learning architectures; heavy     model serving traffic; and complicated end-to-end machine learning     pipelines.</p> </li> </ul> <p>What should be done?</p> <ul> <li>When the dataset is too large to fit in a single machine, we need to     store different parts of the dataset on different machines and then     train the machine learning model by sequentially looping through the     various parts of the dataset on different machines.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   If we have a 30 GB dataset, we can divide it into three partitions     (fig 1.1).</p> <p>Figure 1.1 Dividing a large dataset into three partitions on three separate machines that have sufficient disk storage</p> <ul> <li> <p>The question to ask here is \"what happens if looping through     different parts of the dataset is quite time-consuming?\".</p> </li> <li> <p>Assume that the dataset at hand has been divided into three     partitions. As in figure 1.2, we initialize the machine learning     model on the first machine, and then we train it, using all the data     in the first data partition. Next, we transfer the trained model to     the second machine, which continues training by using the second     data partition.</p> </li> <li> <p>If each partition is large and time-consuming, we'll spend a     significant amount of time waiting.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   We now need to think about adding workers and parallelism</p> <ul> <li> <p>Each worker is responsible for consuming each of the data     partitions, and all workers train the same model in parallel     without waiting for others.</p> </li> <li> <p>This approach is good for speeding up the model training process     but..</p> <ul> <li> <p>What if some workers finish consuming the data partitions that     they are responsible for and want to update the model at the     same time?</p> </li> <li> <p>Which of the worker's results (gradients) should we use to     update the model first?</p> </li> </ul> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   What about the conflicts and tradeoffs between performance and     model quality?</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   If the data partition that the first worker uses has better quality     due to a more rigorous data collection process than the one that the     second worker uses, using the first worker's results first would     produce a more accurate model.</p> <pre><code>-   On the other hand, if the second worker has a smaller partition,\n    it could finish training faster, so we could start using that\n    worker's computational resources to train a new data partition.\n</code></pre> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   When more workers are added, the conflicts in completion time for     data consumption by different workers become even more obvious.</p> <ul> <li> <p>Q: if the application that uses the trained machine learning model     to make predictions observes much heavier traffic, can we simply add     servers, with each new server handling a certain percentage of the     traffic?</p> </li> <li> <p>A: Not really. This solution would need to take other things into     consideration, such as deciding the best load balancer strategy and     processing duplicate requests in different servers.</p> </li> <li> <p>The main takeaway for now is that we have established patterns and     best practices to deal with certain situations, and we will use     those patterns to make the most of our limited computational     resources.</p> </li> <li></li> </ul> <p>Distributed systems</p> <ul> <li> <p>A distributed system is one in which components are located on     different networked computers and can communicate with one another     to coordinate workloads and work together via message passing.</p> </li> <li> <p>Figure 1.3 below illustrates a small distributed system consisting     of two machines communicating with each other via message passing.     One machine contains two CPUs, and the other machine contains three     CPUs. Obviously, a machine contains computational resources other     than the CPUs; we use only CPUs here for illustration purposes.</p> </li> <li> <p>In real-world distributed systems, the number of machines can be     extremely large--- tens of thousands, depending on the use case.</p> </li> <li> <p>Machines with more computational resources can handle larger     workloads and share the results with other machines.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Many patterns and reusable components are available for distributed     systems.</p> <ul> <li> <p>The work-queue pattern in a batch processing system, for example,     makes sure that each piece of work is independent of the others and     can be processed without any interventions within a certain amount     of time.</p> </li> <li> <p>In addition, workers can be scaled up and down to ensure that the     workload can be handled properly.</p> </li> <li> <p>Figure 1.4 depicts seven work items, each of which might be an image     that needs to be modified to grayscale by the system in the     processing queue.</p> </li> <li> <p>Each of the three existing workers takes two to three work items     from the processing queue, ensuring that no worker is idle to avoid     waste of computational resources and maximizing the performance by     processing multiple images at the same time.</p> </li> <li> <p>This performance is possible because each work item is independent     of the others.</p> </li> </ul> <p>Figure 1.4 An example of a batch processing system using the work-queue pattern to modify images to grayscale</p> <p>Distributed machine learning systems</p> <ul> <li> <p>A distributed machine learning system is a distributed system     consisting of a pipeline of steps and components that are     responsible for different steps in machine learning applications,     such as data ingestion, model training, and model serving.</p> </li> <li> <p>It uses patterns and best practices similar to those of a     distributed system, as well as patterns designed specifically to     benefit machine learning applications.</p> </li> <li> <p>With careful design, a distributed machine learning system is more     scalable and reliable for handling large-scale problems, such as     large datasets, large models, heavy model serving traffic, and     complicated model selection or architecture optimization.</p> </li> <li> <p>There are similar patterns in distributed machine learning systems.     As an example, multiple workers can be used to train the machine     learning model asynchronously, with each worker being responsible     for consuming certain partitions of the dataset. This approach,     which is similar to the work-queue pattern used in distributed     systems, can speed up the model training process significantly.</p> </li> <li> <p>Figure 1.5 illustrates how we can apply this pattern to distributed     machine learning systems by replacing the work items with data     partitions. Each worker takes some data partitions from the original     data stored in a database and then uses them to train a centralized     machine learning model.</p> </li> </ul> <p>Figure 1.5 Applying the work-queue pattern in distributed machine learning systems</p> <ul> <li> <p>Another example pattern commonly used in machine learning systems     instead of general distributed systems is the parameter server     pattern for distributed model training.</p> </li> <li> <p>As shown in figure 1.6, the parameter servers are responsible for     storing and updating a particular part of the trained model.</p> </li> <li> <p>Each worker node is responsible for taking a particular part of the     dataset that will be used to update a certain part of the model     parameters.</p> </li> <li> <p>This pattern is useful when the model is too large to fit in a     single server and dedicated parameter servers for storing model     partitions without allocating unnecessary computational resources.</p> </li> </ul> <p>Figure 1.6 An example of applying the parameter server pattern in a distributed machine learning system</p> <p>When do we use a distributed machine learning system?</p> <ul> <li>When any of the following scenarios occurs:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The model is large, consisting of millions of parameters that a     single machine cannot store and that must be partitioned on     different machines.</p> <ul> <li> <p>The machine learning application needs to handle increasing amounts     of heavy traffic that a single server can no longer manage.</p> </li> <li> <p>The task at hand involves many parts of the model's life cycle, such     as data ingestion, model serving, data/model versioning, and     performance monitoring.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   We want to use many computing resources for acceleration, such as     dozens of servers that have many GPUs each.</p> <p>When should we not use a distributed machine learning system?</p> <p>If you encounter any of the following cases:</p> <ul> <li> <p>The dataset is small, such as a CSV file smaller than 10 GBs.</p> </li> <li> <p>The model is simple and doesn't require heavy computation, such as     linear regression.</p> </li> <li> <p>Computing resources are limited but sufficient for the tasks at     hand.</p> </li> </ul> <p>What frameworks will we use?</p> <ul> <li>We'll use several popular frameworks and cutting-edge technologies     to build components of a distributed machine learning workflow,     including the following:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   TensorFlow     (https://www.tensorflow.org)</p> <ul> <li> <p>Kubernetes (https://kubernetes.io)</p> </li> <li> <p>Kubeflow (https://www.kubeflow.org)</p> </li> <li> <p>Docker (https://www.docker.com)</p> </li> <li> <p>Argo Workflows (https://argoproj.github.io/workflows/)</p> </li> </ul> <p>Three model training</p> <p>serving steps. to present to users.</p> <p>Figure 1.7 End-to-end machine learning system that we will be building</p> <p>Table 1.1 shows the key technologies that will be covered in this course and example uses.</p> <p>Table 1.1 The technologies covered in this course and their uses</p> <p>+-------------+--------------------------------------------------------+ | Technology  | Use                                                    | +=============+========================================================+ | TensorFlow  | Building machine learning and deep learning models     | +-------------+--------------------------------------------------------+ | Kubernetes  | &gt; Managing distributed environments and resources      | +-------------+--------------------------------------------------------+ | Kubeflow    | &gt; Submitting and managing distributed training jobs    | |             | &gt; easily on Kubernetes clusters                        | +-------------+--------------------------------------------------------+ | Argo        | &gt; Defining, orchestrating, and managing workflows      | | Workflows   |                                                        | +-------------+--------------------------------------------------------+ | Docker      | &gt; Building and managing images to be used for starting | |             | &gt; containerized environments                           | +-------------+--------------------------------------------------------+</p> <p>https://www.linkedin.com/pulse/install-kubernetes-cluster-your-local-machine-andrea-de-rinaldis</p> <p>[Part 2____________________________________________________________________]{.underline}</p> <p>We will cover:</p> <ul> <li> <p>Using model serving to generate predictions or make inferences on     new data with previously trained machine learning models</p> </li> <li> <p>Handling model serving requests and achieving horizontal scaling     with replicated model serving services</p> </li> <li> <p>Processing large model serving requests using the sharded services     services patterns</p> </li> <li> <p>Assess model serving systems and event driven design</p> </li> </ul> <p>Outcome:</p> <ul> <li> <p>Learn three key patterns used to achieve scaling and performance     outcomes in distributed ML systems</p> </li> <li> <p>Understand the tradeoffs involved</p> </li> </ul> <p>Model Serving</p> <ul> <li> <p>Model serving is the process of loading a previously trained     machine learning model to generate predictions or make inferences on     new input data.</p> </li> <li> <p>It is done after successfully training a machine learning model.     (Fig 4.2)</p> </li> </ul> <p>Model serving is the next step after we have successfully trained a machine learning model. We use the trained</p> <p>Figure 4.1 A diagram showing where model serving fits in the machine learning pipeline</p> <ul> <li> <p>In traditional machine learning applications, model serving is     usually a single program running on a local desktop or machine and     generates predictions on new datasets that are not used for model     training.</p> </li> <li> <p>Both the dataset and the machine learning model used should be small     enough to fit on a single machine for traditional model serving, and     they are stored in the local disk of a single machine.</p> </li> <li> <p>In contrast, distributed model serving usually happens in a cluster     of machines. Both the dataset and the trained machine learning model     used for model serving can be very large and must be stored in a     remote distributed database or partitioned on disks of multiple     machines.</p> </li> <li> <p>The differences between traditional model serving and distributed     model serving systems is summarized in table 4.1.</p> </li> </ul> <p>Table 4.1 Comparison between traditional model serving and distributed model serving systems</p> <p>+-------------------+------------------------+------------------------+ |                   | Traditional model      | Distributed model      | |                   | serving                | serving                | +===================+========================+========================+ | &gt; Computational   | &gt; Personal laptop or   | &gt; Cluster of machines  | | &gt; resources       | &gt; single remote server |                        | +-------------------+------------------------+------------------------+ | &gt; Dataset         | Local disk on a single | &gt; Remote distributed   | | &gt; location        | laptop or machine      | &gt; database or          | |                   |                        | &gt; partitioned on disks | |                   |                        | &gt; of multiple          | |                   |                        | &gt;                      | |                   |                        | &gt; machines             | +-------------------+------------------------+------------------------+ | &gt; Size of model   | &gt; Small enough to fit  | &gt; Medium to large      | | &gt; and dataset     | &gt; on a single machine  |                        | +-------------------+------------------------+------------------------+</p> <p>Pattern 1: Replicated services pattern</p> <ul> <li> <p>Challenge: Building a machine learning model to *tag the main     themes of new videos that the model hasn't seen before*</p> </li> <li> <p>We have used the YouTube8M dataset     (http://research.google.com/youtube8m/)</p> </li> <li> <p>This consists of millions of YouTube video IDs, with high-quality     machine-generated annotations from a diverse vocabulary of 3,800+     visual entities viz, Car, Music, etc. (fig. 4.2) for training. Hence     we have a model</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   We now need to build a model serving system that allows users     to upload new videos.</p> <ul> <li> <p>The system would load the previously trained machine learning     model to tag entities/themes that appear in the uploaded videos.</p> </li> <li> <p>The model serving system is stateless, so users' requests won't     affect the model serving results.</p> </li> <li> <p>The system takes videos uploaded by users and sends requests to the     model server.</p> </li> <li> <p>The model server then retrieves the previously trained     entity-tagging machine learning model from the model storage to     process the videos and eventually generate possible entities that     appear in the videos. (Fig. 4.3)</p> </li> </ul> <p>{width=\"5.666666666666667in\" height=\"2.9133333333333336in\"}</p> <p>Figure 4.2 A screenshot of what the videos in the YouTube-8M dataset looks like. (Source: Sudheendra</p> <p>Vijayanarasimhan et al. Licensed under Nonexclusive License 1.0)</p> <p>Users upload videos and then submit requests to the model serving system to tag the entities within the video</p> <p>Dog,</p> <p>Swimmer</p> <p>Horse</p> <p>...</p> <p>Figure 4.3 The single-node model serving system</p> <ul> <li> <p>This first version of the model server runs on a single machine and     responds to model serving requests from users on a first-come,     first-served basis (figure 4.4) and will work if only very few users     are testing the system.</p> </li> <li> <p>As the number of users or model serving requests increases, users     will experience huge delays while waiting for the system to finish     processing any previous requests. In the real world, this bad user     experience would immediately lose our users' interest in engaging     with this system.</p> </li> </ul> <p>Figure 4.4 The model server only runs on a single machine and responds to model serving requests from users on a first-come, first-served basis.</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#challenge-find-a-better-way-to-handle-model-serving-requests","title":"Challenge: Find a better way to handle model serving requests","text":"<ul> <li> <p>This system can only effectively serve a limited number of model     serving requests on a first-come, first-served basis.</p> </li> <li> <p>As the number of requests grows in the real world, the user     experience worsens when users must wait a long time to receive the     model serving result.</p> </li> <li> <p>All requests are waiting to be processed by the model serving     system, but the computational resources are bound to this single     node.</p> </li> <li> <p>Is there a better way to handle model serving requests than     sequentially?</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#solution","title":"Solution:","text":"<ul> <li> <p>The existing model server is stateless</p> </li> <li> <p>The model serving results for each request aren't affected by other     requests, and the machine learning model can only process a single     request.</p> </li> <li> <p>The model server doesn't require a saved state to operate correctly.</p> </li> <li> <p>Since the model server is stateless, we can add more server     instances to help handle additional user requests without the     requests interfering with each other (figure 4.5).</p> </li> <li> <p>These additional model server instances are exact copies of the     original</p> </li> </ul> <p>Figure 4.5 Additional server instances help handle additional user requests without the requests interfering with each other.</p> <ul> <li> <p>The server addresses are different, and each handles different model     serving requests</p> </li> <li> <p>In other words, they are replicated services for model serving or,     in short, model server replicas.</p> </li> <li> <p>Adding additional resources into our system with more machines is     called horizontal scaling.</p> </li> <li> <p>Horizontal scaling systems handle more and more users or traffic by     adding more replicas.</p> </li> <li> <p>The opposite of horizontal scaling is vertical scaling, which is     usually implemented by adding computational resources to existing     machines.</p> </li> <li> <p>The system now has multiple model server replicas to process the     model serving requests asynchronously.</p> </li> <li> <p>Each model server replica takes a single request, retrieves the     previously trained entity-tagging machine learning model from model     storage, and then processes the videos in the request to tag     possible entities in the videos.</p> </li> <li> <p>As a result, we've successfully scaled up our model server by adding     model server replicas to the existing model serving system (4.6) .</p> </li> <li> <p>The model server replicas are capable of handling many requests at a     time since each replica can process individual model serving     requests independently.</p> </li> </ul> <p>Users upload videos and then submit requests to the model serving system to tag the entities within the videos.</p> <p>Swimmer</p> <p>Horse</p> <p>...</p> <p>Figure 4.6 The system architecture after we've scaled up our model server by adding model server replicas to the system</p> <ul> <li> <p>Here multiple model serving requests from users are sent to the     model server replicas at the same time.</p> </li> <li> <p>Q: How are they being distributed and processed? Which request is     being processed by which model server replica?</p> <ul> <li>We haven't defined a clear mapping relationship between the     requests and the model server replicas.</li> </ul> </li> <li> <p>We need a load balancer, which handles the distribution of model     serving requests among the replicas.</p> </li> <li> <p>It takes multiple model serving requests from our users and then     distributes the requests evenly to each of the model server     replicas, which then are responsible for processing individual     requests, including model retrieval and inference on the new data in     the request. (Figure 4.7).</p> </li> <li> <p>The load balancer can use different algorithms to decide which     request goes to which model server replica.</p> </li> <li> <p>They include round robin, the least connection method, hashing, etc.</p> </li> <li> <p>The replicated services pattern provides a great way to scale our     model serving system horizontally.</p> </li> <li> <p>It can also be generalized for any systems that serve a large amount     of traffic. Whenever a single instance cannot handle the traffic,     introducing this pattern ensures that all traffic can be handled     equivalently and efficiently.</p> </li> </ul> <p>Figure 4.7 Loader balancer distributes the requests evenly across model server replicas</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#points-to-note","title":"Points to note:","text":"<ul> <li> <p>With load-balanced model server replicas in place, we should be able     to support the growing number of user requests, and the entire model     serving system achieves horizontal scaling.</p> </li> <li> <p>The overall model serving system also becomes highly available     (https:// mng.bz/EQBd).</p> </li> <li> <p>High availability is a characteristic of a system that maintains an     agreed-on level of operational performance, usually uptime, for a     longer-than-normal period expressed as a percentage of uptime in a     given year.</p> </li> <li> <p>For example, some organizations may require services to reach a     highly available service-level agreement, which means the service is     up and running 99.9% of the time (known as three-nines     availability). In other words, the service can only get 1.4 minutes     of downtime per day (24 hours \u00d7 60 minutes \u00d7 0.1%).</p> </li> <li> <p>With the help of replicated model services, if any of the model     server replicas crashes or gets preempted on a spot instance, the     remaining model server replicas are still available and ready to     process any incoming model serving requests from users, which     provides a good user experience and makes the system reliable.</p> </li> </ul> <p>Readiness Probes:</p> <ul> <li> <p>Since our model server replicas will need to retrieve previously     trained machine learning models from a remote model storage, they     need to be ready in addition to being alive.</p> </li> <li> <p>It's important to build and deploy readiness probes to inform the     load balancer that the replicas are all successfully established     connections to the remote model storage and are ready to serve model     serving requests from users.</p> </li> <li> <p>A readiness probe helps the system determine whether a particular     replica is ready to serve.</p> </li> <li> <p>With readiness probes, users do not experience unexpected behaviors     when the system is not ready due to internal system problems.</p> </li> </ul> <p>Issues:</p> <ul> <li> <p>Not only the number of serving requests increases but also the size     of each request, which can get extremely large if the data or the     payload is large.</p> </li> <li> <p>In that case, replicated services may not be able to handle the     large requests.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#quiz","title":"Quiz:","text":"<ol> <li> <p>Are replicated model servers stateless or stateful?</p> </li> <li> <p>What happens when we don't have a load balancer as part of the model     serving system?</p> </li> <li> <p>Can we achieve three-nines service-level agreements with only one     model server instance?</p> </li> </ol> <p>Pattern 2: Sharded services pattern</p> <ul> <li> <p>The replicated services pattern efficiently resolves our horizontal     scalability problem so that our model serving system can support a     growing number of user requests. We achieve the additional benefit     of high availability with the help of model server replicas and a     load balancer.</p> </li> <li> <p>Each model server replica has a limited and pre-allocated amount of     computational resources.</p> </li> <li> <p>More important, the amount of computational resources for each     replica must be identical for the load balancer to distribute     requests correctly and evenly.</p> </li> <li> <p>Consider this scenario: A user wants to upload a high-resolution     YouTube video that needs to be tagged with an entity using the model     server application.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The high-resolution video, though too large, may be uploaded     successfully to the model server replica if it has sufficient disk     storage.</p> <ul> <li> <p>Processing the request in any of the individual model server     replicas themselves failed since processing this single large     request would require a larger memory allocated in the model server     replica.</p> </li> <li> <p>This need for a large amount of memory is often due to the     complexity of the trained machine learning model, as it may contain     a lot of expensive matrix computations or mathematical operations.     Eventually, we notify the user of this failure after they have     waited a long time, which results in a bad user experience. A     diagram for this situation is shown in figure 4.8.</p> </li> </ul> <p>User uploads a high-resolution video to the model serving system.</p> <p>This fails as the model server replica that's processing this large request does not have enough computational resources.</p> <p>Figure 4.8 Model server fails to process the large data in the request as the model server replica responsible for processing this request does not have sufficient memory</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#challenge-design-the-model-serving-system-to-handle-large-requests","title":"Challenge: Design the model serving system to handle large requests","text":"<ul> <li>How do we design the model serving system to handle large requests     of high resolution videos successfully?</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#solution-1","title":"Solution:","text":"<ul> <li> <p>Q: Can we scale vertically by increasing each replica's     computational resources so it can handle large requests like     high-resolution videos? Since we are vertically scaling all the     replicas by the same amount, we will not affect our load balancer's     work.</p> </li> <li> <p>A: Unfortunately, we cannot simply scale the model server replicas     vertically since we don't know how many such large requests there     are. A couple of users could have high-resolution videos needing to     be processed, and the remaining vast majority of the users only     upload videos from their smartphones with much smaller resolutions.</p> </li> <li> <p>As a result, most of the added computational resources on the model     server replicas are idling, which results in very low resource     utilization.</p> </li> <li> <p>Note: We will examine the resource utilization perspective     subsequently, but for now, we know that this approach is not     practical due to usage economics (Architecture is money).</p> </li> <li> <p>The parameter server pattern discussed earlier allows us to     partition a very large model.</p> </li> <li> <p>Figure 4.9 shows distributed model training with multiple parameter     servers; the large model has been partitioned, and each partition is     located on different parameter servers.</p> </li> <li> <p>Each worker node takes a subset of the dataset, performs     calculations required in each neural network layer, and then sends     the calculated gradients to update one model partition stored in one     of the parameter servers.</p> </li> </ul> <p>Figure 4.9 Distributed model training with multiple parameter servers where the large model has been sharded and each partition is located on different parameter servers</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#section-unnumbered","title":"{#section .unnumbered}","text":"<ul> <li> <p>To deal with the problem of large model serving requests, the same     idea can be applied here</p> <ul> <li> <p>Divide the original high-resolution video into multiple separate     videos,</p> </li> <li> <p>Each video is processed by multiple model server shards     independently.</p> </li> <li> <p>The model server shards are partitions from a single model     server instance, and each is responsible for processing a subset     of a large request.</p> </li> </ul> </li> </ul> <p>The high-resolution video is divided into two separate videos and sent to each of the model server shards.</p> <p>Figure 4.10 Sharded services pattern: High-resolution video gets divided into two separate videos. Each video represents a subset of the original large request and is processed by different model server shard independently.</p> <ul> <li> <p>After the model server shards receive the sub-requests where each     contains part of the original large model serving request, each     model server shard then retrieves the previously trained     entity-tagging machine learning model from model storage and then     processes the videos in the request to tag possible entities that     appear in the videos, similar to the previous model serving system.</p> </li> <li> <p>Once all the sub-requests have been processed by each of the model     server shards, we merge the model inference result from two     sub-requests to obtain a result for the original large model serving     request with the high-resolution video.</p> </li> </ul> <p>Sharding Function:</p> <ul> <li> <p>Q: How do we distribute the two sub-requests to different model     server shards?</p> </li> <li> <p>A: We use a sharding function, which is very similar to a hashing     function, to determine which shard in the list of model server     shards should be responsible for processing each sub-request.</p> </li> <li> <p>Usually, the sharding function is defined using a hashing function     and the modulo (%) operator.</p> <ul> <li> <p>For example, hash(request) % 10 would return 10 shards even     when the outputs of the hash function are significantly larger     than the number of shards in a sharded service.</p> </li> <li> <p>Characteristics of hashing functions for sharding</p> <ul> <li> <p>The hashing function that defines the sharding function     transforms an arbitrary object into an integer representing     a particular shard index. It has two important     characteristics:</p> </li> <li> <p>The output from hashing is always the same for a given     input.</p> </li> <li> <p>The distribution of outputs is always uniform within the     output space.</p> </li> <li> <p>These characteristics are important and can ensure that a     particular request will always be processed by the same     shard server and that the requests are evenly distributed     among the shards.</p> </li> </ul> </li> </ul> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   When a distributed system has limited computational resources for a     single machine, we can apply this pattern to offload the     computational burden to multiple machines.</p> <p>Points to Consider:</p> <ul> <li> <p>Unlike the replicated services pattern which is useful when building     stateless services, the sharded services pattern is generally used     for building stateful services.</p> </li> <li> <p>In our case, we need to maintain the state or the results from     serving the sub-requests from the original large request using     sharded services and then merge the results into the final response     so it includes all entities from the original high-resolution video.</p> </li> <li> <p>In some cases, this approach may not work well because it depends on     how we divide the original large request into smaller requests.</p> </li> <li> <p>For example, if the original video has been divided into more than     two sub-requests, some may not be meaningful since they don't     contain any complete entities that are recognizable by the machine     learning model we've trained.</p> </li> <li> <p>For situations like that, we need additional handling and cleaning     of the merged result to remove meaningless entities that are not     useful to our application.</p> </li> </ul> <p>Quiz:</p> <ol> <li> <p>Would vertical scaling be helpful when handling large requests?</p> </li> <li> <p>Are the model server shards stateful or stateless?</p> </li> <li></li> </ol> <p>Pattern 3: The event-driven processing pattern</p> <ul> <li> <p>For cases in which we do not know how much model serving traffic     the system will be receiving, or if the demand traffic pattern     is lumpy or shows up in spikes, it's hard to allocate and use     resources efficiently. (e.g. hotel room price prediction)</p> </li> <li> <p>The problem with this traffic pattern is that it could cause very     low resource utilization rates with static resource allocation     schemes.</p> </li> <li> <p>In our current architecture, the underlying computational resources     allocated to the model remain unchanged at all times. This strategy     is not an optimal one.</p> </li> </ul> <p>Users enter date range and location and then submit requests to the serving system.</p> <p>Figure 4.11 A diagram of the model serving system to predict hotel prices</p> <p>Figure 4.12 The traffic changes of the model serving system over time with an equal amount of computational resources allocated all the time.</p> <ul> <li>Since we know, more or less, when those holiday periods are, why     don't we plan accordingly? Unfortunately, some events make it hard     to predict surges in traffic.</li> </ul> <p>Thanksgiving Christmas time</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#unable-to-handle-all-requests-within-this-time-window","title":"unable to handle all requests within this time window","text":"<p>Figure 4.13 The traffic of our model serving system over time with an optimal amount of computational resources allocated for different time windows. In addition, an unexpected event happened before Christmas that suddenly added traffic during that particular time window (solid line).</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#the-challenge-respond-to-model-serving-requests-based-on-events","title":"The challenge: Respond to model serving requests based on events","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#the-solution","title":"The solution","text":"<ul> <li> <p>The solution is in maintaining a pool of computational resources     (e.g., CPUs, memory, disk, etc.) allocated not only to this     particular model serving system but also to model serving of other     applications or other components of the distributed machine learning     pipeline.</p> </li> <li> <p>This shared resource pool gives us enough resources to handle peak     traffic for the model serving system by pre-allocating resources     required during historical peak traffic and autoscaling when the     limit is reached.</p> </li> <li> <p>Therefore, we only use resources when needed and only the specific     amount of resources required for each particular model serving     request.</p> </li> <li> <p>Figure 4.15 shows the traffic of our model serving system over time     with an unexpected bump. The unexpected bump is due to a new very     large international conference that happens before Christmas.</p> </li> </ul> <p>Figure 4.14 An architecture diagram in which a shared resource pool is being used by different components--- for example, data ingestion, model training, model selection, and model deployment---and two different model serving systems at the same time. The arrows with solid lines indicate resources, and the arrows with dashed lines indicate requests.</p> <p>Thanksgiving Christmas days</p> <p>Figure 4.15 The traffic of our model serving system over time. An unexpected bump happened before Christmas that suddenly added traffic. The jump in requests is handled successfully by the model serving system by borrowing the necessary amount of resources from the shared resource pool. The resource utilization rate remains high during this unexpected event.</p> <ul> <li> <p>This event suddenly adds traffic, but the model serving system     successfully handles the surge in traffic by borrowing a necessary     amount of resources from the shared resource pool.</p> </li> <li> <p>With the help of the shared resource pool, the resource utilization     rate remains high during this unexpected event.</p> </li> <li> <p>The shared resource pool monitors the current amount of available     resources and autoscales when needed.</p> </li> <li> <p>This approach, in which the system listens to the user requests and     only responds and utilizes the computational resources when the user     request is being made, is called event-driven processing.</p> </li> </ul> <p>Event-driven processing vs. long-running serving systems</p> <ul> <li> <p>Event-driven processing is different from the model serving systems     that we've looked at in previous sections , where the servers that     handle user requests are always up and running.</p> </li> <li> <p>Those long-running serving systems work well for many applications     that are under heavy load, keep a large amount of data in memory, or     require some sort of background processing.</p> </li> <li> <p>However, for applications that handle very few requests during non     peak periods or respond to specific events, such as our hotel price     prediction system, the event-driven processing pattern is more     suitable.</p> </li> <li> <p>This event-driven processing pattern has flourished in recent years     as cloud providers have developed function-as-a-service     products.</p> </li> <li> <p>Each model serving request made from our hotel price prediction     system represents an event.</p> </li> <li> <p>Our serving system listens for this type of event, utilizes     necessary resources from the shared resource pool, and retrieves and     loads the trained machine learning model from the distributed     database to estimate the hotel prices for the specified     time/location query (Figure 4.16)</p> </li> <li> <p>Using this event-driven processing pattern for our serving system,     we can make sure that our system is using only the resources     necessary to process every request without concerning ourselves with     resource utilization and idling.</p> </li> <li> <p>As a result, the system has sufficient resources to deal with peak     traffic and return the predicted prices without users experiencing     noticeable delays or lags when using the system.</p> </li> </ul> <p>Denial of Service and Rate Limiting</p> <ul> <li> <p>Even though we now have a shared pool of sufficient computational     resources where we can borrow computational resources from the     shared resource pool to handle user requests on demand, we should     also build a mechanism in our model serving system to defend     denial-of-service attacks.</p> </li> <li> <p>Denial-of-service attacks interrupt an authorized user's access to a     computer network, typically caused with malicious intent and often     seen in model serving systems.</p> </li> <li> <p>These attacks can cause unexpected use of computational resources     from the shared resource pool, which may eventually lead to resource     scarcity for other services that rely on the shared resource pool.</p> </li> <li> <p>Denial-of-service attacks may happen in various cases malicious and     non-malicious.</p> </li> </ul> <p>Figure 4.16 A Event-driven model serving system to predict hotel prices</p> <ul> <li> <p>To deal with these situations, which often happen in real-world     applications, it makes sense to introduce a defense mechanism for     denial-of-service attacks</p> </li> <li> <p>One approach to avoid these attacks is via rate limiting, which     adds the model serving requests to a queue and limits the rate the     system is processing the requests in the queue (Fig. 4.17).</p> </li> <li> <p>Figure 4.17 is a flowchart showing four model serving requests sent     to the model serving system. However, only two are under the current     rate limit, which allows a maximum of two concurrent model serving     requests. In this case, the rate-limiting queue for model serving     requests first checks whether the requests received are under the     current rate limit. Once the system has finished processing those     two requests, it will proceed to the remaining two requests in the     queue.</p> </li> <li> <p>If we are deploying and exposing an API for a model serving service     to our users, it's also generally a best practice to have a     relatively small rate limit (e.g., only one request is allowed     within 1 hour) for users with anonymous access and then ask users to     log in to obtain a higher rate limit.</p> </li> <li> <p>This system would allow the model serving system to better control     and monitor the users' behavior and traffic so that we can take     necessary actions to address any potential problems or     denial-of-service attacks.</p> </li> <li> <p>For example, requiring a login provides auditing to find out which     users/events are responsible for the unexpectedly large number of     model serving requests.</p> </li> </ul> <p>Model serving requests</p> <p>Ok to add to the queue?</p> <p>Model serving requests that are under rate limit:</p> <p>2 maximum concurrent requests</p> <p>Figure 4.17 A flowchart of four model serving requests sent to the model serving system. However, only two are under the current rate limit, which allows a maximum of two concurrent model serving requests. Once the system has finished processing those two requests, it will proceed to the remaining two requests in the queue</p> <ul> <li> <p>Figure 4.18 shows the previously described strategy.</p> </li> <li> <p>In the diagram, the flowchart on the left side is the same as figure     4.17 where four total model serving requests from unauthenticated     users are sent to the model serving system.</p> </li> </ul> <p>Model serving requests from authenticated users Model serving requests from unauthenticated users</p> <p>Ok to add to the queue? Ok to add to the queue?</p> <p>Model serving requests that are Model serving requests that are under rate limit: under rate limit:</p> <p>2 maximum concurrent requests 3 maximum concurrent requests for unauthenticated users for authenticated users</p> <p>Figure 4.18 A comparison of behaviors from different rate limits applied to authenticated and unauthenticated users</p> <ul> <li>However, only two can be served by the system due to the current     rate limit, which allows a maximum of two concurrent model serving     requests for unauthenticated users.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Conversely, the model serving requests in the flowchart on the right     side all come from authenticated users. Thus, three requests can be     processed by the model serving system since the limit of maximum     concurrent requests for authenticated users is three.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Rate limits differ depending on whether the user is authenticated.     Rate limits thus effectively control the traffic of the model     serving system and prevent malicious denial-of-service attacks,     which could cause unexpected use of computational resources from the     shared resource pool and eventually lead to resource scarcity of     other services that rely on it.</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#points-to-note-1","title":"Points to Note:","text":"<ul> <li> <p>This pattern is not a universal solution.</p> </li> <li> <p>For machine learning applications with consistent traffic---for     example, model predictions calculated regularly based on a     schedule---an event-driven processing approach is unnecessary as the     system already knows when to process the requests, and there will be     too much overhead trying to monitor this regular traffic.</p> </li> <li> <p>In addition, applications that can tolerate less-accurate     predictions can work well without being driven by events; they can     also recalculate and provide good-enough predictions to a particular     granularity level, such as per day or per week.</p> </li> <li> <p>Event-driven processing is more suitable for applications with     different traffic patterns that are complicated for the system to     prepare beforehand necessary computational resources. With     event-driven processing, the model serving system only requests a     necessary amount of computational resources on demand.</p> </li> <li> <p>The applications can also provide more accurate and real-time     predictions since they obtain the predictions right after the users     send requests instead of relying on precalculated prediction results     based on a schedule.</p> </li> <li> <p>From developers' perspective, one benefit of the event-driven     processing pattern is that it's very intuitive.</p> </li> <li> <p>It greatly simplifies the process of deploying code to running     services since there is no end artifact to create or push beyond the     source code itself. The event-driven processing pattern makes it     simple to deploy code from our laptops or web browser to run code in     the cloud.</p> </li> <li> <p>In our case, we only need to deploy the trained machine learning     model that may be used as a function to be triggered based on user     requests.</p> </li> <li> <p>Once deployed, this model serving function is then managed and     scaled automatically without the need to allocate resources manually     by developers.</p> </li> <li> <p>In other words, as more traffic is loaded onto the service, more     instances of the model serving function are created to handle the     increase in traffic using the shared resource pool.</p> </li> <li> <p>If the model serving function fails due to machine failures, it will     be restarted automatically on other machines in the shared resource     pool.</p> </li> <li> <p>Given the nature of the event-driven processing pattern, each     function that's used to process the model serving requests needs to     be stateless and independent from other model serving requests.</p> </li> <li> <p>Each function instance cannot have local memory, which requires all     states to be stored in a storage service. For example, if our     machine learning models depend heavily on the results from previous     predictions (e.g., a time-series model), in this case, the     event-driven processing pattern may not be suitable.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#quiz-1","title":"Quiz","text":"<ol> <li> <p>Suppose we allocate the same amount of computational resources over     the lifetime of the model serving system for hotel price prediction.     What would the resource utilization rate look like over time?</p> </li> <li> <p>Are the replicated services or sharded services long-running     systems?</p> </li> <li> <p>Is event-driven processing stateless or stateful?</p> </li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%202/#section-1-unnumbered","title":"{#section-1 .unnumbered}","text":"<p>[Part 3___________________________________________________________________]{.underline}</p> <p>What we will cover:</p> <ul> <li> <p>Using workflows to connect machine learning system components</p> </li> <li> <p>Composing complex but maintainable structures within machine     learning workflows with the fan-in and fan-out patterns</p> </li> <li> <p>Accelerating machine learning workloads with concurrent steps using     synchronous and asynchronous patterns</p> </li> <li> <p>Improving performance with the step memoization pattern</p> </li> </ul> <p>Outcomes:</p> <ul> <li> <p>Understand workflows and their use in ML systems</p> </li> <li> <p>Understand step composition and workflow patterns</p> </li> <li> <p>Understand how to improve scale and performance in workflows</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Model serving is a critical step after successfully training a     machine learning model.</p> <ul> <li> <p>It is the final artifact produced by the entire machine learning     workflow, and the results from model serving are presented to users.</p> </li> <li> <p>A Workflow is an essential component in machine learning systems as     it connects all other components in the system.</p> </li> <li> <p>A machine learning workflow can be as easy as chaining data     ingestion, model training, and model serving.</p> </li> <li> <p>However, it can be very complex to handle real-world scenarios     requiring additional steps and performance optimizations as part of     the entire workflow.</p> </li> <li> <p>We need to know what tradeoffs we may see when making design     decisions to meet different business and performance requirements.</p> </li> </ul> <p>What is a workflow?</p> <ul> <li> <p>A Workflow is the process of connecting multiple components or     steps in an end-to-end machine learning system.</p> </li> <li> <p>A workflow consists of arbitrary combinations of the components     commonly seen in real-world machine learning applications, such as     data ingestion, distributed model training, and model serving, as     discussed in the previous chapters.</p> </li> <li> <p>Figure 5.1 shows a simple machine learning workflow. This workflow     connects multiple components or steps in an end-to-end machine     learning system that includes the following steps:</p> </li> <li> <p>Data ingestion---Consumes the Youtube-8M videos dataset</p> </li> <li> <p>Model training---Trains an entity-tagging model</p> </li> <li> <p>Model serving---Tags entities in unseen videos</p> </li> <li> <p>Machine learning workflows are often referred to as machine     learning pipelines.</p> </li> <li> <p>A machine learning workflow may consist of any combination of the     components</p> </li> <li> <p>Machine learning workflows appear in different forms in different     situations</p> </li> <li> <p>Fig 5.2 shows a more complicated workflow where two separate model     training steps are launched after a single data ingestion step, and     then two separate model serving steps are used to serve different     models trained via different model training steps</p> </li> </ul> <p>Figure 5.1 A simple machine learning workflow, including data ingestion, model training, and model serving. The arrows indicate directions. For example, the arrow on the right-hand side denotes the order of the step execution (e.g., the workflow executes the model serving step after the model training step is completed).</p> <p>Figure 5.2 A more complicated workflow, where two separate model training steps are launched after a single data ingestion step, and then two separate model serving steps are used to serve different models trained via different model training steps</p> <ul> <li>The complexity of machine learning workflows varies, which increases     the difficulty of building and maintaining scalable machine learning     systems.</li> </ul> <p>Sequential workflows and Directed Acyclic Graphs (DAGs)</p> <ul> <li> <p>A sequential workflow is a series of steps performed one after     another until the last step in the series is complete.</p> </li> <li> <p>The exact order of execution varies, but steps will always be     sequential. Figure 5.3 is a sequential workflow with three steps     executed sequentially.</p> </li> </ul> <p>A sequential workflow represents a series of steps performed one after another until the last step in the series has completed. The exact order of execution varies, but steps will always be sequential.</p> <p>Step C executes after step B has completed.</p> <p>Figure 5.3 An example sequential workflow with three steps that execute in the following order: A, B, and C.</p> <ul> <li> <p>A workflow can be seen as a DAG if it only consists of steps     directed from one step to another but never form a closed     loop.</p> </li> <li> <p>Figure 5.3 is a valid DAG, figure 5.4, however, is not a valid DAG</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   ML Workflows, in order to meet the requirements of different use     cases (e.g., batch retraining of the models, hyperparameter tuning     experiments, etc.) can get really complicated.</p> <p>A workflow where there's an additional step D that connects from step C and points to step A. These connections form a closed loop and thus the entire workflow is not a valid DAG.</p> <p>Figure 5.4 An example workflow where step D connects from step C and points to step A. These connections form a closed loop and thus the entire workflow is not a valid DAG.</p> <p>The closed loop no longer exists since this arrow is crossed out.</p> <p>Figure 5.5 Workflow where the last step D does not point back to step A. This workflow is a valid DAG since the closed loop no longer exists. Instead, it is a simple sequential workflow similar to figure 5.3.</p> <p>Fan-in and Fan-out Patterns</p> <ul> <li> <p>Q: What if the original YouTube-8M dataset has been updated, and     we'd like to train a new model from scratch using the same model     architecture?</p> </li> <li> <p>In this case, it's pretty easy to containerize each of these     components and chain them together in a machine learning workflow     that can be reused by re-executing the end-to-end workflow when the     data gets updated.</p> </li> <li> <p>As shown in figure 5.6, new videos are regularly being added to the     original YouTube-8M dataset, and the workflow is executed every time     the dataset is updated. The next model training step trains the     entity tagging model using the most recent dataset. Then, the last     model serving step uses the trained model to tag entities in unseen     videos.</p> </li> </ul> <p>Figure 5.6 New videos are regularly added to the original YouTube-8M dataset, and the workflow is executed every time the dataset is updated.</p> <p>Challenge: Build a machine learning workflow that trains different models after the system has ingested data from the data source, selects the top two models and uses the knowledge from both to provide model serving that generates predictions for users.</p> <ul> <li> <p>Building a workflow that includes the end-to-end normal process of a     machine learning system with only data ingestion, model training,     and model serving, where each component only appears once as an     individual step in the workflow, is pretty straightforward.</p> </li> <li> <p>Here, the workflow is more complex as we need to include multiple     model training steps as well as multiple model serving steps.</p> </li> <li> <p>Q: How do we formalize and generalize the structure of this complex     workflow so that it can be easily packaged, reused, and distributed?</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#solution-2","title":"Solution","text":"<ul> <li>Fig 5.7, a basic machine learning workflow that includes only data     ingestion, model training, and model serving, where each of these     components only appears once as an individual step in the workflow..</li> </ul> <p>Baseline workflow that includes only data ingestion, model training, and model serving where each of these components only appears once as individual steps in the workflow</p> <p>Figure 5.7 A baseline workflow including only data ingestion, model training, and model serving, where each of these components only appears once as an individual step in the workflow</p> <ul> <li> <p>We need a machine learning workflow that builds and selects the top     two best-performing models that will be used for model serving to     give better inference results.</p> </li> <li> <p>What is the motivation here?</p> <ul> <li> <p>Figure 5.8 shows two models: the first model has knowledge of     four entities, and the second model has knowledge of three     entities. Thus, each can tag the entities it knows from the     videos.</p> </li> <li> <p>We can use both models to tag entities at the same time and then     aggregate their results. The aggregated result is obviously more     knowledgeable and is able to cover more entities.</p> </li> <li> <p>Thus two models can be more effective and produce more     comprehensive entity-tagging results.</p> </li> </ul> </li> </ul> <p>Figure 5.8 The first model has knowledge of four entities and the second model has knowledge of three entities. Thus, each can tag the entities it knows from the videos. We can use both models to tag entities at the same time and then aggregate their results. The aggregated result covers more entities than each individual model.</p> <ul> <li> <p>Now that we understand the motivation behind building this complex     workflow, let's look at an overview of the entire end-to-end     workflow process.</p> </li> <li> <p>We want to build a machine learning workflow that performs the     following functions sequentially:</p> </li> <li> <p>Ingests data from the same data source</p> </li> <li> <p>Trains multiple different models, either different sets of     hyperparameters of the same model architecture or various model     architectures</p> </li> <li> <p>Picks the two top-performing models to be used for model serving for     each of the trained models</p> </li> <li> <p>Aggregates the models' results of the two model serving systems to     present to users</p> </li> <li> <p>Let's add placeholders to the baseline workflow for multiple     model training steps after data ingestion.</p> </li> <li> <p>We can then add multiple model serving steps once the multiple model     training steps finish.</p> </li> <li> <p>A diagram of the enhanced baseline workflow is shown in figure 5.9.</p> </li> <li> <p>The key difference from what we've dealt with before in the baseline     is the presence of multiple model training and model serving     components.</p> </li> <li> <p>The steps do not have direct, one-to-one relationships i.e., each     model training step may be connected to a single model serving step     or not connected to any steps at all.</p> </li> </ul> <p>Figure 5.9 A diagram of the enhanced baseline workflow where multiple model training steps occur after data ingestion, followed by multiple model serving steps</p> <ul> <li>Figure 5.10 shows that the models trained from the first two model     training steps outperform the model trained from the third model     training step. Thus, only the first two model training steps are     connected to the model serving steps.</li> </ul> <p>The models trained from the first two model training steps outperform the model trained from the third model training step. Thus, only the first two model training steps are connected to model serving steps.</p> <p>Figure 5.10 The models trained from the first two model training steps outperform the model trained from the third model training step. Thus, only the first two model training steps are connected to the model serving steps.</p> <ul> <li> <p>We can compose this workflow as follows:</p> </li> <li> <p>On successful data ingestion, multiple model training steps are     connected to the data ingestion step so that they can use the shared     data that's ingested and cleaned from the original data source.</p> </li> <li> <p>Next, a single step is connected to the model training steps to     select the top two performing models. It produces two model serving     steps that use the selected models to handle model serving requests     from users.</p> </li> <li> <p>A final step at the end of this machine learning workflow is     connected to the two model serving steps to aggregate the model     inference results that will be presented to the users.</p> </li> <li> <p>In figure 5.11, the workflow trains different models via three model     training steps resulting in varying accuracy when tagging entities.     A model selection step picks the top two models with at least 90%     accuracy trained from the first two model training steps that will     be used in the following two separate model serving steps. The     results from the two model serving steps are then aggregated to     present to users via a result aggregation step.</p> </li> </ul> <p>Three model training steps train</p> <p>Figure 5.11 A machine learning workflow that trains different models that result in varying accuracy when tagging entities and then selects the top two models with at least 90% accuracy to be used for model serving. The results from the two model serving steps are then aggregated to present to users.</p> <ul> <li> <p>We can pick out two patterns from this complex workflow. \\</p> </li> <li> <p>The first one we observe is the fan-out pattern.</p> <ul> <li>Fan-out describes the process of starting multiple separate     steps to handle input from the workflow. In our workflow, the     fan-out pattern appears when multiple separate model training     steps connect to the data ingestion step, as shown in figure     5.12.</li> </ul> </li> <li> <p>There's also the fan-in pattern in our workflow, where we have one     single aggregation step that combines the results from the two model     serving steps, as shown in 5.13</p> </li> </ul> <p>Figure 5.12 A diagram of the fan-out pattern that appears when multiple separate model training steps are connected to the data ingestion step</p> <p>figure 5.13. Fan-in describes the process of combining results from multiple steps into one step.</p> <p>Fanning in from two model serving steps to one result aggregation step.</p> <p>Figure 5.13 A diagram of the fan-in pattern, where we have one single aggregation step that combines the results from the two model serving steps</p> <ul> <li>Formalizing these patterns would help us build and organize more     complex workflows by using different patterns for workflows based on     real-world requirements.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#points-to-note-2","title":"Points to note:","text":"<ul> <li> <p>Using the fan-in and fan-out patterns, the system is able to execute     complex workflows that train multiple machine learning models and     pick the most performant ones to provide good entity-tagging results     in the model serving system.</p> </li> <li> <p>These patterns are great abstractions that can be incorporated into     very complex workflows to meet the increasing demand for complex     distributed machine learning workflows in the real world.</p> </li> <li> <p>In general, if both of the following applies, we can consider     incorporating these patterns:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The multiple steps that we are fanning-in or fanning-out are     independent of each other.</p> <ul> <li>It takes a long time for these steps to run sequentially.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The multiple steps need to be order-independent because we don't     know the order in which concurrent copies of those steps will run or     the order in which they will return.</p> <ul> <li> <p>For example, if the workflow also contains a step that trains an     ensemble of other models (also known as ensemble learning;     http://mng.bz/N2vn) to provide a better-aggregated model, this     ensemble model depends on the completion of other model training     steps.</p> </li> <li> <p>Consequently, we cannot use the fan-in pattern because the ensemble     model training step will need to wait for other model training to     complete before it can start running, which would require some extra     waiting and delay the entire workflow.</p> </li> </ul> <p>Ensemble models</p> <ul> <li> <p>An ensemble model uses multiple machine learning models to obtain     better predictive performance than could be obtained from any of the     constituent models alone.</p> </li> <li> <p>It often consists of a number of alternative models that can learn     the relationships in the dataset from different perspectives.</p> </li> <li> <p>Ensemble models tend to yield better results when diversity among     the constituent models is significant.</p> </li> <li> <p>Therefore, many ensemble approaches try to increase the diversity of     the models they combine.</p> </li> </ul> <p>Workflow complexity and performance</p> <ul> <li> <p>The fan-in and fan-out patterns can create very complex workflows     that meet most of the requirements of machine learning systems.</p> </li> <li> <p>However, to achieve good performance on those complex workflows, we     need to determine which parts of the workflows to run first and     which parts of the workflows can be executed in parallel.</p> </li> <li> <p>The result of this optimization is that data science teams would     spend less time waiting for workflows to complete, thus reducing     infrastructure costs.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#section-2-unnumbered","title":"{#section-2 .unnumbered}","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#questions","title":"Questions:","text":"<ol> <li> <p>If the steps are not independent of each other, can we use the     fan-in or fan-out patterns?</p> </li> <li> <p>What's the main problem when trying to build ensemble models with     the fan-in pattern?</p> </li> </ol> <p>Synchronous and asynchronous patterns: Accelerating workflows with concurrency</p> <ul> <li> <p>Each model training step in the system takes a long time to     complete; however, their durations may vary across different model     architectures or model parameters.</p> </li> <li> <p>Imagine an extreme case (LLMs) where one of the model training steps     takes two weeks to complete since it is training a complex machine     learning model that requires a huge amount of computational     resources.</p> </li> <li> <p>All other model training steps only take one week to complete.</p> </li> <li> <p>Many of the steps, such as model selection and model serving, in the     machine learning workflow we built earlier that uses the fan-in and     fan-out patterns will have to wait an additional week until this     long-running model training step is completed.</p> </li> <li> <p>Fig. 5.14 illustrates the duration differences among the three model     training steps.</p> </li> </ul> <p>One of the model training steps takes two weeks to complete since it is training a complex machine learning model that requires a huge amount of computational resources, whereas each of the rest of the model training steps only takes one week to complete.</p> <p>Figure 5.14 A workflow that illustrates the duration differences for the three model training steps</p> <ul> <li> <p>In this case, since the model selection step and the steps following     it require all model training steps to finish, the model training     step that takes two weeks to complete will slow down the workflow by     an entire week.</p> </li> <li> <p>We would rather use that additional week to re-execute all the model     training steps that take one week to complete instead of wasting     time waiting for one step!</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#section-3-unnumbered","title":"{#section-3 .unnumbered}","text":"<p>The Challenge: Accelerate workflows so it will not be affected by the duration of individual steps</p> <ul> <li> <p>We want to build a machine learning workflow that trains different     models and then selects the top two models to use for model serving,     which generates predictions based on the knowledge of both models.</p> </li> <li> <p>Due to varying completion times for each model training step in the     existing machine learning workflow, the start of the following     steps, such as the model selection step and the model serving,     depends on the completion of the previous steps.</p> </li> <li> <p>However, a problem occurs when at least one of the model training     steps takes much longer to complete than the remaining steps because     the model selection step that follows can only start after this long     model training step has completed.</p> </li> <li> <p>As a result, the entire workflow is delayed by this particularly     long-running step. Is there a way to accelerate this workflow so it     will not be affected by the duration of individual steps?</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#section-4-unnumbered","title":"{#section-4 .unnumbered}","text":"<p>The solution:</p> <ul> <li> <p>What if we can exclude the long-running model training step     completely?</p> </li> <li> <p>Once we do that, the rest of the model training steps will have     consistent completion times.</p> </li> <li> <p>Thus, the remaining steps in the workflow can be executed without     waiting for a particular step that's still running. A diagram of the     updated workflow is shown in figure 5.15.</p> </li> <li> <p>This naive approach may resolve our problem of extra waiting time     for long-running steps.</p> </li> <li> <p>However, our original goal was to use this type of complex workflow     to experiment with different machine learning model architectures     and different sets of hyperparameters of those models to select the     best-performing models to use for model serving.</p> </li> <li> <p>If we simply exclude the long-running model training step, we are     essentially throwing away the opportunity to experiment with     advanced models that may better capture the entities in the videos.</p> </li> <li> <p>Is there a better way to speed up the workflow so that it will not     be affected by the duration of this individual step? Let's focus on     the model training steps that only take one week to complete.</p> </li> <li> <p>What can we do when those short-running model training steps are     complete?</p> </li> </ul> <p>After the long-running model training step is excluded, the rest of the model training steps will have consistent completion time. Thus, the remaining steps in the workflow can be executed without having to wait for any particular step that's still running.</p> <p>Figure 5.15 The new workflow after the long-running model training step has been removed</p> <ul> <li> <p>When a model training step finishes, we have successfully obtained a     trained machine learning model.</p> </li> <li> <p>In fact, we can use this trained model in our model serving system     without waiting for the rest of the model training steps to     complete.</p> </li> <li> <p>As a result, the users can see the results of tagged entities from     their model serving requests that contain videos as soon as we have     trained one model from one of the steps in the workflow (figure     5.16).</p> </li> <li> <p>After a second model training step finishes, we can then pass the     two trained models directly to model serving. The aggregated     inference results are presented to users instead of the results from     only the model we obtained initially, as shown in figure 5.17</p> </li> </ul> <p>Uses the trained model from this short-running model training step that finishes earlier directly in our model serving system without waiting for the rest of the model training steps to complete</p> <p>Figure 5.16 A workflow where the trained model from a short-running model training step is applied directly to our model serving system without waiting for the remaining model training steps to complete</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#section-5-unnumbered","title":"{#section-5 .unnumbered}","text":"<p>After a second model training step finishes, we can pass the two trained models directly to be used for model serving, and the aggregated inference results will be presented to users instead of the results from only the one model that we obtained initially.</p> <p>Figure 5.17 After a second model training step finishes, we pass the two trained models directly to model serving. The aggregated inference results are presented to users instead of only the results from the model that we obtained initially.</p> <ul> <li> <p>Note that while we can continue to use the trained models for model     selection and model serving, the long-running model training step is     still running.</p> </li> <li> <p>In other words, the steps are executed asynchronously---they don't     depend on each other's completion.</p> </li> <li> <p>The workflow starts executing the next step before the previous step     finishes.</p> </li> <li> <p>Sequential steps are performed one at a time, and only when one has     completed does the following step become unblocked. In other words,     you must wait for a step to finish to move to the next one.</p> </li> <li> <p>For example, the data ingestion step must be completed before we     start any of the model training steps.</p> </li> <li> <p>Contrary to asynchronous steps, synchronous steps can start running     at the same time once dependencies are met.</p> </li> <li> <p>For example, the model training steps can run concurrently, as soon     as the previous data ingestion step has finished. A different model     training step does not have to wait for another to start.</p> </li> <li> <p>The synchronous pattern is typically useful when you have multiple     similar workloads that can run concurrently and finish near the same     time.</p> </li> <li> <p>By incorporating these patterns, the entire workflow will no longer     be blocked by the long-running model training step. Instead, it can     continue using the already trained models from the short-running     model training steps in the model serving system, which can start     handling users' model serving requests.</p> </li> <li> <p>The synchronous and asynchronous patterns are also extremely useful     in other distributed systems to optimize system performance and     maximize the use of existing computational resources---especially     when the amount of computational resources for heavy workloads is     limited.</p> </li> </ul> <p>Discussion</p> <ul> <li> <p>By mixing synchronous and asynchronous patterns, we can create more     efficient machine learning workflows and avoid any delays due to     steps that prevent others from executing, such as a long-running     model training step.</p> </li> <li> <p>However, the models trained from the short-running model training     steps may not be very accurate.</p> </li> <li> <p>That is, the models with simpler architectures may not discover as     many entities in the videos as the more complex model of the     long-running model training step (figure 5.18).</p> </li> </ul> <p>Figure 5.18 A model trained from two finished short-running model training steps with very simple models that serve as a baseline. They can only identify a small number of entities, whereas the model trained from the most time-consuming step can identify many more entities.</p> <ul> <li> <p>As a result, we should keep in mind that the models we get early on     may not be the best and may only be able to tag a small number of     entities, which may not be satisfactory to our users.</p> </li> <li> <p>When we deploy this end-to-end workflow to real-world applications,     we need to consider whether users seeing inference results faster or     seeing better results is more important. If the goal is to allow     users to see the inference results as soon as a new model is     available, they may not see the results they were expecting.</p> </li> <li> <p>However, if users can tolerate a certain period of delay, it's     better to wait for more model training steps to finish.</p> </li> <li> <p>Then, we can be selective about the models we've trained and pick     the best performing models that provide very good entity-tagging     results.</p> </li> <li> <p>Whether a delay is acceptable is subject to the requirements of     real-world applications.</p> </li> <li> <p>By using synchronous and asynchronous patterns, we can organize the     steps in machine learning workflows from structural and     computational perspectives.</p> </li> <li> <p>As a result, data science teams can spend less time waiting for     workflows to complete to maximize performance, thus reducing     infrastructure costs and idling computational resources.</p> </li> </ul> <p>Quiz:</p> <ol> <li> <p>What causes each step of the model training steps to start?</p> </li> <li> <p>Are the steps blocking each other if they are running     asynchronously?</p> </li> <li> <p>What do we need to consider when deciding whether we want to use any     available trained model as early as possible?</p> </li> </ol> <p>Step memoization pattern: Skipping redundant workloads via memoized steps</p> <ul> <li> <p>The dataset does not always remain unchanged (new YouTube videos are     becoming available and are being added to the YouTube-8M dataset     every week)</p> </li> <li> <p>if, we would like to retrain the model so that it accounts for the     additional videos that arrive on a regular basis, we need to run the     entire workflow regularly from scratch---from the data ingestion     step to the model serving step---as shown in figure 5.19.</p> </li> <li> <p>Say the dataset does not change, but we want to experiment with new     model architectures or new sets of hyperparameters, which is very     common for machine learning practitioners (figure 5.20).</p> </li> <li> <p>For example, we may change the model architecture from simple linear     models to more complex models such as tree-based models or     convolutional neural networks. We can also stick with the particular     model architecture we've used and only change the set of model     hyperparameters, such as the number of layers and hidden units in     each of those layers for neural network models or the maximum depth     of each tree for tree-based models. For cases like these, we still     need to run the end-to-end workflow, which includes the data     ingestion step to re-ingest the data from the original data source     from scratch. Performing data ingestion again is very     time-consuming.</p> </li> </ul> <p>Figure 5.19 A diagram of the entire workflow that is re-executed every time the dataset is updated</p> <p>New model type?</p> <p>1. Linear model</p> <p>Figure 5.20 A diagram where the entire workflow is re-executed every time we experiment with a new model type or hyperparameter even though the dataset has not changed</p> <p>The problem</p> <ul> <li>If the dataset is not updated, but we want to experiment with new     models, we still need to execute the entire workflow, including the     data ingestion step. However, the data ingestion step can take a     long time to complete depending on the size of the dataset. Is there     a way to make this workflow more efficient?</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#the-solution-1","title":"The solution","text":"<ul> <li>We would like to execute the data ingestion step only when we know     that the dataset has been updated, as shown in figure 5.21.</li> </ul> <p>*The dataset has not ***</p> <p>been updated yet.</p> <p>Figure 5.21 A diagram where the data ingestion step is skipped when the dataset has not been updated</p> <ul> <li> <p>Once we have a way to identify that, we can conditionally     reconstruct the machine learning workflow and control whether we     want to include a data ingestion step to be re-executed (figure     5.21).</p> </li> <li> <p>One way to identify whether the dataset has been updated is through     the use of cache. Since our dataset is being updated regularly on a     fixed schedule (e.g., once a month), we can create a time-based     cache that stores the location of the ingested and cleaned dataset     (assuming the dataset is located in a remote database) and the     timestamp of its last updated time. The data ingestion step in the     workflow will then be constructed and executed dynamically based on     whether the last updated timestamp is within a particular window.</p> </li> <li> <p>For example, if the time window is set to two weeks, we consider the     ingested data as fresh if it has been updated within the past two     weeks. The data ingestion step will be skipped, and the following     model training steps will use the already-ingested dataset from the     location that's stored in the cache.</p> </li> <li> <p>Figure 5.22 illustrates the case where a workflow has been     triggered, and we check whether the data has been updated within the     last two weeks by accessing the cache. If the data is fresh, we skip     the execution of the unnecessary data ingestion step and execute the     model training step directly.</p> </li> </ul> <p>Figure 5.22 The workflow has been triggered, and we check whether the data has been updated within the last two weeks by accessing the cache. If the data is fresh, we skip the execution of the unnecessary data ingestion step and execute the model training step directly.</p> <ul> <li> <p>The time window can be used to control how old a cache can be before     we consider the dataset fresh enough to be used directly for model     training instead of re-ingesting the data again from scratch.</p> </li> <li> <p>Alternatively, we can store some of the important metadata about the     data source in the cache, such as the number of records in the     original data source currently available.</p> </li> <li> <p>This type of cache is called content-based cache since it stores     information extracted from a particular step, such as the input and     output information.</p> </li> <li> <p>With this type of cache, we can identify whether the data source has     significant changes (e.g., the number of original records has     doubled in the data source). If there's a significant change, it's     usually a signal to re-execute the data ingestion step since the     current dataset is very old and outdated. A workflow that     illustrates this approach is shown in figure 5.23.</p> </li> </ul> <p>Figure 5.23 The workflow has been triggered, and we check whether the metadata collected from the dataset, such as the number of records in the dataset, has changed significantly. If it's not significant, we then skip the execution of the unnecessary data ingestion step and execute the model training step directly.</p> <ul> <li> <p>This pattern, which uses the cache to determine whether a step     should be executed or skipped, is called step memoization.</p> </li> <li> <p>With the help of step memoization, a workflow can identify the steps     with redundant workloads that can be skipped without being     re-executed and thus greatly accelerate the execution of the     end-to-end workflow. We'll apply this pattern in section 9.4.2.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#points-to-note-3","title":"Points to Note:","text":"<ul> <li> <p>In real-world machine learning applications, many workloads besides     data ingestion are computationally heavy and time-consuming.</p> </li> <li> <p>For example, the model training step uses a lot of computational     resources to achieve high-performance model training and can     sometimes take weeks to complete. If we are only experimenting with     other components that do not require updating the trained model, it     might make sense to avoid re-executing the expensive model training     step. The step memoization pattern comes in handy when deciding     whether you can skip heavy and redundant steps.</p> </li> <li> <p>If we are creating content-based caches, the decision about the type     of information to extract and store in the cache may not be trivial.</p> </li> <li> <p>For example, if we are trying to cache the results from a model     training step, we may want to consider using the trained model     artifact that includes information such as the type of machine     learning model and the set of hyperparameters of the model.</p> </li> <li> <p>When the workflow is executed again, it will decide whether to     re-execute the model training step based on whether we are trying     the same model.</p> </li> <li> <p>Alternatively, we may store information like the performance     statistics (e.g., accuracy, mean-squared error, etc.) to identify     whether it's beyond a threshold and worth training a more performant     model.</p> </li> <li> <p>Furthermore, when applying the step memoization pattern in practice,     be aware that it requires a certain level of maintenance efforts to     manage the life cycle of the created cache. For example, if 1,000     machine learning workflows run every day with an average of 100     steps for each workflow being memoized, 100,000 caches will be     created every day.</p> </li> <li> <p>Depending on the type of information they store, these caches     require a certain amount of space that can accumulate rather     quickly.</p> </li> <li> <p>To apply this pattern at scale, a garbage collection mechanism     must be in place to delete unnecessary caches automatically to     prevent the accumulation of caches from taking up a huge amount of     disk space.</p> </li> <li> <p>For example, one simple strategy is to record the timestamp when the     cache is last hit and used by a step in a workflow and then scan the     existing caches periodically to clean up those that are not used or     hit after a long time.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#quiz-2","title":"Quiz:","text":"<ol> <li> <p>What type of steps can most benefit from step memoization?</p> </li> <li> <p>How do we tell whether a step's execution can be skipped if its     workflow has been triggered to run again?</p> </li> <li> <p>What do we need to manage and maintain once we've used the pattern     to apply the pattern at scale?</p> </li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%202/#summary","title":"Summary","text":"<ul> <li> <p>Workflow is an essential component in machine learning systems as it     connects all other components in a machine learning system. A     machine learning workflow can be as easy as chaining data ingestion,     model training, and model serving.</p> </li> <li> <p>The fan-in and fan-out patterns can be incorporated into complex     workflows to make them maintainable and composable.</p> </li> <li> <p>The synchronous and asynchronous patterns accelerate the machine     learning workloads with the help of concurrency.</p> </li> <li> <p>The step memoization pattern improves the performance of workflows     by skipping duplicate workloads.</p> </li> </ul> <p>[Part 4______________________________________________________________]{.underline}</p> <p>What is covered:</p> <ol> <li> <p>Recognizing the need for areas of improvement in machine learning     systems, such as job scheduling and metadata</p> </li> <li> <p>/Preventing resource starvation and avoiding deadlocks using     scheduling techniques, such as fair-share scheduling, priority     scheduling, and gang scheduling</p> </li> <li> <p>Handling failures more effectively to reduce any negative effect on     users via the metadata path</p> </li> <li> <p>Real-world distributed machine learning workflows are extremely     complex, and a huge amount of operational work is involved to help     maintain and manage the various components of the systems, such as     improvements to system efficiency, observability, monitoring,     deployment, etc.</p> </li> <li> <p>In this section we'll use scheduling techniques to prevent resource     starvation and avoid deadlocks when many team members are working     collaboratively in the same cluster with limited computational     resources.</p> </li> <li> <p>We will also discuss the benefits of the metadata pattern, which can     provide insights into the individual steps in machine learning     workflows and help us handle failures more appropriately to reduce     any negative effects on users.</p> </li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%202/#operations-in-machine-learning-systems","title":"Operations in machine learning systems","text":"<ul> <li> <p>The focus here is on operational techniques and patterns that are     commonly seen in more than one component or step in a machine     learning workflow, instead of patterns that are specific to each     individual component.</p> </li> <li> <p>For example, the workflow shown in figure 6.1 includes three failed     steps in the multiple model training steps that occur after data     ingestion and in the multiple model serving steps that occur after     the multiple model training steps.</p> </li> <li> <p>Unfortunately, each step is like a black box, and we don't know many     details about any of them yet.</p> </li> <li> <p>At this point, we only know whether they fail and whether the     failures have affected the following steps. As a result, they are     really hard to debug.</p> </li> </ul> <p>Three steps failed in this workflow, but we don't know what the root cause of the failures is just by looking at the workflow at a higher level.</p> <p>Perhaps it failed to connect to the database or the workers for model training ran out of memory.</p> <p>Figure 6.1 An example workflow where multiple model training steps occur after data ingestion and multiple model serving steps occur after the multiple model training steps. Note the three failed steps.</p> <ul> <li>The operation patterns introduced here can increase the visibility     of the entire workflow to help us understand the root cause of the     failures and give us some ideas on how to handle the failures     properly.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   In addition, the increased observability may help us develop     improvements in system efficiency that are beneficial to future     executions of similar workflows.</p> <p>MLOps</p> <ul> <li> <p>MLOps, a term derived from machine learning and operations, refers     to a collection of practices for managing machine learning life     cycles in production, including practices from machine learning and     DevOps, to efficiently and reliably deploy and manage machine     learning models in production.</p> </li> <li> <p>MLOps requires communication and collaboration between DevOps and     data science teams as it focuses on improving the quality of     production machine learning and embracing automation while     maintaining business requirements.</p> </li> <li> <p>The scope of MLOps can be extremely large and varies depending on     the context.</p> </li> <li> <p>Given how large the scope of MLOps can be, depending on the context,     we will only focus on a selected set of mature patterns at the time.</p> </li> </ul> <p>Scheduling patterns: Assigning resources effectively in a shared cluster</p> <ul> <li> <p>A scheduler is responsible for assigning computational resources to     perform tasks requested by the system.</p> </li> <li> <p>The scheduler is designed to keep computational resources busy and     allow multiple users to collaborate with shared resources more     easily.</p> </li> <li> <p>Multiple users are trying to build models using the shared     computational resources in the cluster for different scenarios.</p> </li> <li> <p>For example, one user is working on a fraud detection model that     tries to identify fraudulent financial behaviors such as     international money laundering.</p> </li> <li> <p>Another user is working on a condition monitoring model that can     generate a health score to represent the current condition for     industrial assets such as components on trains, airplanes, wind     turbines, etc.</p> </li> <li> <p>Our beginning infrastructure only provides a simple scheduler, which     schedules jobs on a first-come, first-served basis, as shown in     figure 6.2. For example, the third job is scheduled after the second     job has been scheduled, and each job's computational resources are     allocated on scheduling.</p> </li> </ul> <p>The current infrastructure uses a simple scheduler that schedules jobs on a first-come, first-served basis.</p> <p>Job 3 is scheduled after job 2 has been scheduled.</p> <p>Figure 6.2 A diagram of an infrastructure that only provides a simple scheduler, which schedules jobs on a first-come, first-served basis</p> <ul> <li> <p>When users submit multiple model training jobs to experiment with     different sets of models or hyperparameters, these multiple models     block prevent users' model training jobs from executing since those     previously submitted experiments are already utilizing all the     available computational resources.</p> </li> <li> <p>Users end up competing for resources (e.g., waking up in the middle     of the night to submit model training jobs when fewer users are     using the system) resulting in poor collaboration among team     members.</p> </li> <li> <p>Jobs involving training very large machine learning models, which     consume a lot of computational resources increase the time other     users have to wait for their jobs to execute.</p> </li> <li> <p>In addition, if we schedule only some of the requested workers for a     distributed model training job, the model training cannot     execute until all of the requested workers are ready; the nature     of the distribution strategy is distributed training with the     collective communication pattern.</p> </li> <li> <p>If necessary computational resources are lacking, the distributed     model training job will never start, and the already-allocated     computational resources for the existing workers will be wasted.</p> </li> </ul> <p>The Challenge: Find alternative approaches to first come first served scheduling so that computational resources can be utilized much more effectively in a shared cluster</p> <p>The context:</p> <ul> <li> <p>We have set up a distributed infrastructure for users to submit     distributed model training jobs scheduled to run by a default     scheduler responsible for assigning computational resources to     perform various tasks requested by the users.</p> </li> <li> <p>However, the default scheduler only provides a simple scheduler that     schedules jobs on a first-come, first served basis.</p> </li> <li> <p>As a result, when multiple users attempt to use this cluster, they     often need to wait a long time for available computational     resources.</p> </li> <li> <p>Additionally, distributed model training jobs cannot begin to     execute until all of the requested workers are ready due to the     nature of the distributed training strategy, such as a collective     communication strategy.</p> </li> <li> <p>Are there any alternatives to the existing default scheduler so we     could assign the computational resources more effectively in a     shared cluster?</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#the-solution-approach","title":"The solution approach","text":"<ul> <li> <p>An intuitive solution approach would be to limit how much of the     total computational resources each user is allotted.</p> </li> <li> <p>If there are four users (A, B, C, and D), once user A submits a job     that uses 25% of the total available CPU cycles     (https://techterms.com/definition/clockcycle),     they cannot submit another job until those allocated resources are     released and ready to be allocated to new jobs, other users could     submit jobs independent of how much resources user A is using.</p> </li> <li> <p>If user B starts two processes that use the same amount of     resources, those processes will be attributed 12.5% of the total CPU     cycles each, giving user B 25% of total resources</p> </li> <li> <p>Each of the other users still receives 25% of the total cycles.     Figure 6.3 illustrates the resource allocations for these four     users.</p> </li> <li> <p>Finally, if a new user E starts a process on the system, the     scheduler will reapportion the available CPU cycles so that each     user gets 20% of the whole (100% / 5 = 20%).</p> </li> <li> <p>The way we schedule our workloads to execute in our cluster in     figure 6.3 is called fair-share scheduling.</p> </li> <li> <p>It is a scheduling algorithm for computer operating systems in which     the CPU usage is equally distributed among system users or     groups, as opposed to equal distribution among processes.</p> </li> <li></li> </ul> <p>The resources are only split among the</p> <p>total available CPU cycles for user A. User C's resources are independent of A.</p> <p>Figure 6.3 The resource allocations for the four users (A, B, C, and D)</p> <ul> <li> <p>When multiple teams are using the system to train their machine     learning models and each team has multiple members, we can partition     users into different groups and then apply the fair-share scheduling     algorithm to both the users and the groups.</p> </li> <li> <p>We first divide the available CPU cycles among the groups and then     divide further among the users within each group. For example, if     three groups contain three, two, and four users, respectively, each     group will be able to use 33.3% (100% / 3) of the total available     CPU cycles. We can then calculate the available CPU cycles for each     user in each group as follows:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Group 1---33.3% / 3 users = 11.1% per user</p> <ul> <li> <p>Group 2---33.3% / 2 users = 16.7% per user</p> </li> <li> <p>Group 3---33.3% / 4 users = 8.3% per user</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Figure 6.4 summarizes the resource allocation we calculated for each     individual user in the three groups.</p> <ul> <li> <p>Fair-share scheduling would help us resolve the problem of multiple     users running distributed training jobs concurrently.</p> </li> <li> <p>We can apply this scheduling strategy at each level of abstraction,     such as processes, users, groups, etc. All users have their own pool     of available resources without interfering with each other.</p> </li> <li> <p>However, in some situations, certain jobs should be executed     earlier. For example, a cluster administrator would like to submit     jobs for cluster maintenance, such as deleting jobs that have been     stuck and taking up resources for a long time.</p> </li> <li> <p>Executing these cluster maintenance jobs earlier would help make     more computational resources available and thus unblock others from     submitting new jobs.</p> </li> </ul> <p>Figure 6.4 A summary of the resource allocation for each user in three groups</p> <p>Assumptions:</p> <ul> <li> <p>The cluster administrator is user 1 in group 1.</p> </li> <li> <p>Two other non admin users are also in group 1.</p> </li> <li> <p>User 2 is running job 1, which is using all of the 11.1% of the CPU     cycles allocated to them based on the fair-share scheduling     algorithm.</p> </li> </ul> <p>Details:</p> <ul> <li> <p>Even though user 2 has enough computational power to perform job 1,     the job depends on the success of job 2 from user 3. For example,     job 2 from user 3 produces a table in the database that job 1 needs     to perform a distributed model training task. Figure 6.5 summarizes     the resource allocations and usages for each user in the first     group.</p> </li> <li> <p>Job 2 is stuck due to an unstable database connection and keeps     trying to reconnect to produce the data that job 1 needs. To fix the     problem, the administrator needs to submit job 3 that kills and     then restarts the stuck job 2.</p> </li> <li> <p>Now assume that the admin user 1 is already using 11.1% of the total     CPU cycles available.</p> </li> <li> <p>Since maintenance job 3 is submitted later than all previous jobs,     it is added to the job queue and waits to be executed when resources     are released, based on the first-come, first-served nature of our     fair-share scheduling algorithm.</p> </li> <li> <p>As a result, we will encounter a deadlock where no job can     proceed, as illustrated in figure 6.6.</p> </li> <li> <p>To fix this problem, we can allow users to assign priorities to     each of the jobs so that jobs with higher priority are     executed earlier, in contrast to the first-come, first-served     nature of the fair-share scheduling algorithm.</p> </li> </ul> <p>Figure 6.5 A summary of resource allocations and usages for each user in the first group</p> <p>Figure 6.6 The admin user (user 1) in group 1 is trying to schedule a job to restart the stuck job (job 3) but encounters a deadlock where no job can proceed.</p> <ul> <li> <p>In addition, jobs that are already running can be preempted or     evicted to make room for jobs with higher priorities if enough     computational resources are not available. This approach to     scheduling jobs based on priorities is called priority     scheduling.</p> </li> <li> <p>Consider, four jobs (A, B, C, and D) have been submitted     concurrently. Each job has been marked with priorities by the users.     Jobs A and C are high priority, whereas job B is low     priority, and job D is medium priority.</p> </li> <li> <p>With priority scheduling, jobs A and C will be executed first since     they have the highest priorities, followed by the execution of job D     with medium priority and, eventually low-priority job B. Figure 6.7     illustrates the order of execution for the four jobs (A, B, C,     and D) when priority scheduling is used.</p> </li> </ul> <p>Figure 6.7 The order of execution for the four concurrently submitted jobs (A, B, C, and D) when priority scheduling is used</p> <ul> <li> <p>Consider another example. Assume three jobs (B, C, and D) with     different priorities are submitted concurrently and are executed     based on their priorities. If another job (job A) with high priority     is submitted after job B, which is low priority, has already started     running, job B will be preempted, and then job A will start.</p> </li> <li> <p>The computational resources previously allocated to job B will be     released and taken over by job A. Figure 6.8 summarizes the order of     execution for the four jobs (A, B, C, and D) where the low-priority     job B already running is preempted by a new job (job A) with higher     priority.</p> </li> <li> <p>With priority scheduling, we can effectively eliminate the     problem we previously encountered, where jobs can only be     executed sequentially on a first-come, first served basis. Jobs can     now be preempted in favor of tasks with high priorities.</p> </li> <li> <p>However, for distributed machine learning tasks---specifically,     model training tasks---we want to ensure that all workers are ready     before starting distributed training. Otherwise, the ones that     are ready would be waiting for the remaining workers before the     training can proceed, which wastes resources.</p> </li> <li> <p>For example, in figure 6.9, three worker processes in the same     process group are performing an allreduce operation, however, two     workers are not ready because the underlying distributed cluster is     experiencing an unstable network.</p> </li> </ul> <p>1. These three jobs are executed based on their priorities (C \u2192 \u2192D B).</p> <p>2. Job A (high priority) is submitted after job B (low priority) has already started running.</p> <p>3. Job B will be preempted, and then job A will start.</p> <p>Figure 6.8 The order of execution for the four jobs (A, B, C, and D) where the running low-priority job is preempted by a new job with higher priority</p> <ul> <li>As a result, two of the processes (processes 1 and 3) that depend on     those affected communications would not receive some of the     calculated gradient values (v0 and v2) on time (denoted by question     marks in figure 6.9), and the entire allreduce operation is stuck     until everything is received.</li> </ul> <p>Figure 6.9 An example of the allreduce process with an unstable network between the worker processes that blocks the entire model training process</p> <ul> <li> <p>Gang scheduling is usually used to run distributed model training     tasks.</p> </li> <li> <p>It ensures that if two or more workers communicate with each other,     they will be ready to do so at the same time; i.e., gang scheduling     only schedules workers when enough workers are available and ready     to communicate.</p> </li> <li> <p>If they are not gang scheduled, one worker may wait to send or     receive a message while the other worker is sleeping, and vice     versa. When the workers are waiting for other workers to be ready     for communication, we are wasting allocated resources on the workers     that are ready, and the entire distributed model training task is     stuck.</p> </li> <li> <p>For example, for collective communication--based distributed model     training tasks, all workers must be ready to communicate the     calculated gradients and update the models on each worker to     complete an allreduce operation.</p> </li> <li> <p>Assume that the machine learning framework does not support elastic     scheduling yet.</p> </li> <li> <p>As shown in figure 6.10, the gradients are all denoted by question     marks since they have not yet arrived in any of those worker     processes in the second worker group. All worker processes have not     yet started sending the gradients, and they won't until they all     move to the ready state after the network stabilizes.</p> </li> </ul> <p>Figure 6.10 With gang scheduling, the worker processes will not start sending the gradients until they are all in the ready state after the network becomes stable.</p> <ul> <li> <p>With gang scheduling, we can make sure not to start any of the     worker processes until all workers are ready, so none of them will     be waiting for the remaining worker processes. As a result, we can     avoid wasting computational resources.</p> </li> <li> <p>Once the network becomes stable, all of the gradients (v0, v1, and     v2) arrive on each worker process after a successful allreduce     operation, as shown in figure 6.11.</p> </li> <li> <p>The details of different types of gang scheduling are out of the     scope of this course     (https://www.geeksforgeeks.org/gang-scheduling-in-operating-system/).     We will be using an existing open source framework to integrate gang     scheduling into distributed training in the last part of this     course.</p> </li> </ul> <p>Figure 6.11 All of the gradients arrive on each of the worker processes after a successful allreduce operation once the network is stable.</p> <ul> <li> <p>By incorporating different scheduling patterns, we are able to     address various problems that arise when multiple users are using     the infrastructure to schedule different types of jobs.</p> </li> <li> <p>Although we looked at a few specific use cases for these scheduling     patterns, the patterns can be found in many systems that require     careful management of computational resources, especially when     resources are scarce.</p> </li> <li> <p>Many scheduling techniques are applied to even lower-level operating     systems to make sure the applications run efficiently and reasonably     share resources.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#points-to-note-4","title":"Points to Note","text":"<ul> <li> <p>Fair-share scheduling can help solve the problem of multiple users     running distributed training jobs concurrently.</p> </li> <li> <p>Fair-share scheduling allows the application of a scheduling     strategy at each level of abstraction, such as processes, users,     groups, etc.</p> </li> <li> <p>Priority scheduling can be used to effectively eliminate the problem     we encounter when jobs can only be executed sequentially on a first     come, first-served basis.</p> </li> <li> <p>Priority scheduling allows jobs to be executed based on their     priority levels, preempting low-priority jobs to make room for     high-priority jobs.</p> </li> <li> <p>With priority scheduling, if a cluster is used by a large number of     users, a malicious user could create jobs at the highest possible     priority, causing other jobs to be evicted or not get scheduled at     all.</p> </li> <li> <p>To deal with this potential problem, administrators of realworld     clusters usually enforce certain rules and limits to prevent users     from creating a huge number of jobs at high priorities.</p> </li> <li> <p>Gang scheduling ensures that, if two or more workers communicate     with each other, they will all be ready to communicate at the     same time.</p> </li> <li> <p>Gang scheduling is especially helpful for collective     communication--based distributed model training jobs where all     workers need to be ready to communicate the calculated gradients to     avoid wasting computational resources.</p> </li> <li> <p>Some machine learning frameworks support elastic scheduling     (https://github.com/sql-machine-learning/elasticdl/), which allows     distributed model training jobs to start with any number of workers     available without waiting for all the requested workers to be ready;     If this is available, gang scheduling would not be preferable</p> </li> <li> <p>But, because the number of workers may change during model     training, the batch size (sum of the size of mini-batches on each     worker) will affect the model training accuracy. In that case,     additional modifications to the model training strategy are needed.     For example, we can support a customized learning rate scheduler     that will account for epoch or batch or adjust the batch size     dynamically based on the number of workers. Together with these     algorithmic improvements, we can allocate and utilize existing     computational resources more wisely and improve the user experience.</p> </li> <li> <p>In practice, distributed model training jobs greatly benefit from     scheduling patterns like gang scheduling. As a result, we can avoid     wasting computational resources (which represent costs).</p> </li> <li> <p>One problem we need to address is that any of these worker processes     scheduled by gang scheduling may fail, leading to unexpected     consequences.</p> </li> <li> <p>Often it's hard to debug these types of failures. and we will     discuss patterns that will make debugging and handling failures     easier.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#quiz-3","title":"Quiz:","text":"<ol> <li> <p>Can we only apply fair-share scheduling at the user level?</p> </li> <li> <p>Is gang scheduling suitable for all distributed model training jobs?</p> </li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%202/#metadata-pattern","title":"Metadata pattern:","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#handle-failures-appropriately-minimize-negative-effects","title":"Handle failures appropriately minimize negative effects","text":"<ul> <li> <p>In simple ML workflows, we can retry the failed step and easily     continue model training without rerunning the entire data ingestion     process, (figure 6.12).</p> </li> <li> <p>However, when workflows get more complicated, failures becomes     non-trivial to handle</p> </li> <li> <p>For example, consider the workflow from previous sections.</p> </li> <li> <p>That workflow trains models via three model training steps that     arrive at different accuracies when tagging entities. Then, a model     selection step picks the top two models with at least 90% accuracy     trained from the first two model training steps, which will be used     in the following two separate model serving steps.</p> </li> </ul> <p>Baseline workflow that includes only data ingestion, model training, and model serving where each of these components only appears once as individual steps in the workflow</p> <p>If any of the steps fail, we can easily retry the failed step and pick up from what's left.</p> <p>Figure 6.12 A baseline workflow where the model training step has failed to take the ingested data. We retry the failed step and pick up from the failed step to continue model training without rerunning the entire data ingestion process.</p> <ul> <li> <p>The results from the two model serving steps are then aggregated via     a result aggregation step to present to users.</p> </li> <li> <p>Consider the case where the second and the third model training     steps have both failed during execution (e.g., some of the workers     allocated for model training are preempted). These two model     training steps would have provided both the most and the least     accurate model if they had finished successfully, as shown in figure     6.13.</p> </li> <li> <p>At this point, one might think that we should rerun both steps to     proceed to the model selection and model serving steps. However, in     practice, since we already wasted some time training part of the     models, we may not want to start everything from scratch. It would     be much longer before our users can see the aggregated results from     our best models.</p> </li> <li> <p>Question: Is there a better way to handle such kinds of     failures?</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#section-6-unnumbered","title":"{#section-6 .unnumbered}","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#the-challenge-to-find-a-way-to-handle-these-failures-appropriately-so-the-negative-effect-on-users-can-be-minimized.","title":"The Challenge: To find a way to handle these failures appropriately so the negative effect on users can be minimized.","text":"<p>The context:</p> <ul> <li> <p>For complicated machine learning workflows, such as the one we     discussed in earlier sections, where we want to train multiple     models and then select the top-performing models for model serving,     the decision on which strategy to use to handle failures of certain     steps due to real-world requirements is not always trivial.</p> </li> <li> <p>For example, when two out of three model training steps fail due to     preempted workers, we don't want to start training those models from     scratch, which greatly increases the time needed to complete the     workflow.</p> </li> <li> <p>How do we handle these failures appropriately so the negative effect     on users can be minimized?</p> </li> </ul> <p>Three different model training steps train</p> <p>These two model training steps would have provided both the most and the least accurate model if they finished successfully.</p> <p>Figure 6.13 A machine learning workflow that trains models with different accuracies when tagging entities.</p> <p>The model selection step identifies the top two models with at least 90% accuracy to be used for model serving. The accuracies are crossed out in these two steps because the steps failed without arriving at the expected accuracies. The results from the two model serving steps are then aggregated to present to users.</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#the-solution-approach-1","title":"The solution approach:","text":"<ul> <li> <p>Whenever we encounter a failure in a machine learning workflow, we     should first understand the root cause (e.g., loss of network     connections, lack of computational resources, etc).</p> </li> <li> <p>Knowing the root cause is important because we need to understand     the nature of the failure to predict whether retrying the failed     steps would help.</p> </li> <li> <p>If failures are due to long-lasting shortages, that could lead to     repetitive failures when retrying; In this case, we could     better utilize the computational resources to run some other tasks.</p> </li> <li> <p>Figure 6.14 illustrates the difference in the effectiveness of     retrying for permanent and temporary failures. Retrying the model     training step when encountering permanent failures makes the retries     ineffective and leads to repetitive failures.</p> </li> <li> <p>Checking whether the dependencies of a model training step are     met, viz., whether the ingested data from the previous step is     still available is essential.</p> </li> <li> <p>If the data has been persisted to a local disk to a database,     we can proceed to model training. However, if the data was located     in memory and lost when the model training step failed, we cannot     start model training without ingesting the data again.</p> </li> <li> <p>Figure 6.15 shows the process of restarting the data ingestion step     when there's a permanent failure during model training.</p> </li> </ul> <p>Permanent failures: Temporary failures:</p> <ol> <li> <p>Disappeared training data 1. Lack of resources</p> </li> <li> <p>... 2. Loss of network connections</p> </li> <li> <p>...</p> </li> </ol> <p>Figure 6.14 The difference in the effectiveness of retrying for permanent and temporary failures</p> <p>If the data was located in memory and was lost when the model training step failed, then we cannot start model training without starting ingesting the data again.</p> <p>Figure 6.15 The process of restarting the data ingestion step when a permanent failure occurs during model training</p> <ul> <li> <p>Similarly, if the model training step fails due to preempted     training workers or out-of-memory problems, we need to make sure     we still have sufficient computational resources allocated to rerun     the model training step.</p> </li> <li> <p>However, we won't know what information to analyze to determine     the root cause unless we intentionally record it as metadata     during the runtime of each step in the entire machine learning     workflow.</p> </li> <li> <p>For example, for each model training step, we can record     metadata on the availability of the ingested data and whether     different computational resources, such as memory and CPU usage,     exceeded the limit before the step failed.</p> </li> <li> <p>Figure 6.16 is a workflow where the model training step failed.     Metadata is collected every 5 minutes on memory usage (in megabytes)     and the availability of the training data (yes/no) during the     runtime of this step.</p> </li> <li> <p>We can notice a sudden huge memory spike from 23 MB to 200 MB after     30 minutes. In this case, we can retry this step with an increase in     requested memory, and it would then successfully produce a trained     model that will be used for the next model serving step.</p> </li> </ul> <p>Figure 6.16 An example workflow where the model training step failed, with the metadata collected showing an unexpected memory spike during runtime</p> <ul> <li> <p>In practice, for complex workflows like in figure 6.13, even when we     know all the dependencies of model training steps are met (e.g., we     have enough computational resources and a good database connection     to access the data source), we should also think about whether we     want to handle the failures and how we'd like to handle them.</p> </li> <li> <p>We've spent a lot of time on the training steps already, but now,     the steps have suddenly failed, and we've lost all the progress. In     other words, we don't want to start re-training all the models from     scratch, which may add considerable time before we can deliver the     aggregated results from our best models to users.</p> </li> <li> <p>Question: Is there a better way to handle this without a huge     effect on our user experience?</p> </li> <li> <p>In addition to the metadata we've recorded for each of the model     training steps, we could save more useful metadata that can be used     to figure out whether it's worth rerunning all the model training     steps. For example, the model accuracy over time indicates     whether the model is being trained effectively.</p> </li> <li> <p>Model accuracy that remains steady or even decreases (from 30% to     27%, as shown in figure 6.17) may indicate that the model already     converges and continuing training would no longer improve model     accuracy. In this example, even though two model training steps     fail, it's not necessary to retry the third model training step from     scratch since it would lead to a model that converges fast but with     low accuracy. Another example of metadata that can be potentially     useful is the percentage of completed model training (e.g., if we've     iterated through all the requested number of batches and epochs, the     completion is 100%).</p> </li> </ul> <p>The model accuracy decreases, which might indicate that the model already converges and continuing training would no longer improve the model accuracy.</p> <p>Figure 6.17 An example workflow where two model training steps fail and one has decreasing model accuracy</p> <ul> <li> <p>Once we have this additional metadata about model training steps, we     can tell how well each started model training step progresses.</p> </li> <li> <p>For example, for the workflow in figure 6.18, we could potentially     conclude ahead of time that the third model training step was     progressing very slowly (only 1% of completion every 30 minutes) due     to a smaller amount of allocated computational resources or more     complex model architecture.</p> </li> <li> <p>We know that it's highly likely that, given the limited time, we end     up with a model with low accuracy. As a result, we can disregard     this model training step in favor of allocating more computational     resources to the other model training steps with more potential,     which leads to more accurate models faster.</p> </li> <li> <p>Recording these metadata may help us derive more insights specific     to each of the failed steps in the end-to-end machine learning     workflow. We can then decide on a strategy to handle the failed     steps appropriately to avoid wasting computational resources and     minimize the effect on existing users.</p> </li> <li> <p>The metadata patterns provide great visibility into our machine     learning pipelines. They can also be used to search, filter, and     analyze the artifacts produced in each step in the future if we run     a lot of pipelines on a regular basis. For example, we might want to     know which models are performant or which datasets contribute the     most to those models based on the historical training metrics.</p> </li> <li></li> </ul> <p>This model training step was progressing very slowly ^As\\ a\\ result,\\ we\\ can^ due to smaller amount of allocated computational ^disregard\\ this\\ model^ resources or more complex model architecture. ^training\\ step\\ in\\ favor^</p> <p>of allocating more computational resources to the model training steps with more potential, which leads to more accurate models faster.</p> <p>Figure 6.18 An example workflow where two model training steps fail. One is disregarded because it is progressing very slowly, and the model will likely have low accuracy given the limited time.</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#points-to-note-5","title":"Points To Note:","text":"<ul> <li> <p>With the help of the metadata pattern, we can gain additional     insights into the individual steps in machine learning workflows.     Then, if any fail, we can respond based on what's beneficial to our     users and thus reduce any negative effect due to the step failures.</p> </li> <li> <p>One common type of metadata is the various network performance     (http://mng .bz/D4lR) metrics while the model     is being trained (e.g., bandwidth, throughput, latency).</p> </li> <li> <p>This type of information is very useful for detecting when certain     workers experience poor network performance that blocks the entire     training process. We can take down slow workers and start new     workers to continue training, assuming the underlying machine     learning frameworks support elastic scheduling and fault-tolerance     (see chapter 3).</p> </li> <li> <p>For example, in figure 6.19, based on the metadata, the worker on     the right-hand side has extremely high latency (10 times the latency     of the other workers), which slows down the entire model training     process. Ideally, this worker would be taken down and restarted.</p> </li> </ul> <p>This worker node has extremely high latency (10 times the latency of the other workers) that slows down the entire model training process.</p> <p>Figure 6.19 An example parameter server--based model training where the worker on the right-hand side has extremely high latency (10 times the latency of the other workers), which slows down the entire model training process</p> <ul> <li>One additional benefit of introducing the metadata pattern to our     machine learning workflows is to use the metadata recorded to     establish relationships between the individual steps or across     different workflows. For example, modern model management tools can     use the recorded metadata to help users build the lineage of the     trained models and visualize what individual steps/factors     contributed to the model artifacts.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#quiz-4","title":"Quiz:","text":"<ol> <li> <p>If the training step failed due to the loss of training data source,     what should we do?</p> </li> <li> <p>What type of metadata can be collected if we look at individual     workers or parameter servers?</p> </li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%202/#section-7-unnumbered","title":"{#section-7 .unnumbered}","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#summary-1","title":"Summary","text":"<ul> <li> <p>There are different areas of improvement related to operations in     machine learning systems, such as job scheduling and metadata.</p> </li> <li> <p>Various scheduling patterns, such as fair-share scheduling, priority     scheduling, and gang scheduling, can be used to prevent resource     starvation and avoid deadlocks.</p> </li> <li> <p>We can collect metadata to gain insights from machine learning     workflows and handle failures more appropriately to reduce any     negative effects on users.</p> </li> </ul> <p>[Part 5_______________________________]{.underline}</p> <p>Distributed machine learning workflow (Overview &amp; Architecture)</p> <p>What is covered</p> <ul> <li> <p>Providing a high-level overall design of our system</p> </li> <li> <p>Optimizing the data ingestion component for multiple epochs of the     dataset</p> </li> <li> <p>Deciding which distributed model training strategy best minimizes     overhead</p> </li> <li> <p>Adding model server replicas for high performance model serving</p> </li> <li> <p>Accelerating the end-to-end workflow of our machine learning system</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#section-8-unnumbered","title":"{#section-8 .unnumbered}","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#overview","title":"Overview","text":"<ul> <li> <p>For this project, we will build an image classification system that     takes raw images downloaded from the data source, performs necessary     data cleaning steps, builds a machine learning model in a     distributed Kubernetes cluster, and then deploys the trained model     to the model serving system for users to use.</p> </li> <li> <p>We also want to establish an end-to-end workflow that is efficient     and reusable. Next, I will introduce the project background and the     overall system architecture and components.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#background","title":"Background","text":"<ul> <li> <p>We will build an end-to-end machine learning system to apply what we     learned previously.</p> </li> <li> <p>We'll build a data ingestion component that downloads the     Fashion-MNIST dataset and a model training component to train and     optimize the image classification model. Once the final model is     trained, we'll build a high-performance model serving system to     start making predictions using the trained model.</p> </li> <li> <p>As previously mentioned, we will use several frameworks and     technologies to build distributed machine learning workflow     components. For example, we'll use TensorFlow with Python to build     the classification model on the Fashion-MNIST dataset and make     predictions.</p> </li> <li> <p>We'll use Kubeflow to run distributed machine learning model     training on a Kubernetes cluster. Furthermore, we'll use Argo     Workflows to build a machine learning pipeline that consists of many     important components of a distributed machine learning system.</p> </li> <li> <p>The basics of these technologies will be introduced in the next     chapter, and you'll gain hands-on experience with them before diving     into the actual implementation of the project in chapter 9. In the     next section, we'll examine the project's system components.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#section-9-unnumbered","title":"{#section-9 .unnumbered}","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#system-components","title":"System components","text":"<ul> <li> <p>Figure 7.1 is the architecture diagram of the system we will be     building. First, we will build the data ingestion component     responsible for ingesting data and storing the dataset in the cache     using some of the patterns discussed in chapter 2.</p> </li> <li> <p>Next, we will build three different model training steps that train     different models and incorporate the collective communication     pattern addressed in chapter 3. Once we finish the model training     steps, we will build the model selection step that picks the top     model. The selected optimal model will be used for model serving in     the following two steps.</p> </li> <li> <p>At the end of the model serving steps, we aggregate the predictions     and present the result to users. Finally, we want to ensure all     these steps are part of a reproducible workflow that can be executed     at any time in any environment.</p> </li> <li> <p>We'll build the system based on the architecture diagram in Figure     7.1 and dive into the details of the individual components. We'll     also discuss the patterns we can use to address the challenges in     building those components.</p> </li> </ul> <p>Three model training</p> <p>serving steps. to present to users.</p> <p>Figure 7.1 The architecture diagram of the end-to-end machine learning system we will be building</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#data-ingestion","title":"Data ingestion","text":"<ul> <li> <p>For this project, we will use the Fashion-MNIST dataset, introduced     in section 2.2, to build the data ingestion component, as shown in     figure 7.2. This dataset consists of a training set of 60,000     examples and a test set of 10,000 examples.</p> </li> <li> <p>Each example is a 28 \u00d7 28 grayscale image that represents one     Zalando's article image associated with a label from 10 classes.     Recall that the Fashion-MNIST dataset is designed to serve as a     direct drop-in replacement for the original MNIST dataset for     benchmarking machine learning algorithms.</p> </li> <li> <p>It shares the same image size and structure of training and testing     splits.</p> </li> </ul> <p>Figure 7.2 The data ingestion component (dark box) in the end-to-end machine learning system</p> <ul> <li> <p>Figure 7.3 is a screenshot of the collection of images for all 10     classes (T-shirt/top, trouser, pullover, dress, coat, sandal, shirt,     sneaker, bag, and ankle boot) from Fashion-MNIST, where each class     takes three rows in the screenshot.</p> </li> <li> <p>Figure 7.4 is a closer look at the first few example images in the     training set together with their corresponding text labels.</p> </li> <li> <p>The downloaded Fashion-MNIST dataset should only take 30 MBs on disk     if compressed and can be fully loaded into memory.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#section-10-unnumbered","title":"{#section-10 .unnumbered}","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#the-challenge-reduce-sequential-training-inefficiency","title":"The Challenge: Reduce Sequential Training Inefficiency","text":"<p>Context:</p> <ul> <li> <p>Although the Fashion-MNIST data is not large, we may want to perform     additional computations before feeding the dataset into the model,     which is common for tasks that require additional transformations     and cleaning.</p> </li> <li> <p>We may want to resize, normalize, or convert the images to     grayscale. We also may want to perform complex mathematical     operations such as convolution operations, which can require large     additional memory space allocations. Our available computational     resources may or</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#data-ingestion-1","title":"7.2 Data ingestion","text":"<p>Figure 7.3 A screenshot of the collection of images from the Fashion-MNIST dataset for all 10 classes (T-shirt/top, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, and ankle boot)</p> <p>{width=\"5.666666666666667in\" height=\"1.3533333333333333in\"}</p> <p>Figure 7.4 A closer look at the first few example images in the training set with their corresponding labels in text</p> <p>may not be sufficient after we load the entire dataset in memory, depending on the distributed cluster size.</p> <ul> <li>In addition, the machine learning model we are training from this     dataset requires multiple epochs on the training dataset. Suppose     training one epoch on the entire training dataset takes 3 hours. If     we want to train two epochs, the time needed for model training     would double, as shown in figure 7.5.</li> </ul> <p>Figure 7.5 A diagram of model training for multiple epochs at time t0, t1, etc. where we spent 3 hours for each epoch</p> <ul> <li>In real-world machine learning systems, a larger number of epochs is     often needed, and training each epoch sequentially is inefficient.</li> </ul> <p>Question: How can we tackle that inefficiency?</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#the-solution-approach-2","title":"The Solution Approach","text":"<ul> <li> <p>The first challenge we have: the mathematical operations in the     machine learning algorithms may require a lot of additional memory     space allocations while computational resources may or may not be     sufficient.</p> </li> <li> <p>Given that we don't have too much free memory, we should not     load the entire Fashion-MNIST dataset into memory directly.</p> </li> <li> <p>Let's assume that the mathematical operations that we want to     perform on the dataset can be performed on subsets of the     entire dataset.</p> </li> <li> <p>We can use the batching pattern introduced earlier, which would     group a number of data records from the entire dataset into batches,     which will be used to train the machine learning model sequentially     on each batch.</p> </li> <li> <p>To apply the batching pattern, we first divide the dataset into     smaller subsets or mini-batches, load each individual mini-batch of     example images, perform expensive mathematical operations on each     batch, and then use only one mini-batch of images in each model     training iteration.</p> </li> <li> <p>For example, we can perform convolution or other heavy mathematical     operations on the first mini-batch, which consists of only 20     images, and then send the transformed images to the machine learning     model for model training.</p> </li> <li> <p>We then repeat the same process for the remaining mini-batches while     continuing to perform model training.</p> </li> <li> <p>Since we've divided the dataset into many small subsets     (mini-batches), we can avoid any potential problems with running out     of memory when performing various heavy mathematical operations on     the entire dataset necessary for achieving an accurate     classification model on the Fashion-MNIST dataset. We can then     handle even larger datasets using this approach by reducing the size     of the mini-batches.</p> </li> <li> <p>With the help of the batching pattern, we are no longer concerned     about potential out-of-memory problems when ingesting the dataset     for model training. We don't have to load the entire dataset into     memory at once, and instead, we are consuming the dataset batch by     batch sequentially.\\     Fig. 7.6 illustrates this process, where the original dataset gets     divided into two batches and processed sequentially. The first batch     gets consumed to train the model at time t0, and the second batch     gets consumed at time t1.</p> </li> </ul> <p>The two batches of the dataset are consumed sequentially for model training.</p> <p>Figure 7.6 The dataset is divided into two batches and processed sequentially. The first batch is consumed to train the model at time t0, and the second batch is consumed at time t1.</p> <ul> <li> <p>The second challenge is that we want to avoid wasting time if we     need to train a machine learning model that involves iterating on     multiple epochs of the original dataset.</p> </li> <li> <p>The caching pattern can solve this type of problem. With the help of     the caching pattern, we can greatly speed up the re-access to the     dataset for the model training process that involves training on     the same dataset for multiple epochs.</p> </li> <li> <p>We can't do anything special to the first epoch since it's the first     time the machine learning model has seen the entire training     dataset. We can store the cache of the training examples in memory,     making it much faster to re-access when needed for the second and     subsequent epochs.</p> </li> <li> <p>Assuming that the single laptop we use to train the model has     sufficient computational resources such as memory and disk space,     once the machine learning model consumes each training example from     the entire dataset, we can hold off recycling and instead keep the     consumed training examples in memory.</p> </li> <li> <p>In figure 7.7, after we have finished fitting the model for the     first epoch, we can store a cache for both batches used for the     first epoch of model training. Then, we can start training the model     for the second epoch by feeding the stored in-memory cache to the     model directly without repeatedly reading from the data source for     future epochs.</p> </li> </ul> <p>Figure 7.7 A diagram of model training for multiple epochs at time t0, t1, etc. using cache, making reading from the data source repeatedly unnecessary</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#quiz-5","title":"Quiz","text":"<ol> <li> <p>Where do we store the cache?</p> </li> <li> <p>Can we use the batching pattern when the Fashion-MNIST dataset gets     large?</p> </li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%202/#model-training","title":"Model training","text":"<ul> <li>Figure 7.8 is a diagram of the model training component in the     overall architecture.</li> </ul> <p>In the diagram, three different model training steps are followed by a model selection step. These model training steps can train three different models competing with each other for better statistical performance. The dedicated model selection step then picks the top model, which will be used in the subsequent components in the end-to-end machine learning workflow.</p> <p>In the next section, we will look more closely at the model training component in figure 7.8 and discuss potential problems when implementing this component.</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#section-11-unnumbered","title":"{#section-11 .unnumbered}","text":"<p>Three model training steps</p> <p>This step picks the top two train different models. models that will be used in the following two separate model serving steps.</p> <p>Figure 7.8 The model training component (dark boxes) in the end-to-end machine learning system</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#the-challenge","title":"The Challenge:","text":"<ul> <li> <p>We discussed the parameter server and the collective communication     patterns earlier.</p> </li> <li> <p>The parameter server pattern is handy when the model is too large to     fit in a single machine, such as the one for tagging entities in the     8 million YouTube videos.</p> </li> <li> <p>The collective communication pattern is useful to speed up the     training process for medium-sized models when the communication     overhead is significant.</p> </li> </ul> <p>Question: Which pattern should we select for our model training component?</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#the-solution-2","title":"The solution","text":"<ul> <li> <p>With the help of parameter servers, we can effectively resolve the     challenge of building an extremely large machine learning model that     may not fit a single machine.</p> </li> <li> <p>Even when the model is too large to fit in a single machine, we can     still successfully train the model efficiently with parameter     servers.</p> </li> <li> <p>Figure 7.9 is an architecture diagram of the parameter server     pattern using multiple parameter servers.</p> </li> <li> <p>Each worker node takes a subset of the dataset, performs     calculations required in each neural network layer, and sends the     calculated gradients to update one model partition stored in one     of the parameter servers.</p> </li> <li> <p>Because all workers perform calculations in an asynchronous fashion,     the model partitions each worker node uses to calculate the     gradients may not be up to date.</p> </li> <li> <p>For instance, two workers can block each other when sending     gradients to the same parameter server, which makes it hard to     gather the calculated gradients on time and requires a strategy to     resolve the blocking problem.</p> </li> <li> <p>Unfortunately, in real-world distributed training systems where     parameter servers are incorporated, multiple workers may send the     gradients at the same time, and thus many blocking communications     must be resolved.</p> </li> </ul> <p>Figure 7.9 A machine learning training component with multiple parameter servers</p> <ul> <li> <p>Another challenge comes when deciding the optimal ratio between the     number of workers and the number of parameter servers.</p> </li> <li> <p>For example, many workers are sending gradients to the same     parameter server at the same time; the problem gets even worse, and     eventually, the blocking communications between different workers or     parameter servers become a bottleneck.</p> </li> <li> <p>In the Fashion-MNIST classification model, the model we are building     is not as large as large recommendation system models; it can easily     fit in a single machine if we give the machine sufficient     computational resources. It's only 30 MBs in compressed form.</p> </li> <li> <p>Thus, the collective communication model is perfect for the system     we are building.</p> </li> <li> <p>Without parameter servers, each worker node stores a copy of the     entire set of model parameters (figure 7.10).</p> </li> <li> <p>Every worker consumes some portion of data and calculates the     gradients needed to update the model parameters stored locally on     this worker node.</p> </li> <li> <p>We want to aggregate all the gradients as soon as all worker nodes     have successfully completed their calculation of gradients.</p> </li> <li> <p>We also want to make sure every worker's entire set of model     parameters is updated based on the aggregated gradients i.e., each     worker should store a copy of the exact same updated model.</p> </li> <li> <p>Referring to figure 7.8, each model training step uses the     collective communication pattern, taking advantage of the underlying     network infrastructure to perform allreduce operations to     communicate gradients between multiple workers.</p> </li> <li> <p>The collective communication pattern also allows us to train     multiple medium-sized machine learning models in a distributed     setting.</p> </li> <li> <p>Once the model is trained, we can start a separate process to pick     the top model to be used for model serving. This step is pretty     intuitive,</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#model-serving","title":"7.4 Model serving","text":"<p>Each of these workers contains a copy of the entire set of model parameters and consumes partitions of data to calculate the gradients.</p> <p>Figure 7.10 Distributed model training component with only worker nodes, where every worker stores a copy of the entire set of model parameters and consumes partitions of data to calculate the gradients</p> <p>Quiz:</p> <ol> <li> <p>Why isn't the parameter server pattern a good fit for our model?</p> </li> <li> <p>Does each worker store different parts of the model when using the     collective communication pattern?</p> </li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%202/#model-serving-1","title":"Model serving","text":"<ul> <li> <p>We've talked about both the data ingestion and model training     components of the system we are building. Next, let's discuss the     model server component, which is essential to the end-user     experience. Figure 7.11 shows the serving training component in the     overall architecture.</p> </li> <li> <p>Next, let's take a look at a potential problem and its solution we     will encounter when we begin building this component.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#the-challenge-1","title":"The Challenge","text":"<ul> <li> <p>The model serving system needs to take raw images uploaded by users     and send the requests to the model server to make inferences using     the trained model. These model serving requests are being queued and     waiting to be processed by the model server.</p> </li> <li> <p>If the model serving system is a single-node server, it can only     serve a limited number of model serving requests on a first-come,     first-served basis. As the number of requests grows in the real     world, the user experience suffers when users must wait a long time     to receive the model serving result. In other words, all requests     are waiting to be processed by the model serving system, but the     computational resources are limited to this single node.</p> </li> </ul> <p>Question: How do we build a more efficient model serving system?</p> <p>The results from the two model serving steps are then aggregated via a result aggregation step to present to users.</p> <p>Figure 7.11 The model serving component (dark boxes) in the end-to-end machine learning system</p> <p>The solution</p> <ul> <li> <p>The previous section lays a perfect use case for the replicated     services pattern discussed earlier.</p> </li> <li> <p>Our model serving system takes the images uploaded by users and     sends requests to the model server.</p> </li> <li> <p>In addition, unlike the simple single-server design, the system has     multiple model server replicas to process the model serving requests     asynchronously.</p> </li> <li> <p>Each model server replica takes a single request, retrieves the     previously trained classification model from the model training     component, and classifies the images that don't exist in the     Fashion-MNIST dataset.</p> </li> <li> <p>With the help of the replicated services pattern, we can easily     scale up our model server by adding model server replicas to the     single-server model serving system. The new architecture is shown in     figure 7.12.</p> </li> <li> <p>The model server replicas can handle many requests at a time since     each replica can process individual model serving requests     independently.</p> </li> <li> <p>Multiple model serving requests from users are sent to the model     server replicas at the same time after we've introduced them. We     also need to define a clear mapping relationship between the     requests and the model server replicas, which determines which     requests are processed by which of the model server replicas.</p> </li> <li> <p>To distribute the model server requests among the replicas, we need     to add an additional load balancer layer. For example, the load     balancer takes multiple model serving requests from our users.</p> </li> <li> <p>It then distributes the requests evenly among the model server     replicas, which are responsible for processing individual requests,     including model retrieval and inference on the new data in the     request. Figure 7.13 illustrates this process.</p> </li> </ul> <p>Users upload images and then submit requests to the model serving system for classification.</p> <p>Figure 7.12 The system architecture of the replicated model serving services</p> <p>Figure 7.13 Load balancer distributes requests evenly across the model server replicas</p> <ul> <li> <p>The load balancer uses different algorithms to determine which     request goes to which particular model server replica. Example     algorithms for load balancing include round robin, least-connection     method, and hashing.</p> </li> <li> <p>Note that from our original architecture diagram in figure 7.11,     there are two individual steps for model serving, each using     different models. Each model serving step consists of a model     serving service with multiple replicas to handle model serving     traffic for different models.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#quiz-6","title":"Quiz:","text":"<p>1 What happens when we don't have a load balancer as part of the model serving system?</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#end-to-end-workflow","title":"End-to-end workflow","text":"<ul> <li> <p>Now that we've looked at the individual components, let's see how to     compose an end-to-end workflow that consists of all those components     in a scalable and efficient way.</p> </li> <li> <p>Figure 7.14 is a diagram of the end-to-end workflow for the system.</p> </li> </ul> <p>Three model training</p> <p>serving steps. to present to users.</p> <p>Figure 7.14 The architecture diagram of the end-to-end machine learning system we will build</p> <ul> <li>We now examine the entire machine learning system, which chains all     the components together in an end-to-end workflow.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#the-challenges","title":"The Challenges","text":"<ul> <li> <p>First, the Fashion-MNIST dataset is static and does not change over     time. However, to design a more realistic system, let's assume we'll     manually update the Fashion-MNIST dataset regularly.</p> </li> <li> <p>Whenever the updates happen, we may want to rerun the entire machine     learning workflow to train a fresh machine learning model that     includes the new data, i.e., we need to execute the data ingestion     step every time when changes happen.</p> </li> <li> <p>In the meantime, when the dataset is not updated, we want to     experiment with new machine learning models.</p> </li> <li> <p>Thus, we still need to execute the entire workflow, including the     data ingestion step.</p> </li> <li> <p>The data ingestion step is usually very time consuming, especially     for large datasets.</p> </li> <li> <p>Question: Is there a way to make this workflow more efficient?</p> </li> <li> <p>Second, we want to build a machine learning workflow that can train     different models and then select the top model, which will be used     in model serving to generate predictions using the knowledge from     both models.</p> </li> <li> <p>Due to the variance of completion time for each of the model     training steps in the existing machine learning workflow, the start     of each following step, such as model selection and model serving,     depends on the completion of the previous steps.</p> </li> <li> <p>However, this sequential execution of steps in the workflow is quite     time-consuming and blocks the rest of the steps. For example, say     one model training step takes much longer to complete than the rest     of the steps.</p> </li> <li> <p>The model selection step that follows can only start to execute     after this long-running model training step has completed. As a     result, the entire workflow is delayed by this particular step.</p> </li> <li> <p>Question: Is there a way to accelerate this workflow so it will     not be affected by the duration of individual steps?</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#the-solutions","title":"The solutions","text":"<ul> <li> <p>For the first problem, we can use the step memoization pattern     discussed earlier.</p> </li> <li> <p>Recall that step memoization can help the system decide whether a     step should be executed or skipped.</p> </li> <li> <p>With the help of step memoization, a workflow can identify steps     with redundant workloads that can be skipped without being     re-executed and thus greatly accelerate the execution of the     end-to-end workflow.</p> </li> <li> <p>For instance, figure 7.15 contains a simple workflow that only     executes the data ingestion step when we know the dataset has been     updated ie., we don't want to re-ingest the data that's already     collected if the new data has not been updated.</p> </li> <li> <p>Many strategies can be used to determine whether the dataset has     been updated. With a predefined strategy, we can conditionally     reconstruct the machine learning workflow and control whether we     would like to include a data ingestion step to be re-executed, as     shown in figure 7.16.</p> </li> <li> <p>Cache is one way to identify whether a dataset has been updated.     Since we suppose our Fashion-MNIST dataset is being updated     regularly on a fixed schedule (e.g., once a month), we can create a     time-based cache that stores the location of the ingested and     cleaned dataset (assuming the dataset is located in a remote     database) and the timestamp of its last updated time.</p> </li> </ul> <p>The dataset has not been updated yet.</p> <p>Figure 7.15 A diagram of skipping the data ingestion step when the dataset has not been updated</p> <ul> <li> <p>As in figure 7.16, the data ingestion step in the workflow will then     be constructed and executed dynamically based on whether the last     updated timestamp is within a particular window. For example, if the     time window is set to two weeks, we consider the ingested data as     fresh if it has been updated within the past two weeks.</p> </li> <li> <p>The data ingestion step will be skipped, and the following model     training steps will use the already ingested dataset from the     location in the cache.</p> </li> <li> <p>The time window can be used to control how old a cache can be before     we consider the dataset fresh enough to be used directly for model     training instead of re-ingesting the data from scratch.</p> </li> </ul> <p>Figure 7.16 The workflow has been triggered. We check whether the data has been updated within the last two weeks by accessing the cache. If the data is fresh, we can skip the unnecessary data ingestion step and execute the model training step directly.</p> <ul> <li> <p>For the second problem: sequential execution of the steps blocks the     subsequent steps in the workflow and is inefficient.</p> </li> <li> <p>The synchronous and asynchronous patterns introduced earlier can     help.</p> </li> <li> <p>When a short-running model training step finishes---for example,     model training step 2 in figure 7.17---we successfully obtain a     trained machine learning model.</p> </li> <li> <p>In fact, we can use this already-trained model directly in our model     serving system without waiting for the rest of the model training     steps to complete.</p> </li> <li> <p>As a result, users will be able to see the results of image     classification from their model serving requests that contain videos     as soon as we have trained one model from one of the steps in the     workflow.</p> </li> <li> <p>After a second model training step (figure 7.17, model training     step 3) finishes, the two trained models are sent to model serving.     Now, users benefit from the aggregated results obtained from both     models.</p> </li> </ul> <p>After a second model training step finishes, we can pass the two trained models directly to be used for model serving, and the aggregated inference results will be presented to users instead of the results from only the one model that we obtained initially.</p> <p>Figure 7.17 After a second model training step finishes, we can pass the two trained models directly to model serving. The aggregated inference results will be presented to users instead of only the results from the first model.</p> <ul> <li> <p>As a result, we can continue to use the trained models for model     selection and model serving; in the meantime, the long-running model     training steps are still running i.e., they execute asynchronously     without depending on each other's completion.</p> </li> <li> <p>The workflow can proceed and execute the next step before the     previous one finishes. The long-running model training step will no     longer block the entire workflow. Instead, it can continue to use     the already-trained models from the short-running model training     steps in the model serving system. Thus, it can start handling     users' model serving requests.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#quiz-7","title":"Quiz:","text":"<ol> <li> <p>Which component can benefit the most from step memoization?</p> </li> <li> <p>How do we tell whether a step's execution can be skipped if its     workflow has been triggered to run again?</p> </li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%202/#summary-2","title":"Summary","text":"<ul> <li> <p>The data ingestion component uses the caching pattern to speed up     the processing of multiple epochs of the dataset.</p> </li> <li> <p>The model training component uses the collective communication     pattern to avoid the potential communication overhead between     workers and parameter servers.</p> </li> <li> <p>We can use model server replicas, which are capable of handling many     requests at one time since each replica processes individual model     serving requests independently.</p> </li> <li> <p>We can chain all our components into a workflow and use caching to     effectively skip time-consuming components such as data ingestion.</p> </li> </ul> <p>[Part 6_______________________________]{.underline}</p> <p>This part covers</p> <ul> <li> <p>Getting familiar with model building using</p> </li> <li> <p>TensorFlow</p> </li> <li> <p>Understanding key terminologies on Kubernetes</p> </li> <li> <p>Running distributed machine learning workloads with Kubeflow</p> </li> <li> <p>Deploying container-native workflows using Argo</p> </li> <li> <p>Workflow</p> </li> <li> <p>You will learn the basic concepts of the four technologies     (TensorFlow, Kubernetes, Kubeflow, and Argo Workflows) and gain     hands-on experience.</p> </li> <li> <p>Each of these four technologies has a different purpose, but all     will be used to implement the final project.</p> </li> <li> <p>TensorFlow will be used for data processing, model building, and     evaluation.</p> </li> <li> <p>Kubernetes is our core distributed infrastructure.</p> </li> <li> <p>On top of that, Kubeflow will be used for submitting distributed     model training jobs to the Kubernetes cluster</p> </li> <li> <p>Argo Workflows will be used to construct and submit the end-to-end     machine learning workflows.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#section-12-unnumbered","title":"{#section-12 .unnumbered}","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#tensorflow-the-machine-learning-framework","title":"TensorFlow: The machine learning framework","text":"<ul> <li> <p>TensorFlow is an end-to-end machine learning platform. It has been     widely adopted in academia and industries for different applications     and uses cases, such as image classification, recommendation     systems, natural language processing, etc. TensorFlow is highly     portable, deployable on different hardware, and has multi-language     support.</p> </li> <li> <p>TensorFlow has a large ecosystem. The following are some highlighted     projects in this ecosystem:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   TensorFlow.js is a library for machine learning in JavaScript. Users     can use machine learning directly in the browser or in Node.js.</p> <ul> <li> <p>TensorFlow Lite is a mobile library for deploying models on mobile,     microcontrollers, and other edge devices.</p> </li> <li> <p>TFX is an end-to-end platform for deploying production machine     learning pipelines.</p> </li> <li> <p>TensorFlow Serving is a flexible, high-performance serving system     for machine learning models designed for production environments.</p> </li> <li> <p>TensorFlow Hub is a repository of trained machine learning models     ready for fine-tuning and deployable anywhere. Reuse trained models     like BERT and Faster R-CNN with just a few lines of code.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   More can be found in the TensorFlow GitHub organization     (https://github.com/ tensorflow).</p> <ul> <li>We will use TensorFlow Serving in our model serving component. In     the next section, we'll walk through some basic examples in     TensorFlow to train a machine learning model locally using the MNIST     dataset.</li> </ul> <p>The Basics</p> <ul> <li> <p>Let's first install Anaconda for Python 3 for the basic examples we     will use. Anaconda     (https://www.anaconda.com) is a     distribution of the Python and R programming languages for     scientific computing that aims to simplify package management and     deployment. The distribution includes data-science packages suitable     for Windows, Linux, and macOS.</p> </li> <li> <p>Once Anaconda is installed, use the following command in your     console to install a Conda environment with Python 3.9.</p> </li> </ul> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | Creating a Conda environment                             | | &gt; 8.1     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>&gt; conda create --name dist-ml python=3.9 -y</p> <p>Next, we can activate this environment with the following code.</p> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | Activating a Conda environment                           | | &gt; 8.2     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>&gt; conda activate dist-ml</p> <p>Then, we can install TensorFlow in this Python environment.</p> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | Installing TensorFlow                                    | | &gt; 8.3     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>&gt; pip install --upgrade pip</p> <p>&gt; pip install tensorflow==2.10.0</p> <p>If you encounter any problems, please refer to the installation guide (https://www .tensorflow.org/install).</p> <p>In some cases, you may need to uninstall your existing NumPy and reinstall it.</p> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | Installing NumPy                                         | | &gt; 8.4     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>&gt; pip install numpy --ignore-installed</p> <ul> <li> <p>If you are on Mac, check out the Metal plugin for acceleration     (https://developer     .apple.com/metal/tensorflow-plugin/).</p> </li> <li> <p>Once we've successfully installed TensorFlow, we can start with a     basic image classification example! Let's first load and preprocess     our simple MNIST dataset. Recall that the MNIST dataset contains     images for handwritten digits from 0 to 9. Each row represents     images for a particular handwritten digit, as shown in figure 8.1.</p> </li> </ul> <p>Figure 8.1 Some example images for handwritten digits from 0 to 9 where each row represents images for a particular handwritten digit</p> <ul> <li>Keras API (tf.keras) is a high-level API for model training in     TensorFlow, and we will use it for both loading the built-in     datasets and model training and evaluation.</li> </ul> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | Loading the MNIST dataset                                | | &gt; 8.5     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>&gt; import tensorflow as tf</p> <p>&gt; (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()</p> <ul> <li> <p>The function load_data()uses a default path to save the MNIST     dataset if we don't specify a path. This function will return NumPy     arrays for training and testing images and labels. We split the     dataset into training and testing so we can run both model training     and evaluation in our example.</p> </li> <li> <p>A NumPy array is a common data type in Python's scientific computing     ecosystem. It describes multidimensional arrays and has three     properties: data, shape, and dtype. Let's use our training images as     an example.</p> </li> </ul> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | Inspecting the dataset                                   | | &gt; 8.6     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>&gt; x_train.data</p> <p>\\&lt;memory at 0x16a392310&gt;</p> <p>&gt; x_train.shape</p> <p>(60000, 28, 28) &gt; x_train.dtype dtype(\\'uint8\\')</p> <p>&gt; x_train.min()</p> <p>0</p> <p>&gt; x_train.max()</p> <p>255</p> <p>x_train is a 60,000 \u00d7 28 \u00d7 28 three-dimensional array. The data type is uint8 from 0 to 255. In other words, this object contains 60,000 grayscale images with a resolution of 28 \u00d7 28.</p> <ul> <li>Next, we can perform some feature preprocessing on our raw images.     Since many algorithms and models are sensitive to the scale of the     features, we often center and scale features into a range such as     [0, 1] or [-1, 1]. In our case, we can do this easily by     dividing the images by 255.</li> </ul> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | The preprocessing function                               | | &gt; 8.7     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>def preprocess(ds): return ds / 255.0</p> <p>x_train = preprocess(x_train) x_test = preprocess(x_test)</p> <p>&gt; x_train.dtype dtype(\\'float64\\')</p> <p>&gt; x_train.min() 0.0</p> <p>&gt; x_train.max()</p> <p>1.0</p> <ul> <li> <p>After preprocessing the images in the training and testing set, we     can instantiate a simple multilayer neural network model.</p> </li> <li> <p>We use tf.keras to define the model architecture. First, we use     Flatten to expand the two-dimensional images into a one dimensional     array by specifying the input shape as 28 \u00d7 28. The second layer is     densely connected and uses the \\'relu\\' activation function to     introduce some nonlinearity.</p> </li> <li> <p>The third layer is a dropout layer to reduce overfitting and make     the model more generalizable. Since the handwritten digits consist     of 10 different digits from 0 to 9, our last layer is densely     connected for 10-class classification with softmax activation.</p> </li> </ul> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | The sequential model definition                          | | &gt; 8.8     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation=\\'relu\\'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10, activation=\\'softmax\\') ])</p> <p>After we've defined the model architecture, we need to specify three different components: the evaluation metric, loss function, and optimizer.</p> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | Model compilation with optimizer, loss function, and     | | &gt; 8.9     | optimizer                                                | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>model.compile(optimizer=\\'adam\\', loss=\\'sparse_categorical_crossentropy\\', metrics=[\\'accuracy\\'])</p> <ul> <li>We can then start our model training with five epochs as well as     evaluation via the following.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Model training using the training data                  | | &gt; 8.10     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>model.fit(x_train, y_train, epochs=5) model.evaluate(x_test, y_test)</p> <p>We should see training progress in the log:</p> <p>Epoch 1/5</p> <p>1875/1875 [======] - 11s 4ms/step - loss: 0.2949 - accuracy: 0.9150</p> <p>Epoch 2/5</p> <p>1875/1875 [======] - 9s 5ms/step - loss: 0.1389 - accuracy: 0.9581 Epoch 3/5</p> <p>1875/1875 [======] - 9s 5ms/step - loss: 0.1038 - accuracy: 0.9682</p> <p>Epoch 4/5</p> <p>1875/1875 [======] - 8s 4ms/step - loss: 0.0841 - accuracy: 0.9740 Epoch 5/5</p> <p>1875/1875 [======] - 8s 4ms/step - loss: 0.0707 - accuracy: 0.9779</p> <p>10000/10000 [======] - 0s - loss: 0.0726 - accuracy: 0.9788</p> <p>And the log from the model evaluation should look like the following:</p> <p>313/313 [======] - 1s 4ms/step - loss: 0.0789 - accuracy: 0.9763</p> \\[0.07886667549610138, 0.976300060749054\\] <ul> <li> <p>We should observe that as the loss decreases during training, the     accuracy increases to 97.8% on training data. The final trained     model has an accuracy of 97.6% on the testing data. Your result     might be slightly different due to the randomness in the modeling     process.</p> </li> <li> <p>After we've trained the model and are happy with its performance, we     can save it using the following code so that we don't have to     retrain it from scratch next time.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Saving the trained model                                | | &gt; 8.11     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>model.save(\\'my_model.h5\\')</p> <ul> <li>This code saves the model as file my_model.h5 in the current working     directory. When we start a new Python session, we can import     TensorFlow and load the model object from the my_model.h5 file.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Loading the saved model                                 | | &gt; 8.12     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>import tensorflow as tf</p> <p>model = tf.keras.models.load_model(\\'my_model.h5\\')</p> <ul> <li> <p>We've learned how to train a model using TensorFlow's Keras API for     a single set of hyperparameters. These hyperparameters remain     constant over the training process and directly affect the     performance of your machine learning program. Let's learn how to     tune hyperparameters for your TensorFlow program with Keras Tuner     (https://keras.io/keras_tuner/).</p> </li> <li> <p>First, install the Keras Tuner library.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Installing the Keras Tuner package                      | | &gt; 8.13     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>pip install -q -U keras-tuner</p> <p>Once it's installed, you should be able to import all the required libraries.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Importing necessary packages                            | | &gt; 8.14     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>import tensorflow as tf from tensorflow import keras import keras_tuner as kt</p> <ul> <li>We will use the same MNIST dataset and the preprocessing functions     for our hyperparameter tuning example. We then wrap our model     definition into a Python function.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | The model building function using TensorFlow and Keras  | | &gt; 8.15     | Tuner                                                   | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>def model_builder(hp):</p> <p>model = keras.Sequential() model.add(keras.layers.Flatten(input_shape=(28, 28))) hp_units = hp.Int(\\'units\\', min_value=32, max_value=512, step=32) model.add(keras.layers.Dense(units=hp_units, activation=\\'relu\\')) model.add(keras.layers.Dense(10)) hp_learning_rate = hp.Choice(\\'learning_rate\\', values=[1e-2, 1e-3, 1e-4]) model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\\'accuracy\\']) return model</p> <ul> <li> <p>This code is essentially the same as what we used previously for     training a model with a single set of hyperparameters, except that     we also defined hp_units and hp_learning_rate objects that are used     in our dense layer and optimizer.</p> </li> <li> <p>The hp_units object instantiates an integer that will be tuned     between 32 and 512 and used as the number of units in the first     densely connected layer. The hp_learning_rate object will tune the     learning rate for the adam optimizer that will be chosen from among     these values: 0.01, 0.001, or 0.0001.</p> </li> <li> <p>Once the model builder is defined, we can then instantiate our     tuner. There are several tuning algorithms we can use (e.g., random     search, Bayesian optimization, Hyperband). Here we use the hyperband     tuning algorithm. It uses adaptive resource allocation and early     stopping to converge faster on a high-performing model.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | The Hyperband model tuner                               | | &gt; 8.16     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>tuner = kt.Hyperband(model_builder, objective=\\'val_accuracy\\', max_epochs=10, factor=3, directory=\\'my_dir\\', project_name=\\'intro_to_kt\\')</p> <ul> <li> <p>We use the validation accuracy as the objective, and the maximum     number of epochs is 10 during model tuning.</p> </li> <li> <p>To reduce overfitting, we can create an EarlyStopping callback to     stop training as soon as the model reaches a threshold for the     validation loss. Make sure to reload the dataset into memory if     you've started a new Python session.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | The EarlyStopping callback                          | | &gt; 8.17     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>early_stop = tf.keras.callbacks.EarlyStopping( monitor=\\'val_loss\\', patience=4)</p> <ul> <li>Now we can start our hyperparameter search via tuner.search().</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | The Hyperparameter search with early-stopping           | | &gt; 8.18     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>tuner.search(x_train, y_train, epochs=30, validation_split=0.2, callbacks=[early_stop])</p> <ul> <li>Once the search is complete, we can identify the optimal     hyperparameters and train the model on the data for 30 epochs.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Obtaining the best hyperparameters and training the     | | &gt; 8.19     | model                                                   | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>best_hps = tuner.get_best_hyperparameters(num_trials=1)[0] model = tuner.hypermodel.build(best_hps)</p> <p>model.fit(x_train, y_train, epochs=50, validation_split=0.2)</p> <ul> <li>When we evaluate the model on our test data, we should see it's more     performant than our baseline model without hyperparameter tuning.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Model evaluation on the test data                       | | &gt; 8.20     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>model.evaluate(x_test, y_test)</p> <ul> <li>You've learned how to run TensorFlow locally on a single machine. To     take the most advantage of TensorFlow, the model training process     should be run in a distributed cluster, which is where Kubernetes     comes into play.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#quiz-8","title":"Quiz","text":"<ol> <li> <p>Can you use the previously saved model directly for model     evaluation?</p> </li> <li> <p>Instead of using the Hyperband tuning algorithm, could you try the     random search algorithm?</p> </li> </ol> <p>Kubernetes: The distributed container orchestration system</p> <ul> <li> <p>Kubernetes (also known as K8s) is an open source system for     automating the deployment, scaling, and management of containerized     applications.</p> </li> <li> <p>It abstracts away complex container management and provides     declarative configurations to orchestrate containers in different     computing environments.</p> </li> <li> <p>Containers are grouped into logical units for a particular     application for easy management and discovery.</p> </li> <li> <p>Kubernetes builds upon more than 16 years of experience running     production workloads at Google, combined with best-in-class ideas     and practices from the community.</p> </li> <li> <p>Its main design goal is to make it easy to deploy and manage complex     distributed systems, while still benefiting from the improved     utilization that containers enable. It's open source, which gives     the community the freedom to take advantage of on-premises, hybrid,     or public cloud infrastructure and lets you effortlessly move     workloads to where it matters.</p> </li> <li> <p>Kubernetes is designed to scale without increasing your operations     team. Figure 8.2 is an architecture diagram of Kubernetes and its     components. However, we won't be discussing those components because     they are not the focus of this course.</p> </li> <li> <p>We will, however, use kubectl (on the left-hand side of the     diagram), a command-line interface of Kubernetes, to interact with     the Kubernetes cluster and obtain information that we are interested     in.</p> </li> </ul> <p>Figure 8.2 An architecture diagram of Kubernetes</p> <ul> <li>We will go through some basic concepts and examples to build our     knowledge and prepare the following sections on Kubeflow and Argo     Workflows.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#basics","title":"Basics","text":"<ul> <li>First, let's set up a local Kubernetes cluster. We'll use k3d     (https://k3d.io) to bootstrap the local cluster.     k3d is a lightweight wrapper to run k3s (a minimal Kubernetes     distribution provided by Rancher Lab) in Docker. k3d makes it very     easy to create either single-node or multi-node k3s clusters in     Docker for local development that requires a Kubernetes cluster.     Let's create a Kubernetes cluster called distml via k3s.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Creating a local Kubernetes cluster                     | | &gt; 8.21     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; k3d cluster create distml --image rancher/k3s:v1.25.3-rc3-k3s1</p> <p>We can get the list of nodes for the cluster we created via the following listing.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Obtaining the list of nodes in the cluster              | | &gt; 8.22     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl get nodes</p> <p>NAME STATUS ROLES AGE VERSION</p> <p>K3d-distml-server-0 Ready control-plane,master 1m v1.25.3+k3s1</p> <ul> <li> <p>In this case, the node was created 1 minute ago, and we are running     the v1.25.3+k3s1 version of the k3s distribution. The status is     ready so that we can proceed to the next steps.</p> </li> <li> <p>We can also look at the node's details via kubectl describe node     k3d-distmlserver-0. For example, the labels and system info contain     information on the operating system and its architecture, whether     this node is a master node, etc.:</p> </li> </ul> <p>Labels: beta.kubernetes.io/arch=arm64 beta.kubernetes.io/instance-type=k3s beta.kubernetes.io/os=linux kubernetes.io/arch=arm64 kubernetes.io/hostname=k3d-distml-server-0 kubernetes.io/os=linux node-role.kubernetes.io/control-plane=true node-role.kubernetes.io/master=true node.kubernetes.io/instance-type=k3s</p> <p>System Info:</p> <p>Machine ID:</p> <p>System UUID:</p> <p>Boot ID: 73db7620-c61d-432c-a1ab-343b28ab8563</p> <p>Kernel Version: 5.10.104-linuxkit</p> <p>OS Image: K3s dev</p> <p>Operating System: linux</p> <p>Architecture: arm64</p> <p>Container Runtime Version: containerd://1.5.9-k3s1</p> <p>Kubelet Version: v1.22.7+k3s1</p> <p>Kube-Proxy Version: v1.22.7+k3s1 The node's addresses are shown as part of it:</p> <p>Addresses:</p> <p>InternalIP: 172.18.0.3</p> <p>Hostname: k3d-distml-server-0</p> <p>The capacity of the node is also available, indicating how much computational resources are there:</p> <p>Capacity:</p> <p>cpu: 4 ephemeral-storage: 61255492Ki hugepages-1Gi: 0 hugepages-2Mi: 0 hugepages-32Mi: 0 hugepages-64Ki: 0 memory: 8142116Ki pods: 110</p> <ul> <li>Then we'll create a namespace called basics in this cluster for     our project. Namespaces in Kubernetes provide a mechanism for     isolating groups of resources within a single cluster (see     http://mng.bz/BmN1).     Names of resources need to be unique within a namespace but not     across namespaces. The following examples will be in this single</li> </ul> <p>namespace.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Creating a new namespace                                | | &gt; 8.23     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl create ns basics</p> <ul> <li>Once the cluster and namespace are set up, we'll use a convenient     tool called kubectx to help us inspect and navigate between     namespaces and clusters (https://github     .com/ahmetb/kubectx). Note that     this tool is not required for day-to-day work with Kubernetes, but     it should make Kubernetes much easier to work with for developers.     For example, we can obtain a list of clusters and namespaces that we     can connect to via the following listing.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Switching contexts and namespaces                       | | &gt; 8.24     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectx d3d-k3s-default k3d-distml</p> <p>&gt; kubens default kube-system kube-public kube-node-lease basics</p> <p>For example, we can switch to the distml cluster via the k3d-distml context and the basics namespace that we just created using the following listing.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Activate context                                        | | &gt; 8.25     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectx k3d-distml</p> <p>Switched to context \\\"k3d-distml\\\".</p> <p>&gt; kubens basics</p> <p>Active namespace is \\\"basics\\\".</p> <ul> <li> <p>Switching contexts and namespaces is often needed when working with     multiple clusters and namespaces. We are using the basics namespace     to run the examples in this chapter, but we will switch to another     namespace dedicated to our project in the next chapter.</p> </li> <li> <p>Next, we will create a Kubernetes Pod. Pods are the smallest     deployable units of computing that you can create and manage in     Kubernetes. A Pod may consist of one or more containers with shared     storage and network resources and a specification for how to run the     containers. A Pod's contents are always co-located and co-scheduled     and run in a shared context. The concept of the Pod models an     application-specific \"logical host,\" meaning that it contains one or     more application containers that are relatively tightly coupled. In     noncloud contexts, applications executed on the same physical or     virtual machine are analogous to cloud applications executed on the     same logical host. In other words, a Pod is similar to a set of     containers with shared namespaces and shared file system volumes.</p> </li> <li> <p>The following listing provides an example of a Pod that consists of     a container running the image whalesay to print out a \"hello world\"     message. We save the following Pod spec in a file named     hello-world.yaml.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | An example Pod                                          | | &gt; 8.26     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>apiVersion: v1 kind: Pod metadata:</p> <p>name: whalesay spec: containers: - name: whalesay image: docker/whalesay:latest command: [cowsay] args: [\\\"hello world\\\"]</p> <p>To create the Pod, run the following command.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Creating the example Pod in the cluster                 | | &gt; 8.27     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl create -f basics/hello-world.yaml</p> <p>pod/whalesay created</p> <ul> <li>We can then check whether the Pod has been created by retrieving the     list of Pods. Note that pods is plural so we can get the full list     of created Pods. We will use the singular form to get the details of     this particular Pod later.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Getting the list of Pods in the cluster                 | | &gt; 8.28     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl get pods</p> <p>NAME READY STATUS RESTARTS AGE whalesay 0/1 Completed 2 (20s ago) 37s</p> <ul> <li>The Pod status is Completed so we can look at what's being printed     out in the whalesay container like in the following listing.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Checking the Pod logs                                   | | &gt; 8.29     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl logs whalesay</p> <p>_____________</p> <p>\\&lt; hello world &gt;</p> <p>-------------</p> <p>\\</p> <p>\\</p> <p>\\</p> <p>## .</p> <p>## ## ## ==</p> <p>## ## ## ## ===</p> <p>/\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"___/ ===</p> <p>\\~\\~\\~ {\\~\\~ \\~\\~\\~\\~ \\~\\~\\~ \\~\\~\\~\\~ \\~\\~ \\~ / ===- \\~\\~\\~</p> <p>\\______ o __/</p> <p>\\ \\ __/</p> <p>\\____\\______/</p> <ul> <li>We can also retrieve the raw YAML of the Pod via kubectl. Note that     we use -o yaml here to get the plain YAML, but other formats, such     as JSON, are also supported. We use the singular pod to get the     details of this particular Pod instead of the full list of existing     Pods, as mentioned earlier.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Getting the raw Pod YAML                                | | &gt; 8.30     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl get pod whalesay -o yaml</p> <p>apiVersion: v1 kind: Pod metadata:</p> <p>creationTimestamp: \\\"2022-10-22T14:30:19Z\\\" name: whalesay namespace: basics resourceVersion: \\\"830\\\" uid: 8e5e13f9-cd58-45e8-8070-c6bbb2dddb6e spec:</p> <p>containers:</p> <ul> <li> <p>args: - hello world command: - cowsay image: docker/whalesay:latest     imagePullPolicy: Always name: whalesay resources: {}     terminationMessagePath: /dev/termination-log     terminationMessagePolicy: File volumeMounts:</p> </li> <li> <p>mountPath: /var/run/secrets/kubernetes.io/serviceaccount name:     kube-api-access-x826t readOnly: true</p> </li> </ul> <p>dnsPolicy: ClusterFirst enableServiceLinks: true nodeName: k3d-distml-server-</p> <p>\\&lt;...truncated...&gt;</p> <p>volumes:</p> <ul> <li>name: kube-api-access-x826t projected:</li> </ul> <p>defaultMode: 420 sources:</p> <ul> <li>serviceAccountToken:</li> </ul> <p>expirationSeconds: 3607 path: token - configMap: items: - key: ca.crt path: ca.crt name: kube-root-ca.crt - downwardAPI: items: - fieldRef: apiVersion: v1 fieldPath: metadata.namespace path: namespace status: conditions:</p> <ul> <li>lastProbeTime: null lastTransitionTime: \\\"2022-10-22T14:30:19Z\\\"     status: \\\"True\\\" type: Initialized - lastProbeTime: null     lastTransitionTime: \\\"2022-10-22T14:30:19Z\\\" message: \\'containers     with unready status: [whalesay]\\' reason: ContainersNotReady     status: \\\"False\\\" type: Ready</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   You may be surprised how much additional content, such as status and     conditions, has been added to the original YAML we used to create     the Pod. The additional information is appended and updated via the     Kubernetes server so that client-side applications know the current     status of the Pod. Even though we didn't specify the namespace     explicitly, the Pod was created in the basics namespace since we     have used the kubens command to set the current namespace.</p> <ul> <li>That's it for the basics of Kubernetes! In the next section, we will     study how to use Kubeflow to run distributed model training jobs in     the local Kubernetes cluster we just set up.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#exercises","title":"8.2.2 Exercises","text":"<ol> <li> <p>How do you get the Pod information in JSON format?</p> </li> <li> <p>Can a Pod contain multiplier containers?</p> </li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%202/#kubeflow-machine-learning-workloads-on-kubernetes","title":"Kubeflow: Machine learning workloads on Kubernetes","text":"<ul> <li> <p>The Kubeflow project is dedicated to making deployments of machine     learning workflows on Kubernetes simple, portable, and scalable. The     goal of Kubeflow is not to re-create other services but to provide a     straightforward way to deploy best-in-class open source systems for     machine learning to diverse infrastructures. Anywhere you run     Kubernetes, you should be able to run Kubeflow. We will use Kubeflow     to submit distributed machine learning model training jobs to a     Kubernetes cluster.</p> </li> <li> <p>Let's first take a look at what components Kubeflow provides. Figure     8.3 is a diagram that consists of the main components.</p> </li> </ul> <p>{width=\"5.0in\" height=\"3.1533333333333333in\"}</p> <p>Figure 8.3 Main components of Kubeflow</p> <ul> <li> <p>Kubeflow Pipelines (KFP; https://github.com/kubeflow/pipelines)     provides Python SDK to make machine learning pipelines easier. It is     a platform for building and deploying portable and scalable machine     learning workflows using Docker containers. The primary objectives     of KFP are to enable the following:</p> <ul> <li> <p>End-to-end orchestration of ML workflows</p> </li> <li> <p>Pipeline composability through reusable components and pipelines</p> </li> <li> <p>Easy management, tracking, and visualization of pipeline     definitions, runs, experiments, and machine learning artifacts</p> </li> <li> <p>Efficient use of computing resources by eliminating redundant     executions through caching</p> </li> <li> <p>Cross-platform pipeline portability through a platform-neutral     IR YAML pipeline definition</p> </li> <li> <p>KFP uses Argo Workflows as the backend workflow engine, which I     will introduce in the next section, and we'll use Argo Workflows     directly instead of using a higherlevel wrapper like KFP. The ML     metadata project has been merged into KFP and serves as the     backend for logging metadata produced in machine learning     workflows written in KFP.</p> </li> <li> <p>Next is Katib (https://github.com/kubeflow/katib). Katib is a     Kubernetes-native project for automated machine learning. Katib     supports hyperparameter tuning, early stopping, and neural     architecture search.</p> </li> <li> <p>Katib is agnostic to machine learning frameworks. It can tune     hyperparameters of applications written in any language of the     users' choice and natively supports many machine learning     frameworks, such as TensorFlow, Apache MXNet, PyTorch, XGBoost,     and others. Katib can perform training jobs using any Kubernetes     custom resource with out-of-the-box support for Kubeflow     Training Operator, Argo Workflows, Tekton Pipelines, and many     more.</p> </li> <li> <p>Figure 8.4 is a screenshot of the Katib UI that performs     experiment tracking.</p> </li> </ul> </li> </ul> <p>Figure 8.4 A screenshot of the Katib UI that performs experiment tracking</p> <ul> <li> <p>KServe     (https://github.com/kserve/kserve)     was born as part of the Kubeflow project and was previously known as     KFServing. KServe provides a Kubernetes custom resource definition     (CRD) for serving machine learning models on arbitrary frameworks.</p> </li> <li> <p>It aims to solve production model serving use cases by providing     performant, high abstraction interfaces for common ML frameworks.</p> </li> <li> <p>It encapsulates the complexity of autoscaling, networking, health     checking, and server configuration to bring cutting edge serving     features like GPU autoscaling, scale to zero, and canary rollouts to     your machine learning deployments.</p> </li> <li> <p>Figure 8.5 is a diagram that illustrates the position of KServe in     the ecosystem.</p> </li> </ul> <p>{width=\"5.166666666666667in\" height=\"2.61in\"}</p> <p>Figure 8.5 KServe positioning in the ecosystem</p> <ul> <li> <p>Kubeflow provides web UI. Figure 8.6 provides a screenshot of the     UI. Users can access the models, pipelines, experiments, artifacts,     etc. to facilitate the iterative process of the end-to-end model     machine life cycle in each tab on the left side.</p> </li> <li> <p>The web UI is integrated with Jupyter Notebooks to be easily     accessible. There are also SDKs in different languages to help users     integrate with any internal systems.</p> </li> <li> <p>In addition, users can interact with all the Kubeflow components via     kubectl since they are all native Kubernetes custom resources and     controllers. The training operator     (https://github.com/kubeflow/training-operator)     provides Kubernetes custom resources that make it easy to run     distributed or non distributed TensorFlow, PyTorch, Apache MXNet,     XGBoost, or MPI jobs on Kubernetes.</p> </li> <li> <p>The Kubeflow project has accumulated more than 500 contributors and     20,000 GitHub star</p> </li> <li> <p>It's heavily adopted in various companies and has more than 10     vendors, including Amazon AWS, Azure, Google Cloud, IBM, etc. Seven     working groups maintain different subprojects independently.</p> </li> <li> <p>We will use the training operator to submit distributed model     training jobs and KServe to build our model serving component.</p> </li> </ul> <p>the iterative process of the end-to-end model machine life cycle.</p> <p>Figure 8.6 A screenshot of the Kubeflow UI</p> <ul> <li>Once you complete the next chapter, I recommend trying out the other     subprojects in the Kubeflow ecosystem on your own when needed. For     example, if you'd like to tune the performance of the model, you can     use Katib's automated machine learning and hyperparameter tuning     functionalities.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#the-basics","title":"The basics","text":"<ul> <li> <p>Next, we'll take a closer look at the distributed training operator     of Kubeflow and submit a distributed model training job that runs     locally in the Kubernetes local cluster we created in the previous     section.</p> </li> <li> <p>Let's first create and activate a dedicated kubeflow namespace for     our examples and reuse the existing cluster we created earlier.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Creating and switching to a new namespace               | | &gt; 8.31     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl create ns kubeflow</p> <p>&gt; kns kubeflow</p> <p>Then, we must go back to our project folder and apply all the manifests to install all the tools we need.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Applying all manifests and installing all the tools     | | &gt; 8.32     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; cd code/project</p> <p>&gt; kubectl kustomize manifests | k apply -f -</p> <p>Note that we've bundled all the necessary tools in this manifests folder:</p> <ul> <li> <p>Kubeflow Training Operator, which we will use in this chapter for     distributed model training.</p> </li> <li> <p>Argo Workflows (https://github.com/argoproj/argo-workflows), which     we address in chapter 9 when we discuss workflow orchestration and     chain all the components together in a machine learning pipeline. We     can ignore Argo Workflows for now.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   As introduced earlier, the Kubeflow Training Operator provides     Kubernetes custom resources that make it easy to run distributed or     undistributed jobs on Kubernetes, including TensorFlow, PyTorch,     Apache MXNet, XGBoost, MPI jobs, etc.</p> <ul> <li> <p>Before we dive into Kubeflow, we need to understand what custom     resources are. A custom resource is an extension of the Kubernetes     API not necessarily available in a default Kubernetes installation.</p> </li> <li> <p>It is a customization of a particular Kubernetes installation.     However, many core Kubernetes functions are now built using custom     resources, making Kubernetes more modular (http://mng.bz/lWw2).</p> </li> <li> <p>Custom resources can appear and disappear in a running cluster     through dynamic registration, and cluster admins can update custom     resources independently of the cluster. Once a custom resource is     installed, users can create and access its objects using kubectl,     just as they do for built-in resources like Pods.</p> </li> <li> <p>For example, the following listing defines the TFJob custom resource     that allows us to instantiate and submit a distributed TensorFlow     training job to the Kubernetes cluster.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | TFJob CRD                                           | | &gt; 8.33     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata:</p> <p>annotations:</p> <p>controller-gen.kubebuilder.io/version: v0.4.1 name: tfjobs.kubeflow.org spec:</p> <p>group: kubeflow.org names:</p> <p>kind: TFJob listKind: TFJobList plural: tfjobs singular: tfjob</p> <ul> <li>All instantiated TFJob custom resource objects (tfjobs) will be     handled by the training operator. The following listing provides the     definition of the deployment of the training operator that runs a     stateful controller to continuously monitor and process any     submitted tfjobs.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Training operator deployment                            | | &gt; 8.34     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>apiVersion: apps/v1 kind: Deployment metadata:</p> <p>name: training-operator labels:</p> <p>control-plane: kubeflow-training-operator spec: selector: matchLabels: control-plane: kubeflow-training-operator replicas: 1 template: metadata: labels:</p> <p>control-plane: kubeflow-training-operator annotations:</p> <p>sidecar.istio.io/inject: \\\"false\\\" spec: containers: - command: - /manager image: kubeflow/training-operator name: training-operator env:</p> <p>- name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_NAME valueFrom: fieldRef:</p> <p>fieldPath: metadata.name securityContext:</p> <p>allowPrivilegeEscalation: false livenessProbe: httpGet: path: /healthz port: 8081 initialDelaySeconds: 15 periodSeconds: 20 readinessProbe: httpGet: path: /readyz port: 8081 initialDelaySeconds: 5 periodSeconds: 10 resources: limits: cpu: 100m memory: 30Mi requests: cpu: 100m memory: 20Mi serviceAccountName: training-operator terminationGracePeriodSeconds: 10</p> <ul> <li>With this abstraction, data science teams can focus on writing the     Python code in TensorFlow that will be used as part of a TFJob     specification and don't have to manage the infrastructure     themselves. For now, we can skip the low-level details and use TFJob     to implement our distributed model training. Next, let's define our     TFJob in a file named tfjob.yaml.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | An example TFJob definition                         | | &gt; 8.35     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>apiVersion: kubeflow.org/v1 kind: TFJob metadata:</p> <p>namespace: kubeflow generateName: distributed-tfjobspec: tfReplicaSpecs:</p> <p>Worker:</p> <p>replicas: 2 restartPolicy: OnFailure template: spec: containers: - name: tensorflow image: gcr.io/kubeflow-ci/tf-mnist-with-summaries:1.0 command: - \\\"python\\\"</p> <ul> <li> <p>\\\"/var/tf_mnist/mnist_with_summaries.py\\\"</p> </li> <li> <p>\\\"--log_dir=/train/metrics\\\"</p> </li> <li> <p>\\\"--learning_rate=0.01\\\"</p> </li> <li> <p>\\\"--batch_size=100\\\"</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   In this spec, we are asking the controller to submit a distributed     TensorFlow model training model with two worker replicas where each     worker replica follows the same container definition, running the     MNIST image classification example.</p> <ul> <li>Once it's defined, we can submit it to our local Kubernetes cluster     via the following listing.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Submitting TFJob                                    | | &gt; 8.36     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl create -f basics/tfjob.yaml tfjob.kubeflow.org/distributed-tfjob-qc8fh created</p> <ul> <li>We can see whether the TFJob has been submitted successfully by     getting the TFJob list.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Getting the TFJob list                              | | &gt; 8.37     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl get tfjob</p> <p>NAME AGE</p> <p>Distributed-tfjob-qc8fh 1s</p> <ul> <li>When we get the list of Pods, we can see two worker Pods,     distributed-tfjob-qc8fhworker-1 and     distributed-tfjob-qc8fh-worker-0, have been created and started     running. The other Pods can be ignored since they are the Pods that     are running the Kubeflow and Argo Workflow operators.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Getting the list of Pods                                | | &gt; 8.38     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl get pods</p> <p>NAME READY STATUS RESTARTS AGE workflow-controller-594494ffbd-2dpkj 1/1 Running 0 21m training-operator-575698dc89-mzvwb 1/1 Running 0 21m argo-server-68c46c5c47-vfh82 1/1 Running 0 21m distributed-tfjob-qc8fh-worker-1 1/1 Running 0 10s distributed-tfjob-qc8fh-worker-0 1/1 Running 0 12s</p> <ul> <li>A machine learning system consists of many different components. We     only used Kubeflow to submit distributed model training jobs, but     it's not connected to other components yet. In the next section,     we'll explore the basic functionalities of Argo Workflows to connect     different steps in a single workflow so that they can be executed in     a particular order.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#quiz-9","title":"Quiz","text":"<p>1 If your model training requires parameter servers, can you express that in a TFJob?</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#argo-workflows-container-native-workflow-engine","title":"Argo Workflows: Container-native workflow engine","text":"<ul> <li> <p>The Argo Project is a suite of open-source tools for deploying and     running applications and workloads on Kubernetes.</p> </li> <li> <p>It extends the Kubernetes APIs and unlocks new and powerful     capabilities in application deployment, container orchestration,     event automation, progressive delivery, and more.</p> </li> <li> <p>It consists of four core projects: Argo CD, Argo Rollouts, Argo     Events, and Argo Workflows.</p> </li> <li> <p>Besides these core projects, many other ecosystem projects are based     on, extend, or work well with Argo.</p> </li> <li> <p>A complete list of resources related to Argo can be found at     https://github.com/terrytangyuan/     awesome-argo.</p> </li> <li> <p>Argo CD is a declarative, GitOps application delivery tool for     Kubernetes.</p> </li> <li> <p>It manages application definitions, configurations, and environments     declaratively in Git. Argo CD user experience makes Kubernetes     application deployment and life-cycle management automated,     auditable, and easy to grasp.</p> </li> <li> <p>It comes with a UI so engineers can see what's happening in their     clusters and watch for applications deployments, etc. Figure 8.7 is     a screenshot of the resource tree in the Argo CD UI.</p> </li> <li> <p>Argo Rollouts is a Kubernetes controller and set of CRDs that     provides progressive deployment capabilities. It introduces     blue--green and canary deployments, canary analysis,     experimentation, and progressive delivery features to your     Kubernetes cluster.</p> </li> </ul> <p>{width=\"5.666666666666667in\" height=\"2.7466666666666666in\"}</p> <p>Figure 8.7 A screenshot of the resources tree in Argo CD UI</p> <ul> <li> <p>Next is Argo Events. It's an event-based dependency manager for     Kubernetes. It can define multiple dependencies from various event     sources like webhooks, Amazon S3, schedules, and streams and trigger     Kubernetes objects after successful event dependencies resolution. A     complete list of available event sources can be found in figure 8.8.</p> </li> <li> <p>Finally, Argo Workflows is a container-native workflow engine for     orchestrating parallel jobs, implemented as Kubernetes CRD. Users     can define workflows where each step is a separate container, model     multi step workflows as a sequence of tasks or capture the     dependencies between tasks using a graph, and run compute-intensive     jobs for machine learning or data processing.</p> </li> <li> <p>Users often use Argo Workflows together with Argo Events to trigger     event-based workflows. The main use cases for Argo Workflows are     machine learning pipelines, data processing, ETL (extract,     transform, load), infrastructure automation, continuous delivery,     and integration.</p> </li> <li> <p>Argo Workflows also provides interfaces such as a command-line     interface (CLI), server, UI, and SDKs for different languages. The     CLI is useful for managing workflows and performing operations such     as submitting, suspending, and deleting workflows through the     command line.</p> </li> <li> <p>The server is used for integrating with other services. There are     both REST and gRPC service interfaces.</p> </li> <li> <p>The UI is useful for managing and visualizing workflows and any     artifacts/logs created by the workflows, as well as other useful     information, such as resource usage analytics.</p> </li> <li> <p>We will walk through some examples of Argo Workflows to prepare for     our project.</p> </li> </ul> <p>Figure 8.8 Available event sources in Argo Events</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#the-basics-1","title":"The basics","text":"<ul> <li> <p>Before we look at some examples, let's make sure we have the Argo     Workflows UI at hand. It's optional since you can still be     successful in these examples in the command line to interact     directly with Kubernetes via kubectl, but it's nice to see the     directed acyclic graph (DAG) visualizations in the UI as well as     access additional functionalities.</p> </li> <li> <p>By default, the Argo Workflows UI service is not exposed to an     external IP. To access the UI, use the method in the following     listing.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Port-forwarding the Argo server                         | | &gt; 8.39     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl port-forward svc/argo-server 2746:2746</p> <ul> <li> <p>Next, visit the following URL to access the UI: https:/     /localhost:2746. Alternatively, you can expose a load balancer to     get an external IP to access the Argo Workflows UI in your local     cluster.</p> </li> <li> <p>Check out the official documentation for more details: https://     argoproj.github.io/argo-workflows/argo-server/.     Figure 8.9 is a screenshot of what the Argo Workflows UI looks like     for a map-reduce--style workflow.</p> </li> </ul> <p>{width=\"5.666666666666667in\" height=\"3.3333333333333335in\"}</p> <p>Figure 8.9 Argo Workflows UI illustrating a map-reduce--style workflow</p> <ul> <li>The following listing is a basic \"hello world\" example of Argo     Workflows. We can specify the container image and the command to run     for this workflow and print out a \"hello world\" message.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | \"Hello world\" example                                   | | &gt; 8.40     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata:</p> <p>generateName: hello-worldspec:</p> <p>entrypoint: whalesay serviceAccountName: argo templates: - name: whalesay container:</p> <p>image: docker/whalesay command: [cowsay] args: [\\\"hello world\\\"]</p> <ul> <li>Let's go ahead and submit the workflow to our cluster.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Submitting the workflow                                 | | &gt; 8.41     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl create -f basics/argo-hello-world.yaml workflow.argoproj.io/hello-world-zns4g created</p> <ul> <li>We can then check whether it was submitted successfully and has     started running.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Getting the list of workflows                           | | &gt; 8.42     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl get wf</p> <p>NAME STATUS AGE hello-world-zns4g Running 2s</p> <ul> <li>Once the workflow status has changed to Succeeded, we can check the     statuses of the Pods created by the workflow. First, let's find all     the Pods associated with the workflow. We can use a label selector     to get the list of Pods.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Getting the list of Pods belonging to this workflow     | | &gt; 8.43     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl get pods -l workflows.argoproj.io/workflow=hello-world-zns4g</p> <p>NAME READY STATUS RESTARTS AGE hello-world-zns4g 0/2 Completed 0 8m57s</p> <p>Once we know the Pod name, we can get the logs of that Pod.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Checking the Pod logs                                   | | &gt; 8.44     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl logs hello-world-zns4g -c main</p> <p>_____________</p> <p>\\&lt; hello world &gt;</p> <p>-------------</p> <p>\\</p> <p>\\</p> <p>\\</p> <p>## .</p> <p>## ## ## ==</p> <p>## ## ## ## ===</p> <p>/\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"___/ ===</p> <p>\\~\\~\\~ {\\~\\~ \\~\\~\\~\\~ \\~\\~\\~ \\~\\~\\~\\~ \\~\\~ \\~ / ===- \\~\\~\\~</p> <p>\\______ o __/</p> <p>\\ \\ __/</p> <p>\\____\\______/</p> <ul> <li> <p>As expected, we get the same logs as the ones we had with the simple     Kubernetes Pod in the previous sections since this workflow only     runs one \"hello world\" step.</p> </li> <li> <p>The next example uses a resource template where you can specify a     Kubernetes custom resource that will be submitted by the workflow to     the Kubernetes cluster.</p> </li> <li> <p>Here we create a Kubernetes config map named cm-example with a     simple key-value pair. The config map is a Kubernetes-native object     to store key-value pairs.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Resource template                                       | | &gt; 8.45     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata:</p> <p>generateName: k8s-resourcespec:</p> <p>entrypoint: k8s-resource serviceAccountName: argo templates: - name: k8s-resource resource:</p> <p>action: create manifest: | apiVersion: v1 kind: ConfigMap metadata:</p> <p>name: cm-example data: some: value</p> <ul> <li> <p>This example is perhaps most useful to Python users. You can write a     Python script as part of the template definition. We can generate     some random numbers using the built-in random Python module.</p> </li> <li> <p>Alternatively, you can specify the execution logic of the script     inside a container template without writing inline Python code, as     seen in the \"hello world\" example.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Script template                                         | | &gt; 8.46     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata:</p> <p>generateName: script-tmplspec:</p> <p>entrypoint: gen-random-int serviceAccountName: argo templates:</p> <p>- name: gen-random-int script:</p> <p>image: python:alpine3.6 command: [python] source: | import random i = random.randint(1, 100) print(i)</p> <p>Let's submit it.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Submitting the script template workflow                 | | &gt; 8.47     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl create -f basics/argo-script-template.yaml workflow.argoproj.io/script-tmpl-c5lhb created</p> <p>Now, let's check its logs to see whether a random number was generated.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Check the Pod logs                                      | | &gt; 8.48     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl logs script-tmpl-c5lhb</p> <p>25</p> <ul> <li> <p>So far, we've only seen examples of single-step workflows. Argo     Workflow also allows users to define the workflow as a DAG by     specifying the dependencies of each task. The DAG can be simpler to     maintain for complex workflows and allows maximum parallelism when     running tasks.</p> </li> <li> <p>Let's look at an example of a diamond-shaped DAG created by Argo     Workflows. This DAG consists of four steps (A, B, C, and D), and     each has its own dependencies. For example, step C depends on step     A, and step D depends on steps B and C.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | A diamond example using DAG                             | | &gt; 8.49     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata:</p> <p>generateName: dag-diamondspec:</p> <p>serviceAccountName: argo entrypoint: diamond templates: - name: echo inputs: parameters: - name: message container:</p> <p>image: alpine:3.7 command: [echo, \\\"{{inputs.parameters.message}}\\\"]</p> <ul> <li>name: diamond dag: tasks: - name: A template: echo arguments:</li> </ul> <p>parameters: [{name: message, value: A}]</p> <ul> <li> <p>name: B dependencies: [A] template: echo arguments: parameters:     [{name: message, value: B}]</p> </li> <li> <p>name: C dependencies: [A] template: echo arguments:</p> </li> </ul> <p>parameters: [{name: message, value: C}]</p> <ul> <li>name: D dependencies: [B, C] template: echo arguments:</li> </ul> <p>parameters: [{name: message, value: D}] Let's submit it.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Submitting the DAG workflow                             | | &gt; 8.50     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl create -f basics/argo-dag-diamond.yaml workflow.argoproj.io/dag-diamond-6swfg created</p> <ul> <li>When the workflow is completed, we will see four Pods for each of     the steps where each step prints out its step name---A, B, C, and D.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Getting the list of Pods belonging to this workflow     | | &gt; 8.51     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl get pods -l workflows.argoproj.io/workflow=dag-diamond-6swfg</p> <p>NAME READY STATUS RESTARTS AGE dag-diamond-6swfg-echo-4189448097 0/2 Completed 0 76s dag-diamond-6swfg-echo-4155892859 0/2 Completed 0 66s dag-diamond-6swfg-echo-4139115240 0/2 Completed 0 66s dag-diamond-6swfg-echo-4239780954 0/2 Completed 0 56s</p> <ul> <li>The visualization of the DAG is available in the Argo Workflows UI.     It's usually more intuitive to see how the workflow is executed in a     diamond-shaped flow in the UI, as seen in figure 8.10.</li> </ul> <p>Figure 8.10 A screenshot of a diamond-shaped workflow in the UI</p> <ul> <li> <p>Next, we will look at a simple coin-flip example to showcase the     conditional syntax provided by Argo Workflows. We can specify a     condition to indicate whether we want to run the next step.</p> </li> <li> <p>For example, we run the flip-coin step first, which is the Python     script we saw earlier, and if the result returns heads, we run the     template called heads, which prints another log saying it was heads.</p> </li> <li> <p>Otherwise, we print that it was tails. So we can specify these     conditionals inside the when clause in the different steps.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Coin-flip example                                       | | &gt; 8.52     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata:</p> <p>generateName: coinflipspec:</p> <p>serviceAccountName: argo entrypoint: coinflip templates: - name: coinflip steps:</p> <ul> <li> <p>- name: flip-coin template: flip-coin - - name: heads template:     heads when: \\\"{{steps.flip-coin.outputs.result}} == heads\\\"</p> </li> <li> <p>name: tails template: tails when:     \\\"{{steps.flip-coin.outputs.result}} == tails\\\"</p> </li> <li> <p>name: flip-coin script:</p> </li> </ul> <p>image: python:alpine3.6 command: [python] source: | import random result = \\\"heads\\\" if random.randint(0,1) == 0 else \\\"tails\\\" print(result)</p> <ul> <li>name: heads container:</li> </ul> <p>image: alpine:3.6 command: [sh, -c] args: [\\\"echo \\\\\"it was heads\\\\\"\\\"]</p> <ul> <li>name: tails container:</li> </ul> <p>image: alpine:3.6 command: [sh, -c] args: [\\\"echo \\\\\"it was tails\\\\\"\\\"] Let's submit the workflow.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Submitting the coin-flip example                        | | &gt; 8.53     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl create -f basics/argo-coinflip.yaml workflow.argoproj.io/coinflip-p87ff created</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#answers-to-exercises","title":"8.5 Answers to exercises","text":"<ul> <li>Figure 8.11 is a screenshot of what this flip-coin workflow looks     like in the UI.</li> </ul> <p>Figure 8.11 Screenshot of the flip-coin workflow in the UI{width=\"2.0in\" height=\"1.9833333333333334in\"}</p> <ul> <li>When we get the list of workflows, we find only two Pods.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Getting the list of Pods belonging to this workflow     | | &gt; 8.54     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl get pods -l workflows.argoproj.io/workflow=coinflip-p87ff</p> <p>coinflip-p87ff-flip-coin-1071502578 0/2 Completed 0 23s coinflip-p87ff-tails-2208102039 0/2 Completed 0 13s</p> <ul> <li>We can check the logs of the flip-coin step to see whether it prints     out tails since the next step executed is the tails step:</li> </ul> <p>&gt; kubectl logs coinflip-p87ff-flip-coin-1071502578 tails</p> <ul> <li>That's a wrap! We've just learned the basic syntax of Argo     Workflows, which should cover all the prerequisites for the next     chapter!</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#quiz-10","title":"Quiz:","text":"<ol> <li> <p>Besides accessing the output of each step like     {{steps.flip-coin.outputs .result}}, what are other available     variables?</p> </li> <li> <p>Can you trigger workflows automatically by Git commits or other     events?</p> </li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%202/#summary-3","title":"Summary","text":"<ul> <li>We used TensorFlow to train a machine learning model for the MNIST     dataset in a single machine.</li> </ul> <p>We learned the basic concepts in Kubernetes and gained hands-on experience by implementing them in a local Kubernetes cluster.</p> <p>We submitted distributed model training jobs to Kubernetes via Kubeflow.</p> <p>We learned about different types of templates and how to define either DAGs or sequential steps using Argo Workflows.</p> <p>[Part 7_______________________________]{.underline}</p> <p>This Part covers:</p> <ul> <li> <p>Implementing data ingestion component with TensorFlow</p> </li> <li> <p>Defining the machine learning model and submitting distributed model     training jobs</p> </li> <li> <p>Implementing a single-instance model server as well as replicated     model servers</p> </li> <li> <p>Building an efficient end-to-end workflow of our machine learning     system</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#section-13-unnumbered","title":"{#section-13 .unnumbered}","text":"<ul> <li> <p>We'll implement the end-to-end machine learning system with the     architecture we designed in the last part</p> </li> <li> <p>We will completely implement each component, which will incorporate     the previously discussed patterns. We'll use several popular     frameworks and cutting-edge technologies, particularly TensorFlow,     Kubernetes, Kubeflow, Docker, and Argo Workflows.</p> </li> </ul> <p>Data ingestion</p> <ul> <li>The first component in our end-to-end workflow is data ingestion.     We'll be using the Fashion-MNIST dataset introduced in section 2.2     to build the data ingestion component. Figure 9.1 shows this     component in the dark box on the left of the end-to-end workflow.</li> </ul> <p>Figure 9.1 The data ingestion component (dark box) in the end-to-end machine learning system</p> <ul> <li> <p>Recall that this dataset consists of a training set of 60,000     examples and a test set of 10,000 examples. Each example is a 28 \u00d7     28 grayscale image representing one Zalando's article image and     associated with a label from 10 classes.</p> </li> <li> <p>In addition, the Fashion-MNIST dataset is designed to serve as a     direct drop-in replacement for the original MNIST dataset for     benchmarking machine learning algorithms. It shares the same image     size and structure of training and testing splits.</p> </li> <li> <p>Figure 9.2 is a screenshot of the collection of images for all 10     classes (T-shirt/top, trouser, pullover, dress, coat, sandal, shirt,     sneaker, bag, and ankle boot) from Fashion-MNIST, where each class     takes three rows in the screenshot.</p> </li> <li> <p>Figure 9.3 is a closer look at the first few example images in the     training set together with their corresponding labels in text above     each of the images.</p> </li> <li> <p>We'll go through the implementation of a single-node data pipeline     that ingests the Fashion-MNIST dataset. Furthermore, we will cover     the implementation of the distributed data pipeline to prepare the     data for our distributed model training</p> </li> <li> <p>.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#data-ingestion-2","title":"9.1 Data ingestion","text":"<p>Figure 9.2 A screenshot of the collection of images from the Fashion-MNIST dataset for all 10 classes</p> <p>(T-shirt/top, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, and ankle boot)</p> <p>{width=\"5.666666666666667in\" height=\"1.3333333333333333in\"}</p> <p>Figure 9.3 A closer look at the first few example images in the training set with their corresponding labels in text</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#single-node-data-pipeline","title":"Single-node data pipeline","text":"<ul> <li> <p>Let's first take a look at how to build a single-node data pipeline     that works locally on your laptop without using a local Kubernetes     cluster.</p> </li> <li> <p>The best way for a machine learning program written in TensorFlow to     consume data is through methods in the tf.data module.</p> </li> <li> <p>The tf.data API allows users to build complex input pipelines     easily. For example, the pipeline for an image model might aggregate     data from files in various file systems, apply random     transformations to each image, and create batches from the images     for model training.</p> </li> <li> <p>The tf.data API enables it to handle large amounts of data, read     from different data formats, and perform complex transformations. It     contains a tf.data.Dataset abstraction that represents a sequence of     elements, in which each element consists of one or more components.</p> </li> <li> <p>Let's use the image pipeline to illustrate this. An element in an     image input pipeline might be a single training example, with a pair     of tensor components representing the image and its label.</p> </li> <li> <p>The following listing provides the code snippet to load the     Fashion-MNIST dataset into a tf.data.Dataset object and performs     some necessary preprocessing steps to prepare for our model     training:</p> </li> <li> <p>Scale the dataset from the range (0, 255] to (0., 1.].</p> </li> <li> <p>Cast the image multidimensional arrays into float32 type that our     model can accept.</p> </li> <li> <p>Select the training data, cache it in memory to speed up training,     and shuffle it with a buffer size of 10,000.</p> </li> </ul> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | Loading the Fashion-MNIST dataset                        | | &gt; 9.1     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>import tensorflow_datasets as tfds import tensorflow as tf def make_datasets_unbatched(): def scale(image, label):</p> <p>image = tf.cast(image, tf.float32) image /= 255 return image, label datasets, _ = tfds.load(name=\\'fashion_mnist\\', with_info=True, as_supervised=True)</p> <p>return datasets[\\'train\\'].map(scale).cache().shuffle(10000)</p> <ul> <li> <p>Note that we imported the tensorflow_datasets module. The TensorFlow     Datasets, which consists of a collection of datasets for various     tasks such as image classification, object detection, document     summarization, etc., can be used with TensorFlow and other Python     machine learning frameworks.</p> </li> <li> <p>The tf.data.Dataset object is a shuffled dataset where each element     consists of the images and their labels with the shape and data type     information as in the following listing.</p> </li> </ul> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | Inspecting the tf.data object                        | | &gt; 9.2     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>&gt;&gt;&gt; ds = make_datasets_unbatched()</p> <p>&gt;&gt;&gt; ds</p> <p>\\&lt;ShuffleDataset element_spec=( TensorSpec(shape=(28, 28, 1), dtype=tf.float32, name=None),</p> <p>TensorSpec(shape=(), dtype=tf.int64, name=None))&gt;</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#distributed-data-pipeline","title":"Distributed data pipeline","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#now-lets-look-at-how-we-can-consume-our-dataset-in-a-distributed-fashion-well-be-using-tfdistributemultiworkermirroredstrategy-for-distributed-training-in-the-next-section-lets-assume-we-have-instantiated-a-strategy-object","title":"Now let's look at how we can consume our dataset in a distributed fashion. We'll be using tf.distribute.MultiWorkerMirroredStrategy for distributed training in the next section. Let's assume we have instantiated a strategy object.","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#we-will-instantiate-our-dataset-inside-the-strategys-scope-via-python-with-syntax-using-the-same-function-we-previously-defined-for-the-single-node-use-case","title":"We will instantiate our dataset inside the strategy's scope via Python with syntax using the same function we previously defined for the single-node use case.","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#we-will-need-to-tweak-a-few-configurations-to-build-our-distributed-input-pipeline-first-we-create-repeated-batches-of-data-where-the-total-batch-size-equals-the-batch-size-per-replica-times-the-number-of-replicas-over-which-gradients-are-aggregated","title":"We will need to tweak a few configurations to build our distributed input pipeline. First, we create repeated batches of data where the total batch size equals the batch size per replica times the number of replicas over which gradients are aggregated.","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#this-ensures-that-we-will-have-enough-records-to-train-each-batch-in-each-of-the-model-training-workers-in-other-words-the-number-of-replicas-in-sync-equals-the-number-of-devices-taking-part-in-the-gradient-allreduce-operation-during-model-training-for-instance-when-a-user-or-the-training-code-calls-next-on-the-distributed-data-iterator-a-per-replica-batch-size-of-data-is-returned-on-each-replica-the-rebatched-dataset-cardinality-will-always-be-a-multiple-of-the-number-of-replicas","title":"This ensures that we will have enough records to train each batch in each of the model training workers. In other words, the number of replicas in sync equals the number of devices taking part in the gradient allreduce operation during model training. For instance, when a user or the training code calls next() on the distributed data iterator, a per replica batch size of data is returned on each replica. The rebatched dataset cardinality will always be a multiple of the number of replicas.","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#in-addition-we-want-to-configure-tfdata-to-enable-automatic-data-sharding-since-the-dataset-is-in-the-distributed-scope-the-input-dataset-will-be-sharded-automatically-in-multi-worker-training-mode-more-specifically-each-dataset-will-be-created-on-the-cpu-device-of-the-worker-and-each-set-of-workers-will-train-the-model-on-a-subset-of-the-entire-dataset-when-tfdataexperimentalautoshardpolicy-is-set-to-autoshardpolicydata","title":"In addition, we want to configure tf.data to enable automatic data sharding. Since the dataset is in the distributed scope, the input dataset will be sharded automatically in multi worker training mode. More specifically, each dataset will be created on the CPU device of the worker, and each set of workers will train the model on a subset of the entire dataset when tf.data.experimental.AutoShardPolicy is set to AutoShardPolicy.DATA.","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#one-benefit-is-that-during-each-model-training-step-a-global-batch-size-of-non-overlapping-dataset-elements-will-be-processed-by-each-worker-each-worker-will-process-the-whole-dataset-and-discard-the-portion-that-is-not-for-itself","title":"One benefit is that during each model training step, a global batch size of non-overlapping dataset elements will be processed by each worker. Each worker will process the whole dataset and discard the portion that is not for itself.","text":""},{"location":"Spanda%20Bootcamp%20Day%202/#note-that-for-this-mode-to-partition-the-dataset-elements-correctly-the-dataset-needs-to-produce-elements-in-a-deterministic-order-which-should-already-be-guaranteed-by-the-tensorflow-datasets-library-we-use","title":"Note that for this mode to partition the dataset elements correctly, the dataset needs to produce elements in a deterministic order, which should already be guaranteed by the TensorFlow Datasets library we use.","text":"<p>+-----------+----------------------------------------------------------+ | &gt; Listing | Configuring distributed data pipeline                    | | &gt; 9.3     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>BATCH_SIZE_PER_REPLICA = 64</p> <p>BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync with strategy.scope():</p> <p>ds_train = make_datasets_unbatched().batch(BATCH_SIZE).repeat() options = tf.data.Options() options.experimental_distribute.auto_shard_policy = \\ tf.data.experimental.AutoShardPolicy.DATA ds_train = ds_train.with_options(options) model = build_and_compile_model()</p> <p>model.fit(ds_train, epochs=1, steps_per_epoch=70)</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#model-training-1","title":"Model training","text":"<ul> <li> <p>We went through the implementation of the data ingestion component     for both local-node and distributed data pipelines and discussed how     we can shard the dataset properly across different workers so that     it would work with distributed model training. In this section,     let's dive into the implementation details for our model training     component.</p> </li> <li> <p>An architecture diagram of the model training component can be found     in figure 9.4.</p> </li> </ul> <p>Three mode training</p> <p>steps train different</p> <p>~models.~ This step picks the top model</p> <p>that will be used in the following two separate model serving steps.</p> <p>Figure 9.4 A diagram of the model training component in the overall architecture. Three different model training steps are followed by a model selection step. These model training steps would train three different models---namely, CNN, CNN with dropout, and CNN with batch normalization--- competing with each other for better statistical performance.</p> <ul> <li> <p>We will learn how to define those three models with TensorFlow and     execute the distributed model training jobs with Kubeflow.</p> </li> <li> <p>We will implement the model selection step that picks the top model     that will be used in the model serving component in our end-to-end     machine learning workflow.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#model-definition-and-single-node-training","title":"Model definition and single-node training","text":"<ul> <li> <p>Next, we'll look at the TensorFlow code to define and initialize the     first model, a convolutional neural network (CNN) model we     introduced in previous chapters with three convolutional layers.</p> </li> <li> <p>We initialize the model with Sequential(), meaning we'll add the     layers sequentially. The first layer is the input layer, where we     specify the shape of the input pipeline that we defined previously.</p> </li> <li> <p>Note that we also explicitly give a name to the input layer so we     can pass the correct key in our inference inputs, which we will     discuss in more depth in section 9.3.</p> </li> <li> <p>After adding the input layer, three convolutional layers, followed     by max-pooling layers and dense layers, are added to the sequential     model. We'll then print out a summary of the model architecture and     compile the model with Adam as its optimizer, accuracy as the metric     we use to evaluate the model, and sparse categorical cross entropy     as the loss function.</p> </li> </ul> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | Defining the basic CNN model                             | | &gt; 9.4     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>def build_and_compile_cnn_model():</p> <p>print(\\\"Training CNN model\\\") model = models.Sequential() model.add(layers.Input(shape=(28, 28, 1), name=\\'image_bytes\\')) model.add( layers.Conv2D(32, (3, 3), activation=\\'relu\\')) model.add(layers.MaxPooling2D((2, 2)))</p> <p>model.add(layers.Conv2D(64, (3, 3), activation=\\'relu\\')) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation=\\'relu\\')) model.add(layers.Flatten()) model.add(layers.Dense(64, activation=\\'relu\\')) model.add(layers.Dense(10, activation=\\'softmax\\')) model.summary() model.compile(optimizer=\\'adam\\', loss=\\'sparse_categorical_crossentropy\\', metrics=[\\'accuracy\\']) return model</p> <ul> <li> <p>We've successfully defined our basic CNN model.</p> </li> <li> <p>Next, we define two models based on the CNN model. One adds a batch     normalization layer to force the pre-activations to have zero mean     and unit standard deviation for every neuron (activation) in a     particular layer.</p> </li> <li> <p>The other model has an additional dropout layer where half of the     hidden units will be dropped randomly to reduce the complexity of     the model and speed up computation. The rest of the code is the same     as the basic CNN model.</p> </li> </ul> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | Defining the variations of the basic CNN model           | | &gt; 9.5     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>def build_and_compile_cnn_model_with_batch_norm(): print(\\\"Training CNN model with batch normalization\\\") model = models.Sequential() model.add(layers.Input(shape=(28, 28, 1), name=\\'image_bytes\\')) model.add( layers.Conv2D(32, (3, 3), activation=\\'relu\\')) model.add(layers.BatchNormalization()) model.add(layers.Activation(\\'sigmoid\\')) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation=\\'relu\\')) model.add(layers.BatchNormalization()) model.add(layers.Activation(\\'sigmoid\\')) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation=\\'relu\\')) model.add(layers.Flatten())</p> <p>model.add(layers.Dense(64, activation=\\'relu\\')) model.add(layers.Dense(10, activation=\\'softmax\\')) model.summary()</p> <p>model.compile(optimizer=\\'adam\\', loss=\\'sparse_categorical_crossentropy\\', metrics=[\\'accuracy\\']) return model</p> <p>def build_and_compile_cnn_model_with_dropout():</p> <p>print(\\\"Training CNN model with dropout\\\") model = models.Sequential() model.add(layers.Input(shape=(28, 28, 1), name=\\'image_bytes\\')) model.add( layers.Conv2D(32, (3, 3), activation=\\'relu\\')) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation=\\'relu\\')) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Dropout(0.5)) model.add(layers.Conv2D(64, (3, 3), activation=\\'relu\\')) model.add(layers.Flatten()) model.add(layers.Dense(64, activation=\\'relu\\')) model.add(layers.Dense(10, activation=\\'softmax\\'))</p> <p>model.summary()</p> <p>model.compile(optimizer=\\'adam\\', loss=\\'sparse_categorical_crossentropy\\', metrics=[\\'accuracy\\']) return model</p> <ul> <li> <p>Once the models are defined, we can train them locally on our     laptops. Let's use the basic CNN model as an example. We will create     four callbacks that will be executed during model training:</p> </li> <li> <p>PrintLR---Callback to print the learning rate at the end of each     epoch</p> </li> <li> <p>TensorBoard---Callback to start the interactive TensorBoard     visualization to monitor the training progress and model     architecture</p> </li> <li> <p>ModelCheckpoint---Callback to save model weights for model inference     later</p> </li> <li> <p>LearningRateScheduler---Callback to decay the learning rate at the     end of each epoch</p> </li> <li> <p>Once these callbacks are defined, we'll pass it to the fit() method     for training. The fit() method trains the model with a specified     number of epochs and steps per epoch. Note that the numbers here are     for demonstration purposes only to speed up our local experiments     and may not sufficiently produce a model with good quality in     real-world applications.</p> </li> </ul> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | Modeling training with callbacks                         | | &gt; 9.6     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>single_worker_model = build_and_compile_cnn_model() checkpoint_prefix = os.path.join(args.checkpoint_dir, \\\"ckpt_{epoch}\\\")</p> <p>class PrintLR(tf.keras.callbacks.Callback): def on_epoch_end(self, epoch, logs=None):</p> <p>print(\\'\\nLearning rate for epoch {} is {}\\'.format( epoch + 1, multi_worker_model.optimizer.lr.numpy())) callbacks = [ tf.keras.callbacks.TensorBoard(log_dir=\\'./logs\\'), tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True), tf.keras.callbacks.LearningRateScheduler(decay),</p> <p>PrintLR()</p> <p>] single_worker_model.fit(ds_train, epochs=1, steps_per_epoch=70, callbacks=callbacks)</p> <ul> <li>We'll see the model training progress like the following in the     logs:</li> </ul> <p>Learning rate for epoch 1 is 0.0010000000474974513</p> <p>70/70 [========] - 16s 136ms/step - loss: 1.2853</p> <p>- accuracy: 0.5382 - lr: 0.0010</p> <p>Here's the summary of the model architecture in the logs:</p> <p>Model: \\\"sequential\\\"</p> <p>________________________________________________________________</p> <p>Layer (type) Output Shape Param #</p> <p>================================================== conv2d (Conv2D) (None, 26, 26, 32) 320 max_pooling2d (MaxPooling2D) (None, 13, 13, 32) 0 conv2d_1 (Conv2D) (None, 11, 11, 64) 18496 max_pooling2d_1 (MaxPooling2D) (None, 5, 5, 64) 0 conv2d_2 (Conv2D) (None, 3, 3, 64) 36928 flatten (Flatten) (None, 576) 0 dense (Dense) (None, 64) 36928 dense_1 (Dense) (None, 10) 650</p> <p>==================================================</p> <p>Total params: 93,322</p> <p>Trainable params: 93,322</p> <p>Non-trainable params: 0</p> <ul> <li>Based on this summary, 93,000 parameters will be trained during the     process. The shape and the number of parameters in each layer can     also be found in the summary.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#distributed-model-training","title":"Distributed model training","text":"<ul> <li> <p>Now that we've defined our models and can train them locally in a     single machine, the next step is to insert the distributed training     logic in the code so that we can run model training with multiple     workers using the collective communication pattern that we     introduced in the course.</p> </li> <li> <p>We'll use the tf.distribute module that contains     MultiWorkerMirroredStrategy. It's a distribution strategy for     synchronous training on multiple workers. It creates copies of all     variables in the model's layers on each device across all workers.     This strategy uses a distributed collective implementation (e.g.,     all-reduce), so multiple workers can work together to speed up     training.</p> </li> <li> <p>If you don't have appropriate GPUs, you can replace     communication_options with other implementations. Since we want to     ensure the distributed training can run on different machines that     might not have GPUs, we'll replace it with     CollectiveCommunication.AUTO so that it will pick any available     hardware automatically.</p> </li> <li> <p>Once we define our distributed training strategy, we'll initiate our     distributed input data pipeline and the model inside the strategy     scope. Note that defining the model inside the strategy scope is     required since TensorFlow knows how to copy the variables in the     model's layers to each worker adequately based on the strategy. Here     we define different model types (basic CNN, CNN with dropout, and     CNN with batch normalization) based on the command-line arguments we     pass to this Python script.</p> </li> <li> <p>We'll get to the rest of the flags soon. Once the data pipeline and     the model are defined inside the scope, we can use fit() to train     the model outside the distribution strategy scope.</p> </li> </ul> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | Distributed model training logic                         | | &gt; 9.7     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>strategy = tf.distribute.MultiWorkerMirroredStrategy( communication_options=tf.distribute.experimental.CommunicationOptions( implementation=tf.distribute.experimental.CollectiveCommunication.AUTO))</p> <p>BATCH_SIZE_PER_REPLICA = 64</p> <p>BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync</p> <p>with strategy.scope():</p> <p>ds_train = make_datasets_unbatched().batch(BATCH_SIZE).repeat() options = tf.data.Options() options.experimental_distribute.auto_shard_policy = \\ tf.data.experimental.AutoShardPolicy.DATA ds_train = ds_train.with_options(options) if args.model_type == \\\"cnn\\\":</p> <p>multi_worker_model = build_and_compile_cnn_model() elif args.model_type == \\\"dropout\\\":</p> <p>multi_worker_model = build_and_compile_cnn_model_with_dropout() elif args.model_type == \\\"batch_norm\\\":</p> <p>multi_worker_model = build_and_compile_cnn_model_with_batch_norm() else:</p> <p>raise Exception(\\\"Unsupported model type: %s\\\" % args.model_type)</p> <p>multi_worker_model.fit(ds_train, epochs=1, steps_per_epoch=70)</p> <ul> <li> <p>Once the model training is finished via fit() function, we want to     save the model. One common mistake that users can easily make is     saving models on all the workers, which may not save the completed     model correctly and wastes computational resources and storage.</p> </li> <li> <p>The correct way to fix this problem is to save only the model on the     chief worker. We can inspect the environment variable TF_CONFIG,     which contains the cluster information, such as the task type and     index, to see whether the worker is chief. Also, we want to save the     model to a unique path across workers to avoid unexpected errors.</p> </li> </ul> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | Saving a model with a chief worker                       | | &gt; 9.8     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>def is_chief():</p> <p>return TASK_INDEX == 0</p> <p>tf_config = json.loads(os.environ.get(\\'TF_CONFIG\\') or \\'{}\\')</p> <p>TASK_INDEX = tf_config[\\'task\\'][\\'index\\']</p> <p>if is_chief():</p> <p>model_path = args.saved_model_dir else: model_path = args.saved_model_dir + \\'/worker_tmp_\\' + str(TASK_INDEX)</p> <p>multi_worker_model.save(model_path)</p> <ul> <li>So far, we've seen two command-line flags already---namely,     saved_model_dir and model_type. Listing 9.9 provides the rest of the     main function that will parse those command-line arguments. In     addition to those two arguments, there's another checkpoint_dir     argument that we will use to save our model to the TensorFlow     SavedModel format that can be easily consumed for our model serving     component. We will discuss that in detail in section 9.3. We also     disabled the progress bar for the TensorFlow Datasets module to     reduce the logs we will see.</li> </ul> <p>+-----------+----------------------------------------------------------+ | &gt; Listing | Entry point main function                                | | &gt; 9.9     |                                                          | +===========+==========================================================+ +-----------+----------------------------------------------------------+</p> <p>if __name__ == \\'__main__\\': tfds.disable_progress_bar()</p> <p>parser = argparse.ArgumentParser() parser.add_argument(\\'--saved_model_dir\\', type=str, required=True,</p> <p>help=\\'Tensorflow export directory.\\')</p> <p>parser.add_argument(\\'--checkpoint_dir\\', type=str, required=True,</p> <p>help=\\'Tensorflow checkpoint directory.\\')</p> <p>parser.add_argument(\\'--model_type\\', type=str, required=True,</p> <p>help=\\'Type of model to train.\\')</p> <p>parsed_args = parser.parse_args() main(parsed_args)</p> <ul> <li>We've just finished writing our Python script that contains the     distributed model training logic. Let's containerize it and build     the image used to run distributed training in our local Kubernetes     cluster. In our Dockerfile, we'll use the Python 3.9 base image,     install TensorFlow and TensorFlow Datasets modules via pip, and copy     our multi worker distributed training Python script.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Containerization                                        | | &gt; 9.10     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>FROM python:3.9</p> <p>RUN pip install tensorflow==2.11.0 tensorflow_datasets==4.7.0</p> <p>COPY multi-worker-distributed-training.py /</p> <ul> <li>We then build the image from the Dockerfile we just defined. We also     need to import the image to the k3d cluster since our cluster does     not have access to our local image registry. We then set the current     namespace to be \"kubeflow\". Please read chapter 8 and follow the     instructions to install the required components we need for this     project.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Building and importing the docker image                 | | &gt; 9.11     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; docker build -f Dockerfile -t kubeflow/multi-worker-strategy:v0.1 .</p> <p>&gt; k3d image import kubeflow/multi-worker-strategy:v0.1 --cluster distml</p> <p>&gt; kubectl config set-context --current --namespace=kubeflow</p> <ul> <li> <p>Once the worker Pods are completed, all files in the Pod will be     recycled. Since we are running distributed model training across     multiple workers in Kubernetes Pods, all the model checkpoints will     be lost, and we don't have a trained model for model serving. To     address this problem, we'll use PersistentVolume (PV) and     PersistentVolumeClaim (PVC).</p> </li> <li> <p>PV is a storage in the cluster that has been provisioned by an     administrator or dynamically provisioned. It is a resource in the     cluster, just like a node is a cluster resource. PVs are volume     plugins like Volumes, but have a life cycle independent of any     individual Pod that uses the PV. In other words, PVs will persist     and live even after the Pods are completed or deleted.</p> </li> <li> <p>A PVC is a request for storage by a user. It is similar to a Pod.     Pods consume node resources, and PVCs consume PV resources. Pods can     request specific levels of resources (CPU and memory). Claims can     request specific size and access modes (e.g., they can be mounted     ReadWriteOnce, ReadOnlyMany, or ReadWriteMany).</p> </li> <li> <p>Let's create a PVC to submit a request for storage that will be used     in our worker Pods to store the trained model. Here we only submit a     request for 1 Gi storage with ReadWriteOnce access mode.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Persistent volume claim                                 | | &gt; 9.12     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>kind: PersistentVolumeClaim apiVersion: v1 metadata:</p> <p>name: strategy-volume spec:</p> <p>accessModes: [ \\\"ReadWriteOnce\\\" ] resources: requests:</p> <p>storage: 1Gi</p> <p>Next, we'll create the PVC.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Creating the PVC                                        | | &gt; 9.13     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl create -f multi-worker-pvc.yaml</p> <ul> <li> <p>Next, let's define the TFJob spec we introduced in chapter 7 with     the image we just built that contains the distributed training     script. We pass the necessary command arguments to the container to     train the basic CNN model. T4</p> </li> <li> <p>The volumes field in the Worker spec specifies the name of the     persistent volume claim that we just created, and the volumeMounts     field in the containers spec specifies what folder to mount the     files between the volume to the container. The model will be saved     in the /trained_model folder inside the volume.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Distributed model training job definition               | | &gt; 9.14     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>apiVersion: kubeflow.org/v1 kind: TFJob metadata:</p> <p>name: multi-worker-training spec: runPolicy:</p> <p>cleanPodPolicy: None tfReplicaSpecs:</p> <p>Worker:</p> <p>replicas: 2 restartPolicy: Never template: spec: containers: - name: tensorflow</p> <p>image: kubeflow/multi-worker-strategy:v0.1 imagePullPolicy: IfNotPresent command: [\\\"python\\\",</p> <p>\\\"/multi-worker-distributed-training.py\\\",</p> <p>\\\"--saved_model_dir\\\",</p> <p>\\\"/trained_model/saved_model_versions/2/\\\",</p> <p>\\\"--checkpoint_dir\\\",</p> <p>\\\"/trained_model/checkpoint\\\", \\\"--model_type\\\", \\\"cnn\\\"] volumeMounts:</p> <p>- mountPath: /trained_model name: training resources: limits: cpu: 500m volumes: - name: training persistentVolumeClaim:</p> <p>claimName: strategy-volume</p> <p>Then we can submit this TFJob to our cluster to start our distributed model training.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Submitting TFJob                                    | | &gt; 9.15     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl create -f multi-worker-tfjob.yaml</p> <ul> <li>Once the worker Pods are completed, we'll notice the following logs     from the Pods that indicate we trained the model in a distributed     fashion and the workers communicated with each other successfully:</li> </ul> <p>Started server with target: grpc://multi-worker-training-worker-0.kubeflow.svc:2222</p> <p>/job:worker/replica:0/task:1 has connected to coordination service.</p> <p>/job:worker/replica:0/task:0 has connected to coordination service. Coordination agent has successfully connected.</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#model-selection","title":"Model selection","text":"<ul> <li> <p>So far, we've implemented our distributed model training component.     We'll eventually train three different models, as mentioned in     section 9.2.1, and then pick the top model for model serving.</p> </li> <li> <p>Let's assume that we have trained those models successfully by     submitting three different TFJobs with different model types.</p> </li> <li> <p>Next, we write the Python code that loads the testing data and     trained models and then evaluate their performance. We will load     each trained model from different folders by     keras.models.load_model() function and execute model.evaluate(),     which returns the loss and accuracy.</p> </li> <li> <p>Once we find the model with the highest accuracy, we can copy the     model to a new version in a different folder---namely, 4---which     will be used by our model serving component.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Model evaluation                                        | | &gt; 9.16     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>import numpy as np import tensorflow as tf from tensorflow import keras import tensorflow_datasets as tfds import shutil import os def scale(image, label):</p> <p>image = tf.cast(image, tf.float32) image /= 255 return image, label</p> <p>best_model_path = \\\"\\\" best_accuracy = 0 for i in range(1, 4):</p> <p>model_path = \\\"trained_model/saved_model_versions/\\\" + str(i) model = keras.models.load_model(model_path)</p> <p>datasets, _ = tfds.load( name=\\'fashion_mnist\\', with_info=True, as_supervised=True) ds = datasets[\\'test\\'].map(scale).cache().shuffle(10000).batch(64)</p> <p>_, accuracy = model.evaluate(ds) if accuracy &gt; best_accuracy: best_accuracy = accuracy best_model_path = model_path</p> <p>destination = \\\"trained_model/saved_model_versions/4\\\" if os.path.exists(destination): shutil.rmtree(destination)</p> <p>shutil.copytree(best_model_path, destination) print(\\\"Best model with accuracy %f is copied to %s\\\" % ( best_accuracy, destination))</p> <ul> <li> <p>Note that the latest version, 4, in the     trained_model/saved_model_versions folder will be picked up by our     serving component. We will talk about that in the next section.</p> </li> <li> <p>We then add this Python script to our Dockerfile, rebuild the     container image, and create a Pod that runs the model selection     component. The following is the YAML file that configures the model     selection Pod.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Model selection Pod definition                          | | &gt; 9.17     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>apiVersion: v1 kind: Pod metadata:</p> <p>name: model-selection spec: containers: - name: predict image: kubeflow/multi-worker-strategy:v0.1 command: [\\\"python\\\", \\\"/model-selection.py\\\"] volumeMounts: - name: model mountPath: /trained_model volumes: - name: model persistentVolumeClaim: claimName: strategy-volume</p> <ul> <li>When inspecting the logs, we see the third model has the highest     accuracy, so we will copy it to a new version to be used for the     model serving component:</li> </ul> <p>157/157 [======] - 1s 5ms/step - loss: 0.7520 - accuracy: 0.7155</p> <p>157/157 [======] - 1s 5ms/step - loss: 0.7568 - accuracy: 0.7267</p> <p>157/157 [======] - 1s 5ms/step - loss: 0.7683 - accuracy: 0.7282</p> <p>Model serving</p> <ul> <li> <p>Now that we have implemented our distributed model training as well     as model selection among the trained models. The next component we     will implement is the model serving component.</p> </li> <li> <p>The model serving component is essential to the end-user experience     since the results will be shown to our users directly, and if it's     not performant enough, our users will know immediately. Figure 9.5     shows the model training component in the overall architecture.</p> </li> </ul> <p>The results from the two model serving steps are then aggregated via a result aggregation step to present to users</p> <p>.</p> <p>Figure 9.5 Model serving component (dark boxes) in the end-to-end machine learning system</p> <ul> <li>In figure 9.5, the model serving components are shown as the two     dark boxes between the model selection and result aggregation steps.     Let's first implement our single server model inference component in     section 9.3.1 and then make it more scalable and performant in     section 9.3.2.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#single-server-model-inference","title":"Single-server model inference","text":"<ul> <li> <p>The model inference Python code is very similar to the model     evaluation code. The only difference is that we use the     model.predict() method instead of evaluate() after we load the     trained model. This is an excellent way to test whether the trained     model can make predictions as expected.</p> </li> <li></li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Model prediction                                        | | &gt; 9.18     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>import numpy as np import tensorflow as tf from tensorflow import keras import tensorflow_datasets as tfds model = keras.models.load_model(\\\"trained_model/saved_model_versions\\\")</p> <p>def scale(image, label):</p> <p>image = tf.cast(image, tf.float32) image /= 255 return image, label datasets, _ = tfds.load( name=\\'fashion_mnist\\', with_info=True, as_supervised=True) ds = datasets[\\'test\\'].map(scale).cache().shuffle(10000).batch(64) model.predict(ds)</p> <ul> <li> <p>Alternatively, you can start a TensorFlow Serving     (https://github.com/tensorflow/     serving) server locally like     in the following listing once it's installed.</p> </li> <li></li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | TensorFlow Serving command                              | | &gt; 9.19     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>tensorflow_model_server --model_name=flower-sample \\</p> <p>--port=9000 \\</p> <p>--rest_api_port=8080 \\</p> <p>--model_base_path=trained_model/saved_model \\</p> <p>--rest_api_timeout_in_ms=60000</p> <p>- This seems straightforward and works well if we are only experimenting locally. However, there are more performant ways to build our model serving component that will pave our path to running distributed model serving that incorporates the replicated model server pattern that we introduced in previous chapters.</p> <ul> <li>Before we dive into a better solution, let's make sure our trained     model can work with our prediction inputs, which will be a     JSON-structured list of image bytes with the key \\\"instances\\\" and     \\\"image_bytes\\\", like the following:</li> </ul> <p>{</p> <p>\\\"instances\\\":[</p> <p>{</p> <p>\\\"image_bytes\\\":{</p> <p>\\\"b64\\\":\\\"/9j/4AAQSkZJRgABAQAAAQABAAD</p> <p>...</p> <p>\\&lt;truncated&gt;</p> <p>/hWY4+UVEhkoIYUx0psR+apm6VBRUZcUYFSuKZgUAf//Z\\\"</p> <p>}</p> <p>}</p> <p>]</p> <p>}</p> <ul> <li> <p>Now is the time to modify our distributed model training code to     make sure the model has the correct serving signature that's     compatible with our supplied inputs. We define the preprocessing     function that does the following:</p> </li> <li> <p>Decodes the images from bytes</p> </li> <li> <p>Resizes the image to 28 \u00d7 28 that's compatible with our model     architecture</p> </li> <li> <p>Casts the images to tf.uint8</p> </li> <li> <p>Defines the input signature with string type and key as image_bytes</p> </li> <li> <p>Once the preprocessing function is defined, we can define the     serving signature via tf.TensorSpec() and then pass it to     tf.saved_model.save() method to save the model that is compatible     with our input format and preprocess it before TensorFlow Serving     makes inference calls.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Model serving signature definitions                     | | &gt; 9.20     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>def _preprocess(bytes_inputs):</p> <p>decoded = tf.io.decode_jpeg(bytes_inputs, channels=1) resized = tf.image.resize(decoded, size=(28, 28)) return tf.cast(resized, dtype=tf.uint8)</p> <p>def _get_serve_image_fn(model):</p> <p>\\@tf.function( input_signature=[tf.TensorSpec([None], dtype=tf.string, name=\\'image_bytes\\')]) def serve_image_fn(bytes_inputs): decoded_images = tf.map_fn(_preprocess, bytes_inputs, dtype=tf.uint8) return model(decoded_images) return serve_image_fn signatures = { \\\"serving_default\\\":</p> <p>_get_serve_image_fn(multi_worker_model).get_concrete_function( tf.TensorSpec(shape=[None], dtype=tf.string, name=\\'image_bytes\\')</p> <p>)</p> <p>}</p> <p>tf.saved_model.save(multi_worker_model, model_path, signatures=signatures)</p> <ul> <li> <p>Once the distributed model training script is modified, we can     rebuild our container image and retrain our model from scratch,</p> </li> <li> <p>Next, we will use KServe, as we mentioned in the technologies     overview, to create an inference service. Listing 9.21 provides the     YAML to define the KServe inference service. We need to specify the     model format so that KServe knows what to use for serving the model     (e.g., TensorFlow Serving). In addition, we need to supply the URI     to the trained model. In this case, we can specify the PVC name and     the path to the trained model, following the format     pvc://\\&lt;pvc-name&gt;/\\&lt;model-path&gt;.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Inference service definition                            | | &gt; 9.21     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>apiVersion: serving.kserve.io/v1beta1 kind: InferenceService metadata:</p> <p>name: flower-sample spec: predictor: model:</p> <p>modelFormat:</p> <p>name: tensorflow storageUri: \\\"pvc://strategy-volume/saved_model_versions\\\"</p> <ul> <li>Let's install KServe and create our inference service!</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Installing KServe and creating the inference service    | | &gt; 9.22     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; curl -s \\\"https:/ /raw.githubusercontent.com/ kserve/kserve/v0.10.0-rc1/hack/quick_install.sh\\\" | bash &gt; kubectl create -f inference-service.yaml</p> <ul> <li>We can check its status to make sure it's ready for serving.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Getting the details of the inference service            | | &gt; 9.23     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl get isvc</p> <p>NAME URL READY AGE flower-sample \\&lt;truncated...example.com&gt; True 25s</p> <ul> <li>Once the service is created, we port-forward it to local so that we     can send requests to it locally.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Port-forwarding the inference service                   | | &gt; 9.24     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; INGRESS_GATEWAY_SERVICE=$(kubectl get svc --namespace \\ istio-system --selector=\\\"app=istio-ingressgateway\\\" --output \\ jsonpath=\\'{.items[0].metadata.name}\\')</p> <p>&gt; kubectl port-forward --namespace istio-system svc/${INGRESS_GATEWAY_SERVICE} 8080:80</p> <ul> <li>You should be able to see the following if the port-forwarding is     successful:</li> </ul> <p>Forwarding from 127.0.0.1:8080 -&gt; 8080</p> <p>Forwarding from [::1]:8080 -&gt; 8080</p> <ul> <li>Let's open another terminal and execute the following Python script     to send a sample inference request to our model serving service and     print out the response text.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Using Python to send an inference request               | | &gt; 9.25     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>import requests import json input_path = \\\"inference-input.json\\\"</p> <p>with open(input_path) as json_file: data = json.load(json_file)</p> <p>r = requests.post( url=\\\"http:/ /localhost:8080/v1/models/flower-sample:predict\\\", data=json.dumps(data),</p> <p>headers={\\'Host\\': \\'flower-sample.kubeflow.example.com\\'}) print(r.text)</p> <ul> <li>The response from our KServe model serving service, which includes     the predicted probabilities for each class in the Fashion-MNIST     dataset, is as follows:</li> </ul> <p>{</p> <p>\\\"predictions\\\": [[0.0, 0.0, 1.22209595e-11,</p> <p>0.0, 1.0, 0.0, 7.07406329e-32, 0.0, 0.0, 0.0]]</p> <p>}</p> <ul> <li>Alternatively, we can use curl to send requests.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Using curl to send an inference request                 | | &gt; 9.26     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p># Start another terminal export INGRESS_HOST=localhost export INGRESS_PORT=8080</p> <p>MODEL_NAME=flower-sample</p> <p>INPUT_PATH=@./inference-input.json</p> <p>SERVICE_HOSTNAME=$(kubectl get inferenceservice \\ ${MODEL_NAME} -o jsonpath=\\'{.status.url}\\' | \\ cut -d \\\"/\\\" -f 3) curl -v -H \\\"Host: ${SERVICE_HOSTNAME}\\\" \\\"http:/ /${INGRESS_HOST}:${INGRESS_PORT}/v1/ models/$MODEL_NAME:predict\\\" -d $INPUT_PATH</p> <ul> <li>The output probabilities should be the same as the ones we just saw:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Trying ::1:8080...</p> <ul> <li>Connected to localhost (::1) port 8080 (#0)</li> </ul> <p>&gt; POST /v1/models/flower-sample:predict HTTP/1.1</p> <p>&gt; Host: flower-sample.kubeflow.example.com</p> <p>&gt; User-Agent: curl/7.77.0</p> <p>&gt; Accept: */*</p> <p>&gt; Content-Length: 16178</p> <p>&gt; Content-Type: application/x-www-form-urlencoded</p> <p>&gt;</p> <ul> <li>Mark bundle as not supporting multiuse</li> </ul> <p>\\&lt; HTTP/1.1 200 OK</p> <p>\\&lt; content-length: 102</p> <p>\\&lt; content-type: application/json</p> <p>\\&lt; date: Thu, 05 Jan 2023 21:11:36 GMT</p> <p>\\&lt; x-envoy-upstream-service-time: 78</p> <p>\\&lt; server: istio-envoy</p> <p>\\&lt;</p> <p>{</p> <p>\\\"predictions\\\": [[0.0, 0.0, 1.22209595e-11, 0.0,</p> <p>1.0, 0.0, 7.07406329e-32, 0.0, 0.0, 0.0] ]</p> <p>* Connection #0 to host localhost left intact</p> <p>}</p> <ul> <li> <p>As mentioned previously, even though we specified the entire     directory that contains the trained model in the KServe     InferenceService spec, the model serving service that utilizes     TensorFlow Serving will pick the latest version 4 from that     particular folder, which is our best model we selected in section     9.2.3.</p> </li> <li> <p>We can observe that from the logs of the serving Pod.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Inspecting the model server logs                        | | &gt; 9.27     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl logs flower-sample-predictor-default -00001-deployment-f67767f6c2fntx -c kserve-container Here's the logs:</p> <p>Building single TensorFlow model file config:</p> <p>model_name: flower-sample model_base_path: /mnt/models Adding/updating models.</p> <p>...</p> <p>\\&lt;truncated&gt;</p> <p>Successfully loaded servable version</p> <p>{name: flower-sample version: 4}</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#replicated-model-servers","title":"Replicated model servers","text":"<ul> <li> <p>In the previous section, we successfully deployed our model serving     service in our local Kubernetes cluster. This might be sufficient     for running local serving experiments, but it's far from ideal if     it's deployed to production systems that serve realworld model     serving traffic.</p> </li> <li> <p>The current model serving service is a single Kubernetes Pod, where     the allocated computational resources are limited and requested in     advance. When the number of model serving requests increases, the     single-instance model server can no longer support the workloads and     may run out of computational resources.</p> </li> <li> <p>To address the problem, we need to have multiple instances of model     servers to handle a larger amount of dynamic model serving requests.     Fortunately, KServe can autoscale based on the average number of     in-flight requests per Pod, which uses the Knative Serving     autoscaler.</p> </li> <li> <p>The following listing provides the inference service spec with     autoscaling enabled. The scaleTarget field specifies the integer     target value of the metric type the autoscaler watches for. In     addition, the scaleMetric field defines the scaling metric type     watched by autoscaler.</p> </li> <li> <p>The possible metrics are concurrency, RPS, CPU, and memory. Here we     only allow one concurrent request to be processed by each inference     service instance. In other words, when there are more requests, we     will start a new inference service Pod to handle each additional     request.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Replicated model inference services                     | | &gt; 9.28     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>apiVersion: serving.kserve.io/v1beta1 kind: InferenceService metadata:</p> <p>name: flower-sample</p> <p>spec: predictor:</p> <p>scaleTarget: 1 scaleMetric: concurrency model: modelFormat:</p> <p>name: tensorflow</p> <p>storageUri: \\\"pvc://strategy-volume/saved_model_versions\\\"</p> <ul> <li> <p>Let's assume there's no request, and we should only see one     inference service Pod that's up and running. Next, let's send     traffic in 30-second spurts, maintaining five inflight requests. We     use the same service hostname and ingress address, as well as the     same inference input and trained model.</p> </li> <li> <p>Note that we are using the tool hey, a tiny program that sends some     load to a web application. Follow the instructions at     https://github.com/rakyll/hey to install it before executing the     following command.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Sending traffic to test the load                        | | &gt; 9.29     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; hey -z 30s -c 5 -m POST \\</p> <p>-host ${SERVICE_HOSTNAME} \\</p> <p>-D inference-input.json \\\"http:/ /${INGRESS_HOST}:${INGRESS_PORT}</p> <p>/v1/models/$MODEL_NAME:predict\\\"</p> <ul> <li>The following is the expected output from the command, which     includes a summary of how the inference service handled the     requests. For example, the service has processed 230,160 bytes of     inference inputs and 95.7483 requests per second. You can also find     a nice response-time histogram and a latency distribution that might     be useful:</li> </ul> <p>Summary:</p> <p>Total: 30.0475 secs</p> <p>Slowest: 0.2797 secs</p> <p>Fastest: 0.0043 secs</p> <p>Average: 0.0522 secs</p> <p>Requests/sec: 95.7483</p> <p>Total data: 230160 bytes</p> <p>Size/request: 80 bytes Response time histogram:</p> <p>0.004 [1] |</p> <p>0.032 [1437] |\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0</p> <p>0.059 [3] |</p> <p>0.087 [823] |\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0</p> <p>0.114 [527] |\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0</p> <p>0.142 [22] |\u25a0 0.170 [5] |</p> <p>0.197 [51] |\u25a0</p> <p>0.225 [7] |</p> <p>0.252 [0] |</p> <p>0.280 [1] |</p> <p>Latency distribution:</p> <p>10% in 0.0089 secs</p> <p>25% in 0.0123 secs</p> <p>50% in 0.0337 secs</p> <p>75% in 0.0848 secs</p> <p>90% in 0.0966 secs</p> <p>95% in 0.1053 secs</p> <p>99% in 0.1835 secs</p> <p>Details (average, fastest, slowest):</p> <p>DNS+dialup: 0.0000 secs, 0.0043 secs, 0.2797 secs DNS-lookup: 0.0000 secs, 0.0000 secs, 0.0009 secs req write: 0.0000 secs, 0.0000 secs, 0.0002 secs resp wait: 0.0521 secs, 0.0042 secs, 0.2796 secs resp read: 0.0000 secs, 0.0000 secs, 0.0005 secs Status code distribution:</p> <p>[200] 2877 responses</p> <ul> <li>As expected, we see five running inference service Pods processing     the requests concurrently, where each Pod handles only one request.</li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Getting the list of model server Pods                   | | &gt; 9.30     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl get pods</p> <p>NAME READY STATUS RESTARTS AGE flower-\\&lt;truncated&gt;-sr5wd 3/3 Running 0 12s flower--\\&lt;truncated&gt;-swnk5 3/3 Running 0 22s flower--\\&lt;truncated&gt;-t2njf 3/3 Running 0 22s flower--\\&lt;truncated&gt;-vdlp9 3/3 Running 0 22s flower--\\&lt;truncated&gt;-vm58d 3/3 Running 0 42s</p> <p>Once the hey command is completed, we will only see one running Pod.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Getting the list of model server Pods again             | | &gt; 9.31     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl get pods</p> <p>NAME READY STATUS RESTARTS AGE flower-\\&lt;truncated&gt;-sr5wd 3/3 Running 0 62s</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#the-end-to-end-workflow","title":"The end-to-end workflow","text":"<ul> <li>We have just implemented all the components in the previous     sections. Now it's time to put things together! In this section,     we'll define an end-to-end workflow using Argo Workflows that     includes the components we just implemented.</li> </ul> <p>Please go back to previous sections if you are still unfamiliar with all the components and refresh your knowledge of basic Argo Workflows.</p> <p>Here's a recap of what the end-to-end workflow we will implement looks like. Figure 9.6 is a diagram of the end-to-end workflow that we are building. The diagram includes two model serving steps for illustration purposes, but we will only implement one step in our Argo workflow. It will autoscale to more instances based on requests traffic.</p> <p>In the next sections, we will define the entire workflow by connecting the steps sequentially with Argo and then optimize the workflow for future executions by implementing step memoization.</p> <p>Three model training</p> <p>model serving steps. to present to users.</p> <p>Figure 9.6 An architecture diagram of the end-to-end machine learning system we are building</p>"},{"location":"Spanda%20Bootcamp%20Day%202/#sequential-steps","title":"Sequential steps","text":"<ul> <li> <p>First, let's look at the entry point templates and the main steps     involved in the workflow. The entry point template name is tfjob-wf,     which consists of the following steps (for simplicity, each step     uses a template with the same name):</p> </li> <li> <p>data-ingestion-step contains the data ingestion step, which we will     use to download and preprocess the dataset before model training.</p> </li> <li> <p>distributed-tf-training-steps is a step group that consists of     multiple substeps, where each substep represents a distributed model     training step for a specific model type.</p> </li> <li> <p>model-selection-step is a step that selects the top model from among     the different models we have trained in previous steps.</p> </li> <li> <p>create-model-serving-service creates the model serving serve via     KServe.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Workflow entry point templates                          | | &gt; 9.32     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata:</p> <p>generateName: tfjob-wf namespace: kubeflow spec:</p> <p>entrypoint: tfjob-wf podGC:</p> <p>strategy: OnPodSuccess volumes: - name: model persistentVolumeClaim:</p> <p>claimName: strategy-volume</p> <p>templates: - name: tfjob-wf steps:</p> <ul> <li> <p>- name: data-ingestion-step template: data-ingestion-step - - name:     distributed-tf-training-steps template:     distributed-tf-training-steps</p> </li> <li> <p>- name: model-selection-step template: model-selection-step - -     name: create-model-serving-service template:     create-model-serving-service</p> </li> </ul> <p>- Note that we specify the podGC strategy to be OnPodSuccess since we'll be creating a lot of Pods for different steps within our local k3s cluster with limited computational resources, so deleting the Pods right after they are successful can free up computational resources for the subsequent steps. The OnPodCompletion strategy is also available; it deletes Pods on completion regardless of whether they failed or succeeded. We won't use that since we want to keep failed Pods to debug what went wrong.</p> <p>- In addition, we also specify our volumes and PVC to ensure we can persist any files that will be used in the steps. We can save the downloaded dataset into the persistent volume for model training and then persist the trained model for the subsequent model serving step.\\</p> <p>- The first step, the data ingestion step, is very straightforward. It only specifies the container image and the data ingestion Python script to execute. The Python script is a one-line code with tfds.load(name=\\'fashion_mnist\\') to download the dataset to the container's local storage, which will be mounted to our persistent volume.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Data ingestion step                                     | | &gt; 9.33     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <ul> <li>name: data-ingestion-step serviceAccountName: argo container:</li> </ul> <p>image: kubeflow/multi-worker-strategy:v0.1 imagePullPolicy: IfNotPresent command: [\\\"python\\\", \\\"/data-ingestion.py\\\"]</p> <p>The next step is a step group that consists of multiple substeps, where each substep represents a distributed model training step for a specific model type (e.g., basic CNN, CNN with dropout, and CNN with batch norm). The following listing provides the template that defines all the substeps. Distributed training steps for multiple models dictate that these will be executed in parallel.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Distributed training step groups                        | | &gt; 9.34     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <ul> <li> <p>name: distributed-tf-training-steps steps:</p> </li> <li> <p>- name: cnn-model template: cnn-model - name:     cnn-model-with-dropout template: cnn-model-with-dropout - name:     cnn-model-with-batch-norm template: cnn-model-with-batch-norm</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Let's use the first substep, which runs a distributed model training     for the basic CNN model, as an example. The main content of this     step template is the resource field, which includes the following:</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The custom resource definition (CRD) or manifest to take action     upon. In our case, we create a TFJob as part of this step.</p> <ul> <li>The conditions that indicate whether the CRD is created     successfully. In our case, we ask Argo to watch the field     status.replicaStatuses.Worker.succeeded and     status.replicaStatuses.Worker.failed.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Inside the container spec in the TFJob definition, we specify the     model type and save the trained model to a different folder so it's     easy to pick and save the best model for model serving in subsequent     steps. We also want to make sure to attach the persistent volumes so     the trained model can be persisted.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | CNN model training step                                 | | &gt; 9.35     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>- name: cnn-model serviceAccountName: training-operator resource:</p> <p>action: create setOwnerReference: true successCondition: status.replicaStatuses.Worker.succeeded = 2 failureCondition: status.replicaStatuses.Worker.failed &gt; 0 manifest: | apiVersion: kubeflow.org/v1 kind: TFJob metadata:</p> <p>generateName: multi-worker-training spec: runPolicy:</p> <p>cleanPodPolicy: None tfReplicaSpecs:</p> <p>Worker:</p> <p>replicas: 2 restartPolicy: Never template: spec: containers: - name: tensorflow</p> <p>image: kubeflow/multi-worker-strategy:v0.1 imagePullPolicy: IfNotPresent command: [\\\"python\\\",</p> <p>\\\"/multi-worker-distributed-training.py\\\",</p> <p>\\\"--saved_model_dir\\\",</p> <p>\\\"/trained_model/saved_model_versions/1/\\\",</p> <p>\\\"--checkpoint_dir\\\",</p> <p>\\\"/trained_model/checkpoint\\\",</p> <p>\\\"--model_type\\\", \\\"cnn\\\"]</p> <p>volumeMounts:</p> <ul> <li>mountPath: /trained_model name: training resources: limits: cpu:     500m volumes: - name: training persistentVolumeClaim: claimName:     strategy-volume</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   For the rest of the substeps in distributed-tf-training-steps, the     spec is very similar, except the saved model directory and model     type arguments are different. The next step is model selection, for     which we will supply the same container image but execute the model     selection Python script we implemented earlier.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Model selection step Caption here                       | | &gt; 9.36     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <ul> <li>name: model-selection-step serviceAccountName: argo container:</li> </ul> <p>image: kubeflow/multi-worker-strategy:v0.1 imagePullPolicy: IfNotPresent command: [\\\"python\\\", \\\"/model-selection.py\\\"] volumeMounts: - name: model mountPath: /trained_model</p> <ul> <li> <p>Make sure these additional scripts are included in your Dockerfile     and that you have rebuilt the image and re-imported it to your local     Kubernetes cluster.</p> </li> <li> <p>Once the model selection step is implemented, the last step in the     workflow is the model serving step that starts a KServe model     inference service. It's a resource template similar to the model     training steps but with KServe's InferenceService CRD and a success     condition that applies to this specific CRD.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | The model serving step                                  | | &gt; 9.37     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <ul> <li>name: create-model-serving-service serviceAccountName:     training-operator successCondition:     status.modelStatus.states.transitionStatus = UpToDate resource:</li> </ul> <p>action: create setOwnerReference: true manifest: |</p> <p>apiVersion: serving.kserve.io/v1beta1 kind: InferenceService metadata:</p> <p>name: flower-sample spec:</p> <p>predictor: model: modelFormat:</p> <p>name: tensorflow image: \\\"emacski/tensorflow-serving:2.6.0\\\"</p> <p>storageUri: \\\"pvc://strategy-volume/saved_model_versions\\\" Let's submit this workflow now!</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Submitting the end-to-end workflow                      | | &gt; 9.38     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl create -f workflow.yaml</p> <ul> <li> <p>Once the data ingestion step is completed, the associated Pod will     be deleted. When we list the Pods again while it's executing the     distributed model training steps, we'll see the Pods with names     prefixed by tfjob-wf-f4bql-cnn-model-, which are the Pods     responsible for monitoring the status of distributed model training     for different model types.</p> </li> <li> <p>In addition, each model training for each model type contains two     workers with the name matching the pattern     multi-worker-training-*-worker-*.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Getting the list of Pods                                | | &gt; 9.39     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl get pods</p> <p>NAME READY STATUS RESTARTS AGE multi-\\&lt;truncated&gt;-worker-0 1/1 Running 0 50s multi-\\&lt;truncated -worker-1 1/1 Running 0 49s multi-\\&lt;truncated -worker-0 1/1 Running 0 47s multi-\\&lt;truncated -worker-1 1/1 Running 0 47s multi-\\&lt;truncated -worker-0 1/1 Running 0 54s multi-\\&lt;truncated -worker-1 1/1 Running 0 53s \\&lt;truncated&gt;-cnn-model 1/1 Running 0 56s</p> <p>\\&lt;truncated&gt;-batch-norm 1/1 Running 0 56s</p> <p>\\&lt;truncated&gt;-dropout 1/1 Running 0 56s</p> <ul> <li>Once the remaining steps are completed, and the model serving has     started successfully, the workflow should have a Succeeded status.     We've just finished the execution of the end-to-end workflow.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#step-memoization","title":"Step memoization","text":"<ul> <li> <p>To speed up future executions of workflows, we can utilize cache and     skip certain steps that have recently run. In our case, the data     ingestion step can be skipped since we don't have to download the     same dataset again and again.</p> </li> <li> <p>Let's first take a look at the logs from our data ingestion step:</p> </li> </ul> <p>Downloading and preparing dataset 29.45 MiB (download: 29.45 MiB, generated: 36.42 MiB, total: 65.87 MiB) to</p> <p>/root/tensorflow_datasets/fashion_mnist/3.0.1...</p> <p>Dataset fashion_mnist downloaded and prepared to</p> <p>/root/tensorflow_datasets/fashion_mnist/3.0.1.</p> <p>Subsequent calls will reuse this data.</p> <ul> <li> <p>The dataset has been downloaded to a path in the container. If the     path is mounted to our persistent volume, it will be available for     any future workflow runs. Let's use the step memoization feature     provided by Argo Workflows to optimize our workflow.</p> </li> <li> <p>Inside the step template, we supply the memoize field with the cache     key and age of the cache. When a step is completed, a cache will be     saved. When this step runs again in a new workflow, it will check     whether the cache is created within the past hour. If so, this step     will be skipped, and the workflow will proceed to execute subsequent     steps.</p> </li> <li> <p>For our application, our dataset does not change so, theoretically,     the cache should always be used, and we specify 1 hour here for     demonstration purposes only. In real-world applications, you may     want to adjust that according to how frequently the data is updated.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Memoization for the data ingestion step                 | | &gt; 9.40     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>- name: data-ingestion-step serviceAccountName: argo memoize:</p> <p>key: \\\"step-cache\\\" maxAge: \\\"1h\\\" cache: configMap:</p> <p>name: my-config key: step-cache container:</p> <p>image: kubeflow/multi-worker-strategy:v0.1 imagePullPolicy: IfNotPresent command: [\\\"python\\\", \\\"/data-ingestion.py\\\"]</p> <p>Let's run the workflow for the first time and pay attention to the Memoization Status field in the workflow's node status. The cache is not hit because this is the first time the step is run.</p> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Checking the node statuses of the workflow              | | &gt; 9.41     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl get wf tfjob-wf-kjj2q -o yaml The following is the section for node statuses:</p> <p>Status: Nodes:</p> <p>tfjob-wf-crfhx-2213815408:</p> <p>Boundary ID: tfjob-wf-crfhx Children:</p> <p>tfjob-wf-crfhx-579056679 Display Name: data-ingestion-step</p> <p>Finished At: 2023-01-04T20:57:44Z</p> <p>Host Node Name: distml-control-plane Id: tfjob-wf-crfhx-2213815408 Memoization Status:</p> <p>Cache Name: my-config</p> <p>Hit: false</p> <p>Key: step-cache</p> <p>Name: tfjob-wf-crfhx[0].data-ingestion-step</p> <ul> <li>If we run the same workflow again within one hour, we will notice     that the step is skipped (indicated by hit: true in the Memoization     Status field):</li> </ul> <p>Status: Nodes:</p> <p>tfjob-wf-kjj2q-1381200071:</p> <p>Boundary ID: tfjob-wf-kjj2q Children:</p> <p>tfjob-wf-kjj2q-2031651288 Display Name: data-ingestion-step</p> <p>Finished At: 2023-01-04T20:58:31Z Id: tfjob-wf-kjj2q-1381200071 Memoization Status:</p> <p>Cache Name: my-config</p> <p>Hit: true</p> <p>Key: step-cache</p> <p>Name: tfjob-wf-kjj2q[0].data-ingestion-step Outputs:</p> <p>Exit Code: 0</p> <p>Phase: Succeeded</p> <p>Progress: 1/1</p> <p>Started At: 2023-01-04T20:58:31Z Template Name: data-ingestion-step</p> <p>Template Scope: local/tfjob-wf-kjj2q</p> <p>Type: Pod</p> <ul> <li> <p>In addition, note that the Finished At and Started At timestamps are     the same. That is, this step is completed instantly without having     to re-execute from scratch.</p> </li> <li> <p>All the cache in Argo Workflows is saved in a Kubernetes ConfigMap     object. The cache contains the node ID, step outputs, and cache     creation timestamp, as well as the timestamp when this cache is last     hit.</p> </li> </ul> <p>+------------+---------------------------------------------------------+ | &gt; Listing  | Checking the details of the configmap                   | | &gt; 9.42     |                                                         | +============+=========================================================+ +------------+---------------------------------------------------------+</p> <p>&gt; kubectl get configmap -o yaml my-config apiVersion: v1 data:</p> <p>step-cache: \\'{\\\"nodeID\\\":\\\"tfjob-wf-dmtn43886957114\\\",\\\"outputs\\\":{\\\"exitCode\\\":\\\"0\\\"},</p> <p>\\\"creationTimestamp\\\":\\\"2023-01-04T20:44:55Z\\\", \\\"lastHitTimestamp\\\":\\\"2023-01-04T20:57:44Z\\\"}\\' kind: ConfigMap metadata:</p> <p>creationTimestamp: \\\"2023-01-04T20:44:55Z\\\" labels:</p> <p>workflows.argoproj.io/configmap-type: Cache name: my-config namespace: kubeflow resourceVersion: \\\"806155\\\" uid: 0810a68b-44f8-469f-b02c-7f62504145ba</p> <p>Summary</p> <ul> <li> <p>The data ingestion component implements a distributed input pipeline     for the Fashion-MNIST dataset with TensorFlow that makes it easy to     integrate with distributed model training.</p> </li> <li> <p>Machine learning models and distributed model training logic can be     defined in TensorFlow and then executed in a distributed fashion in     the Kubernetes cluster with the help of Kubeflow.</p> </li> <li> <p>Both the single-instance model server and the replicated model     servers can be implemented via KServe. The auto scaling     functionality of KServe can automatically create additional model     serving Pods to handle the increasing number of model serving     requests.</p> </li> <li> <p>We implemented our end-to-end workflow that includes all the     components of our system in Argo Workflows and used step memoization     to avoid time consuming and redundant data ingestion.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%202/#section-14-unnumbered","title":"{#section-14 .unnumbered}","text":""},{"location":"Spanda%20Bootcamp%20Day%203/","title":"Spanda Bootcamp Day 3","text":"<p>[Section 1: Overview_____________________]{.underline}</p> <p>Data Lakes, Federated Learning and Big Data</p> <ul> <li> <p>Data Lakes as a concept have played a key role in ushering in the     massive scales we see when working with data today as they give     companies the ability to store arbitrary types of data observed     during operations.</p> </li> <li> <p>While this freedom allows data lakes to maintain the maximum     potential of the data generated by the company, it also can lead to     a key problem---complacency in understanding the collected data.</p> </li> <li> <p>The ease of storing different types of data in an unstructured     manner can actually lead to a\u00a0store now, sort out later\u00a0mentality.</p> </li> <li> <p>The true difficulty of working with unstructured data actually stems     from its processing; thus, the delayed processing mentality has the     potential to lead to data lakes that become highly cumbersome to     sift through and work with due to unrestricted growth from the     collection of data.</p> </li> <li> <p>Raw data is only as valuable as the models and insights that can be     derived from it.</p> </li> <li> <p>The central data lake approach leads to cases where derivation from     the data is limited by a lack of structure, leading to issues     ranging from storage inefficiency to actual intelligence     inefficiency due to extraction difficulties.</p> </li> <li> <p>On the other side, approaches preceding data lakes suffered from a     simple lack of access to the amount of data potentially available.</p> </li> <li> <p>The fact that Federated Learning (FL) allows for both classes of     problems to be avoided is the key\u00a0driving support for FL as the     vehicle that will advance big data into the collective intelligence     era.</p> </li> <li> <p>This claim is substantiated by the fact that FL flips the big data     flow from collect \u2192 derive intelligence to derive intelligence \u2192     collect.</p> </li> <li> <p>For humans, intelligence can be thought of as the condensed form of     large swaths of experience.</p> </li> <li> <p>In a similar way, the derivation of intelligence at the source of     the generated data--- done by training a model on the data at the     source location---succinctly summarizes the data in a format that     maximizes accessibility for practical applications.</p> </li> <li> <p>The late collection step of FL leads to the creation of the desired     global intelligence with maximal data access and data storage     efficiency.</p> </li> <li> <p>Even cases with partial usage of the generated data sources can     still greatly benefit from the joint storage of intelligence and     data by greatly reducing the number of data formats entering the     residual data lake.</p> </li> </ul> <p>Abundance, Acceptance &amp; Ambivalence</p> <ul> <li> <p>While many\u00a0definitions have been proposed with emphasis on different     aspects, Oxford professor Viktor Mayer-Sch\u00f6nberger and\u00a0The     Economist\u00a0senior editor Kenneth Cukier brilliantly elucidated the     nature of big data.</p> </li> <li> <p>It is not about how big the data in a server is;\u00a0big data is about     three major shifts of a mindset that are interlinked and hence     reinforce one another.</p> </li> <li> <p>Their\u00a0argument can be summarized as the\u00a0Triple-A mindset for big     data*, which consists of an Abundance of observations,     Acceptance of messiness, and Ambivalence of causality***.</p> </li> </ul> <p>Abundance of observations</p> <ul> <li> <p>Big data doesn't have to be\u00a0big\u00a0in terms of columns and rows     or file size.</p> </li> <li> <p>Big data has a number of observations, commonly denominated as\u00a0n,     close or equal to the size of the population\u00a0of interest.</p> </li> <li> <p>In traditional statistics, collecting data from the entire     population---for example, people interested in fitness in New     York---was not possible or feasible, and researchers would have to     randomly select a sample from the population---for example, 1,000     people interested in fitness in New York.</p> </li> <li> <p>Random sampling is often difficult to perform and so is justifying     the narrow focus on particular subgroups: surveying people around     gyms would miss those who run in parks and practice yoga at home,     and why gym goers rather than runners and yoga fans?</p> </li> <li> <p>Thanks to the development and sophistication of\u00a0Information and     Communications Technology\u00a0(ICT) systems, however, researchers     today can access the data of approximately\u00a0all of the population     through multiple sources---for example, records of Google searches     about fitness.</p> </li> <li> <p>This paradigm of\u00a0abundance\u00a0or\u00a0n = all\u00a0is advantageous since what     the data says can be interpreted as a true statement about the     population, whereas the older methods could only\u00a0infer\u00a0such truth     with a significant level of confidence expressed in a\u00a0p-value,     typically supposed to be under 0.05.</p> </li> <li> <p>Small data provides statistics; big data proves states.</p> </li> </ul> <p>Acceptance of messiness</p> <ul> <li> <p>Big data is messy.</p> </li> <li> <p>If we use Google search data as a proxy for someone's interest---for     example---we could mistakenly attribute some of the searches made by     their family or friends on their devices to them, and the estimated     interest will be inaccurate to the degree of the ratio of such     unowned-device searches.</p> </li> <li> <p>In some devices, a significant amount of searches may be made by     multiple users, such as shared computers at an office or a     smartphone belonging to a child whose younger siblings are yet to     own one.</p> </li> <li> <p>Otherwise, people may search for words that pop up in a conversation     with someone else, rather than self-talk, which does not necessarily     reflect their own interests.</p> </li> <li> <p>In studies using traditional methods, researchers would have to make     sure that such devices are not included in their sample data because     the\u00a0mess\u00a0can affect the quality of inference\u00a0significantly, as the     number of observations would be small.</p> </li> <li> <p>This is not the case in big data studies. Researchers would be     willing to accept the\u00a0mess\u00a0as its effect diminishes proportionally     as the number of observations becomes large enough toward\u00a0n = all.</p> </li> <li> <p>In most devices, Google searches would be made by the owner     autonomously most of the time, and the impact of searches in other     contexts would not matter.</p> </li> </ul> <p>Ambivalence of causality</p> <ul> <li> <p>Big data is often used to study correlation but not     causation---in other words, it usually does not tell\u00a0why\u00a0but     only\u00a0what.</p> </li> <li> <p>For many practical questions, correlation alone can provide the     answer. Mayer-Sch\u00f6nberger and Cukier give several examples in     the\u00a0Big Data: A Revolution That Will Transform How We Live, Work,     and Think\u00a0book, among which\u00a0is Fair Isaac Corporation's\u00a0Medication     Adherence Score\u00a0established in 2011.</p> </li> <li> <p>In an era where people's behavioral patterns are\u00a0datafied,     collecting\u00a0n = all\u00a0observations for the variables of interest is     possible, and the correlation found among them is powerful enough to     direct our decision-making. There is no need to know people's     psychological scores     of\u00a0consistency\u00a0or\u00a0conformity\u00a0that\u00a0cause\u00a0their adherence to     medical prescriptions; by looking at how they behave in other     aspects of life, we can directly predict whether they will follow     the prescription or not.</p> </li> <li> <p>By embracing the triple mindset of abundance, acceptance, and     ambivalence, enterprises and governments have generated intelligence     across tasks from pricing services to recommending products,     optimizing transportation routes, and identifying crime suspects.</p> </li> <li> <p>Nevertheless, that mindset has been challenged in recent years, as     shown in the following sections.</p> </li> <li> <p>First, let's glimpse into how the abundance of observations often     taken for granted is currently under pressure.</p> </li> </ul> <p>AI and Data Privacy</p> <ul> <li> <p>FL is often said to be\u00a0one of the most popular privacy-preserving     AI technologies because private data does not have to be     collected or shared with third-party entities to generate     high-quality intelligence.</p> </li> <li> <p>Therefore, in this section, we discuss the data privacy that has     been a bottleneck that FL tries to resolve to create high-quality     intelligence.</p> </li> </ul> <p>Data privacy:</p> <ul> <li> <p>Data privacy, also known as\u00a0information privacy, is the right     of individuals to control how their personal information is\u00a0used,     which mandates third parties to handle, process, store, and use such     information properly in accordance with the\u00a0law.</p> <ul> <li> <p>In May 2021, HCA Healthcare announced that the company had     struck a deal to\u00a0share its patient records and real-time medical     data with Google.</p> </li> <li> <p>Various media quickly responded by warning the public about the     deal, as Google had been mentioned for its\u00a0Project     Nightingale\u00a0where the tech giant allegedly exploited the     sensitive data of millions of American patients.</p> </li> </ul> </li> <li> <p>Given above 80% of the public believes that the potential risks in     data collection by companies outweigh the benefits, according     to a 2019 poll by Pew Research Center, data sharing projects of such     a scale are naturally seen as a threat to people's\u00a0data privacy.</p> </li> <li> <p>It is often confused with\u00a0data security, which ensures that data     is accurate, reliable, and accessible only to authorized users.</p> </li> <li> <p>In the case of Google accounts, data privacy regulates how the     company can use the account holders' information, while data     security requires them to deploy measures such as password     protection and\u00a02-step verification.</p> </li> <li> <p>In explaining these two concepts, the data privacy managers use an     analogy of a\u00a0window\u00a0for security\u00a0and a\u00a0curtain\u00a0for privacy: data     security\u00a0is a prerequisite for data privacy. Put together, they     comprise\u00a0data protection, as shown in the following diagram:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"2.182638888888889in\"}</p> <p>Figure 1.2 -- Data security versus data privacy</p> <ul> <li> <p>We can see from the preceding diagram that while data security     limits\u00a0who\u00a0can access data, data privacy limits\u00a0what\u00a0can be in     the data.</p> </li> <li> <p>Understanding this distinction is very important\u00a0because data     privacy can multiply the consequences of failures in data security.     Let's look into how.</p> </li> </ul> <p>Privata Data Risks</p> <ul> <li> <p>Failing in data protection is costly. According to IBM's\u00a0Cost of a     Data Breach Report 2021, the global average cost of a data breach     in the year marked\u00a0US dollars\u00a0(USD) $4.24 million, which is     considerably higher than $3.86 million a year earlier and is     the\u00a0highest amount in the 17-year history of the report; an     increased number of people working remotely in the aftermath of the     COVID-19 outbreak is considered a major reason for the spike.</p> </li> <li> <p>The top five industries for average total cost are\u00a0healthcare,     finance, pharmaceuticals, technology, and energy.</p> </li> <li> <p>Nearly half of breaches in the year included customer\u00a0personally     identifiable information\u00a0(PII), which costs $180 per record     on average.</p> </li> <li> <p>Once customer PII is breached, negativities such as system downtime     during the response, loss of customers, need for acquiring new     customers, reputation losses, and diminished goodwill ensue; hence,     the hefty cost.</p> </li> <li> <p>The IBM study also found that failing to comply with regulations for     data protection was top among the factors that amplify data breach     costs     ([https://www.ibm.com/downloads/cas/ojdvqgry]{.underline}).</p> </li> </ul> <p>Increased Data Protection Regulations</p> <ul> <li> <p>As technology\u00a0advances, the need to protect customer data has become     more critical. Consumers require and expect privacy protection     during every transaction; many simple activities can risk personal     data, whether online banking or using a phone app.</p> </li> <li> <p>Governments worldwide were initially slow to react by creating laws     and regulations to protect personal data from identity theft,     cybercrime, and data privacy violations.</p> </li> <li> <p>However, times are now changing as data protection laws are     beginning to take shape globally.</p> </li> <li> <p>There are several drivers for the increase in regulations.</p> </li> <li> <p>These include the growth of enormous amounts of data, and we need     more data security and privacy to protect users from nefarious     activities such as identity theft.</p> </li> <li> <p>Let's look at some of the measures taken toward data privacy     in the following subsections.</p> </li> </ul> <p>General Data Protection Regulation (GDPR)</p> <ul> <li> <p>The\u00a0General Data Protection Regulation\u00a0(GDPR) by\u00a0European     Union\u00a0is regarded as\u00a0the first data protection regulation in the     modern data economy and was emulated by many countries to craft     their own.</p> </li> <li> <p>GDPR was proposed in 2012, adapted by the EU Council and Parliament     in 2016, and enforced in May 2018. It superseded the Data Protection     Directive that had been adopted in 1995.</p> </li> <li> <p>What makes GDPR epoch-making is its stress on the protection of PII,     including people's names, locations, racial or ethnic origin,     political or sexual orientation, religious beliefs, association     memberships, and genetic/biometric/health information. Organizations     and individuals both in and outside the EU have to follow the     regulation when dealing with the personal data of EU residents.     There are seven principles of GDPR, among which six were inherited     from the Data Protection Directive; the new principle     is\u00a0accountability, which demands data users maintain documentation     about the purpose and procedure of personal data usage.</p> </li> <li> <p>GDPR has shown the public what the consequences of its violation can     be.</p> </li> <li> <p>Depending on the severity of non-compliance, the GDPR fine can go     from 2% of global annual turnover or \u20ac10 million, whichever is     higher, or 4% of global annual turnover or \u20ac20 million, whichever is     higher.</p> </li> <li> <p>In May 2018, thousands of Europeans filed a complaint against     Amazon.com Inc. through the French organization La Quadrature du     Net, also known as\u00a0Squaring the Net\u00a0in English, accusing the     company of using its\u00a0advertisement targeting system without customer     consent.</p> </li> <li> <p>After 3 years of investigation, Luxembourg's\u00a0National Commission     for Data Protection\u00a0(CNDP) made headlines around the world: it     issued\u00a0Amazon a \u20ac746 million fine.</p> </li> <li> <p>Similarly, WhatsApp was fined by Ireland's Data Protection     Commission in September 2021 for GDPR infringement; again, the     investigation had taken 3 years, and the fine amounted to \u20ac225     million.</p> </li> <li> <p>Currently, in the US, a majority of states have privacy protections     in place or soon will. Additionally, several states have     strengthened existing regulations, such as California, Colorado, and     Virginia.</p> </li> <li> <p>Let's look at each to get an idea of these changes.</p> </li> </ul> <p>California Consumer Privacy Act (CCPA)</p> <ul> <li> <p>The state of California followed suit. The\u00a0California Consumer     Privacy Act\u00a0(CCPA) became effective on January 1, 2020.</p> </li> <li> <p>As the name suggests, the aim of the regulation is to\u00a0protect     consumers' PII just as GDPR does.</p> </li> <li> <p>Compared to GDPR, the scope of the CCPA is significantly limited.</p> </li> <li> <p>The CCPA is applicable only to for-profit organizations that collect     data from over 50,000 points (residents, households, or devices in     the state) in a year, generate annual revenue over $25 million, or     make half of their annual revenue by selling such information.</p> </li> <li> <p>However, CCPA infringement can be much more costly than GDPR     infringement since the former has no ceiling for its fine ($2,500     per record for each unintentional violation; $7,500 per record for     each intentional violation).</p> </li> </ul> <p>Colorado Privacy Act (CPA)</p> <ul> <li>Under the\u00a0Colorado Privacy Act\u00a0(CPA), starting July 1, 2024,     data collectors and controllers\u00a0will have to follow universal     opt-outs that users have selected for generating targeted     advertising and sales. This rule protects residents in Colorado from     targeted sales and advertising as well as certain types of     profiling.</li> </ul> <p>Virginia Consumer Data Protection Act (CDPA)</p> <ul> <li> <p>Virginia's\u00a0Consumer Data Protection Act\u00a0(CDPA) will make     several changes to increase security\u00a0and privacy on January 1, 2023.     These changes will be applicable to organizations that do business     in Virginia or with residents in Virginia. Data collectors need to     obtain approval to utilize their private data. These changes also     try to determine the adequacy of privacy and security of AI vendors,     which may require the removal of that data.</p> </li> <li> <p>These are just a few simple examples of how data regulations will     take shape in the US. What does this look like for the rest of the     world? Some estimate that by 2024, 75% of the global population will     have personal data covered by privacy regulations of one type or     another.</p> </li> <li> <p>Another example of major data protection regulation is     Brazil's\u00a0Lei Geral de Prote\u00e7\u00e3o de Dados Pessoais\u00a0(LGPD)     which has been in force since September 2020.</p> </li> <li> <p>It replaced\u00a0dozens of laws in the country related to data privacy.     LGPD was modeled after GDPR, and the contents are almost identical.</p> </li> <li> <p>In Asia, Japan was the first country to introduce a data protection     regulation: the\u00a0Act on the Protection of Personal     Information\u00a0(APPI) was adopted\u00a0in 2003 and amended in 2015. In     April 2022, the latest version of APPI was put in force to address     modern concerns over data privacy.</p> </li> <li> <p>FL has been\u00a0identified as a critical technology that can work     well with privacy regulations and regulatory compliance in different     domains.</p> </li> </ul> <p>Data Minimalism</p> <ul> <li> <p>Organizations have been acclimatizing to these regulations.</p> </li> <li> <p>TrustArc's\u00a0Global Privacy Benchmarks Survey 2021\u00a0found that the     number of enterprises with a dedicated privacy office is increasing:     83% of respondents in the survey had a privacy office, whereas the     rate was only 67% in 2020. 85% had a strategic and reportable     privacy management program in place, yet 73% of them believed that     they could do more to protect privacy.</p> </li> <li> <p>Their eagerness is hardly surprising as 34% of the respondents     claimed that they had faced a data breach in the previous 3 years,     the\u00a0costly consequences of which was mentioned previously Here.</p> </li> <li> <p>A privacy\u00a0office would be led by a\u00a0data protection     officer\u00a0(DPO) who is responsible for the company's\u00a0Data     Protection Impact Assessment\u00a0(DPIA) in order to comply with     regulations such as GDPR that demand accountability and     documentation of personal data handling.</p> </li> <li> <p>DPOs are also responsible for monitoring and ensuring that personal     data is treated by their organizations in compliance with the law,     and the top management and board are supposed to provide necessary     support and resources to DPOs to allow them to complete their task.</p> </li> <li> <p>In the face of GDPR, the current trend in data protection is     shifting toward\u00a0data minimalism.</p> </li> <li> <p>Data minimalism in this context does not necessarily encourage     minimization of the size\u00a0of data; it pertains more directly to     minimizing PII factors in data so that individuals cannot be     identified with its data points.</p> </li> <li> <p>Therefore, data minimalism affects AI sectors in their ability to     create a high-performing AI application because a shortage in data     variety for the ML process simply generates ML model biases with     unsatisfying performance in prediction.</p> </li> <li> <p>The abundance mindset for big data introduced at the beginning of     the section has thus been disciplined by the public concern over     data privacy. The risk of being fined for violating data protection     regulations, coupled with the wasteful cost of having a data     graveyard, calls for practicing data minimalism rather than data     abundance.</p> </li> <li> <p>That is why FL is becoming a\u00a0must-have\u00a0solution for many AI     solution providers such as medical sectors that are struggling with     public concerns and data privacy, which basically becomes\u00a0an issue     when a third-party entity needs to collect private data for     improving the quality of ML models and their applications.</p> </li> <li> <p>As mentioned, FL is a promising framework for privacy-preserving AI     because learning of the data can happen anywhere; even if the data     is not available for the AI service providers, all we have to do is     collect and aggregate trained ML models in a consistent way.</p> </li> <li> <p>Now, let's consider another facet of the Triple-A mindset for big     data being challenged: acceptance of messy data.</p> </li> </ul> <p>Data Volume, Training Data and Model Bias</p> <ul> <li> <p>Does the sheer volume of big data truly annihilate the treacherous     reality of\u00a0garbage in, garbage out?</p> </li> <li> <p>In\u00a0fact, the messiness of data can only be accepted if\u00a0enough data     from a variety of sources and distributions can be fully learned     without causing any biases in the outcomes of the learning.</p> </li> <li> <p>The actual training of the big data in a centralized location does     take a lot of time and huge computational resources and storage.</p> </li> <li> <p>Also, we would probably have to find methods to measure and reduce     model bias without directly collecting and accessing sensitive and     private data, which would conflict with some of the privacy     regulations discussed previously.</p> </li> <li> <p>FL also has an aspect of distributed and collaborative learning,     which becomes critical to eliminate data and model bias to absorb     the messiness of the data.</p> </li> <li> <p>With collaborative and distributed learning, we could significantly     increase the data accessibility and efficiency of an entire learning     process that is often very expensive and time-consuming.</p> </li> <li> <p>It gives us a chance to break through the limitation that big data     training used to have, as discussed in the following sections.</p> </li> </ul> <p>Training Costs</p> <ul> <li> <p>According to the report:     https://www.flexera.com/blog/cloud/cloud-computing-trends-2022-state-of-the-cloud-report,     37% of enterprises annually spend more than $12 million and 80%     spend over $1.2 million\u00a0per year for public cloud.</p> </li> <li> <p>The training cost over the cloud is not cheap, and it can easily be     assumed that this cost is going to boost significantly, together     with the increasing demand for AI and ML.</p> </li> <li> <p>Sometimes, big data\u00a0cannot be fully trained for ML because\u00a0of the     following issues:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Big data storage: Big data storage is an architecture for     compute and storage that collects and manages large amounts of     datasets for AI applications or real-time analytics. Worldwide     enterprise companies are paying more than $100 billion just for     cloud storage and data center costs     (https://a16z.com/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-repatriation-optimization/).     While some of the datasets are critical for the\u00a0applications they     provide, what they really want is often\u00a0business     intelligence\u00a0that can be extracted from the data, not just the     data itself.</p> <ul> <li> <p>Significant training time: Building and training an ML model     that can be delivered as an authentic product basically takes a     significant amount of time, not only for the training process but     also for the preparation of the ML pipelines. Therefore, in many     cases, the true value of the intelligence is going to be lost by the     time the ML model is delivered.</p> </li> <li> <p>Huge computation: Training of an ML model often consumes     significant computational resources. For example, an ML task of     manipulating pieces such as a Rubik's Cube using a robotic hand     could sometimes require more than 1,000 computers. It could also     take a dozen machines just to run some specialized graphics chips     for several months.</p> </li> <li> <p>Communications latency: To form big data, especially in the     cloud, a significant amount of data needs to be transferred to the     server, which in itself causes communications latency. In most use     cases, FL requires much less data to be transferred from local     devices or learning environments to a server called an aggregator     that is there to synthesize the local ML models collected from those     devices.</p> </li> <li> <p>Scalability: In traditional centralized systems, scalability     becomes an issue because of the complexity of big data and its     costly infrastructures such as huge storage and computing resources     in the cloud server environment. In an FL server, only an     aggregation is conducted to synthesize the multiple local models     that have been trained to update the global model. Therefore, both     the system and\u00a0learning scalability increase significantly as ML     training is conducted on edge devices in a distributed manner, not     only in a single centralized learning server.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   FL effectively utilizes distributed computational resources that can     be used for light training of the ML models.</p> <ul> <li> <p>Whether training happens on actual physical devices or virtual     instances of the cloud system, parallelizing the model training     process into distributed environments often accelerates the speed of     learning itself.</p> </li> <li> <p>In addition, once the trained\u00a0models are collected, the FL system     can quickly synthesize them to generate an updated ML model called a     global model that absorbs enough learnings at the edge sides, and     thus delivering the intelligence in near real time is possible.</p> </li> </ul> <p>Model bias and training data</p> <ul> <li> <p>ML bias happens when an ML\u00a0algorithm generates results that are     systemically prejudiced because\u00a0of erroneous assumptions in\u00a0the ML     process. ML bias is also\u00a0sometimes called algorithm bias or AI bias.</p> </li> <li> <p>Yann LeCun, the 2018 Turing Award winner for his outstanding     contribution to the development of DL, says \"ML systems are biased     when data is biased\"     ([https://twitter.com/ylecun/status/1274782757907030016]{.underline}).</p> </li> <li> <p>This comes from a\u00a0computer vision\u00a0(CV) model trained with     the\u00a0Flickr-Faces-HQ\u00a0dataset compiled by the Nvidia team.</p> </li> <li> <p>Based on the face upsampling system, many people are classified     as\u00a0white as the network was pre-trained on\u00a0Flickr-Faces-HQ\u00a0data     mainly containing pictures of white people.</p> </li> <li> <p>For this problem of misclassification of the people, the     architecture of the model is not the issue that mandates this     output.</p> </li> <li> <p>Hence, the conclusion is that a racially skewed dataset generated a     neutral model to produce biased outcomes.</p> </li> <li> <p>Productive conversations about AI and ML biases have been led by the     former lead of AI Ethics at Google.</p> </li> <li> <p>The 2018 publication of the\u00a0Gender Shades\u00a0paper demonstrated race     and gender bias in major facial recognition models, and lawmakers in     Congress have sought to prohibit the use of the technology by the US     federal government.</p> </li> <li> <p>Tech companies including Amazon, IBM, and Microsoft also agreed to     suspend or terminate sales of facial recognition models to the     police.</p> </li> <li> <p>They are encouraged to use an interventionist approach to data     collection by advising scientists and engineers to specify the     objectives of model development, form a strict policy for data     collection, and conduct a thorough appraisal of collected data to     avoid biases---details are available on the\u00a0FATE/CV\u00a0website     (https://sites.google.com/view/fatecv-tutorial/home).</p> </li> <li> <p>FL could be one of the most promising ML technologies to overcome     data-silo issues.</p> </li> <li> <p>Very often, the data is not even be accessible or usable for the     training, causing a significant bias in data\u00a0and models.</p> </li> <li> <p>Naturally, FL is useful for overcoming bias by\u00a0resolving the issues     of data privacy and silos that become the bottleneck to     fundamentally avoiding data bias.</p> </li> <li> <p>In this context, FL is becoming a breakthrough in the implementation     of big data services and applications, as thoroughly investigated     in\u00a0[https://arxiv.org/pdf/2110.04160.pdf]{.underline}.</p> </li> <li> <p>Also, there are several techniques that try to mitigate model bias     in FL itself, such as\u00a0Reweighing\u00a0and\u00a0Prejudice Remover, both     detailed     in\u00a0[https://arxiv.org/pdf/2012.02447.pdf]{.underline}.</p> </li> </ul> <p>Model drift &amp; performance degradation</p> <ul> <li> <p>Model drift\u00a0is generally\u00a0about the degradation of ML model     performance because of\u00a0changes in data and relationships     between\u00a0input and output\u00a0(I/O) variables, known as\u00a0model     decay, as well.</p> </li> <li> <p>Model\u00a0drift can be addressed by continuous learning to adapt to     the\u00a0latest changes in datasets or environments in near real time.</p> </li> <li> <p>One of the important aspects of FL is realizing a continuous     learning framework by updating an ML model instantly whenever the     learning happens in the local distributed environment anytime, in a     consistent manner.</p> </li> <li> <p>That way, FL could resolve the situation often seen in enterprise AI     applications where the intelligence is useless by the time it is     delivered for production.</p> </li> <li> <p>We will now touch\u00a0on how models could get degraded or stop working,     and then some of the current efforts of\u00a0model     operations\u00a0(ModelOps) to continuously improve the performance     of models and achieve sustainable AI operations.</p> </li> </ul> <p>How models can stop working</p> <ul> <li> <p>Any AI and ML model with\u00a0fixed parameters, or\u00a0weights, generated     from the training data and adjusted to the test data can perform     fairly well when deployed in an environment where the model receives     data similar to the training and test data.</p> </li> <li> <p>If an autonomous driving model is well trained with data recorded     during sunny daytime, the model can drive vehicles safely on sunny     days because it is doing what it has been trained to do.</p> </li> <li> <p>On a rainy night, however, nobody should be in or near the vehicle     if it is autonomously driven: the model is fed with totally     unfamiliar, dark, and blurry images; its decisions will not be     reliable at all. In such a situation, the\u00a0model's decision will be     far off the track, hence the name\u00a0model drift.</p> </li> <li> <p>Again, model drift is not likely to happen\u00a0if the model is deployed     in an environment similar to the training and testing environment     and if the environment does not change significantly over time.</p> </li> <li> <p>But in many business situations, that assumption does not always     hold, and model drift becomes a serious issue.</p> </li> <li> <p>There are\u00a0two types of model drifts:\u00a0data drift\u00a0and\u00a0concept     drift.</p> </li> <li> <p>Data drift happens when input\u00a0data to a deployed model is     significantly different from the data the\u00a0model has been trained     with. In other words, changes in\u00a0data distribution\u00a0are the cause     of data drift. The aforementioned diurnal autonomous vehicle model     not performing well in the nighttime is an example of data drift.     Another example would be an ice-cream sale prediction model trained     in California being deployed in New Zealand; seasonality in the     southern hemisphere is opposite to that in the northern hemisphere,     and the estimated sales of ice cream will be low for\u00a0summer\u00a0and     high for\u00a0winter, on the contrary to the actual sales volume.</p> </li> <li> <p>Concept drift, on the other hand, is a result of changes in how     variables correlate with each other. In the terminology of     statistics, this implies that the\u00a0data-generating process\u00a0has been     altered. And this is what\u00a0Google Flu Trends (GFT)\u00a0suffered from,     as the author of\u00a0The Undercover Economist\u00a0put it in the     following\u00a0Financial Times\u00a0article:     https://www.ft.com/content/21a6e7d8-b479-11e3-a09a-00144feabdc0#axzz30qfdzLCB.</p> </li> <li> <p>Prior to the period, search queries were meaningfully correlated     with the spread of flu as mainly people who suspected that they were     infected typed those words in the browser, and therefore the model     worked successfully.</p> </li> <li> <p>This may no longer have been the case in 2013 since people in other     categories, such as those who were precocious about a potential     pandemic or those who were just curious, were searching for\u00a0those     words, and they may have been led to do so by Google's     recommendations.</p> </li> <li> <p>This concept drift likely made GFT overestimate the spread vis-\u00e0-vis     medical reports provided by the\u00a0Centers for Disease Control and     Prevention\u00a0(CDC).</p> </li> <li> <p>Either by data or by concept, model drift causes model performance     degradation, and it occurs because of our focus on correlation.</p> </li> <li> <p>The\u00a0ground truth\u00a0in data science parlance does not mean something     like the universal truth in hard science such as physics and     chemistry---that is, causation.</p> </li> <li> <p>It is merely a true statement about how variables in given data     correlate with each other in a particular environment, and it     provides no guarantee that the correlation holds when the     environment changes or differs.</p> </li> <li> <p>This is to say that what we estimate as the\u00a0ground truth\u00a0can vary     over time and locations, just like the\u00a0ground\u00a0has been reshaped by     seismic events throughout history and geography.</p> </li> </ul> <p>Continuous monitoring</p> <ul> <li> <p>In a survey commissioned by Redis Labs     (https://venturebeat.com/business/redis-survey-finds-ai-is-stressing-it-infrastructure-to-breaking-point/),     about half of the respondents cited model reliability (48%), model     performance (44%), accuracy over time (57%), and latency of running     the model (51%) as the top challenges for getting models deployed.</p> </li> <li> <p>Given the risk and concern of model drift, AI and ML model     stakeholders need to\u00a0work on two additional tasks after deployment.     First, model performance must be continuously monitored to detect     model drift. Both data drift and concept drift can take place     gradually or suddenly. Once model drift is detected, the model needs     to be retrained with new training data, and when concept drift     occurs, even the use of a new model architecture may be necessary to     upgrade the model.</p> </li> <li> <p>In order to address\u00a0these requirements, a new ML principle     called\u00a0Continuous Delivery for Machine Learning\u00a0(CD4ML) has     been proposed.</p> </li> <li> <p>In the framework of CD4ML,</p> <ul> <li> <p>A model is coded and trained with training data in the first     step.</p> </li> <li> <p>The model is then tested with a separate dataset and evaluated     based on some metrics, and more often than not, the best model     is selected from multiple candidates.</p> </li> <li> <p>Next, the selected model is productionized with a further test     to make sure that the model performs well after the deployment,     and once it passes the test, it is deployed.</p> </li> <li> <p>Here, the monitoring process starts.</p> </li> <li> <p>When model drift is observed, the model will be retrained with     new data or given a new architecture, depending on the severity     of the drift.</p> </li> <li> <p>If you are familiar with software\u00a0engineering, you might have     noticed that CD4ML is the adoption of\u00a0continuous     integration/continuous delivery\u00a0(CI/CD) in the field of     ML.</p> </li> <li> <p>In a similar vein,\u00a0ModelOps, an AI and ML\u00a0operational     framework stemming from     the\u00a0development-operations\u00a0(DevOps) software engineering     framework is gaining popularity.</p> </li> <li> <p>ModelOps bridges\u00a0ML operations\u00a0(MLOps: the integration     of\u00a0data engineering and data science) and application     engineering; it can be seen as the enabler of CD4ML.</p> </li> </ul> </li> <li> <p>The third factor of the\u00a0Triple-A mindset for big data\u00a0lets us     focus on correlation and has helped in building AI and ML models     rapidly over the last decade.</p> </li> <li> <p>Finding correlation is much easier than discovering causation.</p> </li> <li> <p>For many AI and ML models that have been telling us what we need to     know from people's Google search patterns over years, we have to     check if it still works today. And so do we tomorrow.</p> </li> <li> <p>That is why FL is one of the important approaches for continuous     learning. When creating and operating an FL system, it is also     important to develop the system with ModelOps functionalities,     as\u00a0the critical role of FL is to keep improving models constantly     from various learning environments in a collaborative manner.</p> </li> <li> <p>It is even possible to realize a\u00a0crowdsourced learning\u00a0framework     with FL so that people in the platform can take the\u00a0desired ML model     to adapt and train it locally and return an updated model to the FL     server with an aggregator.</p> </li> <li> <p>With an advanced model aggregation framework to filter out poisonous     ML models that could potentially degrade the current models, FL can     consistently integrate other learnings, and thus realize a     sustainable continuous learning operation that is key for the     platform with ModelOps functionalities.</p> </li> </ul> <p>The FL Solution Approach</p> <ul> <li> <p>We confirmed that big data has issues to be addressed.</p> </li> <li> <p>Data privacy must be\u00a0preserved in order to protect not only     individuals but also data users who would face risks of data     breaches and subsequent fines.</p> </li> <li> <p>Biases in a set of big data can affect inference significantly     through proxies, even when factors about gender and race are     omitted, and focus on correlation rather than causation makes     predictive models vulnerable to model drift.</p> </li> <li> <p>Let us discuss\u00a0the difference between a\u00a0traditional big data ML     system and an FL system in terms of their architectures, processes,     issues, and benefits.</p> </li> <li> <p>The following diagram depicts a visual comparison between a     traditional big data ML system and an FL system:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.716666666666667in\"}</p> <p>Figure 1.3 -- Comparison between traditional big data ML system and FL system</p> <ul> <li> <p>In the traditional\u00a0big data system, data is\u00a0gathered to create large     data stores. These large data stores are used to solve a specific     problem using ML. The resulting model displays strong     generalizability due to the volume of data it is trained on and is     eventually deployed.</p> </li> <li> <p>However, continuous data collection uses large amounts of     communication bandwidth. In privacy-focused applications, the     transmission of data may be banned entirely, making model creation     impossible. Training large ML models on big data stores is     computationally expensive, and traditional centralized training     efficiency is limited by single-machine performance. Slow training     processes lead to long delays between incremental model updates,     leading to a lack of flexibility in accommodating new data trends.</p> </li> <li> <p>On the other hand, in an FL system, ML training is performed     directly at the location of the data. The resulting\u00a0trained models     are collected at the central server. Aggregation algorithms are used     to produce an aggregated model from the collected models. The     aggregated model is sent back to the data locations for further     training.</p> </li> <li> <p>FL approaches often incur overhead to set up and maintain training     performance with distributed-system settings. However, even with a     bit more complicated architecture and settings, there are benefits     that exceed its complexity drawbacks.</p> </li> <li> <p>Training is performed at the data location, so data is never     transmitted, maintaining data privacy.</p> </li> <li> <p>Training can be performed asynchronously across a variable number of     nodes, which results in efficient and easily scalable distributed     learning.</p> </li> <li> <p>Only model weights are transmitted between server and nodes, thus FL     is efficient in communication.</p> </li> <li> <p>Advanced aggregation algorithms can maintain training performance     even in restricted scenarios and increase efficiency in standard ML     scenarios too.</p> </li> <li> <p>The vast majority of all AI projects do not seem to be delivered, or     simply fall short altogether.</p> </li> <li> <p>To deliver an authentic AI application and product, all the issues     discussed previously need to be considered seriously. It is obvious     that FL, together with other key\u00a0technologies to deal with local     data processed by the ML pipeline and engine, is getting to be a     critical solution to resolve data-related problems in a continuous     and collaborative manner.</p> </li> <li> <p>How can we harness the power of AI and ML to optimize the technical     system for society in its entirety---that is, bring about a more     joyous, comfortable, convenient, and safe world while being data     minimalistic and ethical, as well as delivering improvements     continuously?</p> </li> <li> <p>We contend that the key is a\u00a0collective     intelligence\u00a0or\u00a0intelligence-centric\u00a0platform.</p> </li> <li> <p>In subsequent sections, we introduce the concept, design, and     implementation of an FL system as a promising technology for     orchestrating collective intelligence with networks of AI and ML     models to fulfill those requirements discussed so far.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>This section provided an overview of how FL could potentially solve     many of the big data issues by first understanding the definition of     big data and its nature, involving an abundance of observations,     acceptance of messiness, and ambivalence of causality.</p> </li> <li> <p>We have learned about privacy regulations in a variety of forms from     many regions and the risk of data breaches and privacy violations     that eventually lead to loss of profits, as well as a bottleneck in     creating authentic AI applications. Federated learning, by design,     will not collect any raw data and can preserve data privacy and     follow those regulations.</p> </li> <li> <p>In addition, with an FL framework, we can reduce inherent bias that     affects the performance of ML models and minimize model drift with a     continuous learning framework. Thus, a distributed and collaborative     learning framework such as FL is required for a more cost-effective     and efficient approach based on FL.</p> </li> <li> <p>This introductory section concluded with the potential of FL as a     primary solution for the aforementioned big data problems based on     the paradigm-shifting idea of collective intelligence that could     potentially replace the current mainstream data-centric platforms.</p> </li> <li> <p>In the next section, we will see where in the landscape of data     science FL fits and how it can open a new era of ML.</p> </li> </ul> <p>[Section 2: All about FL __________________]{.underline}</p> <p>What Is Federated Learning?</p> <ul> <li> <p>This section frames\u00a0federated learning\u00a0(FL) as the answer to     the desire for this new ML approach.</p> </li> <li> <p>In a nutshell, FL is an approach to ML that allows models to be     trained in parallel across data sources without the transmission of     any data.</p> </li> <li> <p>The goal of This section is to build up the case for the FL     approach, with explanations of the necessary conceptual building     blocks in order to ensure that you can achieve a similar     understanding of the technical aspects and practical usage of FL.</p> </li> <li> <p>After going through this section, you should have a high-level     understanding of the FL process and should be able to visualize     where the approach slots into real-world problem domains.</p> </li> </ul> <p>Topics covered:</p> <ul> <li> <p>Understanding the current state of ML</p> </li> <li> <p>Distributed learning nature -- toward scalable AI</p> </li> <li> <p>Understanding FL</p> </li> <li> <p>FL system considerations</p> </li> </ul> <p>ML - Current State</p> <ul> <li>To understand why the\u00a0benefits derived from the application of FL     can outweigh the increased complexity of this approach, it is     necessary to understand how ML is currently practiced and the     associated limitations. The goal of this section is to provide you     with this context.</li> </ul> <p>The Model</p> <ul> <li> <p>The term \"model\" finds usage across numerous different disciplines;     however, the generalized definition we are interested in can be     narrowed down to a working representation of the dynamics within     some desired system.</p> </li> <li> <p>Simply put, we develop a model B of some\u00a0phenomenon A as a means of     better understanding A through the increased interaction offered by     B.</p> </li> <li> <p>Consider the phenomenon of an object being dropped from some point     in a vacuum.</p> </li> <li> <p>Using kinematic equations, we can compute exactly how long it will     take for the object to hit the ground -- this is a model of the     aforementioned phenomenon.</p> </li> <li> <p>The power of this approach is the ability to observe results from     the created model without having to explicitly interact with the     phenomenon in question.</p> </li> <li> <p>For example, the model of the falling object allows us to determine     the difference in fall time between a 10 kg object and a 50 kg     object at some height without having to physically drop real objects     from said height in a real vacuum.</p> </li> <li> <p>Evidently, the modeling of natural phenomena plays a key role in     being able to claim a true understanding of said phenomena.</p> </li> <li> <p>Removing the need for the comprehensive observation of a phenomenon     allows for true generalization in the decision-making process.</p> </li> <li> <p>The concept of a model is greatly narrowed down within the context     of computer science. In this context, models are algorithms that     allow for some key values of a phenomenon to be output given some     initial characterization of the phenomenon in question.</p> </li> <li> <p>Going back to the falling object example, a computer science model     could entail the computation of values such as the time to hit the     ground and the maximum speed given the mass of the object and the     height from which it is dropped.</p> </li> <li> <p>These computer science models are uniquely powerful due to the     superhuman ability of computers to compute the output from countless     starting phenomenon configurations, offering us even greater     understanding and generalization.</p> </li> <li> <p>So, how do we\u00a0create such models?</p> </li> <li> <p>The first and simplest approach is building rule-based systems     or\u00a0white-box\u00a0models.</p> </li> <li> <p>A white-box (also known as\u00a0glass-box or clear-box) model is made\u00a0by     writing down the underlying functions of a system of interest     explicitly.</p> </li> <li> <p>This is only possible when information about the system is     available\u00a0a priori.</p> </li> <li> <p>Naturally, in this case, the underlying functions are relatively     simple.</p> </li> <li> <p>One such example is the problem of classifying a randomly selected     integer as odd or even; we can easily write an algorithm to do this     by\u00a0checking the remainder after dividing the integer by two.</p> </li> <li> <p>If you want to see how much it costs to fill up your gas tank, given     how empty the tank is and the price per gallon, you can just     multiply those values together.</p> </li> <li> <p>Despite their simplicity, these examples illustrate that simple     models can have a lot of practical applications in various fields.</p> </li> <li> <p>Unfortunately, the white-box modeling of underlying functions can     quickly become too complex to perform directly. In general, systems     are often too complex for us to be able to construct a\u00a0white-box     model for.</p> </li> <li> <p>For example, let's say you want to predict the future values of your     property.</p> </li> <li> <p>You have a lot of metrics about the property, such as the area, how     old it is, its location, and interest rate to name but a few.</p> </li> <li> <p>You believe that there is likely a linear relationship between the     property value and all of those metrics, such that the weighted sum     of all of them would give you the property value.</p> </li> <li> <p>Now, if you actually try to build a white-box model under that     assumption, you will have to directly figure out what the parameter     (weight) for each metric is, which implies that you must know the     underlying function of the real\u00a0estate pricing system. Usually, this     is not the case.</p> </li> <li> <p>Therefore, we need another approach:\u00a0black box\u00a0modeling.</p> </li> </ul> <p>Automating Model Creation</p> <ul> <li> <p>The concept of a black box\u00a0system was first developed in the field     of electric circuits during the WWII period.</p> </li> <li> <p>It was the famous cybernetician Norbert Wiener who began treating     the black box as an abstract concept, and a general theory was     established by Mario Augusto Bunge in the 1960s.</p> </li> <li> <p>The function for estimating future property values, as illustrated     earlier, is a good example of a black box.</p> </li> <li> <p>As you might expect, the function is complex enough that it is     unreasonable for us to try to write a white-box model to represent     it.</p> </li> <li> <p>This is where ML comes in, allowing us to create a model as a black     box.</p> </li> </ul> <p>Reference</p> <ul> <li> <p>You might be aware that black box modeling has been criticized for     its lack of interpretability, an important concept outside the scope     of this tutorial; Interpretable Machine Learning covers this area.</p> </li> <li> <p>ML is a type of artificial intelligence that is used to     automatically generate model parameters for making decisions and     predictions.\u00a0</p> </li> <li> <p>Figure 2.1\u00a0illustrates this in a very simple way: those cases     where the known values and the unknown value have a linear     relationship allow a popular algorithm, called\u00a0ordinary least     squares\u00a0(OLS), to be applied. OLS computes\u00a0the unknown     parameters of the linear relationship by finding the set of     parameters that produces the closest predictions on some set of     known examples (pairs of input feature value sets and the true     output value):</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.7034722222222225in\"}</p> <p>Figure 2.1 -- ML determining model parameters</p> <ul> <li> <p>The preceding diagram\u00a0displays a simple two-dimensional linear     regression problem with one feature/input variable and one output     variable. In this toy two-dimensional case, it might be relatively     straightforward for us to come up with the parameters representing     the best-fit relationship directly, either through implicit     knowledge or through testing different values.</p> </li> <li> <p>However, it should be clear that this approach quickly becomes     intractable as the number of feature variables increases.</p> </li> <li> <p>OLS allows us to attack this problem from the reverse direction:     instead of producing linear relationships and evaluating them on the     data, we can use the data to compute the parameters of the best-fit     relationship directly instead.</p> </li> <li> <p>Revisiting the real estate problem, let's assume that we have     collected a large number of property valuation data points,     consisting of the associated metric values and the sale price.</p> </li> <li> <p>We can apply OLS to take these points and find the relationship     between each metric and the sale\u00a0price for any property (still under     the assumption that the true relationship is linear).</p> </li> <li> <p>From this, we can pass in the metric values of our property and get     the predicted sale price.</p> </li> <li> <p>The power of this approach is the abstraction of this relationship     computation from any implicit knowledge of the problem.</p> </li> <li> <p>The OLS algorithm doesn't care what the data represents -- it just     finds the best line for the data it is given.</p> </li> <li> <p>This class of approaches is exactly what ML entails, granting the     power to create models of phenomena without any required knowledge     of the internal relationship, given a sufficient amount of data.</p> </li> <li> <p>In a nutshell, ML lets us program algorithms that can learn to     create models from data, and our motivation to do so is to     approximate complex systems.</p> </li> <li> <p>It is important to keep in mind that the underlying functions of a     complex system can change over time due to outside factors, quickly     making models created from old data obsolete.</p> </li> <li> <p>For example, the preceding linear regression model might not work to     estimate property values in a far distant future or a faraway     district.</p> </li> <li> <p>Variance in such a macroscopic scale is not taken into account in a     model containing only a few dozen parameters, and we would need     different models for separate groups of adjacent data points --     unless we employ even more sophisticated ML approaches such     as\u00a0deep learning.</p> </li> </ul> <p>Deep Learning (DL)</p> <ul> <li> <p>So, how did deep learning\u00a0become synonymous with ML in common usage?</p> </li> <li> <p>Deep learning\u00a0involves the application of a\u00a0deep neural     network\u00a0(DNN), which is a type of highly-parameterized model     inspired by the transmission of signals between neurons in the     brain.</p> </li> <li> <p>The foundation of deep learning was established in the early 1960s     by Frank Rosenblatt, who is known as the\u00a0father of deep learning.</p> </li> <li> <p>His work was further developed in the 1970s and 1980s by computer     scientists including Geoffrey Hinton, Yann LeCun, and Yoshua Bengio,     and the term\u00a0deep learning\u00a0was popularized by the University of     California, Irvine's distinguished Professor Rina Dechter. Deep     learning can conduct much more complex tasks compared to simpler ML     algorithms such as linear regression.</p> </li> <li> <p>While the specifics are beyond the scope of this book, the key     problem that deep learning was able to solve was the modeling of     complex non-linear relationships, pushing ML as a whole to the     forefront of numerous fields due to the increased modeling ability     it provided.</p> </li> <li> <p>This ability has been\u00a0mathematically proven via specific universal     approximation theorems for different model size cases.</p> </li> <li> <p>Over the past decade, ever-increasingly powerful models have been     built by tech giants against the backdrop of big data.</p> </li> <li> <p>If we look at the state-of-the-art deep learning models today, they     could have up to trillions of parameters; expectedly, this gives     them unparalleled flexibility in modeling complex functions.</p> </li> <li> <p>The reason deep learning models can be scaled up to arbitrarily     increase performance, unlike other ML model types used previously,     is due to\u00a0a phenomenon called\u00a0double descent.</p> </li> <li> <p>This refers to the ability for a certain parameterization/training     threshold to overcome the standard bias-variance trade-off (where     increasing complexity leads to fine-tuning on training data,     reducing bias but increasing variance) and continuing to increase     performance.</p> </li> <li> <p>The key takeaway is that the performance of deep learning models can     be considered to be limited by just the available compute power and     data, two factors that have surged in growth in the past 10 years     due to advances in computing and the ever-increasing number of     devices and software collecting data, respectively.</p> </li> <li> <p>Deep learning has become intertwined with ML, with deep learning     playing a significant role within the current state of ML and big     data.</p> </li> <li> <p>This section focused on establishing a case for the importance of     the modeling performed by current ML techniques. In a sense, this     can be considered the\u00a0what\u00a0-- what exactly FL is trying to do.</p> </li> <li> <p>Next, we will focus on the\u00a0where\u00a0in terms of the desired setting     for numerous ML applications.</p> </li> </ul> <p>Distributed Learning and Scalability</p> <ul> <li> <p>In this section, we introduce the distributed computing setting and     discuss the intersection of this setting with ML approaches to fully     establish the support for why FL is necessary.</p> </li> <li> <p>The goal of the section is for the user to understand both the     benefits and limitations imposed by the distributed computing     setting, in order to understand how FL addresses some of these     limitations.</p> </li> </ul> <p>Distributed computing</p> <ul> <li> <p>The past several years have\u00a0shown a large but predictable rise in     the development of new approaches and the conversion of existing     server infrastructure within the lens of distributed computing.</p> </li> <li> <p>To generalize further, distributed approaches themselves have     shifted more and more from research implementations to extensive use     in production settings; one significant example of this phenomenon     is the usage of cloud computing platforms such as AWS     from\u00a0Amazon,\u00a0Google Cloud Platform\u00a0(GCP) from Google, and     Azure from Microsoft. It turns out that the flexibility of     on-demand resources allows for cost-saving and efficiency in     numerous applications that would, otherwise, be bottlenecked by     on-premise servers and computational power.</p> </li> <li> <p>While a parallel cannot exactly be drawn between cloud computing and     the concept of distributed computing, the key benefits stemming from     the distributed nature are similar.</p> </li> <li> <p>At a high level, distributed computing involves spreading the work     necessary for some computational task over a number of computational     agents in a way that allows each to act near-autonomously.</p> </li> <li> <p>The following figure shows the difference between centralized and     distributed approaches in the high-level context of answering     questions:</p> </li> </ul> <p>Figure 2.2 -- Centralized versus distributed question answering</p> <ul> <li> <p>In this simple example, the centralized approach involves processing     the input questions sequentially, whereas the distributed\u00a0approach     is able to process each question at the same time. It should be     clear that the parallel approach trades off computational resource     usage for increased answering speed.</p> </li> <li> <p>The question, then, is whether this trade-off is beneficial for     real-world applications.</p> </li> </ul> <p>A Real-world example</p> <ul> <li> <p>To understand the\u00a0practical benefits of distributed computing     approaches, let's analyze an example business problem through a     traditional and a distributed computing lens.</p> </li> <li> <p>Consider an e-commerce business that is trying to host its website     using on-premise servers.</p> </li> <li> <p>The traditional way to do this would be to perform enough analysis     on the business side to determine the expected volume of traffic at     some future time and invest in one or a couple of server machines     large enough to handle that calculated volume.</p> </li> <li> <p>Several cases immediately lend themselves to showing the flaws of     such an approach.</p> </li> <li> <p>Consider a scenario where usage of the websites greatly exceeds the     initial projections.</p> </li> <li> <p>A fixed number of servers means that all upgrades must be hardware     upgrades, resulting in old hardware that had to be purchased and is     no longer used.</p> </li> <li> <p>Going further, there are no guarantees that the now-increased usage     will stay fixed. Further increases in usage will result in more     scaling-up costs, while decreases in usage will lead to wasted     resources (maintaining large servers when smaller machines would be     sufficient).</p> </li> <li> <p>A key point is that the integration of additional servers is     non-trivial due to the single-machine approach used to manage     hosting.</p> </li> <li> <p>Additionally, we have to consider the hardware limitations of     handling large numbers of requests in parallel with one or a few     machines.</p> </li> <li> <p>The ability to handle requests in parallel is limited for each     machine -- significant volumes of traffic would be almost guaranteed     to eventually be bottlenecked regardless of the power available to     each server.</p> </li> <li> <p>In comparison, consider the distributed computing-based solution for     this problem.</p> </li> <li> <p>Based on the initial business projections, a number of smaller     server machines are purchased and each is set up to handle some     fixed volume of traffic.</p> </li> <li> <p>If the scenario of incoming traffic\u00a0exceeding projects arises, no     modification to the existing machines is necessary; instead, more     similarly-sized servers can be purchased and configured to handle     their designated volume of new traffic.</p> </li> <li> <p>If the incoming traffic decreases, the equivalent number of servers     can be shut down or shifted to handle other tasks. This means that     the same hardware can be used for variable volumes of traffic.</p> </li> <li> <p>This ability to scale quickly to handle the necessary computational     task at any moment is precisely due to how distributed computing     approaches allow for computational agents to seamlessly start and     stop working on said task.</p> </li> <li> <p>In addition, the use of many smaller machines in parallel, versus     using fewer larger machines, means that the number of requests that     can be handled at the same time is notably higher. It is clear that     a distributed computing approach, in this case, lends itself to     cost-saving and flexibility that cannot be matched with more     traditional methods.</p> </li> </ul> <p>Distributed Computing - The benefits</p> <ul> <li> <p>In general, distributed computing\u00a0approaches offer three main     benefits for any computational task -- scalability, throughput, and     resilience.</p> </li> <li> <p>In the previous case of web hosting, scalability referred to the     ability to scale the number of servers deployed based on the amount     of incoming traffic, whereas throughput refers to the ability to     reduce request processing latency through the inherent parallelism     of smaller servers.</p> </li> <li> <p>In this example, resilience could refer to the ability of other     deployed servers to take on the load from a server that stops     working, allowing the hosting to continue relatively unfazed.</p> </li> <li> <p>Distributed computing often finds uses when working with large     stores of data, especially when attempting to perform analyses on     the data using a single machine would be computationally infeasible     or otherwise undesirable.</p> </li> <li> <p>In these cases, scalability allows for the deployment of a variable     number of agents based on factors such as the desired runtime and     amount of data at any given time, whereas the ability of each agent     to\u00a0autonomously work on processing a subset of the data in parallel     allows for processing throughput that would be impossible for a     single high-power machine to achieve.</p> </li> <li> <p>It turns out that this lack of reliance on cutting-edge hardware     leads to further cost savings, as hardware price-to-performance     ratios are often not linear.</p> </li> <li> <p>While the development of parallelized software to operate in a     distributed computing setting is non-trivial, hopefully, it is clear     that many practical computational tasks greatly benefit from the     scalability and throughput achieved by such approaches.</p> </li> </ul> <p>Distributed ML</p> <ul> <li> <p>When thinking about the types of computational tasks that have     proven to be valuable in practical applications\u00a0and that might be     directly benefited from increased scalability and throughput, it is     clear that the rapidly growing field of ML is near the top.</p> </li> <li> <p>In fact, we can frame ML tasks as a specific example of the     aforementioned tasks of analyzing large stores of data, placing     emphasis on the data being processed and the nature of the analysis     being performed.</p> </li> <li> <p>The joint growth of cheap computational power (for example, smart     devices) and the proven benefits of data analysis and modeling have     led to companies with both the storage of excessive amounts of data     and the desire to extract meaningful insights and predictions from     said data.</p> </li> <li> <p>The second part is exactly what ML is geared to solve, and large     amounts of work have already been completed to do so in various     domains.</p> </li> <li> <p>However, like other computational tasks, performing ML on large     stores of data often leads to a time-computational power trade-off     in which more powerful machines are needed to perform such tasks in     reasonable amounts of time.</p> </li> <li> <p>As ML algorithms become more computationally and memory-intensive,     such as recent state-of-the-art deep learning models with billions     of parameters, hardware bottlenecks make increasing the     computational power infeasible.</p> </li> <li> <p>As a result, current ML tasks must apply distributed computing     approaches to stay cutting-edge while producing results in usable     timeframes.</p> </li> </ul> <p>ML at the Edge</p> <ul> <li> <p>Although the prevalence of deep learning described earlier, besides     the paradigm shift from big data to collective\u00a0intelligence gives     enough motivation for distributed ML, its physical foundation came     from the recent development of\u00a0edge computing.</p> </li> <li> <p>The\u00a0edge\u00a0represents the close\u00a0proximity around deployed solutions;     it follows that edge computing refers to processing data at or near     the location of the data source.</p> </li> <li> <p>Extending the concept of computation to ML leads to the idea     of\u00a0edge AI, where models\u00a0are integrated directly into edge     devices.</p> </li> <li> <p>A few popular examples would be Amazon Alexa, where edge AI takes     care of speech recognition, and self-driving cars that collect     real-world data and incrementally improve with edge AI.</p> </li> <li> <p>The most ubiquitous example is the smartphone -- some potential uses     are the recommendation of content to the user, searches with voice     assistance and auto-complete, auto-sorting of pictures into an album     and gallery search, and more.</p> </li> <li> <p>To capitalize on this potential, smartphone manufacturers have     already begun integrating ML-focused processor components into the     chips they integrate with their newest phones, such as the\u00a0Neural     Processing Unit\u00a0from\u00a0Samsung\u00a0and the\u00a0Tensor Processing Unit\u00a0on     the\u00a0Google Tensor chip.</p> </li> <li> <p>Google has also worked to develop ML-focused APIs for Android     applications through their\u00a0Android ML Kit SDK.</p> </li> <li> <p>From this, it should be clear that ML applications are shifting     toward the edge computing paradigm.</p> </li> <li> <p>Let's say that smartphones need to use a deep learning model for     word recommendation. This is so that when you type words on your     phone, it gives you suggestions for the next word, with the goal     being to save you some time.</p> </li> <li> <p>In the scheme of a centralized computing process, the central server     is the only component that has access to this text prediction model     and none of the phones have the model stored locally.</p> </li> <li> <p>The central server handles all of the requests sent from the phones     to return word recommendations.</p> </li> <li> <p>As you type, your phone has to send what has been typed along with     some personal information about you, all the way to the central     server. The server receives this information, makes a prediction     using the deep learning model, and then sends the result back to the     phone.</p> </li> <li> <p>The following figure reflects this scenario:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"2.5076388888888888in\"}</p> <p>Figure 2.3 -- Centralized inference scenario</p> <ul> <li> <p>There are a few problems that become apparent when you look at this     scenario.</p> </li> <li> <p>First, even a half to one second of latency makes the recommendation     slower than typing everything yourself, making the system useless.</p> </li> <li> <p>Furthermore, if there is no internet connection, the recommendation     simply does not work.</p> </li> <li> <p>Another restriction of this scheme is the need\u00a0for the central     server to process all of these requests. Imagine how many     smartphones are being used in the world, and you will realize a lack     of feasibility due to the extreme scale of this solution.</p> </li> <li> <p>Now, let's look at the same problem from the edge computing     perspective.</p> </li> <li> <p>What if the smartphones themselves contain the deep learning model?</p> </li> <li> <p>The central server is only in charge of managing the latest trained     model and communicating this model with each phone.</p> </li> <li> <p>Now, whenever you start typing, your phone can use the received     model locally to make recommendations from what you typed. The     following figure reflects this scenario:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"2.704861111111111in\"}</p> <p>Figure 2.4 -- Edge inference scenario</p> <ul> <li> <p>This removes both the latency problem and prevents the need to     handle the incoming inference requests\u00a0at a central location.</p> </li> <li> <p>In addition, the phones no longer have to maintain a connection with     the server to make a recommendation.</p> </li> <li> <p>Each phone is in charge of fulfilling requests from its user.</p> </li> <li> <p>This is the core benefit of edge computing:\u00a0we have moved the     computing load from the central server to the edge devices/servers.</p> </li> </ul> <p>Training at the Edge</p> <ul> <li> <p>The distinction between\u00a0centralized and decentralized computing can     be extended to the concept of model training.</p> </li> <li> <p>Let's stick to the smartphone example but think about how we would     train the predictive model instead.</p> </li> <li> <p>First, in the centralized ML process, all of the data used to train     the recommendation model must be collected from the users' devices     and stored on the central server.</p> </li> <li> <p>Then, the collected data is used to train a model, which is     eventually sent to all the phones.</p> </li> <li> <p>This means that the central server still has to be able to handle     the large volume of user data coming in and store it in an efficient     way to be able to train the model.</p> </li> <li> <p>This design leads to the problems found in the centralized computing     approach: as the number of phones connected to the server increases,     the server's ability to work with the incoming data needs to scale     in order to maintain the training process. In addition, since the     data needs to be transmitted and stored centrally in this approach,     there is always the possibility of the interception of transmissions     or even attacks on the stored data.</p> </li> <li> <p>There are several cases where data confidentiality and privacy are     required or strongly\u00a0desired; for example, applications in the     financial and medical industries.\u00a0</p> </li> <li> <p>Centralized model training thus limits use cases, and an     alternative way to work with data directly on edge devices is     required. This exact setting is the motivation for FL.</p> </li> </ul> <p>FL - The technical bits</p> <ul> <li> <p>This section focuses\u00a0on providing a high-level technical     understanding of how FL actually slots in as a solution to the     problem setting described in the previous section.</p> </li> <li> <p>The goal of this section is for you to understand how FL fits as a     solution, and to provide a conceptual basis that will be filled in     by the subsequent sections.</p> </li> </ul> <p>FL Definition</p> <ul> <li> <p>Federated learning is a method to synthesize global models from     local models trained on the edge.</p> </li> <li> <p>FL was first developed by Google in 2016 for their Gboard     application, which incorporates the context of an Android user's     typing history to suggest corrections and propose candidates for     subsequent words.</p> </li> <li> <p>Indeed, this is the exact word recommendation\u00a0problem discussed in     the\u00a0Edge inference\u00a0and\u00a0Edge training\u00a0sections.</p> </li> <li> <p>The solution that Google produced was a decentralized training     approach where an iterative process would compute model training     updates at the edge, aggregating these updates to produce the global     update to be applied to the model.</p> </li> <li> <p>This core concept of aggregating model updates was key in allowing     for a single, performant model to be produced from edge training.</p> </li> <li> <p>Let's break this concept down further.</p> </li> <li> <p>The desired model is distributed across the edge and is trained on     data collected locally at the edge.</p> </li> <li> <p>Of course, we can expect that a model trained on one specific data     source is not going to be representative of the entire dataset.</p> </li> <li> <p>As a result, we dub such models trained with limited data\u00a0local     models. One\u00a0immediate benefit of this approach is the enabling of     ML on data that would otherwise not be collected in the centralized     case, due to issues with privacy and efficiency.</p> </li> <li> <p>Aggregation, the key\u00a0theoretical step of FL, allows for our desired     single\u00a0global model\u00a0to be created from the set of local models     produced at some iteration.</p> </li> <li> <p>The most well-known aggregation algorithm, popular for its     simplicity\u00a0and surprising performance, is called\u00a0federated     averaging\u00a0(FedAvg).</p> </li> <li> <p>FedAvg is performed on a set of local models by computing the     parameter-wise arithmetic mean across the models, producing an     aggregate model.</p> </li> <li> <p>It is important to understand that performing aggregation once     is not enough to produce a good global aggregate model; instead,     it is the iterative process of locally training the previous     global model and aggregating the produced local models into a     new global model that allows for global training progress to be     made.</p> </li> </ul> <p>The FL process</p> <p>To better understand FL from an iterative process perspective, we break it down into the core constituent\u00a0steps of a single iteration, or\u00a0round.</p> <p>The steps for a round can be described as follows:</p> <ol> <li> <p>The aggregate global model parameters are sent to each user's     device.</p> </li> <li> <p>The received ML models located on the user devices are trained with     local data.</p> </li> <li> <p>After a certain amount of training, the local model parameters are     sent to the central server.</p> </li> <li> <p>The central server aggregates the local models by applying an     aggregation function, producing a new aggregate global model.</p> </li> </ol> <p>These steps are depicted in\u00a0Figure 2.5:</p> <p>{width=\"6.268055555555556in\" height=\"4.667361111111111in\"}</p> <p>Figure 2.5 -- FL steps</p> <ul> <li> <p>The flow from\u00a0steps 1 to 4\u00a0constitutes\u00a0a single round of FL.</p> </li> <li> <p>The next round begins as the user servers/devices receive the newly     created aggregate model and start training on the local data.</p> </li> <li> <p>Let's revisit Google's word recommendation for Gboard.</p> </li> <li> <p>At some point in time, each phone stores a sufficient amount of its     user's typing data.</p> </li> <li> <p>The edge training process can create a local model from it, and the     parameters will be sent to the central server.</p> </li> <li> <p>After receiving parameters from a certain number of phones,     the server aggregates them to create a global model and sends it to     the phones.</p> </li> <li> <p>This way, every phone connected to the server receives a model that     reflects local data in all of the phones without ever transmitting     the data from them.</p> </li> <li> <p>In turn, each phone retrains the model when another batch of     sufficient data is collected, sends the model to the server, and     receives a new global model.</p> </li> <li> <p>This cycle repeats itself over and over according to the     configuration of the FL system, resulting in the continuous     monitoring and updating of the global model.</p> </li> <li> <p>Note that the user data never leaves the edge, only the model     parameters; nor is there a need to put all the data in a central     server to generate a global model, allowing for data minimalism.</p> </li> <li> <p>Moreover, model bias can be mitigated with FL methods, as discussed.</p> </li> <li> <p>That is why FL can be\u00a0regarded as a solution to the three issues of     big data, which were introduced earlier (Triple A).</p> </li> </ul> <p>Transfer learning</p> <ul> <li> <p>FL is closely related to an ML concept called\u00a0transfer     learning\u00a0(TL).</p> </li> <li> <p>TL allows us to use large deep learning\u00a0models that have been     trained by researchers using plentiful compute power and resources     on very generalized datasets.</p> </li> <li> <p>These\u00a0models can be applied to more specific problems.</p> </li> <li> <p>For example, we can take an object detection model trained to locate     and name specific objects in images and retrain it on a limited     dataset containing specific objects we are interested in, which were     not included in the original data.</p> </li> <li> <p>If you were to take the original data, add to it the data of those     objects of our interest, and then train a model from scratch, a lot     of computational time and power would be required.</p> </li> <li> <p>With TL, you can quicken the process by leveraging a key fact about     those existing large, generalized models.</p> </li> <li> <p>There is a tendency for the intermediate layers of large DNNs to be     excellent at extracting features, used by the following layers for     the specific ML task.</p> </li> <li> <p>We can maintain its learned ability to extract features by     preserving the parameters in those layers.</p> </li> <li> <p>In other words, parameters in certain layers of existing pre-trained     models can be preserved and used to detect new objects -- we do     not need to reinvent the wheel.</p> </li> <li> <p>This technique is called\u00a0parameter freezing.</p> </li> <li> <p>In FL, model training often takes place in local devices/servers     with limited\u00a0computational power.</p> </li> <li> <p>One example using the Gboard scenario is performing parameter     freezing on a pre trained word embedding layer to allow training to     focus on task-specific information, leveraging prior training of the     embeddings to greatly reduce the trainable parameter count.</p> </li> <li> <p>Taking this concept further, the\u00a0intersection of FL and TL is     called\u00a0federated transfer learning\u00a0(FTL).</p> </li> <li> <p>FTL allows for the FL approach to be applied in cases where the     local datasets differ in structure by performing FL on a shared     subset of the model that can later be extended for specific tasks.</p> </li> <li> <p>For example, a sentiment analysis model and a text summarization     model could both share a sentence encoding component, which can be     trained using FL and used for both tasks.</p> </li> <li> <p>TL (and, by extension, FTL) are key concepts that allow for training     efficiency and incremental improvement to be realized in FL.</p> </li> </ul> <p>Personalization</p> <ul> <li> <p>When edge devices are dealing\u00a0with data that is not\u00a0independent     and identically distributed\u00a0(IID), each device can customize     the global model.</p> </li> <li> <p>This is an idea called\u00a0personalization, which can be\u00a0considered     as fine-tuning the global model\u00a0with local data, or the strategic     use of bias in the data.</p> </li> <li> <p>For example, consider a shopping mall chain that operates in two     areas with distinct local demographics (that is, the chain deals     with non-IID data).</p> </li> <li> <p>If the chain seeks tenant recommendations for both locations using     FL, each of the locations can be better served with personalized     models than a single global model, helping attract local customers.     Since the personalized model is fine-tuned or\u00a0biased\u00a0with local     data, we can expect that its performance on general data would not     be as good as that of the global model.</p> </li> <li> <p>On the other hand, we can also expect that the personalized model     performs better than the global model on the local data for which     the model is personalized.</p> </li> <li> <p>There is a trade-off between user-specific performance and     generalizability, and the power of an FL system comes from its     flexibility to balance them according to the requirements.</p> </li> </ul> <p>Horizontal and Vertical FL</p> <ul> <li> <p>There are two\u00a0types of FL:\u00a0horizontal\u00a0or\u00a0homogeneous     FL\u00a0and\u00a0vertical\u00a0or\u00a0heterogeneous FL.</p> </li> <li> <p>Horizontal\u00a0FL, also called\u00a0sample-based FL, is applicable when     all local datasets\u00a0connected with the aggregator server have the     same features but\u00a0contain different samples.</p> </li> <li> <p>The Gboard application discussed earlier is a good example of     horizontal FL in the\u00a0form of\u00a0cross-device FL, that is, local     training taking place in edge devices.</p> </li> <li> <p>The datasets in all Android phones have identical formats but unique     contents that reflect their user's typing history.</p> </li> <li> <p>On the other hand, vertical FL, or\u00a0feature-based FL, is a\u00a0more     advanced technology that allows parties holding different features     for the same samples to cooperatively generate a global model.</p> </li> <li> <p>For example, a bank and an e-commerce company might both store the     data of residents in a city but their features would differ: the     former knows the credit and expenditure patterns of the citizens,     the latter their shopping behavior.</p> </li> <li> <p>Both of them can benefit by sharing valuable insights without     sharing customer data.</p> </li> <li> <p>First, the bank and e-commerce company can identify their common     users with a technique called\u00a0private set intersection\u00a0(PSI)     while\u00a0preserving data privacy     using\u00a0Rivest-Shamir-Adleman\u00a0(RSA) encryption.</p> </li> <li> <p>Next, each party trains a preliminary model\u00a0with local data     containing unique features.</p> </li> <li> <p>Those models are then aggregated to construct a global model.</p> </li> <li> <p>Usually, vertical FL involves multiple data silos, and when that is     the case, it is also\u00a0called\u00a0cross-silo FL.</p> </li> <li> <p>In China,\u00a0federated Ai ecosystem\u00a0(FATE) is well known for     its seminal demonstration of vertical FL involving WeBank. If you     are interested in\u00a0further conceptual details of FL, there is a very     illustrative and well-written report by Cloudera Fast Forward Labs,     at https://federated.fastforwardlabs.com/.</p> </li> <li> <p>The information on FL contained in this section should be sufficient     to understand the following parts, which examine, in further depth,     some of the key concepts introduced here.</p> </li> <li> <p>The final section aims to cover some of the auxiliary concepts     focused on the practical application of FL.</p> </li> </ul> <p>System Considerations for FL</p> <ul> <li> <p>This section mainly focuses on the multi-party\u00a0computation aspects     of FL, including theoretical security measures and full     decentralization approaches.</p> </li> <li> <p>The goal of this section is for you to be aware of some of the more     practical considerations that should be taken into account for     practical FL applications.</p> </li> </ul> <p>Security Considerations for FL</p> <ul> <li> <p>Despite the\u00a0nascency of the technology, experimental usage of FL has     emerged in a few sectors.</p> </li> <li> <p>Specifically,\u00a0anti-money laundering\u00a0(AML) in the financial     industry and\u00a0drug discovery and diagnosis in the medical industry     have seen promising results, as proofs of concepts in those fields     have been successfully conducted by companies such as Consilient and     Owkin.</p> </li> <li> <p>In AML use cases, banks can cooperate with one another to identify     fraudulent transactions efficiently without sharing their account     data; and hospitals can keep their patient data to themselves while     improving ML models for detecting health issues.</p> </li> <li> <p>These solutions exploit the power of relatively simple horizontal     cross-silo FL, as explained in the\u00a0Understanding FL\u00a0section, and     its application is spreading to other areas.</p> </li> <li> <p>For example, Edgify is a UK-based company contributing to the     automation of cashiers at retail stores in collaboration with Intel     and Hewlett Packard.</p> </li> <li> <p>In Munich, Germany, another UK-based company, Fetch.ai, is     developing a smart city infrastructure with their FL-based     technology. It is clear that the practical application of FL is     rapidly growing.</p> </li> <li> <p>Although FL can circumvent the concern over data privacy thanks to     its privacy-by-design (model parameters do not expose privacy) and     data minimalist (data is not collected in the central server)     approach, there are potential obstructions against its     implementation; one such example is\u00a0mistrust\u00a0among the     participants of an FL project.</p> </li> <li> <p>Consider a situation where\u00a0Bank A\u00a0and\u00a0Bank B\u00a0agree to use FL for     developing a collaborative AML solution.</p> </li> <li> <p>They decide on the common model architecture so that each can train     a local model with their own data and aggregate the results to     create a global model to be used by both.</p> </li> <li> <p>Na\u00efve implementations of FL might allow for one bank to reconstruct     the local model from the other bank, using their local model and the     aggregate model.</p> </li> <li> <p>From this, the bank might be able to extract key information on the     data used to train the other bank's model.</p> </li> <li> <p>As a result, there might be a dispute regarding which party should     host the server to\u00a0aggregate the local models.</p> </li> <li> <p>A possible solution is having a third party host the server and take     responsibility for model aggregation.</p> </li> <li> <p>Yet, how would\u00a0Bank A\u00a0know that the third party is not colluding     with\u00a0Bank B, and vice versa?</p> </li> <li> <p>Going further, the integration of an FL system into a     security-focused domain leads to new concerns regarding the security     and stability of each system component.</p> </li> <li> <p>Known security issues tied to different FL system approaches might     incur an additional potential weakness to adversarial attacks that     outweighs the benefits of the approach.</p> </li> <li> <p>There are several security measures to allow FL collaboration     without forcing the participants to trust one\u00a0another.</p> </li> <li> <p>With a statistical method called\u00a0differential privacy\u00a0(DP),     each participant can add random noise to their local model     parameters to prevent the ability to glean information on the     training data distribution or specific elements from the transmitted     parameters.</p> </li> <li> <p>By sampling the random noise from a symmetric distribution with zero     mean and relatively low variance (for example, Gaussian, Laplace),     the random differences added to the local models are expected to     cancel out when aggregation is performed.</p> </li> <li> <p>As a result, the global model is expected to be very similar to what     would have been generated without DP.</p> </li> <li> <p>However, there is a critical limitation to this approach; for the     sum of the added random noise to converge to zero, a sufficient     number of parties must participate in the coalition.</p> </li> <li> <p>This might not be the case for projects involving only a few banks     or hospitals, and using DP in such cases would harm the global     model's integrity.</p> </li> <li> <p>Some additional measures would be necessary, for example, each     participant sending multiple copies of their local model to increase     the number of models so that the noise will be offset.</p> </li> <li> <p>Another possibility in certain\u00a0fully-decentralized FL systems     is\u00a0secure multi-party computation\u00a0(MPC).</p> </li> <li> <p>MPC-based aggregation allows agents to communicate among themselves     and compute the aggregate model without involving a trusted     third-party server, maintaining model parameter privacy.</p> </li> <li> <p>How could the participants secure the system from outside attacks?\u00a0</p> </li> <li> <p>Homomorphic encryption\u00a0(HE), which preserves the\u00a0effects of     addition and multiplication on data across encryption, allows the     local models to be aggregated into the global model in an encrypted     form.</p> </li> <li> <p>This precludes the exposure of model parameters to outsiders who do     not possess the key for decryption.</p> </li> <li> <p>Yet, HE's effectiveness in securing the communication between     the\u00a0participants comes with a prohibitively high computational cost:     processing the operation on data with the HE algorithm can take     hundreds of trillions of times longer than otherwise!</p> </li> <li> <p>A solution to mitigate this challenge is the use of partial HE,     which is compatible with only one of the additive or multiplicative     operations across encryption; therefore it is computationally much     lighter than the fully homomorphic counterpart.</p> </li> <li> <p>Using this scheme, each participant in a coalition can encrypt and     send their local model to the aggregator, which then sums up all     local models and sends the aggregated model back to the     participants, who, in turn, decrypt the model and divide its     parameters by the number of participants to receive the global     model.</p> </li> <li> <p>Both HE and DP are essential technology for the practical     application of FL.</p> </li> <li> <p>Those interested in the implementation of FL in real-world scenarios     can learn a great deal from\u00a0Federated AI for Real-World Business     Scenarios\u00a0written by IBM Research Fellow Dinesh C. Verma.</p> </li> </ul> <p>Decentralized FL and Blockchain</p> <ul> <li> <p>The architecture of FL discussed so far is based on client-server     networks, that is, edge devices exchanging models with a central     aggregator server.</p> </li> <li> <p>Due to the issues surrounding trust between the participants of FL     coalitions discussed earlier; however, building a system with an     aggregator as a separate and central entity can be problematic.</p> </li> <li> <p>It can be difficult for the\u00a0host of an aggregator to be impartial     and unbiased toward their own data.</p> </li> <li> <p>Also, having a central server inevitably leads to a single point of     failure in the FL system, which results in low resilience.</p> </li> <li> <p>Furthermore, if the aggregator is set up in a cloud server, the     implementation of such an FL system would require a skilled DevOps     engineer, who might be difficult to find and expensive to hire.</p> </li> <li> <p>Given these concerns, Kiyoshi Nakayama co-authored an article about     the first-ever experimentation of a fully decentralized FL system     using blockchain technology     ([http://www.kiyoshi-nakayama.com/publications/BAFFLE.pdf]{.underline}).</p> </li> <li> <p>Leveraging\u00a0smart contracts\u00a0to coordinate model updates and     aggregation, a\u00a0private Ethereum network was constructed to perform     FL in a\u00a0serverless manner.</p> </li> <li> <p>The results of the experiment showed that a peer-to-peer,     decentralized FL can be much more efficient and scalable than an     aggregator-based, centralized FL.</p> </li> <li> <p>The superiority of decentralized architecture was confirmed in a     more recent experiment conducted by Hewlett Packard and German     research\u00a0institutes who gave a unique name to decentralized FL with     blockchain technology:\u00a0swarm learning.</p> </li> <li> <p>While research\u00a0and development in the field of FL are shifting to a     decentralized model, the rest of this book assumes centralized     architecture with an aggregator server. There are two reasons for     this design. First, blockchain is still a nascent technology that AI     and ML researchers are not necessarily familiar with. Incorporating     a peer-to-peer communication scheme can overcomplicate the subject     matter. And second, the logic of FL itself is independent of the     network architecture, and there is no problem with the centralized     model to illustrate how FL works.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>Here, we covered the two key developments that have resulted from     the recent growth in accessible computational power at all levels.</p> </li> <li> <p>First, we looked at the importance of models and how this has     enabled ML to grow considerably in practical usage, with increases     in computational power allowing stronger models that surpass     manually created white-box systems to continuously be produced.</p> </li> <li> <p>We called this the\u00a0what\u00a0of FL -- ML is what we are trying to     perform using FL.</p> </li> <li> <p>Then, we took a step back to look at how edge devices are reaching a     stage where complex computations can be performed within reasonable     timeframes for real-world applications, such as the text     recommendation models on our phones.</p> </li> <li> <p>We called this the\u00a0where\u00a0of FL -- the setting where we want to     perform ML.</p> </li> <li> <p>From the what and the where, we get the intersection of these two     developments -- the usage of ML models directly on edge devices.</p> </li> <li> <p>Remember that the standard central training approach for ML models     greatly suffers from the need to centrally collect all of the data     in the edge ML case, as this prevents applications requiring     efficient communication or data privacy from being possible.</p> </li> <li> <p>We showed that\u00a0FL\u00a0directly addresses this problem by performing     all training at the edge to produce\u00a0local models, at the same     location as the requisite data stores.\u00a0Aggregation\u00a0algorithms take     these local models and produce a\u00a0global model. By iteratively     switching between local training and aggregation,</p> </li> <li> <p>FL allows for the creation of a model that has effectively been     trained across all data stores without ever needing to centrally     collect the data.</p> </li> <li> <p>We concluded the part by stepping outside the theory behind     effective aggregation, looking at system and architecture design     considerations regarding aspects such as model privacy and full     decentralization.</p> </li> <li> <p>After reading, it should be clear that the current state of ML, edge     computing, and fledgling growth in practical FL applications makes     it clear that FL is poised for serious growth in the near future.</p> </li> <li> <p>In the next part, we will examine the implementation of FL from a     system-level perspective.</p> </li> </ul> <p>[Section 3: FL - The System Details_______]{.underline}</p> <p>Federated Learning Systems</p> <ul> <li> <p>This section will provide an overview of the architecture, procedure     flow, sequence of messages, and basics of model aggregation of     the\u00a0federated learning\u00a0(FL) system.</p> </li> <li> <p>As discussed earlier, the basics of the FL framework is quite easy     to understand. However, the real implementation of the FL framework     needs to come with a good understanding of both AI and distributed     systems.</p> </li> <li> <p>The content of this section is based on the most standard foundation     of FL systems, which is used in hands-on exercises later.</p> </li> <li> <p>First, we will introduce the building blocks of FL systems, such as     an aggregator with an FL server, an agent with an FL client, a     database server, and communication between these components.</p> </li> <li> <p>The architecture introduced here is designed in a decoupled way so     that further enhancement to the system will be relatively easier     than with an FL system that contains everything on one machine.</p> </li> <li> <p>Then, an explanation of the flow of the operation of FL from     initialization to aggregation will follow.</p> </li> <li> <p>Finally, we will examine the way an FL system is scaled with a     horizontal design of decentralized FL setups.</p> </li> </ul> <p>This section covers the following topics:</p> <ul> <li> <p>FL system architecture</p> </li> <li> <p>Understanding the FL system flow -- from initialization to     continuous operation</p> </li> <li> <p>Basics of model aggregation</p> </li> <li> <p>Furthering scalability with horizontal design</p> </li> </ul> <p>The System Architecture</p> <ul> <li> <p>FL systems are distributed\u00a0systems that are comprised of servers and     distributed clients.</p> </li> <li> <p>Here, we will define a representative architecture of an FL system     with the following components: an aggregator with an FL server, an     agent with an FL client, and a database:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Cluster aggregator\u00a0(or\u00a0aggregator): A\u00a0system\u00a0with an FL     server that collects and\u00a0aggregates\u00a0machine learning\u00a0(ML)     models that are trained at multiple distributed agents (defined     shortly) and creates global ML models that are sent back to the     agents. This system serves as a\u00a0cluster aggregator, or more     simply, an\u00a0aggregator\u00a0of FL systems.</p> <ul> <li> <p>Distributed agent\u00a0(or\u00a0agent): A\u00a0distributed     learning\u00a0environment with an FL client such as a local edge device,     mobile application, tablet, or any distributed cloud environment     where ML models are trained in a distributed manner and sent to an     aggregator. The agent can be connected to an FL server of the     aggregator through the FL client-side communications module. The FL     client-side codes contain a collection of libraries that can be     integrated into the local ML application, which is designed and     implemented by individual ML engineers and data scientists.</p> </li> <li> <p>Database server\u00a0(or\u00a0database): A database and its server to     store the data related to the aggregators, agents, and global and     local ML models and their performance metrics. The database server     handles the incoming queries from the aggregators and sends the     necessary data back to the aggregators. Agents do not have to be     connected to the database server directly for the simplicity of the     FL system design.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Figure 3.1\u00a0shows the typical overall architecture consisting of a     single cluster aggregator and a database server, as well as multiple     distributed agents:</p> <p>{width=\"6.268055555555556in\" height=\"4.414583333333334in\"}</p> <p>Figure 3.1 -- Overall architecture of an FL system</p> <ul> <li> <p>One advantage\u00a0of the FL system's architecture is that users do not     have to send private raw data to the server, especially that owned     by a third party. Instead, they only have to send locally trained     models to the aggregator.</p> </li> <li> <p>The locally trained models can be in a variety of formats such as     the weights of the entire ML models, the changes of weights     (gradients), or even a subset of them.</p> </li> <li> <p>Another advantage includes reducing the communication load because     the users only have to exchange models that are usually much lighter     than raw data.</p> </li> </ul> <p>The Cluster aggregators</p> <ul> <li> <p>A cluster aggregator\u00a0consists\u00a0of an FL server module, FL state     manager module, and model aggregation module, as in\u00a0Figure 3.1.</p> </li> <li> <p>We just call a cluster aggregator with an FL server an aggregator.</p> </li> <li> <p>While these modules are the foundation of the aggregator, advanced     modules can be added to ensure further security and flexibility of     the aggregation of ML models.</p> </li> <li> <p>Some of the advanced modules are not\u00a0implemented in     the\u00a0simple-fl\u00a0GitHub repository provided with exercises because the     main purpose of this effort is to understand the basic structure and     system flow of the FL system.</p> </li> <li> <p>In the aggregator system, the following modules related to the FL     server, the state manager of FL, and model aggregation are the keys     to implementing the aggregator-side functionalities.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   FL server module: There\u00a0are three primary\u00a0functionalities for     the FL server module, which include the communication handler,     system configuration handler, and model synthesis routine:</p> <pre><code>-   **Communication handler**: Serves\u00a0as a module of the aggregator\n    that supports\u00a0*communications with agents and the database*.\n    Usually, this module accepts polling messages from agents and\n    sends responses back to them. The types of messages they receive\n    include the registration of agents themselves with secure\n    credentials and authentication mechanisms, the initialization of\n    the ML model that serves as an\u00a0*initial model*\u00a0for the future\n    aggregation process, confirmation about whether or not agents\n    participate in a round, and local ML models that are retrained\n    at distributed agents such as mobile devices and local edge\n    machines. The communication handler can also query the database\n    server in order to access the system data and ML models in the\n    database, as well as push and store this data and those models\n    once the aggregator receives or creates new models. This module\n    can utilize HTTP, WebSocket, or any other communication\n    framework for its implementation.\n\n-   **System configuration handler**: Deals with the\u00a0*registration\n    of agents*\u00a0and tracking the\u00a0connected agents and their statuses.\n    The aggregator needs to be aware of the connections and\n    registration statuses of the agents. If the agents are\n    registered with an established authentication mechanism, they\n    will accept the messages and process them accordingly.\n    Otherwise, this module\u00a0will go through the authentication\n    process, such as validating the token sent from the agent, so\n    that next time this agent is connected to the FL server, the\n    system will recognize the agent properly.\n\n-   **Model synthesis routine**: Supports\u00a0checking the collection\n    status of the local ML models and aggregating them once the\n    collection criteria are satisfied. Collection criteria include\n    the number of local models collected by the connected agents.\n    For example, aggregation can happen when 80% of the connected\n    agents send the trained local models to the aggregator. One of\n    the design patterns to do so is to periodically check the number\n    of ML models uploaded by the agents, which keep running while\n    the FL server is up and running. The model synthesis routine\n    will access the database or local buffer periodically to check\n    the status of the local model collection and aggregate those\n    models, to produce the global model that will be stored in the\n    database server and sent back to the agents.\n</code></pre> <ul> <li> <p>FL state manager: A\u00a0state manager keeps track of the\u00a0state     information of an aggregator and connected agents. It stores     volatile information for an aggregator, such as local and global     models delivered by agents, cluster models pulled from the database,     FL round information, or agents connected to the aggregator. The     buffered local models are used by the model aggregation module to     generate a global model that is sent back to each active agent     connected to the aggregator.</p> </li> <li> <p>Model aggregation module: The\u00a0model aggregation module\u00a0is a     collection of the model aggregation algorithms introduced in     the\u00a0Basics of model aggregation\u00a0section here and in\u00a0Model     Aggregation, in further depth. The most typical aggregation     algorithm is\u00a0federated averaging, which averages the weights of     the collected ML models, considering the number of samples that each     model has used for its local training.</p> </li> </ul> <p>Distributed agents</p> <ul> <li>A distributed agent\u00a0consists\u00a0of an FL client module that includes     the communication handler and client libraries as well as local ML     applications connected to the FL system through the FL client     libraries:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   FL client module: There\u00a0are primarily\u00a0four key functionalities     for the FL client module, which include a communication handler,     agent participation handler, model exchange routine, and client     libraries:</p> <pre><code>-   **Communication handler**: Serves as a\u00a0channel to communicate\n    with the aggregator that is assigned to the agent. The message\n    sent to the aggregator includes the registration payload of the\n    agent itself and an initial model that will be the basis of\n    aggregated models. The message also contains locally trained\n    models together with the performance data of those models. This\n    module supports both\u00a0*push*\u00a0and\u00a0*polling*\u00a0mechanisms and can\n    utilize HTTP or WebSocket frameworks for its implementation.\n\n-   **FL participation handler**: Deals\u00a0with the agent participation\n    in the FL process and cycle by sending an aggregator a message\n    including the agent information itself to be registered in the\n    FL platform and initialize the FL process if needed. The\n    response message will set the agent up for the continuous and\n    ongoing FL process and often includes the most updated global\n    model for the agent to utilize and train locally.\n\n-   **Model exchange routine**: Supports a synchronizing\n    functionality that constantly checks whether a new global model\n    is available or not. If the new global model is available, this\n    module downloads the global model from the aggregator and the\n    global model replaces the local model if needed. This module\n    also checks the client state and sends the retrained model if\n    the local training process is done.\n\n-   **Client libraries**: Include administrative libraries and\n    general FL client libraries:\n\n    -   The administrative libraries are used when registering the\n        initial model that will be used by other agents. Any\n        configuration changes for FL systems can be also requested\n        by administrative agents that have higher control\n        capabilities.\n\n    -   General FL client libraries provide basic functionalities\n        such as starting FL client core threads, sending local\n        models to an aggregator, saving models in some specific\n        location on the local machine, manipulating the client\n        state, and downloading the global models. This book mainly\n        talks about this general type of library.\n</code></pre> <ul> <li>Local ML engine and data pipelines: These parts are designed by     individual\u00a0ML engineers and\u00a0scientists and can be independent of the     FL client functionalities. This module has an ML model itself that     can be put into play immediately by the user for potentially more     accurate inference, a training and testing environment that can be     plugged into the FL client libraries, and for the implementation of     data pipelines. While the aforementioned module and libraries can be     generalized and provided as\u00a0application programming     interfaces\u00a0(APIs) or libraries for any ML applications, this     module is\u00a0unique depending on the requirements of AI applications to     be developed.</li> </ul> <p>The Database Components</p> <ul> <li> <p>A\u00a0database server\u00a0consists of a database query handler and a     database, as storage.</p> </li> <li> <p>The database server can reside on the server side, such as on the     cloud, and is tied closely to aggregators, while the recommended     design is to separate this database server\u00a0from aggregator servers     to decouple the functionalities to enhance the system's simplicity     and resilience.</p> </li> <li> <p>The functionality of the database query handler and sample database     tables are as follows:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Database query handler: Accepts the incoming requests from an     aggregator and sends the requested data and ML models back to the     aggregator.</p> <ul> <li> <p>Database: Stores\u00a0all the related information to FL processes. We     list some potential entries for the database here:</p> <ul> <li> <p>Aggregator information: This\u00a0aggregator-related information     includes the ID of the aggregator itself, the IP address and     various port numbers, system registered and updated times, and     system status. In addition, this entry can include model     aggregation-related information, such as the round of FL and its     information and aggregation criteria.</p> </li> <li> <p>Agent information: This\u00a0agent-related information includes     the ID of the agent itself, the IP address and various port     numbers, system registered and updated times, and system status.     This entry can also contain the opt-in/out status that is used     for synchronous FL (explained in the\u00a0Synchronous and     asynchronous FL\u00a0section Here) and a flag to record whether the     agent has been a bad actor in the past (for example, involved in     poisoning attacks, or very slow at returning results).</p> </li> <li> <p>Base model information: Base\u00a0model information is used for     the registration of initial ML models whose architecture and     information are used for the entire process of FL rounds.</p> </li> <li> <p>Local models: The information of local models includes the     model ID that is unique to individual ML models, generated time     of the model, agent ID that uploaded the model, aggregator ID     that received the model from the agent, and so on. Usually, the     model ID is uniquely mapped to the location of the actual ML     model file that can be stored in the database server or in some     cloud storage services such as S3 buckets of Amazon Web     Services, and so on.</p> </li> <li> <p>Cluster global models: The information of the cluster global     models is similar to what local models could record in the     database including the model ID, aggregator ID, generated time     of the model, and so on. Once the aggregated model is created by     an aggregator, the\u00a0database server will accept the global models     and store them in the database server or any cloud storage     services. Any global model can be requested by an aggregator.</p> </li> <li> <p>Performance data: The\u00a0performance of the local and global     models can be tracked, as metadata attached to those models.     This performance data will be used to ensure that the aggregated     model performs well enough before it is actually deployed to the     user ML application.</p> </li> </ul> </li> </ul> <p>Note</p> <ul> <li> <p>In the code sample of the\u00a0simple-fl\u00a0repository, only the database     tables related to the local models and cluster models are covered to     simplify the explanation of the entire FL process.</p> </li> <li> <p>Now that the basic architecture of the FL system has been     introduced, next, we will talk about how to enhance the FL system's     architecture if the computation resources are limited on the     agent-side devices.</p> </li> </ul> <p>Low Computational Capacity Agent Devices and Intermediate servers</p> <ul> <li> <p>Sometimes, the\u00a0computational capability of local user devices is     limited -- ML training may be difficult in those devices, but     inference or predictions can be made\u00a0possible by just downloading     the global model. In these cases, an FL platform may be able to set     up an additional intermediate server layer, such as with     smartphones, tablets, or edge servers.</p> </li> <li> <p>For example, in a healthcare AI application, users manage their     health information on their smart watches, which can be transferred     to their smart tablets or synched with laptops. In those devices, it     is easy to retrain ML models and integrate the distributed agent     functionalities.</p> </li> <li> <p>Therefore, the system architecture needs to be modified or     redesigned depending on the applications into which the FL system is     integrated, and the concept of intermediate servers can be applied     using distributed agents to realize FL processes.</p> </li> <li> <p>We do not have to modify the interactions and communication     mechanisms between the\u00a0aggregators and the intermediate servers.     Just by implementing\u00a0APIs between the user devices and the     intermediate servers, FL will be possible in most use cases.</p> </li> </ul> <p>Figure 3.2\u00a0illustrates the interaction between the aggregators, intermediate servers, and user devices:</p> <p>{width=\"6.268055555555556in\" height=\"3.623611111111111in\"}</p> <p>Figure 3.2 -- An FL system with intermediate servers</p> <ul> <li>Now that we have learned about the basic architecture and components     of an FL system, let us look into how an FL system operates in the     following section.</li> </ul> <p>FL System Process -- from initialization to continuous operation</p> <ul> <li> <p>Each distributed\u00a0agent belongs to an aggregator that is managed by     an FL server, where ML model aggregation is conducted to synthesize     a global model that is going to be sent back to the agents.</p> </li> <li> <p>An agent uses its local data to train an ML model and then uploads     the trained model to the corresponding aggregator. The concept     sounds straightforward, so we will look into a bit more detail to     realize the entire flow of those processes.</p> </li> <li> <p>We also define a\u00a0cluster global model, which we simply call     a\u00a0cluster model\u00a0or\u00a0global model, which is an aggregated ML     model of local models collected from distributed agents.</p> </li> </ul> <p>Note</p> <ul> <li> <p>In the next two sections, we will guide you on how to implement the     procedure and sequence of messages discussed Here.</p> </li> <li> <p>However, some of the system operation perspectives, such as an     aggregator or agent system registration in the database, are not     introduced in the code sample of the\u00a0simple-fl\u00a0repository in order     to simplify the explanation of the entire FL process.</p> </li> </ul> <p>Database, Aggregator, and Agent initialization</p> <ul> <li> <p>The\u00a0sequence\u00a0of the initialization processes is quite simple. The     initialization and registration processes need to happen in the     order of database, aggregator, and agents.</p> </li> <li> <p>The overall registration sequence of an aggregator and an agent with     a database is depicted in\u00a0Figure 3.3\u00a0as follows:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"6.290972222222222in\"}</p> <p>Figure 3.3 -- The process of aggregator and agent registration in the database server</p> <p>Here is the\u00a0initialization and registration procedure of each component in the FL system:</p> <ul> <li> <p>Database server initialization: The\u00a0first step of the operation     of an FL system is to initiate the database server. There are some     simple frameworks that are provided by multiple organizations that     do not include databases or database servers. However, in order to     maintain the process of federating the ML models, it is recommended     that you use a database, even a lightweight one such as an SQLite     database.</p> </li> <li> <p>Aggregator initialization and registration: An aggregator should     be set up and\u00a0running before any agents start running and uploading     the ML models. When the aggregator starts running and first gets     connected to the database server, the registration process happens     automatically by also checking whether the aggregator is safe to be     connected. If it fails to go through the registration process, it     receives the registration failure message sent back from the     database. Also, in case the aggregator is trying to connect to     the\u00a0database again after losing the connection to the database, the     database server always checks whether the aggregator has already     been registered or not. If this is the case, the response from the     database server includes the system information of the registered     aggregator so that the aggregator can start from the point where it     left off. The aggregator may need to publish an IP address and port     number for agents to be connected if it uses HTTP or WebSocket.</p> </li> <li> <p>Agent initialization and registration: Usually, if an agent     knows the aggregator\u00a0that the agent wants to connect to, the     registration is similar to how an aggregator connects to a database     server. The connection process should be straightforward enough to     just send a participation message to that aggregator using an IP     address, the port number of the aggregator (if we are using some     frameworks such as HTTP or WebSocket), and an authentication token.     In case the agent is trying to connect to the aggregator again after     losing the connection to the aggregator, the database server always     checks whether the agent already has been registered or not. If the     agent is already registered, the response from the database server     includes the system information of the registered agent so that the     agent can start from the point where it was disconnected from the     aggregator.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   In particular, when it receives the participation message from the     agent, the aggregator goes through the following procedure, as     in\u00a0Figure 3.4.</p> <ul> <li> <p>The key process after receiving the participation request is (i)     checking whether the agent is trusted or not, or whether the agent     is already registered or not, and (ii) checking\u00a0whether the initial     global model is already registered or not. If (i) is met, the     registration process keeps going. If the (initial) global model is     already registered, the agent will be able to receive the global     model and start\u00a0using that global model for the local training     process on the agent side.</p> </li> <li> <p>The agent participation and registration process at an aggregator     side is depicted in\u00a0Figure 3.4:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"6.81875in\"}</p> <p>Figure 3.4 -- The registration process of an agent by an aggregator</p> <ul> <li>Now that we\u00a0understand the initialization and registration process     of the FL system components, let us move on to the basic     configuration of the ongoing FL process, which is about uploading     the initial ML model.</li> </ul> <p>Initial model upload process by initial agent</p> <ul> <li> <p>The\u00a0next step in running an FL process is\u00a0to register the initial ML     model whose architecture will be used in the entire and continuous     process of FL by all the aggregators and agents.</p> </li> <li> <p>The initial model can be distributed by the company that owns the ML     application and FL servers.</p> </li> <li> <p>They'll likely provide the initial base model as part of the     aggregator configuration.</p> </li> <li> <p>We call the initial ML model used as a reference for model     aggregation a\u00a0base model.</p> </li> <li> <p>We also call the agent that uploads the initial base model     an\u00a0initial agent. The base model info could include the ML model     itself as well as the time it was generated and the initial     performance data.</p> </li> <li> <p>That being said, the process of initializing the base model can be     seen in\u00a0Figure 3.5:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"5.909722222222222in\"}</p> <p>Figure 3.5 -- Base model upload process for the initial agent</p> <ul> <li>Now, the FL process is ready to be conducted. Next, we will learn     about the FL cycle, which is a very core part of the FL process.</li> </ul> <p>Overall FL System Process Sequence</p> <ul> <li> <p>In this section, we\u00a0will only give an example with a single agent     and aggregator, but in real cases and operations, the agent     environments are various and dispersed into distributed devices.</p> </li> <li> <p>The following is the list\u00a0of the processes for how the local models     are uploaded, aggregated, stored, and sent back to agents as a     global model:</p> </li> <li> <p>The agents other than the initial agent will request the global     model, which is an updated aggregated ML model, in order to deploy     it to their own applications.</p> </li> <li> <p>Once the agent gets the updated model from the aggregator and     deploys it, the agent retrains the ML model locally with new data     that is obtained afterward to reflect the freshness and timeliness     of the data. An agent can also participate in multiple rounds with     different data to absorb its local examples and tendencies. Again,     this local data will not be shared with the aggregator and stays     local to the distributed devices.</p> </li> <li> <p>After retraining the local ML model (which, of course, has the same     architecture as the global or base model of the FL), the agent calls     an FL client API to send the model to the aggregator.</p> </li> <li> <p>The aggregator receives the local ML model and pushes the model to     the database. The aggregator keeps track of the number of collected     local models and will keep accepting the local models as long as the     federation round is open. The round can be closed with any defined     criteria, such as the aggregator receiving enough ML models to be     aggregated. When the criteria are met, the aggregator aggregates the     local models and produces an updated global model that is ready to     be sent back to the agent.</p> </li> <li> <p>During that process, agents constantly poll the aggregator on     whether the global model is ready or not, or in some cases, the     aggregator may push the global model to the agents that are     connected to the aggregator, depending on the communications system     design and network constraints. Then, the updated model is sent back     to the agent.</p> </li> <li> <p>After receiving the updated global model, the agent deploys and     retrains the global model locally whenever it is ready. The whole     process described is repeated until the termination criteria are met     for the FL to end. In some cases, there are no termination     conditions to stop this FL cycle and retraining process\u00a0so that the     global model constantly keeps learning about the latest phenomena,     current trends, or user-related tendencies. FL rounds can just be     stopped\u00a0manually in preparation for some evaluation before a     rollout.</p> </li> </ul> <p>Figure 3.6\u00a0shows the overall process of how FL is continuously conducted between an agent, an aggregator, and a database typically:</p> <p>{width=\"6.268055555555556in\" height=\"6.572222222222222in\"}</p> <p>Figure 3.6 -- Overview of the continuous FL cycle</p> <ul> <li>Now that we understand the overall procedure of the FL process, we     will look into the different round management approaches in the FL     process next: synchronous FL and asynchronous FL.</li> </ul> <p>Synchronous and asynchronous FL</p> <ul> <li> <p>When the model aggregation happens at the aggregator, there are     multiple criteria related to how many local models it needs to     collect from which agents.</p> </li> <li> <p>In this section, we will briefly talk about the differences between     synchronous and asynchronous FL, which have been discussed in a lot     of literature, such as     https://iqua.ece.toronto.edu/papers/ningxinsu-iwqos22.pdf, so please     refer to it to learn about these concepts further.</p> </li> </ul> <p>Synchronous FL</p> <ul> <li> <p>Synchronous FL\u00a0requires the aggregator to select the\u00a0agents that     need to send the local models for each round in order to proceed     with the model aggregation.</p> <ul> <li> <p>This synchronous FL approach is simple to design and implement     and suitable for FL applications that require a clear selection     of agents.</p> </li> <li> <p>However, if the number of agents becomes too large, the     aggregator may have to wait for a long time to wrap up the     current round, as the computational capability of the agents     could vary and some of them may have problems uploading or fail     to upload their local models.</p> </li> <li> <p>Thus, some of the agents can become slow or totally     dysfunctional when sending their models to the aggregator.</p> </li> <li> <p>These slow agents are known as\u00a0stragglers\u00a0in distributed ML,     which motivates us to use the asynchronous FL mode.</p> </li> </ul> </li> </ul> <p>Asynchronous FL</p> <ul> <li> <p>Asynchronous FL\u00a0does not require the aggregator\u00a0to select the agents     that have to upload their local models. Instead, it opens the door     for any trusted agents to upload the model anytime.</p> </li> <li> <p>Furthermore, it is fine to wrap up the federation round whenever the     aggregator wants to generate the global model, with or without     criteria such as the minimum number of local models that needs to be     collected, or some predefined interval or deadline for which the     aggregator needs to wait to receive the local models from the agents     until the aggregation for that round happens.</p> </li> <li> <p>This asynchronous FL approach gives the FL system much more     flexibility for model aggregation for each FL round, but the design     may be more complicated than the simple synchronous aggregation     framework.</p> </li> <li> <p>When managing the FL rounds, you need to consider the practicalities     of running rounds, such as scheduling and dealing with delayed     responses, the minimum levels of participation required, the details     of example stores, using the downloaded or trained models for     improved inference in the applications on the edge devices, and     dealing with bad or slow agents.</p> </li> <li> <p>We will look into the FL process and procedure flow next, focusing     on the aggregator side.</p> </li> </ul> <p>FL cycle and process- The aggregator perspective</p> <ul> <li>An\u00a0aggregator has two threads\u00a0running to accept and cache the local     models and aggregate the collected local ML models. In this section,     we describe those procedures.</li> </ul> <p>Accepting and caching local ML models</p> <ul> <li> <p>The aggregator side process\u00a0of accepting\u00a0and caching local ML models     is depicted in\u00a0Figure 3.7\u00a0and explained as follows:</p> </li> <li> <p>The aggregator will wait for a local ML model to be uploaded by an     agent. This method sounds like asynchronous FL. However, if the     aggregator has already decided which agents to accept models from,     it just needs to exclude the model uploads sent by undesired agents.     Some other system or module may have already told the undesired     agents not to participate in the round as well.</p> </li> <li> <p>Once an ML model is received, the aggregator checks whether the     model is uploaded by the trusted agents or not. Also, if the agent     that uploads the local model is not listed in the agents that the FL     operator wants to accept, the aggregator will discard the model.     Furthermore, an aggregator needs to have a mechanism to only filter     the valid models -- otherwise, there is a risk of poisoning the     global model and messing up the entire FL process.</p> </li> <li> <p>If the uploaded local ML model is valid, the aggregator will push     the model to the database. If the database resides on a different     server, the aggregator will package the model and send it to the     database server.</p> </li> <li> <p>While the\u00a0uploaded\u00a0models are stored in the database, they should be     buffered in the memory of the state manager of the aggregator in an     appropriate format, such as NumPy arrays.</p> </li> <li> <p>This procedure keeps running until the termination conditions are     satisfied or the operator of the FL system opts to stop the     process.\u00a0Figure 3.7\u00a0depicts the procedure of accepting and caching     local ML models:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"6.972916666666666in\"}</p> <p>Figure 3.7 -- Procedure for accepting and caching local ML models</p> <ul> <li>Once the\u00a0local ML models are\u00a0accepted and cached, the FL system     moves on to the next procedure: aggregating the local models.</li> </ul> <p>Aggregating local ML models</p> <ul> <li> <p>The\u00a0aggregator-side procedure of aggregating local ML models     depicted in\u00a0Figure 3.8\u00a0is as follows:</p> </li> <li> <p>The aggregator constantly checks whether the aggregation criteria     are satisfied. The typical aggregation criteria are as follows:</p> <ul> <li> <p>The number of local models collected so far in this FL round.     For example, if the number of agents is 10 nodes, after 8 nodes     (meaning 80% nodes) report the locally trained models, the     aggregator starts aggregating the models.</p> </li> <li> <p>The combination of the number of collected models and the time     that the FL round has spent. This can automate the aggregation     process and prevent systems from getting stuck.</p> </li> </ul> </li> <li> <p>Once\u00a0the aggregation criteria are met, the aggregator starts a model     aggregation process. Usually, federated averaging is a very typical     but powerful aggregation method. Further explanation of the model     aggregation methods is in the\u00a0Basics of model aggregation\u00a0section     of This section and in\u00a0section     7,\u00a0Model     Aggregation. The aggregated model is defined as a global model in     this FL round.</p> </li> <li> <p>In a case where time for the FL round has expired and not enough     agents that participated in the round have uploaded a model, the     round can be abandoned or forced to conduct aggregation for the     local models collected so far.</p> </li> <li> <p>Once the model aggregation is complete, the aggregator pushes the     aggregated global model to the database. If the database resides on     a different server, the aggregator will package the global model and     send it to the database server.</p> </li> <li> <p>Then, the aggregator sends the global model to all the agents, or     when the agents poll to check whether the global model is ready, the     aggregator will notify the agent that the global model is ready and     put it in the response message to the agents.</p> </li> <li> <p>After the whole process of model aggregation, the aggregator updates     the number of the FL round by just incrementing it.</p> </li> <li> <p>Figure 3.8\u00a0shows the aggregator's process from checking the     aggregation criteria to synthesizing the global model when enough     models are collected:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"8.41388888888889in\"}</p> <p>Figure 3.8 -- Model synthesis routine: aggregating local ML models</p> <ul> <li>Aggregating\u00a0local models to generate the global model has been     explained. Now, let us look into the agent-side FL cycle, including     the retraining process of the local ML models.</li> </ul> <p>Local Retraining Cycle - The agent-perspective</p> <ul> <li> <p>In the\u00a0distributed agent, the\u00a0following state transition happens and     is repeated for the continuous operation of the FL cycle:</p> </li> <li> <p>In the state of\u00a0waiting_gm, the agent polls the aggregator to     receive any updates related to the global model. Basically, a     polling method is used to regularly query the updated global model.     However, under some specific settings, an aggregator can push the     updated global model to all agents.</p> </li> <li> <p>gm_ready\u00a0is the state after the global model is formed by the     aggregator and downloaded by the agent. The model parameters are     cached in the agent device. The agent replaces its local ML model     with the downloaded global model. Before completely replacing the     local model with the downloaded model, the agent can check whether     the output of the global model is sufficiently performant for the     local ML engine. If the performance is not what is expected, the     user can keep using the old model locally until it receives the     global model that has the desired performance.</p> </li> <li> <p>Next, in the\u00a0training\u00a0state, the agent can locally train the model     in order to maximize its performance. The trained model is saved in     a local data storage where training examples are kept. The FL client     libraries of the agent ascertain its readiness to manipulate the     local model that can be protected with asynchronous function access.</p> </li> <li> <p>After the local model is trained, the agent checks whether the new     global model has been sent to the agent or not. If the global model     has arrived, then the locally trained ML model is discarded and goes     back to the\u00a0gm_ready\u00a0state.</p> </li> <li> <p>After local training, the agent proceeds with the\u00a0sending\u00a0state to     send the updated local model back to the aggregator, and then, the     agent goes back to the\u00a0waiting_gm\u00a0state.</p> </li> <li> <p>Figure 3.9\u00a0depicts the state transition of an agent to adapt and     update the ML model:</p> </li> </ul> <p>{width=\"5.709722222222222in\" height=\"7.125in\"}</p> <p>Figure 3.9 -- Agent-side state transition to adapt and update the ML model</p> <ul> <li>Next, we\u00a0touch on a model interpretation based on deviation from the     baseline outputs that are used for anomaly detection and preventing     model degradation.</li> </ul> <p>Model interpretation based on deviation from baseline outputs</p> <ul> <li> <p>We can also\u00a0provide an interpretation framework by looking at the     output of each local model. The following procedure can be     considered to ensure the local model is always good to use and can     be deployed in production:</p> </li> <li> <p>Obtain the most recent ML output generated by an agent as well as a     baseline output that can be a typical desired output prepared by     users. The baseline output could include an average output based on     the past windows or reference points defined by an operator, subject     expert, or rule-based algorithm.</p> </li> <li> <p>The deviation between the output of the local model and the baseline     output is computed.</p> </li> <li> <p>An anomaly or performance degradation can be detected by checking     whether the deviation exceeds the operator-specified threshold. If     an anomaly is detected, an alarm can be sent to an operator to     indicate a fault or that the ML model is in an anomalous state.</p> </li> <li> <p>Now that the process of the FL has been explained, let us look into     the basics of model aggregation, which comprise the critical part of     FL.</p> </li> </ul> <p>Model Aggregation - The Basics</p> <ul> <li> <p>Aggregation\u00a0is a\u00a0core concept within FL. In fact, the strategies     employed to aggregate models are the key theoretical driver for the     performance of FL systems.</p> </li> <li> <p>The purpose of this section is to introduce the high-level concepts     of aggregation within the context of an FL system -- the underlying     theory and examples of advanced aggregation strategies will be     discussed in greater depth later when we discuss\u00a0Model     Aggregation.</p> </li> </ul> <p>What does aggregation of models mean?</p> <ul> <li> <p>Let's revisit the\u00a0aggregator-side cycle discussed in earlier, at the     point in the process where the agents assigned to a certain     aggregator have finished training locally and have transmitted these     models back to this aggregator.</p> </li> <li> <p>The goal of any aggregation strategy, or any way of aggregating     these models together, is to produce new models that gradually     increase in performance across all of the data collected by the     constituent agents.</p> </li> <li> <p>An important point\u00a0to remember is that FL is, by definition, a     restricted version of the distributed learning setting, in which the     data collected locally by each agent cannot be directly accessed by     other agents.</p> </li> <li> <p>If this restriction were not in place, a model could be made to     perform well trivially on all of the data by collecting the data     from each agent and training on the joint dataset; thus, it makes     sense to treat this\u00a0centrally-trained\u00a0model as the target model     for an FL approach.</p> </li> <li> <p>At a high level, we can consider this unrestricted distributed     learning scenario as aggregation before model training (where in     this case, aggregation refers to combining the data from each     agent).</p> </li> <li> <p>Because FL does not allow data to be accessed by other agents, we     consider the scenario as aggregation after model training instead;     in this context, aggregation refers to the combination of the     intelligence captured by each of the trained models from their     differing local datasets.</p> </li> <li> <p>To summarize, the goal of an aggregation strategy is to combine     models in a way that eventually leads to a generalized model whose     performance approaches that of the respective centrally trained     model.</p> </li> </ul> <p>FedAvg -- Federated averaging</p> <ul> <li>To make\u00a0some of these\u00a0ideas more concrete, let's take an initial     look into one of the most well-known and straightforward aggregation     strategies, known as\u00a0Federated Averaging\u00a0(FedAvg). The     FedAvg algorithm is performed as follows:     let\u00a0{width=\"1.2670089676290464in\"     height=\"0.2487379702537183in\"}\u00a0be the parameters of the models     from\u00a0{width=\"0.20833333333333334in\"     height=\"0.20833333333333334in\"}\u00a0agents, each with a local dataset     size     of\u00a0{width=\"1.1828608923884514in\"     height=\"0.2213538932633421in\"}.     Also,\u00a0{width=\"0.20833333333333334in\"     height=\"0.20833333333333334in\"}\u00a0is the total dataset size defined     as\u00a0{width=\"2.617024278215223in\"     height=\"0.29885826771653545in\"}. Then, FedAvg returns the following     ML model as the aggregated model:</li> </ul> <p>{width=\"4.697647637795275in\" height=\"0.54582239720035in\"}</p> <ul> <li> <p>Essentially, we\u00a0perform FedAvg over a set\u00a0of models by taking the     weighted average of the models, with weights proportional to the     size of the dataset used to train the model.</p> </li> <li> <p>As a result, the types of models to which FedAvg can be applied are     models that can be represented as some set of parameter values.</p> </li> <li> <p>Deep neural networks are currently the most notable of these kinds     of models -- most of the results analyzing the performance of FedAvg     work with deep learning models.</p> </li> <li> <p>It is rather surprising that this relatively simple approach can     lead to generalization in the resulting model.</p> </li> <li> <p>We can visually examine what FedAvg looks like within a toy     two-dimensional parameter space to observe the benefits of the     aggregation strategy:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.636111111111111in\"}</p> <p>Figure 3.10 -- Two-dimensional parameter space with local models from two agents (the circle and square) and a target model (the black x)</p> <ul> <li> <p>Let's consider a case where we have two newly initialized models     (the circle and square points) belonging to separate agents.</p> </li> <li> <p>The space in the preceding figure represents the parameter space of     the models, where each toy model is defined by two parameters.</p> </li> <li> <p>As\u00a0the models are trained, these points will move in the parameter     space -- the goal is to approach a local optimum in the parameter     space, generally corresponding to the aforementioned centrally     trained model:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.461111111111111in\"}</p> <p>Figure 3.11 -- Change in local model parameters without aggregation</p> <ul> <li> <p>Each model\u00a0converges to separate dataset-specific optima (two x     points from the circle and square) that do not generalize.</p> </li> <li> <p>Because each agent only has access to a subset of the data, the     local optima reached by training each model locally will differ from     the true local optima; this difference depends on how similar the     underlying data distributions are for each agent.</p> </li> <li> <p>If the models are only trained locally, the resulting models will     likely not generalize over all of the data:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.631944444444445in\"}</p> <p>Figure 3.12 -- Adding aggregation moves the local model parameters to the average for both models at each step, leading to convergence at the target model</p> <ul> <li> <p>Applying\u00a0FedAvg at each movement\u00a0step allows us to create an     aggregate model that eventually comes close to the true local optima     in the parameter space.</p> </li> <li> <p>This example\u00a0displays the basic capability of FedAvg to produce     generalized models.</p> </li> <li> <p>However, working with real models (such as highly parameterized deep     learning models) introduces additional complexity that is handled by     FedAvg but not by simpler approaches.</p> </li> <li> <p>For example, we might wonder why we don't simply fully train each     local model and only average at the end; while this approach would     work in this toy case, it has been observed that only averaging once     with real models leads to poor performance across all of the data.</p> </li> <li> <p>The FedAvg process allows for a more robust way to reach the     generalized model within high-dimension parameter spaces.</p> </li> <li> <p>This section only aims to give an overview of aggregation in     FL;\u00a0the\u00a0Model Aggregation section, contains more detailed     explanations and examples for aggregation in different scenarios.</p> </li> <li> <p>We now understand the entire process of how the FL system works with     basic model aggregation. In some applications, the FL system may     have to support a huge number of agents to realize its scalability.</p> </li> <li> <p>The following section will give you some idea about how to scale     more smoothly, especially with a decentralized horizontal design.</p> </li> </ul> <p>Horizontal Design for Enhanced Scalability</p> <ul> <li> <p>In this section, we will look into how to further scalability when     we need to support a large number of devices and users.</p> </li> <li> <p>There are\u00a0practical cases where control, ease of maintenance and     deployment, and low communication overhead are provided by     centralized FL. If the number of agents is not large, it makes more     sense to stick to centralized FL than decentralized FL.</p> </li> <li> <p>However, when the number of participating agents becomes quite     large, it may be worth looking into horizontal scaling with a     decentralized FL architecture.</p> </li> <li> <p>The latest developments of auto-scaling frameworks these days, such     as\u00a0the\u00a0Kubernetes\u00a0framework (https://kubernetes.io/, can be a     nice integration with the topic that is discussed in this section,     although actual integration and implementation with Kubernetes is     beyond the scope of this material.</p> </li> </ul> <p>Horizontal design with semi-global model</p> <ul> <li> <p>There will be some\u00a0use cases where many aggregators are needed to     cluster groups of agents and create a global model on top of those     many aggregators.</p> </li> <li> <p>Google uses a centralized approach for this, as in the     paper\u00a0Towards Federated Learning at Scale, while setting up a     centralized node for managing multiple aggregators may have some     resilience issues.</p> </li> <li> <p>The idea is simple: periodically aggregate all the cluster models at     some central master node.</p> </li> <li> <p>On the other hand, we can realize the decentralized way of     aggregating cluster models created by multiple aggregators. The     architecture for that is based on two crucial ideas:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Model aggregation conducted among individual cluster aggregators     without master nodes</p> <ul> <li>Semi-global model synthesis to aggregate cluster models generated by     other aggregators</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   To create semi-global models, decentralized cluster aggregators     exchange their aggregated cluster models with each other and     approximate optimal global models.</p> <ul> <li> <p>The cluster aggregators can also use a database to periodically     collect other cluster models to generate the semi-global models.</p> </li> <li> <p>This framework allows for the absorption of training results from     diverse sets of users dispersed across many aggregators by     synthesizing the most updated global models without a master node     concept.</p> </li> <li> <p>Based on\u00a0this decentralized architecture, the robustness of the     entire FL system can be enhanced, as the semi-global model can be     independently computed at each cluster aggregator.</p> </li> <li> <p>The FL system can be scaled further, as each cluster aggregator is     responsible for creating its own semi-global model by itself -- not     via the master node of those aggregators -- and therefore,     decentralized semi-global model formation comes with resiliency and     mobility.</p> </li> <li> <p>We can even decouple the database that stores the uploaded local     models, cluster global models, and semi-global models.</p> </li> <li> <p>By introducing a distributed database into the FL system, the entire     system could be made more scalable, resilient, and secure together     with some failover mechanism.</p> </li> <li> <p>For example, each cluster aggregator stores the cluster model in a     distributed database.</p> </li> <li> <p>The cluster aggregators can retrieve cluster models of other     aggregators by pulling the models periodically from the databases.     At each cluster aggregator, a semi-global ML model is generated by     synthesizing the pulled models.</p> </li> </ul> <p>Figure 3.13\u00a0illustrates the overall architecture of the decentralized horizontal design of a multi-aggregator FL system:</p> <p>{width=\"6.268055555555556in\" height=\"2.0625in\"}</p> <p>Figure 3.13 -- Architecture of a decentralized FL system with multiple aggregators (horizontal design)</p> <ul> <li>Now that we have\u00a0discussed how to enhance the FL system with a     horizontal design using the semi-global model concept, next, we will     look at distributed database frameworks to further ensure     scalability and resiliency.</li> </ul> <p>Distributed database</p> <ul> <li> <p>Furthermore, the\u00a0accountability of the model updates can be provided     by storing historical model data in a data-driven distributed     database.</p> </li> <li> <p>The\u00a0InterPlanetary File System\u00a0(IPFS) and Blockchain     are\u00a0well-known distributed databases that ensure the accountability     of global model updates.</p> </li> <li> <p>After a cluster aggregator generates a semi-global model based on     other cluster models, the semi-global model is stored in a     distributed database.</p> </li> <li> <p>The distributed database manages the information of those models     with a unique identifier.</p> </li> <li> <p>To maintain all the models consistently, including local, cluster,     and semi-global models, each ML model is assigned a globally unique     identifier, such as a hash value, which could be realized using the     concept of\u00a0a\u00a0Chord Distributed Hash Table\u00a0(Chord DHT).</p> </li> <li> <p>The Chord DHT is a scalable peer-to-peer lookup protocol for     internet applications.</p> </li> <li> <p>The cluster aggregator can store metadata on the cluster models,     such as timestamps and hash identifiers.</p> </li> <li> <p>This gives us further accountability for model synthesis by ensuring     the cluster models haven\\'t been altered.</p> </li> <li> <p>It is also possible to identify a set of aggregators that are     sending harmful cluster models to destroy the semi-global models     once the malicious models are detectable.</p> </li> <li> <p>These models can be filtered by analyzing the patterns of the     weights of the cluster model or deviation from the other cluster     models when the difference is too big to rely on.</p> </li> <li> <p>The nature of the distributed database is to store all the volatile     state information of the distributed FL system.</p> </li> <li> <p>The FL system can restore from the distributed database in the case     of failure.</p> </li> <li> <p>The cluster aggregators also exchange their cluster models based on     a certain interval defined by the system operator.</p> </li> <li> <p>Therefore, the mapping table between cluster models and aggregators     needs to be logged in the database together with meta-information on     the local, cluster, and semi-global models, such as the generation     time of those models and the size of training samples.</p> </li> </ul> <p>Asynchronous agent participation in a multiple-aggregator scenario</p> <ul> <li> <p>Distributed agents\u00a0can broadcast participation messages to     connectable aggregators when they want to join their FL process.</p> </li> <li> <p>The participation messages can contain the unique ID of the agent.     One of the cluster aggregators then returns a cluster aggregator ID,     potentially the value generated based on a common hash function, to     which the agent should belong.\u00a0</p> </li> <li> <p>Figure 3.14\u00a0depicts how the agent is assigned to a certain cluster     aggregator using a hash function:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"7.308333333333334in\"}</p> <p>Figure 3.14 -- The sequence of an agent joining one of the cluster aggregators in an FL system</p> <ul> <li>In the following section, we will look into how the semi-global     model is generated based on aggregating the multiple cluster global     models.</li> </ul> <p>Semi-global model synthesis</p> <ul> <li> <p>After the\u00a0agent is assigned to a specific cluster aggregator, the     agent starts to participate in the FL process.</p> </li> <li> <p>It\u00a0requests a base ML model if it is registered -- otherwise, it     needs to upload the base model to start local training.</p> </li> <li> <p>The procedure of uploading local models and generating cluster and     semi-global models will continue until the agent or aggregator is     disconnected from the system.</p> </li> <li> <p>The sequence of the local and cluster model upload process,     aggregation process, and semi-global model synthesis and pulling is     illustrated in\u00a0Figure 3.15:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.134027777777778in\"}</p> <p>Figure 3.15 -- The sequence of the semi-global model synthesis processes from uploading local models to pulling semi-global models</p> <ul> <li> <p>Let's look at semi-global model synthesis using the flowchart     between the agent, aggregator, and distributed database.</p> </li> <li> <p>The aggregator receives a local model from an agent. When receiving     the local model, the model filtering process will decide whether to     accept the uploaded model or not.</p> </li> <li> <p>This framework can be implemented using many different methods, such     as a basic scheme of checking the difference between the weights of     the global and local models. If the model is not valid, just discard     the local model.</p> </li> <li> <p>Then, a cluster model is created by aggregating all the accepted     local models.</p> </li> <li> <p>The aggregator stores the cluster model in a database, as well as     simultaneously retrieving the cluster models generated by other     cluster aggregators.</p> </li> <li> <p>A semi-global model is then synthesized from those cluster models     and will be used in the agents that are assigned to the cluster     aggregator.</p> </li> <li> <p>Figure 3.16\u00a0shows\u00a0how the\u00a0cluster aggregator proceeds with cluster     and semi-global model synthesis using a distributed database:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"8.567361111111111in\"}</p> <p>Figure 3.16 -- The procedure and flow of semi-global model synthesis</p> <ul> <li> <p>An\u00a0aggregator does not\u00a0need to retrieve all the cluster models     generated at each round to create a semi-global model. To synthesize     a semi-global model, the global model can eventually converge based     on the subset of models randomly selected by each aggregator.</p> </li> <li> <p>Using this approach, the robustness and\u00a0independence of aggregators     will be enhanced by compromising on the conditions to create the     global model at every update.</p> </li> <li> <p>This\u00a0framework can also resolve the bottlenecks in terms of     computation and communication typical to centralized FL systems.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>Here, we discussed the potential architecture, procedure flow, and     message sequences within an FL system.</p> </li> <li> <p>The typical FL system architecture consists of an aggregator,     agents, and a database server.</p> </li> <li> <p>These three components are constantly communicating with each other     to exchange system information and ML models to achieve model     aggregation.</p> </li> <li> <p>The key to implementing a good FL system is decoupling the critical     components and carefully designing the interfaces between them.</p> </li> <li> <p>We focused on the aspect of the simplicity of its design so that     further enhancement can be achieved by just adding additional     components to the systems. Horizontal decentralized design can also     help implement a scalable FL system.</p> </li> <li> <p>In the following section, we will discuss the implementation details     of achieving FL on the server side.</p> </li> <li> <p>As some critical aspects of the functionalities have been introduced     Here, you will be able to implement the basic system and smoothly     run the simulation with some ML applications.</p> </li> </ul> <p>[Section 4: FL Server - Python Implementation]{.underline}</p> <ul> <li> <p>The server-side implementation of a\u00a0federated learning\u00a0(FL)     system is critical for realizing authentic FL-enabled applications.</p> </li> <li> <p>We have discussed the basic system architecture and flow in the     previous section. Here, more hands-on implementation will be     discussed so that you can create a simple server and aggregator of     the FL system that various\u00a0machine learning\u00a0(ML)     applications can be connected to and tested on.</p> </li> <li> <p>This section describes an actual implementation aspect of FL     server-side components discussed in\u00a0section 3.</p> </li> <li> <p>Based on the understanding of how the entire process of the FL     system works, you will be able to go one step further to make it     happen with example code provided here and on GitHub.</p> </li> <li> <p>Once you understand the basic implementation principles using the     example code, it is a fun aspect to be able enhance the FL server     functionalities based on your own design.</p> </li> <li> <p>Here, we're going to cover the following topics:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Main software components of the aggregator</p> <ul> <li> <p>Implementing FL server-side functionalities</p> </li> <li> <p>Maintaining models for aggregation with the state manager</p> </li> <li> <p>Aggregating local models</p> </li> <li> <p>Running the FL server</p> </li> <li> <p>Implementing and running the database server</p> </li> <li> <p>Potential enhancements to the FL server</p> </li> </ul> <p>Technical requirements</p> <ul> <li>All the code files introduced Here can be found on GitHub     here:\u00a0[https://github.com/keshavaspanda/simple-fl]{.underline}</li> </ul> <p>Main software components of the aggregator \u2028and database</p> <ul> <li> <p>The architecture\u00a0of an aggregator with the FL server was introduced     in the previous section. Here, we will introduce the code that     realizes the basic functionalities of an FL system.</p> </li> <li> <p>The aggregator and database-side Python-based software components     are listed in the\u00a0aggregator\u00a0directory of\u00a0fl_main, as well     as\u00a0lib/util\u00a0and\u00a0pseudodb\u00a0folders, as in\u00a0Figure 4.1:</p> </li> </ul> <p>{width=\"3.407120516185477in\" height=\"3.9518897637795276in\"}</p> <p>Figure 4.1 -- Python software components for the aggregator as well as internal libraries and pseudo database</p> <ul> <li>The following is a brief description of the Python code files in the     aggregator.</li> </ul> <p>The Aggregator-side code</p> <ul> <li> <p>In this section, we\u00a0will touch on the\u00a0main Python files of the     aggregator-side related to the FL server thread, FL state manager,     and model aggregation itself.</p> </li> <li> <p>These aggregator-side code files are found in the\u00a0aggregator\u00a0folder.     The code in the repo only captures the model aggregation     perspective, not the entire engineering aspects of creating a     thorough FL platform.</p> </li> </ul> <p>FL server code (server_th.py)</p> <ul> <li> <p>This is the\u00a0main code that realizes\u00a0the whole basic flow of the FL     process from the communication processes between an aggregator     itself, agents, and a database to coordinating agent participation     and the aggregation of the ML models.</p> </li> <li> <p>It also initializes the global cluster model sent from the first     connected agent. It manages receiving local models and the cluster     model synthesis routine in which the cluster global model is formed     after collecting enough local models.</p> </li> </ul> <p>FL state manager (state_manager.py)</p> <ul> <li> <p>The state\u00a0manager\u00a0buffers the local model and cluster model data     that is needed for aggregation processes.</p> </li> <li> <p>The buffers will be filled out when the aggregator receives local     models from the agents and cleared when proceeding to the next round     of the FL process.</p> </li> <li> <p>The checking function of the aggregation criteria is also defined in     this file.</p> </li> </ul> <p>Aggregation code (aggregation.py)</p> <ul> <li> <p>The\u00a0aggregation\u00a0Python code will list the basic algorithms for     aggregating the model.</p> </li> <li> <p>In the code example used here Here, we will only introduce the     averaging method called\u00a0federated averaging\u00a0(FedAvg),     which\u00a0averages the weights of the collected local models considering     local dataset sizes to generate a cluster global model.</p> </li> </ul> <p>lib/util codes</p> <ul> <li>The Python\u00a0files for the internal\u00a0libraries     (communication_handler.py,\u00a0data_struc.py,\u00a0helpers.py,\u00a0messengers.py,     and\u00a0states.py) will be explained in the\u00a0Appendix,\u00a0Exploring     Internal Libraries.</li> </ul> <p>Database-side code</p> <ul> <li> <p>Database-side code consists of the pseudo database and the SQLite     database Python code files that can be found in the\u00a0pseudodb\u00a0folder.</p> </li> <li> <p>The pseudo database code is hosting a server to receive messages     from the aggregator and parse them to process as the ML model data     that can be utilized for the FL process.</p> </li> </ul> <p>Pseudo database code (pseudo_db.py)</p> <ul> <li>The function of pseudo database Python code is to accept the     messages related to the local and global cluster models from the     aggregator and push the information to the database. It also saves     the ML model binary files in the local file system.</li> </ul> <p>SQLite database code (sqlite_db.py)</p> <ul> <li> <p>The SQLite database Python code creates an actual SQLite database at     the specified path. It also has the function to insert data entries     related to the local and global cluster models into the database.</p> </li> <li> <p>Now that the aggregator and database-side software components are     defined, let\\'s move on to the configuration of the aggregator.</p> </li> </ul> <p>Configuring the aggregator</p> <ul> <li>The following\u00a0code is an example of the aggregator-side     configuration parameters defined in the\u00a0config_aggregator.json\u00a0file,     which can be found in the\u00a0setups\u00a0folder:</li> </ul> <p>{</p> <p>\\\"aggr_ip\\\": \\\"localhost\\\",</p> <p>\\\"db_ip\\\": \\\"localhost\\\",</p> <p>\\\"reg_socket\\\": \\\"8765\\\",</p> <p>\\\"exch_socket\\\": \\\"7890\\\",</p> <p>\\\"recv_socket\\\": \\\"4321\\\",</p> <p>\\\"db_socket\\\": \\\"9017\\\",</p> <p>\\\"round_interval\\\": 5,</p> <p>\\\"aggregation_threshold\\\": 1.0,</p> <p>\\\"polling\\\": 1</p> <p>}</p> <p>CopyExplain</p> <ul> <li> <p>The parameters include the aggregator's IP (the FL server's IP), the     database server's IP, and the various port numbers of the database     and agents.</p> </li> <li> <p>The round interval is the time of the interval at which the criteria     of aggregation are checked and the aggregation threshold defines the     percentage of collected local ML models needed to start the     aggregation process. The polling flag is related to whether to     utilize the\u00a0polling\u00a0method for communications between the aggregator     and agents or not.</p> </li> <li> <p>Now that we have covered the concept of the configuration file for     the aggregator side, let's move on to how the code is designed and     implemented.</p> </li> </ul> <p>FL server-side functions</p> <ul> <li> <p>In this section, we\u00a0will explain how you can implement the very     first version of an aggregator with an FL server system using the     actual code examples, which are in\u00a0server_th.py\u00a0in     the\u00a0aggregator\u00a0directory.</p> </li> <li> <p>This way, you will understand the core functionalities of the FL     server system and how they are implemented so that you can further     enhance a lot more functionalities on your own.</p> </li> <li> <p>Therefore, we will only cover the important and core functionalities     that are critical to conducting a simple FL process. The potential     enhancements will be listed in the later section of This     section,\u00a0Potential enhancements to the FL server.</p> </li> <li> <p>server_th.py\u00a0handles all the aspects of basic functionalities     related to the FL server side, so let's look into that in the     following section.</p> </li> </ul> <p>FL Server Library Imports</p> <ul> <li> <p>The FL server-side\u00a0code starts with importing the necessary     libraries. In particular,\u00a0lib.util\u00a0handles the basic supporting     functionalities to make the implementation of FL easy. The details     of the code can be found in the GitHub repository.</p> </li> <li> <p>The server code imports\u00a0StateManager\u00a0and\u00a0Aggregator\u00a0for the FL     processes. The code about the state manager and aggregation will be     discussed in later sections Here about\u00a0Maintaining models for     aggregation with the state manager\u00a0and\u00a0Aggregating local models.</p> </li> <li> <p>Here is the code for importing the necessary libraries:</p> </li> </ul> <p>import asyncio, logging, time, numpy as np</p> <p>from typing import List, Dict, Any</p> <p>from fl_main.lib.util.communication_handler import init_fl_server, send, send_websocket, receive</p> <p>from fl_main.lib.util.data_struc import convert_LDict_to_Dict</p> <p>from fl_main.lib.util.helpers import read_config, set_config_file</p> <p>from fl_main.lib.util.messengers import generate_db_push_message, generate_ack_message, generate_cluster_model_dist_message, generate_agent_participation_confirmation_message</p> <p>from fl_main.lib.util.states import ParticipateMSGLocation, ModelUpMSGLocation, PollingMSGLocation, ModelType, AgentMsgType</p> <p>from .state_manager import StateManager</p> <p>from .aggregation import Aggregator</p> <p>CopyExplain</p> <ul> <li>After we import\u00a0the necessary libraries, let us move on to designing     an FL\u00a0Server\u00a0class.</li> </ul> <p>Defining the FL Server class</p> <ul> <li>In practice, it is\u00a0wise to define the\u00a0Server\u00a0class, using which you     can create an instance of the FL server that has the functionalities     discussed in\u00a0earlier, as follows:</li> </ul> <p>class Server:</p> <p>\\\"\\\"\\\"</p> <p>FL Server class defining the functionalities of</p> <p>agent registration, global model synthesis, and</p> <p>handling mechanisms of messages by agents.</p> <p>\\\"\\\"\\\"</p> <p>CopyExplain</p> <ul> <li> <p>Again, the\u00a0server\u00a0class primarily provides the functionalities of     agent registration and global model synthesis and handles the     mechanisms of uploaded local models and polling messages sent from     agents. It also serves as the interface between the aggregator and     database and between the aggregator and agents.</p> </li> <li> <p>The FL server class functionality is now clear -- next is     initializing and configuring the server.</p> </li> </ul> <p>Initializing the FL server</p> <ul> <li>The\u00a0following code inside the\u00a0__init__\u00a0constructor is an example     of the initialization process of the\u00a0Server\u00a0instance:</li> </ul> <p>def __init__(self):</p> <p>config_file = set_config_file(\\\"aggregator\\\")</p> <p>self.config = read_config(config_file)</p> <p>self.sm = StateManager()</p> <p>self.agg = Aggregator(self.sm)</p> <p>self.aggr_ip = self.config[\\'aggr_ip\\']</p> <p>self.reg_socket = self.config[\\'reg_socket\\']</p> <p>self.recv_socket = self.config[\\'recv_socket\\']</p> <p>self.exch_socket = self.config[\\'exch_socket\\']</p> <p>self.db_ip = self.config[\\'db_ip\\']</p> <p>self.db_socket = self.config[\\'db_socket\\']</p> <p>self.round_interval = self.config[\\'round_interval\\']</p> <p>self.is_polling = bool(self.config[\\'polling\\'])</p> <p>self.sm.agg_threshold =</p> <p>self.config[\\'aggregation_threshold\\']</p> <p>CopyExplain</p> <ul> <li> <p>Then,\u00a0self.config\u00a0stores the information from     the\u00a0config_aggregator.json\u00a0file discussed in the preceding code     block.</p> </li> <li> <p>self.sm\u00a0and\u00a0self.agg\u00a0have instances of the state manager class and     aggregator class discussed as follows, respectively.</p> </li> <li> <p>self.aggr_ip\u00a0reads an IP address from the aggregator's configuration     file.</p> </li> <li> <p>Then,\u00a0reg_socket\u00a0and\u00a0recv_socket\u00a0will be set up, where\u00a0reg_socket\u00a0is     used for agents to register themselves together with an aggregator     IP address stored as\u00a0self.aggr_ip, and\u00a0recv_socket\u00a0is used for     receiving local models from agents, together with an aggregator IP     address stored as\u00a0self.aggr_ip. Both\u00a0reg_socket\u00a0and\u00a0recv_socket\u00a0in     this example code can be read from the aggregator's configuration     file.</p> </li> <li> <p>The\u00a0exch_socket\u00a0is the port number used to send the global model     back to the agent together with the agent IP address, which is     initialized with the configuration parameter in the initialization     process.</p> </li> <li> <p>The information to get connected to the database server will then be     configured, where\u00a0dp_ip\u00a0and\u00a0db_socket\u00a0will be the IP address and the     port number of the database server, respectively, all read from     the\u00a0config_aggregator.json\u00a0file.</p> </li> <li> <p>round_interval\u00a0is an interval time to check whether the aggregation     criteria for starting the model aggregation process are met or not.</p> </li> <li> <p>The\u00a0is_polling\u00a0flag is related to whether to use the\u00a0polling\u00a0method     from the agents or not. The polling flag must be the same as the one     used in the agent-side configuration file.</p> </li> <li> <p>agg_threshold\u00a0is also the percentage over the number of collected     local models that is used in     the\u00a0ready_for_local_aggregation\u00a0function where if the percentage of     the collected models is equal to or more than\u00a0agg_threshold, the FL     server starts the aggregation process of the local models.</p> </li> <li> <p>Both\u00a0self.round_interval\u00a0and\u00a0self.agg_threshold\u00a0are read from the     configuration file in this example code too.</p> </li> <li> <p>Now that the\u00a0configuration has been set up, we will talk about how     to register agents that are trying to participate in the FL process.</p> </li> </ul> <p>Agent Registration</p> <ul> <li> <p>In this\u00a0section, the simplified and asynchronous\u00a0register\u00a0function     is described to receive the participation message specifying the     model structures and return socket information for future model     exchanges. It also sends the welcome message back to the agent as a     response.</p> </li> <li> <p>The registration process of agents is described in the following     example code:</p> </li> </ul> <p>async def register(self, websocket: str, path):\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0</p> <p>msg = await receive(websocket)</p> <p>es = self._get_exch_socket(msg)</p> <p>agent_nm = msg[int(ParticipateMSGLocation.agent_name)]</p> <p>agent_id = msg[int(ParticipateMSGLocation.agent_id)]</p> <p>ip = msg[int(ParticipateMSGLocation.agent_ip)]</p> <p>id, es = self.sm.add_agent(agent_nm, agent_id, ip, es)</p> <p>if self.sm.round == 0:</p> <p>await self._initialize_fl(msg)</p> <p>await self._send_updated_global_model( \\</p> <p>websocket, id, es)</p> <p>CopyExplain</p> <ul> <li> <p>In this example code, the received message from an agent, defined     here as\u00a0msg, is decoded by the\u00a0receive\u00a0function imported from     the\u00a0communication_handler\u00a0code.</p> </li> <li> <p>In particular, the\u00a0self.sm.add_agent(agent_name, agent_id, addr,     es)\u00a0function takes the agent name, agent ID, agent IP address, and     the\u00a0exch_socket\u00a0number included in the\u00a0msg\u00a0message in order to     accept the messages from this agent, even if the agent is     temporarily disconnected and then connected again.</p> </li> <li> <p>After that, the registration function checks whether it should move     on to the process of initial models or not, depending on the FL     round that is tracked with\u00a0self.sm.round. If the FL process is not     happening yet, that is, if\u00a0self.sm.round\u00a0is\u00a00, it calls     the\u00a0_initialize_fl(msg)\u00a0function in order to initialize the FL     process.</p> </li> <li> <p>Then, the FL server sends the updated global model back to the agent     by calling the\u00a0_send_updated_global_model(websocket, id,     es)\u00a0function. The function takes the WebSocket, agent ID,     and\u00a0exch_socket\u00a0as parameters and creates a reply\u00a0message to the     agent to notify it whether the participation message has been     accepted or not.</p> </li> <li> <p>The registration process of agents with the FL server is simplified     in this example code here. In a production environment, all the     system information from the agent will be pushed to the database so     that an agent that loses the connection to the FL server can be     recovered anytime by reconnecting to the FL server.</p> </li> <li> <p>Usually, if the FL server is installed in the cloud and agents are     connected to the FL server from their local environment, this     push-back mechanism from the aggregator to agents will not work     because of security settings such as firewalls.</p> </li> <li> <p>We do not discuss the topic of security issues in detail, so you are     encouraged to use the\u00a0polling\u00a0method implemented in     the\u00a0simple-fl\u00a0code to communicate between the\u00a0cloud-based aggregator     and local agents.</p> </li> </ul> <p>Getting socket information to push the global model back to agents</p> <ul> <li>The following\u00a0function\u00a0called\u00a0_get_exch_socket\u00a0takes     a\u00a0participation\u00a0message from the agent and decides which port to use     to reach out to the agent depending on the simulation flag in the     message:</li> </ul> <p>def _get_exch_socket(self, msg):</p> <p>if msg[int(ParticipateMSGLocation.sim_flag)]:</p> <p>es = msg[int(ParticipateMSGLocation.exch_socket)]</p> <p>else:</p> <p>es = self.exch_socket</p> <p>return es</p> <p>CopyExplain</p> <ul> <li> <p>We support a simulation run in this implementation exercise by which     you can run all the FL system components of a database, aggregator,     and multiple agents in one machine.</p> </li> <li> <p>Initializing the FL process if necessary</p> </li> <li> <p>The\u00a0asynchronous\u00a0_initialize_fl\u00a0function is for initializing an FL     process that is only called when the round of FL is\u00a00. The following     is the code to do so:</p> </li> </ul> <p>async def _initialize_fl(self, msg):</p> <p>agent_id = msg[int(ParticipateMSGLocation.agent_id)]</p> <p>model_id = msg[int(ParticipateMSGLocation.model_id)]</p> <p>gene_time = msg[int(ParticipateMSGLocation.gene_time)]</p> <p>lmodels = msg[int(ParticipateMSGLocation.lmodels)]</p> <p>perf_val = msg[int(ParticipateMSGLocation.meta_data)]</p> <p>init_flag = \\</p> <p>bool(msg[int(ParticipateMSGLocation.init_flag)])</p> <p>self.sm.initialize_model_info(lmodels, init_flag)</p> <p>await self._push_local_models( \\</p> <p>agent_id, model_id, lmodels, gene_time, perf_val)</p> <p>self.sm.increment_round()</p> <p>CopyExplain</p> <ul> <li> <p>After extracting the agent ID (agent_id), the model ID (model_id),     local models from an agent (lmodels), the generated time of the     model (gene_time), the performance data (perf_val), and the value     of\u00a0init_flag\u00a0from the received message,     the\u00a0initialize_model_info\u00a0function of the state manager code is     called, which is explained in a later section of This section.</p> </li> <li> <p>This function then pushes the local model to the database by calling     the\u00a0_push_local_models\u00a0function, which is also described in this     section. You can refer to\u00a0the\u00a0Functions to push the local and     global models to the database\u00a0section.</p> </li> <li> <p>After that, the round is incremented to proceed to the first round     in FL.</p> </li> </ul> <p>Confirming agent participation with an updated global model</p> <ul> <li>After\u00a0initializing\u00a0the (cluster) global model, the global models     need to be sent to the agent connected to the aggregator through     this registration process. The     asynchronous\u00a0_send_updated_global_model\u00a0function as follows handles     the process of sending the global models to the agent by taking the     WebSocket information, agent ID, and the port to use to reach out to     the agent as parameters. The following code block describes the     procedure:</li> </ul> <p>async def _send_updated_global_model( \\</p> <p>self, websocket, agent_id, exch_socket):</p> <p>model_id = self.sm.cluster_model_ids[-1]</p> <p>cluster_models = \\</p> <p>convert_LDict_to_Dict(self.sm.cluster_models)</p> <p>reply = generate_agent_participation_confirm_message(</p> <p>self.sm.id, model_id, cluster_models, self.sm.round,</p> <p>agent_id, exch_socket, self.recv_socket)</p> <p>await send_websocket(reply, websocket)</p> <p>CopyExplain</p> <ul> <li> <p>If the FL process has already started, that is, the\u00a0self.sm.round\u00a0is     more than 0 already, we get the cluster models from their buffer and     convert them into a dictionary format with     the\u00a0convert_LDict_to_Dict\u00a0library function.</p> </li> <li> <p>Then, the reply message is packaged using     the\u00a0generate_\u00a0agent_participation_confirm_message\u00a0function and sent     to the agent that just connected or reconnected to the aggregator by     calling the\u00a0send_websocket(reply, websocket)\u00a0function. Please also     refer to the\u00a0Functions to send the global models to the     agents\u00a0section.</p> </li> <li> <p>Now that\u00a0we\u00a0understand the agents' registration process, let's move     on to the implementation of handling the local ML models and polling     messages.</p> </li> </ul> <p>The server for handling messages from local agents</p> <ul> <li>The\u00a0asynchronous\u00a0receive_msg_from_agent\u00a0process\u00a0at the FL server is     constantly running to receive local model updates and to push them     to the database and the memory buffer temporally saving local     models. It also responds to the polling messages from the local     agents. The following code explains this functionality:</li> </ul> <p>async def receive_msg_from_agent(self, websocket, path):</p> <p>msg = await receive(websocket)</p> <p>if msg[int(ModelUpMSGLocation.msg_type)] == \\</p> <p>AgentMsgType.update:</p> <p>await self._process_lmodel_upload(msg)</p> <p>elif msg[int(PollingMSGLocation.msg_type)] == \\</p> <p>AgentMsgType.polling:</p> <p>await self._process_polling(msg, websocket)\u00a0\u00a0</p> <p>CopyExplain</p> <ul> <li>We will then look into the two functions called by     the\u00a0receive_msg_from_agent\u00a0function as\u00a0shown in the\u00a0preceding code     blocks, which are     the\u00a0_process_lmodel_upload\u00a0and\u00a0_process_polling\u00a0functions.</li> </ul> <p>Processing a model upload by local agents</p> <ul> <li>The\u00a0asynchronous\u00a0_process_lmodel_upload\u00a0function deals\u00a0with     the\u00a0AgentMsgType.update\u00a0message. The following code block is about     the function related to receiving the local ML models and putting     them into the buffer in the state manager:</li> </ul> <p>async def _process_lmodel_upload(self, msg):</p> <p>lmodels = msg[int(ModelUpMSGLocation.lmodels)]</p> <p>agent_id = msg[int(ModelUpMSGLocation.agent_id)]</p> <p>model_id = msg[int(ModelUpMSGLocation.model_id)]</p> <p>gene_time = msg[int(ModelUpMSGLocation.gene_time)]</p> <p>perf_val = msg[int(ModelUpMSGLocation.meta_data)]</p> <p>await self._push_local_models( \\</p> <p>agent_id, model_id, lmodels, gene_time, perf_val)</p> <p>self.sm.buffer_local_models( \\</p> <p>lmodels, participate=False, meta_data=perf_val)</p> <p>CopyExplain</p> <ul> <li> <p>First, it extracts the agent ID (agent_id), the model ID (model_id),     local models from an agent (lmodels), the generated time of the     model (gene_time), and the performance data (perf_val) from the     received message, and then calls the\u00a0_push_local_models\u00a0function to     push the local models to the database.</p> </li> <li> <p>The\u00a0buffer_local_models\u00a0function is then called to save the local     models (lmodels) in the memory buffer.     The\u00a0buffer_local_models\u00a0function is described in the\u00a0Maintaining     models for aggregation with the state manager\u00a0section.</p> </li> </ul> <p>Processing polling by agents</p> <ul> <li>The following\u00a0asynchronous\u00a0_process_polling\u00a0function\u00a0deals with     the\u00a0AgentMsgType.polling\u00a0message:</li> </ul> <p>async def _process_polling(self, msg, websocket):</p> <p>if self.sm.round &gt; \\</p> <p>int(msg[int(PollingMSGLocation.round)]):</p> <p>model_id = self.sm.cluster_model_ids[-1]</p> <p>cluster_models = \\</p> <p>convert_LDict_to_Dict(self.sm.cluster_models)</p> <p>msg = generate_cluster_model_dist_message( \\</p> <p>self.sm.id, model_id, self.sm.round, \\</p> <p>cluster_models)</p> <p>await send_websocket(msg, websocket)</p> <p>else:</p> <p>msg = generate_ack_message()</p> <p>await send_websocket(msg, websocket)\u00a0\u00a0</p> <p>CopyExplain</p> <ul> <li> <p>If the FL round (self.sm.round) is greater than the local FL round     included in the received message that is maintained by the local     agent itself, it means that the model aggregation is done during the     period between the time when the agent polled to the aggregator last     time and now.</p> </li> <li> <p>In this case,\u00a0cluster_models\u00a0that are converted into a dictionary     format are packaged into a response message     by\u00a0generate_cluster_model_dist_message\u00a0and sent back to the agent     via the\u00a0send_websocket\u00a0function.</p> </li> <li> <p>Otherwise, the aggregator just returns the\u00a0ACK\u00a0message to the     agent, generated by the\u00a0generate_ack_message\u00a0function.</p> </li> <li> <p>Now we are ready to aggregate the local models received from the     agents, so let us look into the model aggregation routine.</p> </li> </ul> <p>The global model synthesis routine</p> <ul> <li> <p>The global\u00a0model synthesis\u00a0routine process designed in\u00a0async def     model_synthesis_routine(self)\u00a0in the FL server periodically checks     the number of stored models and executes global model synthesis if     there are enough local models collected to meet the aggregation     threshold.</p> </li> <li> <p>The following code describes the model synthesis routine process     that periodically checks the aggregation criteria and executes model     synthesis:</p> </li> </ul> <p>async def model_synthesis_routine(self):</p> <p>while True:</p> <p>await asyncio.sleep(self.round_interval)</p> <p>if self.sm.ready_for_local_aggregation():\u00a0\u00a0</p> <p>self.agg.aggregate_local_models()</p> <p>await self._push_cluster_models()</p> <p>if self.is_polling == False:</p> <p>await self._send_cluster_models_to_all()</p> <p>self.sm.increment_round()</p> <p>CopyExplain</p> <ul> <li> <p>This process is asynchronous, running with a\u00a0while\u00a0loop.</p> </li> <li> <p>In particular, once the criteria set     by\u00a0ready_for_local_aggregation\u00a0(explained in the\u00a0Maintaining models     for aggregation with the state manager\u00a0section) are met,     the\u00a0aggregate_local_models\u00a0function imported from     the\u00a0aggregator.py\u00a0file is called, where this function averages the     weights of the collected local models based on\u00a0FedAvg. Further     explanation of the\u00a0aggregate_local_models\u00a0function can be found in     the\u00a0Aggregating local models\u00a0section.</p> </li> <li> <p>Then,\u00a0await self._push_cluster_models()\u00a0is called to push the     aggregated cluster global model to the database.</p> </li> <li> <p>await self._send_cluster_models_to_all()\u00a0is for sending the updated     global model to all the agents connected to the aggregator if     the\u00a0polling\u00a0method is not used.</p> </li> <li> <p>Last but not least, the FL round is incremented     by\u00a0self.sm.increment_round().</p> </li> <li> <p>Once the cluster global model is generated, the models need to be     sent to the connected\u00a0agents with\u00a0the functions described in the     following section.</p> </li> </ul> <p>Functions to send the global models to the agents</p> <ul> <li>The\u00a0functionality\u00a0of sending global models to the connected agents     is dealt with by the\u00a0_send_cluster_models_to_all\u00a0function. This is     an asynchronous function to send out cluster global models to all     agents under this aggregator as follows:</li> </ul> <p>async def _send_cluster_models_to_all(self):</p> <p>model_id = self.sm.cluster_model_ids[-1]</p> <p>cluster_models = \\</p> <p>convert_LDict_to_Dict(self.sm.cluster_models)</p> <p>msg = generate_cluster_model_dist_message( \\</p> <p>self.sm.id, model_id, self.sm.round, \\</p> <p>cluster_models)</p> <p>for agent in self.sm.agent_set:</p> <p>await send(msg, agent[\\'agent_ip\\'], agent[\\'socket\\'])</p> <p>CopyExplain</p> <ul> <li> <p>After getting the cluster models' information, it creates the     message including the cluster models, round, model ID, and     aggregator ID information using     the\u00a0generate_cluster_model_dist_message\u00a0function and calls     the\u00a0send\u00a0function from the\u00a0communication_handler\u00a0libraries to send     the global models to all the agents in the\u00a0agent_set\u00a0registered     through the agent participation process.</p> </li> <li> <p>Sending the cluster global models to the connected agents has now     been explained. Next, we explain\u00a0how to push the local and cluster     models to the database.</p> </li> </ul> <p>Functions to push the local and global models to the database</p> <ul> <li>The\u00a0_push_local_models\u00a0and\u00a0_push_cluster_models\u00a0functions are both     called internally to push and send the local models and cluster     global models to the database.</li> </ul> <p>Pushing local models to the database</p> <ul> <li>Here is\u00a0the\u00a0_push_local_models\u00a0function\u00a0for pushing a given set of     local models to the database:</li> </ul> <p>async def _push_local_models(self, agent_id: str, \\</p> <p>model_id: str, local_models: Dict[str, np.array], \\</p> <p>gene_time: float, performance: Dict[str, float]) \\</p> <p>-&gt; List[Any]:</p> <p>return await self._push_models(</p> <p>agent_id, ModelType.local, local_models, \\</p> <p>model_id, gene_time, performance)</p> <p>CopyExplain</p> <ul> <li> <p>The\u00a0_push_local_models\u00a0function takes parameters such as the agent     ID, local models, the model ID, the generated time of the model, and     the performance data, and returns a response message if there is     one.</p> </li> <li> <p>Pushing cluster models to the database</p> </li> <li> <p>The\u00a0following\u00a0_push_cluster_models\u00a0function\u00a0is for pushing the     cluster global models to the database:</p> </li> </ul> <p>async def _push_cluster_models(self) -&gt; List[Any]:</p> <p>model_id = self.sm.cluster_model_ids[-1]</p> <p>models = convert_LDict_to_Dict(self.sm.cluster_models)</p> <p>meta_dict = dict({ \\</p> <p>\\\"num_samples\\\" : self.sm.own_cluster_num_samples})</p> <p>return await self._push_models( \\</p> <p>self.sm.id, ModelType.cluster, models, model_id, \\</p> <p>time.time(), meta_dict)</p> <p>CopyExplain</p> <ul> <li> <p>_push_cluster_models\u00a0in this code does not take any parameters, as     those parameters can be obtained from the instance information and     buffered memory data of the state manager.</p> </li> <li> <p>For example,\u00a0self.sm.cluster_model_ids[-1]\u00a0obtains the ID of the     latest cluster model, and\u00a0self.sm.cluster_models\u00a0stores the latest     cluster model itself, which is converted into\u00a0models\u00a0with a     dictionary format to be sent\u00a0to the\u00a0database. It also     creates\u00a0mata_dict\u00a0to store the number of samples.</p> </li> <li> <p>Pushing ML models to the database</p> </li> <li> <p>Both the\u00a0preceding\u00a0functions call the\u00a0_push_models\u00a0function as     follows:</p> </li> </ul> <p>async def _push_models(</p> <p>self, component_id: str, model_type: ModelType,</p> <p>models: Dict[str, np.array], model_id: str,</p> <p>gene_time: float, performance_dict: Dict[str, float])</p> <p>-&gt; List[Any]:</p> <p>msg = generate_db_push_message(component_id, \\</p> <p>self.sm.round, model_type, models, model_id, \\</p> <p>gene_time, performance_dict)</p> <p>resp = await send(msg, self.db_ip, self.db_socket)</p> <p>return resp</p> <p>CopyExplain</p> <ul> <li> <p>In this code example, the\u00a0_push_models\u00a0function takes parameters     such as\u00a0component_id\u00a0(the ID of the aggregator or     agent),\u00a0model_type, such as local or cluster     model,\u00a0models\u00a0themselves,\u00a0model_id,\u00a0gene_time\u00a0(the time the model is     created), and\u00a0performance_dict\u00a0as the performance metrics of the     models.</p> </li> <li> <p>Then, the message to be sent to the database (using     the\u00a0send\u00a0function) is created by     the\u00a0generate_db_push_message\u00a0function, taking these parameters     together with the FL round information. It returns a response     message from the database.</p> </li> <li> <p>Now that we have explained all the core functionalities related to     the FL server, let us look into the\u00a0role of\u00a0the state manager, which     maintains all the models needed for the aggregation process.</p> </li> </ul> <p>Maintaining models for aggregation with the \u2028state manager</p> <ul> <li>In this section, we\u00a0will explain\u00a0state_manager.py,\u00a0which handles     maintaining the models and necessary volatile information related to     the aggregation of local models.</li> </ul> <p>State Manager Library Imports</p> <ul> <li>This code imports\u00a0the\u00a0following. The internal libraries     for\u00a0data_struc,\u00a0helpers, and\u00a0states\u00a0are introduced in     the\u00a0Appendix,\u00a0Exploring Internal Libraries:</li> </ul> <p>import numpy as np</p> <p>import logging</p> <p>import time</p> <p>from typing import Dict, Any</p> <p>from fl_main.lib.util.data_struc import LimitedDict</p> <p>from fl_main.lib.util.helpers import generate_id, generate_model_id</p> <p>from fl_main.lib.util.states import IDPrefix</p> <p>CopyExplain</p> <ul> <li>After importing the necessary libraries, let's define the state     manager class.</li> </ul> <p>Defining the state manager class</p> <ul> <li>The state\u00a0manager\u00a0class (Class StateManager), as seen     in\u00a0state_manager.py, is defined in the following code:</li> </ul> <p>class StateManager:</p> <p>\\\"\\\"\\\"</p> <p>StateManager instance keeps the state of an aggregator.</p> <p>Functions are listed with this indentation.</p> <p>\\\"\\\"\\\"</p> <p>CopyExplain</p> <ul> <li> <p>This keeps track of the state information of an aggregator. The     volatile state of an aggregator and agents should also be stored,     such as local models, agents' info connected to the aggregator,     cluster models generated by the aggregation process, and the current     round number.</p> </li> <li> <p>After defining the state manager, let us move on to initializing the     state manager.</p> </li> </ul> <p>Initializing the state manager</p> <ul> <li>In the\u00a0__init__\u00a0constructor, the\u00a0information\u00a0related to the FL     process is\u00a0configured. The following code is an example of how to     construct the state manager:</li> </ul> <p>def __init__(self):</p> <p>self.id = generate_id()</p> <p>self.agent_set = list()</p> <p>self.mnames = list()</p> <p>self.round = 0</p> <p>self.local_model_buffers = LimitedDict(self.mnames)</p> <p>self.local_model_num_samples = list()</p> <p>self.cluster_models = LimitedDict(self.mnames)</p> <p>self.cluster_model_ids = list()</p> <p>self.initialized = False</p> <p>self.agg_threshold = 1.0</p> <p>CopyExplain</p> <ul> <li> <p>The ID of the\u00a0self.id\u00a0aggregator can be generated randomly using     the\u00a0generate_id()\u00a0function from the\u00a0util.helpers\u00a0library.</p> </li> <li> <p>self.agent_set\u00a0is a set of agents connected to the aggregator where     the format of the set is a collection of dictionary information,     related to agents in this case.</p> </li> <li> <p>self.mnames\u00a0stores the names of each layer of the ML models to be     aggregated in a list format.</p> </li> <li> <p>self.round\u00a0is initialized to be\u00a00\u00a0so that the round of FL is     initialized.</p> </li> <li> <p>local_model_buffers\u00a0is a list of local models collected by agents     stored in the memory\u00a0space.\u00a0local_model_buffers\u00a0accepts the local     models sent from the agents for\u00a0each FL round, and once the round is     completed by the aggregation process, this buffer is cleared and     starts accepting the next round's local models.</p> </li> <li> <p>self.local_model_num_samples\u00a0is a list that stores the number of     data samples for the models that are collected in the buffer.</p> </li> <li> <p>self.cluster_models\u00a0is a collection of global cluster models in     the\u00a0LimitedDict\u00a0format, and\u00a0self.cluster_model_ids\u00a0is a list of IDs     of cluster models.</p> </li> <li> <p>self.initialized\u00a0becomes\u00a0True\u00a0once the initial global model is set     and is\u00a0False\u00a0otherwise.</p> </li> <li> <p>self.agg_threshold\u00a0is initialized to be\u00a01.0, which is overwritten by     the value specified in the\u00a0config_aggregator.json\u00a0file.</p> </li> <li> <p>After initializing the state manager, let us investigate     initializing a global model next.</p> </li> </ul> <p>Initializing a global model</p> <ul> <li>The\u00a0following\u00a0initialize_model_info\u00a0function\u00a0sets up the initial     global\u00a0model to be used by the other agents:</li> </ul> <p>def initialize_model_info(self, lmodels, \\</p> <p>init_weights_flag):</p> <p>for key in lmodels.keys():</p> <p>self.mnames.append(key)</p> <p>self.local_model_buffers = LimitedDict(self.mnames)</p> <p>self.cluster_models = LimitedDict(self.mnames)</p> <p>self.clear_lmodel_buffers()</p> <p>if init_weights_flag:</p> <p>self.initialize_models(lmodels, \\</p> <p>weight_keep=init_weights_flag)</p> <p>else:</p> <p>self.initialize_models(lmodels, weight_keep=False)</p> <p>CopyExplain</p> <ul> <li> <p>It fills up the model names (self.mnames) extracted from the local     models (lmodels) sent from an initial agent.</p> </li> <li> <p>Together with the model     names,\u00a0local_model_buffers\u00a0and\u00a0cluster_models\u00a0are re-initialized     too. After clearing the local model buffers, it calls     the\u00a0initialize_models\u00a0function.</p> </li> <li> <p>The following\u00a0initialize_models\u00a0function initializes the structure     of neural networks (numpy.array) based on the initial base models     received as parameters of models\u00a0with a dictionary format     (str\u00a0or\u00a0np.array):</p> </li> </ul> <p>def initialize_models(self, models: Dict[str, np.array], \\</p> <p>weight_keep: bool = False):</p> <p>self.clear_saved_models()</p> <p>for mname in self.mnames:</p> <p>if weight_keep:</p> <p>m = models[mname]</p> <p>else:</p> <p>m = np.zeros_like(models[mname])</p> <p>self.cluster_models[mname].append(m)</p> <p>id = generate_model_id(IDPrefix.aggregator, \\</p> <p>self.id, time.time())</p> <p>self.cluster_model_ids.append(id)</p> <p>self.initialized = True</p> <p>CopyExplain</p> <ul> <li> <p>For each layer of the model, defined here as model names, this     function fills out the model parameters. Depending on     the\u00a0weight_keep\u00a0flag, the model is initialized with zeros or     parameters that are received.</p> </li> <li> <p>This way, the initial cluster global model is constructed together     with the randomized model ID. If an agent sends a different ML model     than the model architecture defined here, the aggregator rejects the     acceptance of the model or gives an error message to the agent.     Nothing is returned.</p> </li> <li> <p>So, we have\u00a0covered\u00a0initializing the global model. In the following     section, we will explain the core part of the FL process, which is     checking aggregation criteria.</p> </li> </ul> <p>Checking the aggregation criteria</p> <ul> <li>The\u00a0following\u00a0code, called\u00a0ready_for_local_aggregation, is     for\u00a0checking the aggregation criteria:</li> </ul> <p>def ready_for_local_aggregation(self) -&gt; bool:</p> <p>if len(self.mnames) == 0:</p> <p>return False</p> <p>num_agents = int(self.agg_threshold * \\</p> <p>len(self.agent_set))</p> <p>if num_agents == 0: num_agents = 1</p> <p>num_collected_lmodels = \\</p> <p>len(self.local_model_buffers[self.mnames[0]])</p> <p>if num_collected_lmodels &gt;= num_agents:</p> <p>return True</p> <p>else:</p> <p>return False\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0</p> <p>CopyExplain</p> <ul> <li> <p>This\u00a0ready_for_local_aggregation\u00a0function returns a\u00a0bool\u00a0value to     identify whether the aggregator can start the aggregation process.     It returns\u00a0True\u00a0if it satisfies the aggregation criteria (such as     collecting enough local models to aggregate) and\u00a0False\u00a0otherwise.     The aggregation threshold,\u00a0agg_threshold, is     configured\u00a0in\u00a0the\u00a0config_aggregator.json\u00a0file.</p> </li> <li> <p>The following section is about buffering the local models that are     used for the aggregation process.</p> </li> </ul> <p>Buffering the local models</p> <ul> <li>The following\u00a0code\u00a0on\u00a0buffer_local_models\u00a0stores local models from     an agent in the local model buffer:</li> </ul> <p>def buffer_local_models(self, models: Dict[str, np.array],</p> <p>participate=False, meta_data: Dict[Any, Any] = {}):</p> <p>if not participate:\u00a0\u00a0</p> <p>for key, model in models.items():</p> <p>self.local_model_buffers[key].append(model)</p> <p>try:</p> <p>num_samples = meta_data[\\\"num_samples\\\"]</p> <p>except:</p> <p>num_samples = 1</p> <p>self.local_model_num_samples.append( \\</p> <p>int(num_samples))</p> <p>else:\u00a0\u00a0</p> <p>pass</p> <p>if not self.initialized:</p> <p>self.initialize_models(models)</p> <p>CopyExplain</p> <ul> <li> <p>The parameters include the local\u00a0models\u00a0formatted as a dictionary as     well as meta-information such as the number of samples.</p> </li> <li> <p>First, this function checks whether the local model sent from an     agent is either the initial model or not by checking the     participation flag. If it is an initial model, it calls     the\u00a0initialize_model\u00a0function, as shown in the preceding code block.</p> </li> <li> <p>Otherwise, for each layer of the model defined with model names, it     stores the\u00a0numpy\u00a0array in the\u00a0self.local_model_buffers. The\u00a0key\u00a0is     the model name and\u00a0model\u00a0mentioned in the preceding code are the     actual parameters of the model.</p> </li> <li> <p>Optionally, it can accept the number of samples or data sources that     the agent has used for the retraining process and push it to     the\u00a0self.\u00a0local_model_num_samples\u00a0buffer.</p> </li> <li> <p>This function is called when the FL server receives the local models     from an agent during the\u00a0receive_msg_from_agent\u00a0routine.</p> </li> <li> <p>With that, the local model buffer has been explained. Next, we will     explain how to clear the saved\u00a0models\u00a0so that aggregation can     continue without having to store unnecessary models in the buffer.</p> </li> </ul> <p>Clearing the saved models</p> <ul> <li>The\u00a0following\u00a0clear_saved_models\u00a0function clears all cluster models     stored in this\u00a0round:</li> </ul> <p>def clear_saved_models(self):</p> <p>for mname in self.mnames:</p> <p>self.cluster_models[mname].clear()</p> <p>CopyExplain</p> <ul> <li> <p>This function is called when initializing the FL process at the very     beginning and the cluster global model is emptied to start a fresh     FL round again.</p> </li> <li> <p>The following function, the\u00a0clear_lmodel_buffers\u00a0function, clears     all the buffered local models to prepare for the next FL round:</p> </li> </ul> <p>def clear_lmodel_buffers(self):</p> <p>for mname in self.mnames:</p> <p>self.local_model_buffers[mname].clear()</p> <p>self.local_model_num_samples = list()</p> <p>CopyExplain</p> <ul> <li> <p>Clearing the local models in\u00a0local_model_buffers\u00a0is critical when     proceeding to the next FL round. Without this process, the models to     be aggregated are mixed up with the non-relevant models from other     rounds, and eventually, the performance\u00a0of the\u00a0FL is sometimes     degraded.</p> </li> <li> <p>Next, we will explain the basic framework of adding agents during     the FL process.</p> </li> </ul> <p>Adding agents</p> <ul> <li>This\u00a0add_agent\u00a0function\u00a0deals\u00a0with brief agent registration using     system memory:</li> </ul> <p>def add_agent(self, agent_name: str, agent_id: str, \\</p> <p>agent_ip: str, socket: str):</p> <p>for agent in self.agent_set:</p> <p>if agent_name == agent[\\'agent_name\\']:</p> <p>return agent[\\'agent_id\\'], agent[\\'socket\\']</p> <p>agent = {</p> <p>\\'agent_name\\': agent_name,</p> <p>\\'agent_id\\': agent_id,</p> <p>\\'agent_ip\\': agent_ip,</p> <p>\\'socket\\': socket</p> <p>}</p> <p>self.agent_set.append(agent)</p> <p>return agent_id, socket</p> <p>CopyExplain</p> <ul> <li> <p>This function just adds agent-related information to     the\u00a0self.agent_set\u00a0list. The agent information includes the agent     name, agent ID, agent IP address, and the\u00a0socket\u00a0number to reach out     to the agent.</p> </li> <li> <p>The\u00a0socket\u00a0number can be used when sending the cluster global model     to the agent connected to the aggregator and when the\u00a0push\u00a0method is     used for communication between an aggregator and an agent.</p> </li> <li> <p>This function is only called during the agent registration process     and returns the agent ID and the\u00a0socket\u00a0number.</p> </li> <li> <p>If the agent is already registered, which means there is already an     agent with the same name in\u00a0agent_set, it returns the agent ID and     the\u00a0socket\u00a0number of the existing agent.</p> </li> <li> <p>Again, this\u00a0push\u00a0communication method from an aggregator to agents     does not work under certain security circumstances. It is     recommended to use the\u00a0polling\u00a0method that the agents use to     constantly check whether the aggregator has an updated global model     or not.</p> </li> <li> <p>The agent\u00a0registration\u00a0mechanism can be expanded using a database,     which will give you better management of the distributed systems.</p> </li> <li> <p>Next, we will touch on incrementing the FL round.</p> </li> </ul> <p>Incrementing the FL round</p> <ul> <li>The\u00a0increment_round\u00a0function\u00a0just\u00a0increments the round number     precisely managed by the state manager:</li> </ul> <p>def increment_round(self):</p> <p>self.round += 1</p> <p>CopyExplain</p> <ul> <li> <p>Incrementing rounds is a critical part of the FL process for     supporting the continuous learning operation. This function is only     called after registering the initial global model or after each     model aggregation process.</p> </li> <li> <p>Now that we understand how the FL works with the state manager, in     the following section, we will talk about the model aggregation     framework.</p> </li> </ul> <p>Aggregating local models</p> <ul> <li>The\u00a0aggregation.py\u00a0code\u00a0handles aggregating local models with a     bunch of aggregation algorithms. In the code example, we only     support\u00a0FedAvg, as discussed in the following sections.</li> </ul> <p>Importing the libraries for the aggregator</p> <ul> <li>The\u00a0aggregation.py\u00a0code\u00a0imports the following:</li> </ul> <p>import logging</p> <p>import time</p> <p>import numpy as np</p> <p>from typing import List</p> <p>from .state_manager import StateManager</p> <p>from fl_main.lib.util.helpers import generate_model_id</p> <p>from fl_main.lib.util.states import IDPrefix</p> <p>CopyExplain</p> <ul> <li>The imported state manager's role and functionalities are discussed     in the\u00a0Maintaining models for aggregation with the state     manager\u00a0section, and the\u00a0helpers\u00a0and\u00a0states\u00a0libraries are     introduced in the\u00a0Appendix,\u00a0Exploring Internal Libraries.</li> </ul> <p>- After importing the necessary libraries, let's define the aggregator class.</p> <p>Defining and initializing the aggregator class</p> <ul> <li>The following\u00a0code for\u00a0class Aggregator\u00a0defines the core process of     the aggregator, which provides a set of mathematical functions for     computing the aggregated models:</li> </ul> <p>class Aggregator:</p> <p>\\\"\\\"\\\"</p> <p>Aggregator class instance provides a set of</p> <p>mathematical functions to compute aggregated models.</p> <p>\\\"\\\"\\\"</p> <p>CopyExplain</p> <ul> <li>The following\u00a0__init__\u00a0function just sets up the state manager     of the aggregator to access the model buffers:</li> </ul> <p>def __init__(self, sm: StateManager):</p> <p>self.sm = sm</p> <p>CopyExplain</p> <ul> <li>Once the\u00a0aggregator class is defined and initialized, let's look at     the actual FedAvg algorithm implementation.</li> </ul> <p>Defining the aggregate_local_models function</p> <ul> <li>The following\u00a0aggregate_local_models\u00a0function is the code for     aggregating the local models:</li> </ul> <p>def aggregate_local_models(self):</p> <p>for mname in self.sm.mnames:</p> <p>self.sm.cluster_models[mname][0] \\</p> <p>= self._average_aggregate( \\</p> <p>self.sm.local_model_buffers[mname], \\</p> <p>self.sm.local_model_num_samples)</p> <p>self.sm.own_cluster_num_samples = \\</p> <p>sum(self.sm.local_model_num_samples)</p> <p>id = generate_model_id( \\</p> <p>IDPrefix.aggregator, self.sm.id, time.time())</p> <p>self.sm.cluster_model_ids.append(id)</p> <p>self.sm.clear_lmodel_buffers()</p> <p>CopyExplain</p> <ul> <li> <p>This function can be called after the aggregation criteria are     satisfied, such as the aggregation threshold defined in     the\u00a0config_aggregator.json\u00a0file. The aggregation process uses local     ML models buffered in the memory of the state manager.</p> </li> <li> <p>Those local ML models are sent from the registered agents. For each     layer of the models defined by\u00a0mname, the weights of the model are     averaged by the\u00a0_average_aggregate\u00a0function as follows to realize     FedAvg. After averaging the model parameters of all the     layers,\u00a0cluster_models\u00a0is updated, which is sent to all the agents.</p> </li> <li> <p>Then, the local\u00a0model buffer is cleared to be ready for the next     round of the FL process.</p> </li> </ul> <p>The FedAvg function</p> <ul> <li>The following\u00a0function,\u00a0_average_aggregate, called by the     preceding\u00a0aggregate_local_models\u00a0function, is the code that realizes     the\u00a0FedAvg\u00a0aggregation method:</li> </ul> <p>def _average_aggregate(self, buffer: List[np.array],</p> <p>num_samples: List[int]) -&gt; np.array:</p> <p>denominator = sum(num_samples)</p> <p>model = float(num_samples[0])/denominator * buffer[0]</p> <p>for i in range(1, len(buffer)):</p> <p>model += float(num_samples[i]) /</p> <p>denominator * buffer[i]</p> <p>return model</p> <p>CopyExplain</p> <ul> <li> <p>In the\u00a0_average_aggregate\u00a0function, the computation is simple     enough that, for each buffer of the given list of ML models, it     takes averaged parameters for the models.</p> </li> <li> <p>The basics of model aggregation are discussed earlier. It returns     the weighted aggregated models with\u00a0np.array.</p> </li> <li> <p>Now that we have covered all the essential functionalities of the FL     server and aggregator, next, we will talk about how to run the FL     server itself.</p> </li> </ul> <p>Running the FL server</p> <ul> <li>Here is an\u00a0example of running the FL server. In order to run the FL     server, you will just execute the following code:</li> </ul> <p>if __name__ == \\\"__main__\\\":</p> <p>s = Server()</p> <p>init_fl_server(s.register,</p> <p>s.receive_msg_from_agent,</p> <p>s.model_synthesis_routine(),</p> <p>s.aggr_ip, s.reg_socket, s.recv_socket)</p> <p>CopyExplain</p> <ul> <li> <p>The\u00a0register,\u00a0receive_msg_from_agnet,     and\u00a0model_synthesis_routine\u00a0functions of the instance of the FL     server are for starting the registration process of the agents,     receiving messages from the agents, and starting the model synthesis     process to create a global model, which are all started using     the\u00a0init_fl_server\u00a0function from     the\u00a0communication_handler\u00a0libraries.</p> </li> <li> <p>We have covered\u00a0all the core modules of the aggregator with the FL     server. They can work with the database server, which will be     discussed in the following section.</p> </li> </ul> <p>Implementing and running the database server</p> <ul> <li> <p>The database\u00a0server\u00a0can be hosted either on the same machine as the     aggregator server or separately from the aggregator server.</p> </li> <li> <p>Whether the database server is hosted on the same machine or not,     the code introduced here is still applicable to both cases.</p> </li> <li> <p>The database-related code is found in the\u00a0fl_main/pseudodb\u00a0folder of     the GitHub repository provided alongside this book.</p> </li> </ul> <p>Configuring the database</p> <ul> <li>The following\u00a0code is an example of the database-side configuration     parameters saved as\u00a0config_db.json:</li> </ul> <p>{</p> <p>\\\"db_ip\\\": \\\"localhost\\\",</p> <p>\\\"db_socket\\\": \\\"9017\\\",</p> <p>\\\"db_name\\\": \\\"sample_data\\\",</p> <p>\\\"db_data_path\\\": \\\"./db\\\",</p> <p>\\\"db_model_path\\\": \\\"./db/models\\\"</p> <p>}</p> <p>CopyExplain</p> <ul> <li> <p>In particular,\u00a0db_data_path\u00a0is the location of the SQLite database     and\u00a0db_model_path\u00a0is the location of the ML model binary files.</p> </li> <li> <p>The\u00a0config_db.json\u00a0file\u00a0can be found in the\u00a0setup\u00a0folder.</p> </li> <li> <p>Next, let's define the database server and import the necessary     libraries.</p> </li> </ul> <p>Defining the database server</p> <ul> <li> <p>The main\u00a0functionality of the\u00a0pseudo_db.py\u00a0code is accepting     messages that contain local and cluster global models.</p> </li> <li> <p>Importing the libraries for the pseudo database</p> </li> <li> <p>First, the\u00a0pseudo_db.py\u00a0code\u00a0imports\u00a0the following:</p> </li> </ul> <p>import pickle, logging, time, os</p> <p>from typing import Any, List</p> <p>from .sqlite_db import SQLiteDBHandler</p> <p>from fl_main.lib.util.helpers import generate_id, read_config, set_config_file</p> <p>from fl_main.lib.util.states import DBMsgType, DBPushMsgLocation, ModelType</p> <p>from fl_main.lib.util.communication_handler import init_db_server, send_websocket, receive</p> <p>CopyExplain</p> <ul> <li>It imports the basic general libraries as well     as\u00a0SQLiteDBHandler\u00a0(discussed later in the\u00a0Defining the database     with SQLite\u00a0section) and the functions from the\u00a0lib/util\u00a0libraries     that are discussed in the\u00a0Appendix,\u00a0Exploring Internal     Libraries.</li> </ul> <p>Defining the PseudoDB class</p> <ul> <li>The\u00a0PseudoDB\u00a0class\u00a0is then defined to\u00a0create an instance that     receives models and their data from an aggregator and pushes them to     an actual database (SQLite, in this case):</li> </ul> <p>class PseudoDB:</p> <p>\\\"\\\"\\\"</p> <p>PseudoDB class instance receives models and their data</p> <p>from an aggregator, and pushes them to database</p> <p>\\\"\\\"\\\"</p> <p>CopyExplain</p> <ul> <li>Now, let us move on to initializing the instance of\u00a0PseudoDB.</li> </ul> <p>Initializing PseudoDB</p> <ul> <li>Then, the\u00a0initialization process,\u00a0__init__, is\u00a0defined as     follows:</li> </ul> <p>def __init__(self):</p> <p>self.id = generate_id()</p> <p>self.config = read_config(set_config_file(\\\"db\\\"))</p> <p>self.db_ip = self.config[\\'db_ip\\']</p> <p>self.db_socket = self.config[\\'db_socket\\']</p> <p>self.data_path = self.config[\\'db_data_path\\']</p> <p>if not os.path.exists(self.data_path):</p> <p>os.makedirs(self.data_path)</p> <p>self.db_file = \\</p> <p>f\\'{self.data_path}/model_data{time.time()}.db\\'</p> <p>self.dbhandler = SQLiteDBHandler(self.db_file)</p> <p>self.dbhandler.initialize_DB()</p> <p>self.db_model_path = self.config[\\'db_model_path\\']</p> <p>if not os.path.exists(self.db_model_path):</p> <p>os.makedirs(self.db_model_path)</p> <p>CopyExplain</p> <ul> <li> <p>The initialization process generates the ID of the instance and sets     up various parameters such as the database socket (db_socket), the     database IP address (db_ip), the path to the database (data_path),     and the database file (db_file), all configured from\u00a0config_db.json.</p> </li> <li> <p>dbhandler\u00a0stores the instance of\u00a0SQLiteDBHandler\u00a0and calls     the\u00a0initialize_DB\u00a0function to create an SQLite database.</p> </li> <li> <p>Folders for\u00a0data_path\u00a0and\u00a0db_model_path\u00a0are created if they do not     already exist.</p> </li> <li> <p>After the initialization process of\u00a0PseudoDB, we need to design the     communication module that accepts the messages from the aggregators.</p> </li> <li> <p>We again use WebSocket for communicating with an aggregator and     start this module as a server to accept and respond to\u00a0messages from     an aggregator.</p> </li> <li> <p>In\u00a0this design, we do not push messages from the database server to     an aggregator or agents in order to make the FL mechanism simpler.</p> </li> <li> <p>Handling messages from the aggregator</p> </li> <li> <p>The following\u00a0code for the\u00a0async def handler\u00a0function,     which\u00a0takes\u00a0websocket\u00a0as a parameter, receives messages from the     aggregator and returns the requested information:</p> </li> </ul> <p>async def handler(self, websocket, path):</p> <p>msg = await receive(websocket)</p> <p>msg_type = msg[DBPushMsgLocation.msg_type]</p> <p>reply = list()</p> <p>if msg_type == DBMsgType.push:</p> <p>self._push_all_data_to_db(msg)</p> <p>reply.append(\\'confirmation\\')</p> <p>else:</p> <p>raise TypeError(f\\'Undefined DB Message Type: \\</p> <p>{msg_type}.\\')</p> <p>await send_websocket(reply, websocket)</p> <p>CopyExplain</p> <ul> <li> <p>In the\u00a0handler\u00a0function, once it decodes the received message from     an aggregator, the\u00a0handler\u00a0function checks whether the message type     is\u00a0push\u00a0or not.</p> </li> <li> <p>If so, it tries to push the local or cluster models to the database     by calling the _push_all_data_to_db\u00a0function.</p> </li> <li> <p>Otherwise, it will show an error message. The confirmation message     about pushing the models to the database can then be sent back to     the aggregator.</p> </li> <li> <p>Here, we only\u00a0defined the type of the\u00a0push\u00a0message, but\u00a0you can     define as many types as possible, together with the enhancement of     the database schema and design.</p> </li> </ul> <p>Pushing all the data to the database</p> <ul> <li>The\u00a0following code for\u00a0_push_all_data_to_db\u00a0pushes the models'     information to the database:</li> </ul> <p>def _push_all_data_to_db(self, msg: List[Any]):</p> <p>pm = self._parse_message(msg)</p> <p>self.dbhandler.insert_an_entry(*pm)</p> <p>model_id = msg[int(DBPushMsgLocation.model_id)]</p> <p>models = msg[int(DBPushMsgLocation.models)]</p> <p>fname = f\\'{self.db_model_path}/{model_id}.binaryfile\\'</p> <p>with open(fname, \\'wb\\') as f:</p> <p>pickle.dump(models, f)</p> <p>CopyExplain</p> <ul> <li> <p>The models' information is extracted by the\u00a0_parse_message\u00a0function     and passed to the\u00a0_insert_an_entry\u00a0function.</p> </li> <li> <p>Then, the actual models are saved in the local server filesystems,     where the filename of the models and the path are defined     by\u00a0db_model_path\u00a0and\u00a0fname\u00a0here.</p> </li> </ul> <p>Parsing the message</p> <ul> <li>The\u00a0_parse_message\u00a0function\u00a0just extracts the\u00a0parameters from the     received message:</li> </ul> <p>def _parse_message(self, msg: List[Any]):</p> <p>component_id = msg[int(DBPushMsgLocation.component_id)]</p> <p>r = msg[int(DBPushMsgLocation.round)]</p> <p>mt = msg[int(DBPushMsgLocation.model_type)]</p> <p>model_id = msg[int(DBPushMsgLocation.model_id)]</p> <p>gene_time = msg[int(DBPushMsgLocation.gene_time)]</p> <p>meta_data = msg[int(DBPushMsgLocation.meta_data)]</p> <p>local_prfmc = 0.0</p> <p>if mt == ModelType.local:</p> <p>try: local_prfmc = meta_data[\\\"accuracy\\\"]</p> <p>except: pass</p> <p>num_samples = 0</p> <p>try: num_samples = meta_data[\\\"num_samples\\\"]</p> <p>except: pass</p> <p>return component_id, r, mt, model_id, gene_time, \\</p> <p>local_prfmc, num_samples</p> <p>CopyExplain</p> <ul> <li> <p>This function parses the received message into parameters related to     agent ID or aggregator ID (component_id), round number (r), message     type (mt),\u00a0model_id, time of generation of the models (gene_time),     and performance data as a dictionary format (meta_data).</p> </li> <li> <p>The local performance data,\u00a0local_prfmc, is extracted when the model     type is local. The amount of sample data used at the local device is     also extracted from\u00a0meta_dect.</p> </li> <li> <p>All these extracted parameters are returned at\u00a0the end.</p> </li> <li> <p>In the following section, we will explain the database     implementation using the SQLite framework.</p> </li> </ul> <p>Defining the database with SQLite</p> <ul> <li> <p>The\u00a0sqlite_db.py\u00a0code\u00a0creates the SQLite database and deals with     storing and retrieving data from the database.</p> </li> <li> <p>Importing libraries for the SQLite database</p> </li> <li> <p>sqlite_db.py\u00a0imports\u00a0the basic general libraries and\u00a0ModelType\u00a0as     follows:</p> </li> </ul> <p>import sqlite3</p> <p>import datetime</p> <p>import logging</p> <p>from fl_main.lib.util.states import ModelType</p> <p>CopyExplain</p> <ul> <li> <p>The\u00a0ModelType\u00a0from\u00a0lib/util\u00a0defines the type of the models: local     models and (global) cluster models.</p> </li> <li> <p>Defining and initializing the SQLiteDBHandler class</p> </li> <li> <p>Then, the\u00a0following code related to     the\u00a0SQLiteDBHandler\u00a0class\u00a0creates and initializes the SQLite     database and inserts models into the SQLite database:</p> </li> </ul> <p>class SQLiteDBHandler:</p> <p>\\\"\\\"\\\"</p> <p>SQLiteDB Handler class that creates and initialize</p> <p>SQLite DB, and inserts models to the SQLiteDB</p> <p>\\\"\\\"\\\"</p> <p>CopyExplain</p> <ul> <li>The initialization is very simple -- just setting     the\u00a0db_file\u00a0parameter passed from the\u00a0PseudoDB\u00a0instance     to\u00a0self.db_file:</li> </ul> <p>def __init__(self, db_file):</p> <p>self.db_file = db_file</p> <p>CopyExplain</p> <p>Initializing the database</p> <ul> <li>In the\u00a0following\u00a0initialize_DB\u00a0function, the database tables are     defined with local and cluster models using SQLite (sqlite3):</li> </ul> <p>def initialize_DB(self):</p> <p>conn = sqlite3.connect(f\\'{self.db_file}\\')</p> <p>c = conn.cursor()</p> <p>c.execute(\\'\\'\\'CREATE TABLE local_models(model_id, \\</p> <p>generation_time, agent_id, round, performance, \\</p> <p>num_samples)\\'\\'\\')</p> <p>c.execute(\\'\\'\\'CREATE TABLE cluster_models(model_id, \\</p> <p>generation_time, aggregator_id, round, \\</p> <p>num_samples)\\'\\'\\')</p> <p>conn.commit()</p> <p>conn.close()</p> <p>CopyExplain</p> <ul> <li> <p>The tables are simplified in this example so that you can easily     follow the uploaded local models and their performance as well as     the global models created by an aggregator.</p> </li> <li> <p>The\u00a0local_models\u00a0table has a model ID (model_id), the time the model     is generated (generation_time), an agent ID uploaded of the local     model (agent_id), round information (round), the performance data of     the local model (performance), and the number of samples used for     FedAvg aggregation (num_samples).</p> </li> <li> <p>cluster_models\u00a0has a model ID (model_id), the time the model is     generated (generation_time), an aggregator ID (aggregator_id), round     information (round), and the number of samples (num_samples).</p> </li> </ul> <p>Inserting an entry into the database</p> <ul> <li>The following\u00a0code for\u00a0insert_an_entry\u00a0inserts the data received as     parameters using\u00a0sqlite3\u00a0libraries:</li> </ul> <p>def insert_an_entry(self, component_id: str, r: int, mt: \\</p> <p>ModelType, model_id: str, gtime: float, local_prfmc: \\</p> <p>float, num_samples: int):</p> <p>conn = sqlite3.connect(self.db_file)</p> <p>c = conn.cursor()</p> <p>t = datetime.datetime.fromtimestamp(gtime)</p> <p>gene_time = t.strftime(\\'%m/%d/%Y %H:%M:%S\\')</p> <p>if mt == ModelType.local:</p> <p>c.execute(\\'\\'\\'INSERT INTO local_models VALUES \\</p> <p>(?, ?, ?, ?, ?, ?);\\'\\'\\', (model_id, gene_time, \\</p> <p>component_id, r, local_prfmc, num_samples))</p> <p>elif mt == ModelType.cluster:</p> <p>c.execute(\\'\\'\\'INSERT INTO cluster_models VALUES \\</p> <p>(?, ?, ?, ?, ?);\\'\\'\\', (model_id, gene_time, \\</p> <p>component_id, r, num_samples))</p> <p>conn.commit()</p> <p>conn.close()</p> <p>CopyExplain</p> <ul> <li> <p>This function takes\u00a0the parameters of\u00a0component_id\u00a0(agent ID or     aggregator ID), round number (r), message type (mt), model ID     (model_id), the time the model is generated (gtime), the local     model's performance data (local_prfmc), and the number of samples     (num_samples) to insert an entry with the\u00a0execute\u00a0function of the     SQLite library.</p> </li> <li> <p>If the model type is\u00a0local, the information of the models is     inserted into the\u00a0local_models\u00a0table. If the model type     is\u00a0cluster, the information of the models is inserted into     the\u00a0cluster_models\u00a0table.</p> </li> <li> <p>Other functions, such as updating and deleting data from the     database, are not implemented in this example code and it's up to     you to write those additional functions.</p> </li> <li> <p>In the following section, we will explain how to run the database     server.</p> </li> </ul> <p>Running the database server</p> <ul> <li>Here is the code for\u00a0running the database server with the SQLite     database:</li> </ul> <p>if __name__ == \\\"__main__\\\":</p> <p>pdb = PseudoDB()</p> <p>init_db_server(pdb.handler, pdb.db_ip, pdb.db_socket)</p> <p>CopyExplain</p> <ul> <li> <p>The instance of\u00a0PseudoDB\u00a0class is created as\u00a0pdb. The\u00a0pdb.handler,     the database's IP address (pdb.db_ip), and the database socket     (pdb.db_socket) are used to start the process of receiving local and     cluster models from an aggregator enabled by\u00a0init_db_server\u00a0from     the\u00a0communication_handler\u00a0library in the\u00a0util/lib\u00a0folder.</p> </li> <li> <p>Now, we understand how to implement and run the database server. The     database tables and schema discussed here are minimally designed so     that we can understand the fundamentals of the FL server's     procedure. In the following section, we will discuss potential     enhancements to the FL server.</p> </li> </ul> <p>FL Server Potential enhancements</p> <ul> <li>Here are some\u00a0of the key potential enhancements to the FL server     discussed Here.</li> </ul> <p>Redesigning the database</p> <ul> <li> <p>The database was\u00a0intentionally designed with minimal table     information in this book and needs to be extended, such as by having     tables of the aggregator itself, agents, the initial base model, and     the project info, among other things, in the database.</p> </li> <li> <p>For example, the FL system described here does not support the     termination and restart of the server and agent processes.</p> </li> <li> <p>Thus, the FL server implementation is not complete, as it loses most     of the information when any of the systems is stopped or failed.</p> </li> </ul> <p>Automating the registry of an initial model</p> <ul> <li> <p>In order to\u00a0simplify the explanation of the process of registering     the initial model, we defined the layers of the ML models using     model names.</p> </li> <li> <p>This registration of the model in the system can be automated so     that just loading a certain ML model, such as PyTorch or Keras     models, with file extensions such as\u00a0.pt/.pth\u00a0and\u00a0.h5, will be     enough for the users of the FL systems to start the process.</p> </li> </ul> <p>Performance metrics for local and global models</p> <ul> <li> <p>Again, to\u00a0simplify\u00a0the explanation of the\u00a0FL server and the     database-side functionalities, an accuracy value is just used as one     of the performance criteria of the models.</p> </li> <li> <p>Usually, ML applications have many more metrics to keep track of as     performance data and they need to be enhanced together with the     database and communications protocol design.</p> </li> </ul> <p>Fine-tuned aggregation</p> <ul> <li> <p>In order to simplify\u00a0the process of\u00a0aggregating the local models, we     just used FedAvg, a weighted averaging method.</p> </li> <li> <p>The number of samples can dynamically change depending on the local     environment, and that aspect is enhanced by you.</p> </li> <li> <p>There are also a variety of model aggregation methods, which will be     explained later,\u00a0Model Aggregation, of this work so that you can     accommodate the best aggregation method depending on the ML     applications to be created and integrated into the FL system.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>Here, the basics and principles of FL server-side implementation     were explained with actual code examples. Having followed the     contents of This section, you should now be able to construct the FL     server-side functionalities with model aggregation mechanisms.</p> </li> <li> <p>The server-side components that were introduced here involve basic     communications and the registration of the agents and initial     models, managing state information used for the aggregation, and the     aggregation mechanisms for creating the global cluster models.</p> </li> <li> <p>In addition, we discussed the implementation of the database to just     store the information of the ML models.</p> </li> <li> <p>The code was simplified so that you were able to understand the     principles of server-side functionalities. Further enhancements to     many other aspects of constructing a more sustainable, resilient,     and scalable FL system are up to you.</p> </li> <li> <p>In the next section, we will discuss the principle of implementing     the functionalities of the FL client and agent. The client side     needs to provide some well-designed APIs for the ML applications for     plugin use. Therefore, the section will discuss the FL client\\'s     core functionalities and libraries as well as the library     integration into the very simple ML applications to enable the whole     FL process.</p> </li> </ul> <p>[Section 5: FL Client Side Implementation]{.underline}</p> <ul> <li> <p>The client-side\u00a0modules of a\u00a0federated learning\u00a0(FL) system     can be implemented based on the system architecture, sequence, and     procedure flow, as discussed earlier.</p> </li> <li> <p>FL client-side\u00a0functionalities can connect distributed\u00a0machine     learning\u00a0(ML) applications that conduct local training and     testing with an aggregator, through a communications module embedded     in the client-side libraries.</p> </li> <li> <p>In the example of using the FL client libraries in a local ML     engine, the minimal engine package example will be discussed, with     dummy ML models to understand the process of integration with the FL     client libraries that are designed Here.</p> </li> <li> <p>By following the example code about integration, you will understand     how to actually enable the whole process related to the FL client     side, as discussed earlier, while an analysis on what will happen     with the minimal example will be discussed in\u00a0\u00a0Running the     Federated Learning System and Analyzing the Results.</p> </li> <li> <p>Here, an overview of the design and implementation principle of FL     client-side functionalities used in local ML engines will be     discussed. \u00a0you will be able to code the FL client-side modules and     libraries as well as\u00a0distributed\u00a0local ML engines, such as image     classification with\u00a0Convolutional Neural Networks\u00a0(CNNs).</p> </li> <li> <p>Here, we will cover the following topics:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   An overview of FL client-side components</p> <ul> <li> <p>Implementing FL client-side main functionalities</p> </li> <li> <p>Designing FL client libraries</p> </li> <li> <p>Local ML engine integration into an FL system</p> </li> <li> <p>An example of integrating image classification into an FL system</p> </li> </ul> <p>Technical requirements</p> <p>All the code files introduced Here can be found on GitHub ([https://github.com/keshavaspanda/simple-fl]{.underline}).</p> <p>An overview of FL client-side components</p> <ul> <li> <p>The architecture\u00a0of an FL client as an agent was introduced earlier.</p> </li> <li> <p>Here, we will introduce code that realizes the basic functionalities     of an FL client.</p> </li> <li> <p>The client side of software architecture is simplified here, where     only the\u00a0client.py\u00a0file can be used in this example, together with     supporting functions from the\u00a0lib/util\u00a0folder, as shown in\u00a0Figure     5.1:</p> </li> </ul> <p>{width=\"3.751388888888889in\" height=\"3.3020833333333335in\"}</p> <p>Figure 5.1 -- Python software components for an FL client as an agent</p> <ul> <li>The following section gives a brief description of the Python files     for an agent of the FL system.</li> </ul> <p>Distributed agent-side code</p> <ul> <li>For the\u00a0agent side, there is\u00a0one main file,\u00a0client.py, in     the\u00a0fl_main/agent\u00a0directory that deals with most of the FL     client-side functionalities.</li> </ul> <p>FL client code (client.py)</p> <ul> <li> <p>The\u00a0client.py\u00a0file in the\u00a0agent\u00a0folder has functions to participate     in an FL cycle, an ML\u00a0model exchange framework with an aggregator,     and\u00a0push\u00a0and\u00a0polling\u00a0mechanisms to communicate with the     aggregator.</p> </li> <li> <p>The client's functions can also serve as interfaces between the     local ML application and the FL system itself, providing FL     client-side libraries to the ML engine.</p> </li> <li> <p>This is the main code that connects locally trained ML models to the     FL server and aggregator.</p> </li> <li> <p>You need to prepare a local ML application by yourself, and we will     help you understand how to integrate your ML engine into an FL     system using the FL client libraries, which is another main topic of     This section.</p> </li> </ul> <p>lib/util code</p> <ul> <li>An explanation\u00a0of the supporting Python code     (communication_handler.py,\u00a0data_struc.py,\u00a0helpers.py,\u00a0messengers.py,     and\u00a0states.py) as internal libraries will be covered in\u00a0Appendix,     Exploring Internal Libraries.</li> </ul> <p>Configuration of an agent</p> <ul> <li>The following\u00a0is an example of client-side configuration parameters     saved as\u00a0config_agent.json\u00a0in the code we are using:</li> </ul> <p>{</p> <p>\\\"aggr_ip\\\": \\\"localhost\\\",</p> <p>\\\"reg_socket\\\": \\\"8765\\\",</p> <p>\\\"model_path\\\": \\\"./data/agents\\\",</p> <p>\\\"local_model_file_name\\\": \\\"lms.binaryfile\\\",</p> <p>\\\"global_model_file_name\\\": \\\"gms.binaryfile\\\",</p> <p>\\\"state_file_name\\\": \\\"state\\\",</p> <p>\\\"init_weights_flag\\\": 1,</p> <p>\\\"polling\\\": 1</p> <p>}</p> <ul> <li> <p>CopyExplain</p> </li> <li> <p>The aggregator's IP (aggr_ip) and its port number (reg_socket) are     used to get connected to the FL server, where the aggregation of the     local models happens. In addition, the model path     parameter,\u00a0model_path, specifies the location of both the local     model (named\u00a0local_model_file_name) and the global model     (named\u00a0global_model_file_name).</p> </li> <li> <p>The local and global models are stored as binary files     (lms.binaryfile\u00a0and\u00a0gms.binaryfile\u00a0in this example). The state file     (named\u00a0state_file_name) writes the local state of the client that     defines waiting for the global models, training the models, sending     the trained models, and so on.\u00a0init_weights_flag\u00a0is used when the     system operator wants to initialize the global model\u00a0with certain     weights.</p> </li> <li> <p>If the flag is\u00a01, the agent will send the pre-configured model;     otherwise, the model will be filled with zeros on the aggregator     side. The polling flag (polling) concerns whether to utilize the     polling method or not for communication between agents and an     aggregator.</p> </li> <li> <p>Now that we've discussed FL client-side modules, let's look into the     actual implementation and some code to realize the functionalities     of an FL client.</p> </li> </ul> <p>Implementing FL client-side main functionalities</p> <ul> <li> <p>In this section, we will explain how you can implement basic FL     client-side code, which is\u00a0described in the\u00a0client.py\u00a0file in     the\u00a0agent\u00a0directory.</p> </li> <li> <p>By learning about this client-side code, you will understand how to     implement an agent's registration process, model exchange     synchronization, and\u00a0push/polling\u00a0mechanisms, as well as the     communication protocol\u00a0between the agent and aggregator, with some     functions that will be called from other ML applications     as\u00a0Application Programming Interfaces\u00a0(APIs).</p> </li> <li> <p>Let's first see what libraries will be imported for implementing FL     client functions.</p> </li> </ul> <p>Importing libraries for an agent</p> <ul> <li>In this\u00a0client.py\u00a0file\u00a0example, the agent imports general libraries     such as\u00a0asyncio\u00a0and\u00a0time\u00a0(a detailed explanation of which is out of     scope for this book):</li> </ul> <p>import asyncio, time, logging, sys, os</p> <p>from typing import Dict, Any</p> <p>from threading import Thread</p> <p>from fl_main.lib.util.communication_handler import \\</p> <p>init_client_server, send, receive</p> <p>from fl_main.lib.util.helpers import read_config, \\</p> <p>init_loop, save_model_file, load_model_file, \\</p> <p>read_state, write_state, generate_id, \\</p> <p>set_config_file, get_ip, compatible_data_dict_read, \\</p> <p>generate_model_id, create_data_dict_from_models, \\</p> <p>create_meta_data_dict</p> <p>from fl_main.lib.util.states import ClientState, \\</p> <p>AggMsgType, ParticipateConfirmationMSGLocation, \\</p> <p>GMDistributionMsgLocation, IDPrefix</p> <p>from fl_main.lib.util.messengers import \\</p> <p>generate_lmodel_update_message, \\</p> <p>generate_agent_participation_message, \\</p> <p>generate_polling_message</p> <p>CopyExplain</p> <ul> <li> <p>As for the\u00a0communication_handler,\u00a0helpers,\u00a0states,     and\u00a0messengers\u00a0libraries imported from\u00a0fl_main.lib.util\u00a0that are     designed for enabling the FL general functionalities, please refer     to the\u00a0Appendix, Exploring Internal Libraries.</p> </li> <li> <p>After importing the necessary libraries, you will define     the\u00a0Client\u00a0class.</p> </li> </ul> <p>Defining the Client class</p> <ul> <li>Let's\u00a0define the\u00a0Client\u00a0class that implements the core     functionalities of an FL client, including the participation     mechanism of the agent itself, the model exchange framework, and a     communication interface between the agent and an aggregator, as well     as libraries provided for use in the agent-side local ML engine:</li> </ul> <p>class Client:</p> <p>\\\"\\\"\\\"</p> <p>Client class instance with FL client-side functions</p> <p>and libraries used in the agent\\'s ML engine</p> <p>\\\"\\\"\\\"</p> <p>CopyExplain</p> <ul> <li>Then, you will initialize the\u00a0Client\u00a0class under     the\u00a0__init__\u00a0function, as discussed in the next section.</li> </ul> <p>Initializing the client</p> <ul> <li>The\u00a0following code inside the\u00a0__init__\u00a0constructor is an example     of the initialization process of the client:</li> </ul> <p>def __init__(self):</p> <p>self.agent_name = \\'default_agent\\'</p> <p>self.id = generate_id()</p> <p>self.agent_ip = get_ip()</p> <p>self.simulation_flag = False</p> <p>if len(sys.argv) &gt; 1:</p> <p>self.simulation_flag = bool(int(sys.argv[1]))</p> <p>config_file = set_config_file(\\\"agent\\\")</p> <p>self.config = read_config(config_file)</p> <p>self.aggr_ip = self.config[\\'aggr_ip\\']</p> <p>self.reg_socket = self.config[\\'reg_socket\\']</p> <p>self.msend_socket = 0</p> <p>self.exch_socket = 0</p> <p>if self.simulation_flag:</p> <p>self.exch_socket = int(sys.argv[2])</p> <p>self.agent_name = sys.argv[3]</p> <p>self.model_path = f\\'{self.config[\\\"model_path\\\"]}</p> <p>/{self.agent_name}\\'</p> <p>if not os.path.exists(self.model_path):</p> <p>os.makedirs(self.model_path)</p> <p>self.lmfile = self.config[\\'local_model_file_name\\']</p> <p>self.gmfile = self.config[\\'global_model_file_name\\']</p> <p>self.statefile = self.config[\\'state_file_name\\']</p> <p>self.round = 0</p> <p>self.init_weights_flag = \\</p> <p>bool(self.config[\\'init_weights_flag\\'])</p> <p>self.is_polling = bool(self.config[\\'polling\\'])</p> <p>CopyExplain</p> <ul> <li> <p>First, the client generates a unique ID for itself as an identifier     that will be used in many\u00a0scenarios to conduct FL.</p> </li> <li> <p>Second, the client gets its own IP address by using     the\u00a0get_ip()\u00a0function.</p> </li> <li> <p>Also, simulation runs are supported in this implementation exercise,     where we can run all the FL system components of a database, server,     and multiple agents within one machine. If simulation needs to be     done, then the\u00a0simulation_flag\u00a0parameter needs to be\u00a0True\u00a0(refer to     the\u00a0README\u00a0file on GitHub for how to set up a simulation mode).</p> </li> <li> <p>Then,\u00a0self.cofig\u00a0reads and stores the information     of\u00a0config_agent.json.</p> </li> <li> <p>The client then configures the aggregator's information to connect     to its server, where\u00a0self.aggr_ip\u00a0reads the IP address of the     aggregator machine or instance from the agent configuration file.</p> </li> <li> <p>After that, the\u00a0reg_socket\u00a0port will be set up, where\u00a0reg_socket\u00a0is     used for registration of the agent, together with an aggregator IP     address stored as\u00a0self.aggr_ip. The\u00a0reg_socket\u00a0value in this example     can be read from the agent configuration file as well.</p> </li> <li> <p>msend_socket, which is used in the model exchange routine to send     the local ML models to the aggregator, will be configured after     participating in the FL process by sending a message to the FL     server and receiving the response.</p> </li> <li> <p>exch_socket\u00a0is used when communication is not in\u00a0polling\u00a0mode for     receiving global models sent from the aggregator, together with an     agent IP address stored as\u00a0self.agent_ip.</p> </li> <li> <p>exch_socket\u00a0in this example can either be read from the arguments     from the command line or decided by the aggregator, depending on the     simulation mode.</p> </li> <li> <p>In this\u00a0example, when the aggregator is set to be able to push     messages to the connected agents, which is not the case when polling     mode is on,\u00a0exch_socket\u00a0can be dynamically configured by the     aggregator.</p> </li> <li> <p>self.model_path\u00a0stores the path to the local and global models and     can either be read from the agent configuration file or arguments     from the command line, depending on the simulation mode as well. If     there is no directory to save those model files, it makes sure to     create the directory.</p> </li> <li> <p>self.lmfile,\u00a0self.gmfile, and\u00a0self.statefile\u00a0are the filenames for     local models, global models, and the state of the client     respectively, and read from the configuration file of the agent. In     particular, in\u00a0self.statefile, the value of\u00a0ClientState\u00a0is     saved.\u00a0ClientState\u00a0is the enumeration value of the client itself     where there is a state waiting for the global model (waiting_gm), a     state for local training (training), a state for sending local     models (sending), and a state for having the updated global models     (gm_ready).</p> </li> <li> <p>The round information of the FL process, defined as\u00a0self.round, is     initialized as\u00a00\u00a0and later updated as the FL round proceeds with     model aggregation, where the aggregator will notify the change of     the round usually.</p> </li> <li> <p>self.init_weights_flag\u00a0is the flag used when a system operator wants     to initialize a global model with certain parameters, as explained     in the configuration of the agent.</p> </li> <li> <p>The\u00a0self.is_polling\u00a0flag concerns whether to use the polling method     in communication between the agents and aggregator or not. The     polling flag must be the\u00a0same as the one set up on the aggregator     side.</p> </li> <li> <p>The code about the\u00a0__init__\u00a0constructor discussed here can be     found in\u00a0client.py\u00a0in the\u00a0fl_main/agent\u00a0folder on GitHub     ([https://github.com/keshavaspanda/simple-fl]{.underline}).</p> </li> <li> <p>Now that we have discussed how to initialize a client-side module,     in the next section, we will look into how the participation     mechanism works with some sample code.</p> </li> </ul> <p>Agent participation in an FL cycle</p> <ul> <li> <p>This participation or registration process is needed for an agent to     be able to participate\u00a0in an FL process together with other agents.     Therefore, the agent needs to be added to the list of authorized     agents that can send locally trained ML models to an aggregator.</p> </li> <li> <p>The asynchronous\u00a0participate\u00a0function sends the first message to an     aggregator to join the FL cycle and will receive state and     communication information, such as socket numbers from the     aggregator.</p> </li> <li> <p>An agent knows the IP address and port number to join the FL     platform through the\u00a0config_agent.json\u00a0file. When joining the FL     platform, an agent sends a participation message that contains the     following information:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   agent_name: A unique name of an agent itself.</p> <ul> <li> <p>id: A unique identifier of an agent itself.</p> </li> <li> <p>model_id: A unique identifier of models to be sent to an aggregator.</p> </li> <li> <p>models: A dictionary of models keyed by model names. The weights of     models need not be trained if\u00a0init_flag\u00a0is\u00a0False, since it is only     used by an aggregator to remember the shapes of models.</p> </li> <li> <p>init_weights_flag: A Boolean flag to indicate whether the sent model     weights should be used as a base model. If it is\u00a0True\u00a0and there are     no global models ready, an aggregator sets this set of local models     as the first global models and sends it to all agents.</p> </li> <li> <p>simulation_flag: This is\u00a0True\u00a0if it is a simulation run; otherwise,     it is\u00a0False.</p> </li> <li> <p>exch_socket: The port number waiting for global models from the     aggregator.</p> </li> <li> <p>gene_time: The time that models are generated.</p> </li> <li> <p>performance_dict: Performance data related to models in a dictionary     format.</p> </li> <li> <p>agent_ip: The IP address of an agent itself.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   With all the aforementioned participation messages defined, the     agent is ready to exchange\u00a0models with the aggregator, and the code     to realize the participation process is as follows:</p> <p>async def participate(self):</p> <p>data_dict, performance_dict = \\</p> <p>load_model_file(self.model_path, self.lmfile)</p> <p>_, gene_time, models, model_id = \\</p> <p>compatible_data_dict_read(data_dict)</p> <p>msg = generate_agent_participation_message(</p> <p>self.agent_name, self.id, model_id, models,</p> <p>self.init_weights_flag, self.simulation_flag,</p> <p>self.exch_socket, gene_time, performance_dict,</p> <p>self.agent_ip)</p> <p>resp = await send(msg, self.aggr_ip, self.reg_socket)</p> <p>self.round = resp[ \\</p> <p>int(ParticipateConfirmaMSGLocation.round)]</p> <p>self.exch_socket = resp[ \\</p> <p>int(ParticipateConfirmationMSGLocation.exch_socket)]</p> <p>self.msend_socket = resp[ \\</p> <p>int(ParticipateConfirmationMSGLocation.recv_socket)]</p> <p>self.id = resp[ \\</p> <p>int(ParticipateConfirmationMSGLocation.agent_id)]</p> <p>self.save_model_from_message(resp, \\</p> <p>ParticipateConfirmationMSGLocation)</p> <p>CopyExplain</p> <ul> <li> <p>The\u00a0agent reads the local models to tell the structure of the ML     models to the aggregator, and the initial model does not necessarily     need to be trained.\u00a0data_dict\u00a0and\u00a0performance_dict\u00a0store the models     and their performance data respectively.</p> </li> <li> <p>Then, a message,\u00a0msg, containing information such as the     ML\u00a0models\u00a0and its\u00a0model_id, is packaged using     the\u00a0generate_agent_participation_message\u00a0function.</p> </li> <li> <p>When sending the message, in this example, the WebSocket is     constructed using the aggregator's IP address (aggr_ip) and the     registration port number (reg_socket) to be connected to the     aggregator.</p> </li> <li> <p>After sending the message to the aggregator via an     asynchronous\u00a0send\u00a0function imported from\u00a0communication_handler, the     agent receives a response message,\u00a0resp, from the aggregator. The     response will include the round info, the port number to receive the     global models'\u00a0exch_socket, the port number to send the local models     to the aggregator's\u00a0msend_socket, and an updated agent ID.</p> </li> <li> <p>Finally, the global model within the response message is saved     locally by calling the\u00a0save_model_from_message\u00a0function.</p> </li> <li> <p>The participation mechanism of an agent has been explained. In the     next section, we will learn about the framework of model exchange     synchronization.</p> </li> </ul> <p>Model exchange synchronization</p> <ul> <li>Model\u00a0exchange synchronization, as shown in the following code, is     for checking the state of the agent and calling a proper function     based on the state:</li> </ul> <p>Async def model_exchange_routine(self):</p> <p>while True:</p> <p>await asyncio.sleep(5)</p> <p>state = read_state(self.model_path, self.statefile)</p> <p>if state == ClientState.sending:</p> <p>await self.send_models()</p> <p>elif state == ClientState.waiting_gm:</p> <p>if self.is_polling == True:</p> <p>await self.process_polling()</p> <p>else: pass</p> <p>elif state == ClientState.training: pass</p> <p>elif state == ClientState.gm_ready: pass</p> <p>else: pass</p> <p>CopyExplain</p> <ul> <li> <p>Basically, this process is always running while the client is alive,     whereas the\u00a0while\u00a0loop is used periodically to check the     client's\u00a0state\u00a0and proceed with the next steps if necessary.</p> </li> <li> <p>In the\u00a0while\u00a0loop, after waiting a few seconds, it first checks the     client state by the\u00a0read_state\u00a0function. The parameters in     the\u00a0read_state\u00a0function are to locate the\u00a0state\u00a0file stored in the     local environment.</p> </li> <li> <p>As mentioned,\u00a0ClientState\u00a0has the enumeration value of the client     state itself, defining a state for sending local models (sending), a     state waiting for the global model (waiting_sgm), a state for local     training (training), and a state for receiving the updated global     models (gm_ready).</p> </li> <li> <p>If the client is in the\u00a0sending\u00a0state (state ==     ClientState.sending), it means it is ready to send the locally     trained model to the aggregator. Therefore, the agent calls     the\u00a0send_models\u00a0function to send the locally trained ML model to the     aggregator.</p> </li> <li> <p>When the state is\u00a0waiting_gm\u00a0(state == ClientState.waiting_gm), it     either proceeds with\u00a0process_polling\u00a0to poll from the agent to the     aggregator if polling mode is on, or just does nothing if polling     mode is off.</p> </li> <li> <p>If the\u00a0client is in the\u00a0training\u00a0state (state ==     ClientState.training), it means that the client is training the     local model now and just waits for a few seconds, printing the     training status if necessary. You can also add any procedure if     needed.</p> </li> <li> <p>If the client is in the\u00a0gm_ready\u00a0state (state ==     ClientState.gm_ready), it means that the client received the global     model. This state will be handled by a local ML application, and it     does nothing but show the readiness of the global models.</p> </li> <li> <p>In the next section, we will talk about how     the\u00a0push\u00a0and\u00a0polling\u00a0mechanisms can be implemented for an FL     cycle.</p> </li> </ul> <p>Push and polling implementation</p> <ul> <li> <p>Once an\u00a0agent is initialized\u00a0and confirmed for participation in an     FL process, it starts waiting for the global models sent from an     aggregator. There are two ways\u00a0to receive\u00a0global models from the     aggregator: the\u00a0push\u00a0method\u00a0and the\u00a0polling\u00a0method. Although     the\u00a0Secure Sockets Layer\u00a0(SSL) or\u00a0Transport Layer     Security\u00a0(TSL) frameworks are not\u00a0implemented in FL     client-side code here for simplification, it is recommended to     support them to secure constant communication.</p> </li> <li> <p>Let's look into the mechanism for each communication framework.</p> </li> <li> <p>The push method from aggregator to agent</p> </li> <li> <p>With the\u00a0push\u00a0method, the\u00a0aggregator will push the message that     includes global models to all the connected agents right after the     global models are generated.</p> </li> <li> <p>The following code shows the\u00a0push\u00a0mechanism accepting and saving     global models from the aggregator:</p> </li> </ul> <p>async def wait_models(self, websocket, path):</p> <p>gm_msg = await receive(websocket)</p> <p>self.save_model_from_message( \\</p> <p>gm_msg, GMDistributionMsgLocation)</p> <p>CopyExplain</p> <ul> <li> <p>The\u00a0wait_models\u00a0asynchronous function accepts\u00a0websocket\u00a0as a     parameter. When\u00a0the aggregator sends a message to the agent, it     receives the\u00a0gm_msg\u00a0message through\u00a0await recieve(websocket)\u00a0and     saves the global models locally by calling     the\u00a0save_model_from_message\u00a0function, as defined in the\u00a0Toward     designing FL client libraries\u00a0section.</p> </li> <li> <p>The polling method from agent to aggregator</p> </li> <li> <p>With the\u00a0polling\u00a0method, an agent will keep asking (polling) an     aggregator to see whether\u00a0global models are already formed or not.     Once it has been created and is ready to be sent to the connected     agents, the polled message will be returned to the agent with the     updated global models in the response.</p> </li> <li> <p>The following code about the\u00a0process_polling\u00a0asynchronous function     illustrates the\u00a0polling\u00a0method:</p> </li> </ul> <p>async def process_polling(self):</p> <p>msg = generate_polling_message(self.round, self.id)</p> <p>resp = await send(msg, self.aggr_ip, self.msend_socket)</p> <p>if resp[int(PollingMSGLocation.msg_type)] \\</p> <p>== AggMsgType.update:</p> <p>self.save_model_from_message(resp, \\</p> <p>GMDistributionMsgLocation)</p> <p>else: pass</p> <p>CopyExplain</p> <ul> <li> <p>It first generates the polling message with     the\u00a0generate_polling_message\u00a0function to be sent to the aggregator.     After receiving the response message,\u00a0resp, from the aggregator, if     the message type is\u00a0AggMsgType.update, meaning the response message     contains the updated global models, it calls     the\u00a0save_model_from_message\u00a0function. Otherwise, it does nothing.</p> </li> <li> <p>The aforementioned functions are the basic but core features of an     FL client, and those functions need to be efficiently used by a     user-side ML application as libraries.</p> </li> <li> <p>Now that FL client design, including initialization, participation,     and model exchanges, has been explained, we will learn about how to     design FL client libraries.</p> </li> </ul> <p>Designing FL client libraries</p> <ul> <li> <p>In this section, we will explain how to package essential functions     to be provided as libraries to\u00a0users. In this example, the simplest     way to package them as libraries will be discussed.</p> </li> <li> <p>This will need to be expanded, depending on your needs and the     design of your own FL client framework. By packaging FL client-side     modules as libraries, developers will be easily able to integrate     the FL client's functions into the local ML engine.</p> </li> <li> <p>Let's start with how to define a library to start and register an FL     client.</p> </li> </ul> <p>Starting FL client core threads</p> <ul> <li> <p>For local ML application developers to be able to integrate FL     client-related functions, they sometimes need to be packaged as     threading functions.</p> </li> <li> <p>The\u00a0following code to register an agent in the FL system simply puts     a\u00a0participate\u00a0function into the\u00a0run_until_complete\u00a0function of     an\u00a0asyncio.get_event_loop\u00a0function:</p> </li> </ul> <p>def register_client(self):</p> <p>asyncio.get_event_loop().run_until_complete( \\</p> <p>self.participate())</p> <p>CopyExplain</p> <ul> <li> <p>Also, the\u00a0start_wait_model_server\u00a0function is packaged, as shown in     the following code block, where the\u00a0Thread\u00a0function takes care of     the constant run.</p> </li> <li> <p>This way, you will be able to run the local ML module in parallel     and receive global models in the\u00a0wait_models\u00a0thread when the FL     system is in\u00a0push\u00a0communication mode:</p> </li> </ul> <p>def start_wait_model_server(self):</p> <p>th = Thread(target = init_client_server, \\</p> <p>args=[self.wait_models, self.agent_ip, \\</p> <p>self.exch_socket])</p> <p>th.start()</p> <p>CopyExplain</p> <ul> <li>Similarly, the\u00a0start_model_exhange_server\u00a0function can be a thread     to run a model\u00a0exchange routine to synchronize the local and global     models, while the local ML module is running in parallel. You can     just call the following\u00a0start_model_exchange_server\u00a0function as a     library to enable this functionality:</li> </ul> <p>def start_model_exchange_server(self):</p> <p>self.agent_running = True</p> <p>th = Thread(target = init_loop, \\</p> <p>args=[self.model_exchange_routine()])</p> <p>th.start()</p> <p>CopyExplain</p> <ul> <li>Finally, it may be helpful to package all these three functions to     execute at the same time when they are called outside     the\u00a0Client\u00a0class. Therefore, we introduce the following code     concerning\u00a0start_fl_client\u00a0that aggregates the functions of     registering agents, waiting for global models and a model exchange     routine to start the FL client core functions:</li> </ul> <p>def start_fl_client(self):</p> <p>self.register_client()</p> <p>if self.is_polling == False:</p> <p>self.start_wait_model_server()</p> <p>self.start_model_exchange_server()</p> <p>CopyExplain</p> <ul> <li>The initiation of the FL client is now packaged     into\u00a0start_fl_client. Next, we will define the libraries of saved ML     models.</li> </ul> <p>Saving global models</p> <ul> <li> <p>While the\u00a0load\u00a0and\u00a0save\u00a0model functions are provided by the helper     functions in\u00a0lib/util, which will be explained later in     the\u00a0Appendix,\u00a0Exploring Internal Libraries, it is helpful to     provide an\u00a0interface for ML developers to save global models from a     message sent from an aggregator.</p> </li> <li> <p>The following\u00a0save_model_from_message\u00a0function is one that extracts     and saves global models in an agent and also changes the client     state to\u00a0gm_ready. This function takes the message (msg) and message     location (MSG_LOC) information as parameters:</p> </li> </ul> <p>def save_model_from_message(self, msg, MSG_LOC):</p> <p>data_dict = create_data_dict_from_models( \\</p> <p>msg[int(MSG_LOC.model_id)],</p> <p>msg[int(MSG_LOC.global_models)],</p> <p>msg[int(MSG_LOC.aggregator_id)])</p> <p>self.round = msg[int(MSG_LOC.round)]</p> <p>save_model_file(data_dict, self.model_path, \\</p> <p>self.gmfile)</p> <p>self.tran_state(ClientState.gm_ready)</p> <p>CopyExplain</p> <ul> <li> <p>The global models, model ID, and aggregator ID are extracted from     the message and put into a dictionary using     the\u00a0create_data_dict_from_models\u00a0library. The round information is     also updated based on the received message.</p> </li> <li> <p>Then, the received global models are saved to the local file using     the\u00a0save_model_file\u00a0library, in which the data dictionary, model     path, and global model file name are specified to save the models.</p> </li> <li> <p>After receiving the global models, it changes the client state     to\u00a0gm_ready, the state indicating that the global model is ready for     the local ML to be utilized by calling the\u00a0tran_state\u00a0function,     which will be explained in the next section.</p> </li> <li> <p>With the function of saving global models defined, we are ready to     move on to how to manipulate the client state in the next section.</p> </li> </ul> <p>Manipulating client state</p> <ul> <li> <p>In order to manipulate the client state so that it can logically     handle local and global models, we prepare     the\u00a0read_state\u00a0and\u00a0tran_state\u00a0functions, which can be accessed\u00a0both     from inside and outside the code.</p> </li> <li> <p>The following\u00a0read_state\u00a0function reads the value written     in\u00a0statefile, stored in the location specified by\u00a0model_path. The     enumeration value of\u00a0ClientState\u00a0is used to change the client state:</p> </li> </ul> <p>def read_state(self) -&gt; ClientState:</p> <p>return read_state(self.model_path, self.statefile)</p> <p>CopyExplain</p> <ul> <li>The following\u00a0tran_state\u00a0function changes the state of the agent. In     this code sample, the state is maintained in the local\u00a0state\u00a0file     only:</li> </ul> <p>def tran_state(self, state: ClientState):</p> <p>write_state(self.model_path, self.statefile, state)</p> <p>CopyExplain</p> <ul> <li>Next, let's define the functions that can send local models to an     aggregator.</li> </ul> <p>Sending local models to aggregator</p> <ul> <li>The\u00a0following asynchronous\u00a0send_models\u00a0function is about sending     models that have been saved locally to the aggregator:</li> </ul> <p>async def send_models(self):</p> <p>data_dict, performance_dict = \\</p> <p>load_model_file(self.model_path, self.lmfile)</p> <p>, _, models, model_id = \\</p> <p>compatible_data_dict_read(data_dict)</p> <p>msg = generate_lmodel_update_message( \\</p> <p>self.id, model_id, models, performance_dict)</p> <p>await send(msg, self.aggr_ip, self.msend_socket)</p> <p>self.tran_state(ClientState.waiting_gm)</p> <p>CopyExplain</p> <ul> <li> <p>It first extracts\u00a0data_dict\u00a0and\u00a0performance_dict\u00a0using     the\u00a0load_model_file\u00a0helper function and then pulls out the models     and their ID from\u00a0data_dict, based on     the\u00a0compatible_data_dict_read\u00a0function. Then, the message is     packaged with the\u00a0generate_lmodel_update_message\u00a0library and sent to     the aggregator, with the\u00a0send\u00a0function from\u00a0communication_handler.     After that, the client state is changed to\u00a0waiting_gm\u00a0by     the\u00a0tran_state\u00a0function. Again, the SSL/TSL framework can be added     to secure communication, which is not implemented here to keep the     FL client-side coding simple.</p> </li> <li> <p>The\u00a0following\u00a0send_initial_model\u00a0function is called when you want to     send the initial\u00a0base model\u00a0to an aggregator of the model     architecture for registration purposes. It takes initial models, the     number of samples, and performance value as input and     calls\u00a0setup_sending_model, which will be explained later in this     section:</p> </li> </ul> <p>def send_initial_model(self, initial_models, \\</p> <p>num_samples=1, perf_val=0.0):</p> <p>self.setup_sending_models( \\</p> <p>initial_models, num_samples, perf_val)</p> <p>CopyExplain</p> <ul> <li>The following\u00a0send_trained_model\u00a0function is called when you want to     send trained local models to the aggregator during the FL cycle. It     takes trained models, the number of samples, and performance value     as input and only calls\u00a0setup_sending_model\u00a0if the client state     is\u00a0not\u00a0gm_ready:</li> </ul> <p>def send_trained_model(self, models, \\</p> <p>num_samples, perf_value):</p> <p>state = self.read_state()</p> <p>if state == ClientState.gm_ready:</p> <p>pass</p> <p>else:</p> <p>self.setup_sending_models( \\</p> <p>models, num_samples, perf_value)</p> <p>CopyExplain</p> <ul> <li>The following\u00a0setup_sending_models\u00a0function is designed to serve as     an internal library to set up sending locally trained models to the     aggregator. It takes parameters\u00a0of models as\u00a0np.array, the number of     samples as an integer, and performance data as a float value:</li> </ul> <p>def setup_sending_models(self, models, \\</p> <p>num_samples, perf_val):</p> <p>model_id = generate_model_id( \\</p> <p>IDPrefix.agent, self.id, time.time())</p> <p>data_dict = create_data_dict_from_models( \\</p> <p>model_id, models, self.id)</p> <p>meta_data_dict = create_meta_data_dict( \\</p> <p>perf_val, num_samples)</p> <p>save_model_file(data_dict, self.model_path, \\</p> <p>self.lmfile, meta_data_dict)</p> <p>self.tran_state(ClientState.sending)</p> <p>CopyExplain</p> <ul> <li> <p>Basically, this function creates a unique model ID with     the\u00a0generate_model_id\u00a0helper function,\u00a0data_dict\u00a0to store the local     ML models data created with the\u00a0create_data_dict_from_models\u00a0helper     function, and\u00a0meta_data_dict\u00a0to store the performance data created     with the\u00a0create_meta_data_dict\u00a0helper function. And then, all the     aforementioned data related to the models and performance is saved     locally with the\u00a0save_model_file\u00a0function, in the location specified     with\u00a0self.model_path. Then, it changes the client state     to\u00a0sending\u00a0so that the\u00a0mode_exchange_routine\u00a0function can note the     change in the client state and start sending trained local models to     the aggregator.</p> </li> <li> <p>Now that we know about the libraries to send ML models to the     aggregator, let's learn about an important function to wait for a     global model on the agent side.</p> </li> </ul> <p>Waiting for global models from an aggregator</p> <ul> <li>The\u00a0following\u00a0wait_for_global_model\u00a0function is very important to     conduct an FL cycle consistently:</li> </ul> <p>def wait_for_global_model(self):</p> <p>while (self.read_state() != ClientState.gm_ready):</p> <p>time.sleep(5)</p> <p>data_dict, _ = load_model_file( \\</p> <p>self.model_path, self.gmfile)</p> <p>global_models = data_dict[\\'models\\']</p> <p>self.tran_state(ClientState.training)</p> <p>return global_models</p> <p>CopyExplain</p> <ul> <li> <p>The principle is that the function waits until the client state     becomes\u00a0gm_ready. The transition of the client state     to\u00a0gm_ready\u00a0happens when the global model is received on the agent     side. Once the client state changes to\u00a0gm_ready, it proceeds to load     global models from\u00a0data_dict, extracted with     the\u00a0load_model_file\u00a0function, changes the client state to\u00a0training,     and returns the global models to the local ML module.</p> </li> <li> <p>We have discussed how to design the libraries of FL client-side     functions. In the next section, we will discuss how to integrate     those libraries into a local ML process.</p> </li> </ul> <p>Local ML engine integration into an FL system</p> <ul> <li> <p>The\u00a0successful integration of FL client libraries\u00a0into a local ML     engine is key to conducting FL in distributed environments later on.</p> </li> <li> <p>The\u00a0minimal_MLEngine.py\u00a0file in the\u00a0examples/minimal\u00a0directory found     in the GitHub repository     at\u00a0[https://github.com/tie-set/simple-fl]{.underline},     as shown in\u00a0Figure 5.2, provides an example of integrating FL     client-side libraries into a minimal ML engine package:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"1.1395833333333334in\"}</p> <p>Figure 5.2 -- The minimal ML engine package</p> <ul> <li>Next, we\u00a0will explain what libraries\u00a0need to be imported into the     local ML engine in the following section.</li> </ul> <p>Importing libraries for a local ML engine</p> <ul> <li> <p>The following\u00a0code shows the importing process, where general     libraries such as\u00a0numpy,\u00a0time, and\u00a0Dict\u00a0are imported first. The key     part of this process is that\u00a0Client\u00a0is imported from     the\u00a0client.py\u00a0file in the\u00a0fl_main.agent\u00a0folder. This way, a     developer does not need to know too much about the code inside an FL     system and just calls the important functionalities defined as     libraries, as discussed in the\u00a0Toward designing FL client     libraries\u00a0section.</p> </li> <li> <p>We will not cover the\u00a0pip\u00a0installation packaging here in this book,     but it is possible to host the client-side code with either a     private or public PyPI server:</p> </li> </ul> <p>import numpy as np</p> <p>import time, logging, sys</p> <p>from typing import Dict</p> <p>from fl_main.agent.client import Client</p> <p>CopyExplain</p> <ul> <li>After importing the necessary libraries, let's look at the functions     defined for local training and testing.</li> </ul> <p>Defining the ML models, training, and test functions</p> <ul> <li> <p>You first\u00a0define the models, training, and testing\u00a0functions to be     integrated into the FL system. In\u00a0this code example, we will use     dummy models and training/testing functions, allowing users to be     able to understand the minimal FL procedure without being bothered     by specific ML complications.</p> </li> <li> <p>The following function called\u00a0init_models\u00a0returns the templates of     models (in a dictionary format) to inform the ML model structure.     The models do not need to be trained\u00a0necessarily. In this case, the     models have two\u00a0layers defined by\u00a0model1\u00a0and\u00a0model2, where     some\u00a0random NumPy array is assigned to each layer, as follows:</p> </li> </ul> <p>def init_models() -&gt; Dict[str,np.array]:</p> <p>models = dict()</p> <p>models[\\'model1\\'] = np.array([[1, 2, 3], [4, 5, 6]])</p> <p>models[\\'model2\\'] = np.array([[1, 2], [3, 4]])</p> <p>return models</p> <p>CopyExplain</p> <ul> <li>After initializing the models, you will design the     following\u00a0training\u00a0function that can be a placeholder function for     each ML application:</li> </ul> <p>def training(models: Dict[str,np.array],</p> <p>init_flag: bool = False) -&gt; Dict[str,np.array]:</p> <p># return templates of models to tell the structure</p> <p># This model is not necessarily actually trained</p> <p>if init_flag:</p> <p>return init_models()</p> <p># ML Training. In this example, no actual training.</p> <p>models = dict()</p> <p>models[\\'model1\\'] = np.array([[1, 2, 3], [4, 5, 6]])</p> <p>models[\\'model2\\'] = np.array([[1, 2], [3, 4]])</p> <p>return models</p> <p>CopyExplain</p> <ul> <li> <p>The logic of this function should be in the order of taking models     as input, training them, and returning trained local models. As     input parameters, it takes models with     the\u00a0Dict[str,np.array]\u00a0format and the\u00a0init_flag\u00a0Boolean value,     indicating whether it is the initialization step or not.</p> </li> <li> <p>init_flag\u00a0is\u00a0True\u00a0when you want to call and return the     predefined\u00a0init_models, and it is\u00a0False\u00a0if it's an actual training     step.</p> </li> <li> <p>Eventually, this function returns the trained models that are     decomposed into NumPy arrays, with a dictionary     of\u00a0Dict[str,np.array]\u00a0in this example.</p> </li> <li> <p>In this\u00a0dummy example, we are just giving you\u00a0dummy models that skip     the actual training process.</p> </li> <li> <p>Then, the\u00a0following\u00a0compute_performance\u00a0function is designed to     compute the performance of models given a set of models and a test     dataset:</p> </li> </ul> <p>def compute_performance(models: Dict[str,np.array], \\</p> <p>testdata) -&gt; float:</p> <p># replace with actual performance computation logic</p> <p>accuracy = 0.5</p> <p>return</p> <p>CopyExplain</p> <ul> <li> <p>Again, in this example, just a dummy accuracy value is given,\u00a00.5,     to keep things simple.</p> </li> <li> <p>Then, you may want to define the     following\u00a0judge_termination\u00a0function to decide the criteria to     finish the training process and exit from the FL process:</p> </li> </ul> <p>def judge_termination(training_count: int = 0,</p> <p>global_arrival_count: int = 0) -&gt; bool:</p> <p># Depending on termination criteria, change the return bool value</p> <p># Call a performance tracker to check if the current models satisfy the required performance</p> <p>return True</p> <p>CopyExplain</p> <ul> <li> <p>It is up to you how to design this termination condition. This     function takes parameters such as the number of completed training     processes (training_count), the number of times it received global     models (global_arrival_count), and so on, returning a Boolean value     where the flag is\u00a0True\u00a0if it continues the FL process and\u00a0False\u00a0if     it stops. Here, it just gives a\u00a0True\u00a0Boolean value, meaning the FL     process will not stop unless the agent is forced to stop outside of     this function.</p> </li> <li> <p>If preparing the test data is needed, you can define a function such     as\u00a0prep_test_data:</p> </li> </ul> <p>def prep_test_data():</p> <p>testdata = 0</p> <p>return</p> <p>CopyExplain</p> <ul> <li> <p>In this example, it is just set as\u00a00.</p> </li> <li> <p>Now that\u00a0the necessary functions for testing\u00a0and training are     defined, we will integrate\u00a0client libraries into the local ML engine     to run the FL agent working with the FL server-side components, such     as an aggregator and a database.</p> </li> </ul> <p>Integration of client libraries into your local ML engine</p> <ul> <li> <p>Now, everything is ready to start your very first FL process,     although the models, training, and testing functions are set with     dummy variables.</p> </li> <li> <p>The very\u00a0first thing to do is to create a\u00a0Client\u00a0instance as follows     so that you can call its libraries:</p> </li> </ul> <p># Step1: Create Client instance</p> <p>cl = Client()</p> <p>CopyExplain</p> <p>Second, you create the\u00a0initial_models\u00a0with the training function, as follows:</p> <p># Step2: Create template models (to tell the shapes)</p> <p>initial_models = training(dict(), init_flag=True)</p> <p>CopyExplain</p> <ul> <li>After that, it sends the initial models to the FL aggregator by     calling\u00a0cl.send_initial_model, with\u00a0initial_models\u00a0as a parameter:</li> </ul> <p># Step3: Send initial models</p> <p>cl.send_initial_model(initial_model)</p> <p>CopyExplain</p> <ul> <li>Then, let's just start the client-side FL process by     calling\u00a0cl.start_fl_client(). As explained earlier in the\u00a0Starting     FL client core threads\u00a0section, this function can start three     processes at the same time: registering the agent, waiting for     global models, and the model exchange routine:</li> </ul> <p># Step4: Start the FL client</p> <p>cl.start_fl_client()</p> <p>CopyExplain</p> <ul> <li>Then, we\u00a0design the client-side FL cycle of local training/testing     and sending/receiving models by effectively integrating the several     FL client libraries, as follows:</li> </ul> <p># Step5: Run the local FL loop</p> <p>training_count, gm_arrival_count = 0, 0</p> <p>while judge_termination(training_count, gm_arrival_count):</p> <p>global_models = cl.wait_for_global_model()</p> <p>gm_arrival_count += 1</p> <p>global_model_performance_data = \\</p> <p>compute_performance(global_models, prep_test_data())</p> <p>models = training(global_models)</p> <p>training_count += 1</p> <p>perf_value = compute_performance( \\</p> <p>models, prep_test_data())</p> <p>cl.send_trained_model(models, 1, perf_value)</p> <p>CopyExplain</p> <ul> <li> <p>We use a\u00a0while\u00a0loop and the\u00a0judge_termination\u00a0function to check     whether the system needs to leave the loop. It is up to you to     use\u00a0training_count\u00a0and\u00a0gm_arrival_count\u00a0to judge the termination of     the FL cycle.</p> </li> <li> <p>Then, the agent proceeds to wait for the global models     with\u00a0cl.wait_for_global_model(). Upon the arrival of the global     models from the aggregator, it extracts\u00a0global_models,     increments\u00a0gm_arrival_count, and sets the client state to     the\u00a0training\u00a0state in the\u00a0wait_for_global_model\u00a0function.</p> </li> <li> <p>Next,\u00a0global_model_performance_data\u00a0is calculated with     the\u00a0compute_performance\u00a0function,     taking\u00a0global_models\u00a0and\u00a0prep_test_data\u00a0as input.</p> </li> <li> <p>While executing\u00a0training(global_models)\u00a0in the\u00a0training\u00a0state, the     client might receive new global models from the aggregator. This     scenario happens when the client's local training was too slow, and     the aggregator decided to utilize other local models to create a new     set of global models. If the new global models have already\u00a0arrived     at the agent, the client's state is changed to\u00a0gm_ready\u00a0and the     current ML model being trained will be discarded.</p> </li> <li> <p>After the local training phase has finished with\u00a0models\u00a0generated     by\u00a0training(global_models), an agent increments\u00a0training_count\u00a0and     calculates the performance data,\u00a0perf_value, of the current ML model     with the\u00a0compute_performance\u00a0function.</p> </li> <li> <p>Then, the agent tries to upload the trained local models to the     aggregator via\u00a0cl.send_trained_model, taking the trained models and     the performance value calculated previously as parameters.</p> </li> <li> <p>In the\u00a0send_trained_model\u00a0function, the client state is set     to\u00a0sending. Once the client's\u00a0model_exchange_routine\u00a0observes the     state transition to the\u00a0sending\u00a0state, it sends the trained local     models (stored as a binary file) to the aggregator. After sending     the models, it goes back to the\u00a0waiting_gm\u00a0state in     the\u00a0send_models\u00a0function.</p> </li> <li> <p>After sending the local models, the aggregator stores the uploaded     local models in its buffers and waits for another round of global     model aggregation, until enough local models are uploaded by agents.</p> </li> <li> <p>In the next section, we will briefly talk about how to integrate     image classification ML into the FL system we have discussed.</p> </li> </ul> <p>An example of integrating image classification \u2028into an FL system</p> <ul> <li> <p>We learned\u00a0about how to initiate\u00a0an FL process with a minimal     example. In this section, we will give a brief example of FL     with\u00a0image classification\u00a0(IC) using a CNN.</p> </li> <li> <p>First, the package that contains the image classification example     code is found in the\u00a0examples/image_classification/\u00a0folder in the     GitHub repository     at\u00a0[https://github.com/keshavaspanda/simple-fl]{.underline},     as shown in\u00a0Figure 5.3:</p> </li> </ul> <p>{width=\"4.041666666666667in\" height=\"2.627083333333333in\"}</p> <p>Figure 5.3 -- The image classification package</p> <ul> <li> <p>The main code in charge of integrating the IC algorithms into the FL     systems is found in the\u00a0classification_engine.py\u00a0file.</p> </li> <li> <p>When importing the libraries, we use a couple of extra files that     include CNN models, converter functions, and data managers related     to IC algorithms. The details are provided in the GitHub code     at\u00a0[https://github.com/keshavaspanda/simple-fl]{.underline}.</p> </li> <li> <p>Next, let's import some standard ML libraries as well as client     libraries from the FL code we discussed:</p> </li> </ul> <p>import logging</p> <p>import numpy as np</p> <p>import torch</p> <p>import torch.nn as nn</p> <p>import torch.optim as optim</p> <p>from typing import Dict</p> <p>from .cnn import Net</p> <p>from .conversion import Converter</p> <p>from .ic_training import DataManger, execute_ic_training</p> <p>from fl_main.agent.client import Client</p> <p>CopyExplain</p> <ul> <li>In this case, we define\u00a0TrainingMetaData, which just gives you the     amount of training\u00a0data that will be sent to\u00a0the aggregator and used     when conducting the\u00a0FedAvg\u00a0algorithm. The aggregation algorithm was     discussed in\u00a0section     4,\u00a0Federated     Learning Server Implementation with Python, as well as in\u00a0section     7,\u00a0Model     Aggregation:</li> </ul> <p>class TrainingMetaData:</p> <p># The number of training data used for each round</p> <p># This will be used for the weighted averaging</p> <p># Set to a natural number &gt; 0</p> <p>num_training_data = 8000</p> <p>CopyExplain</p> <ul> <li>The content of the\u00a0init_models\u00a0function is now replaced with a CNN     that is converted into a NumPy array. It returns the template of the     CNN in a dictionary format to inform the structure:</li> </ul> <p>def init_models() -&gt; Dict[str,np.array]:</p> <p>net = Net()</p> <p>return Converter.cvtr().convert_nn_to_dict_nparray(net)</p> <p>CopyExplain</p> <ul> <li> <p>The training function,\u00a0training, is now filled with actual training     algorithms using the CIFAR-10 dataset. It takes the models     and\u00a0init_flag\u00a0as parameters and returns the trained models     as\u00a0Dict[str,np.array]. The\u00a0init_flag\u00a0is a\u00a0bool\u00a0value, where it     is\u00a0True\u00a0if it's at the initial step and\u00a0False\u00a0if it's an actual     training step. When preparing for the training data, we use a     certain threshold for training due to batch size. In this case, the     threshold is\u00a04.</p> </li> <li> <p>Then, we create a CNN-based cluster global model with\u00a0net =     Converter.cvtr().convert_dict_nparray_to_nn(models).</p> </li> <li> <p>We define the loss function and optimizer as the following:</p> </li> </ul> <p>criterion = nn.CrossEntropyLoss()</p> <p>optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</p> <p>CopyExplain</p> <ul> <li> <p>Then, the actual training will be conducted with\u00a0trained_net =     execute_ic_training(DataManger.dm(), net, criterion, optimizer),     where the actual code of the IC training can be found in     the\u00a0ic_training.py\u00a0file.</p> </li> <li> <p>After\u00a0the training, the converted\u00a0models will be returned.</p> </li> <li> <p>The algorithm is summarized as follows:</p> </li> </ul> <p>def training(models: Dict[str,np.array], \\</p> <p>init_flag: bool=False) -&gt; Dict[str,np.array]:</p> <p>if init_flag:</p> <p>DataManger.dm( \\</p> <p>int(TrainingMetaData.num_training_data / 4))</p> <p>return init_models()</p> <p>net = \\</p> <p>Converter.cvtr().convert_dict_nparray_to_nn(models)</p> <p>criterion = nn.CrossEntropyLoss()</p> <p>optimizer = optim.SGD(net.parameters(), \\</p> <p>lr=0.001, momentum=0.9)</p> <p>trained_net = execute_ic_training(DataManger.dm(), \\</p> <p>net, criterion, optimizer)</p> <p>models = Converter.cvtr(). \\</p> <p>convert_nn_to_dict_nparray(trained_net)</p> <p>return models</p> <p>CopyExplain</p> <ul> <li>The following\u00a0compute_performance\u00a0function is filled with an     algorithm to calculate the accuracy, which is simple enough -- just     divide the number of correct outcomes\u00a0by the number of total\u00a0labels.     With a given set of models and a test dataset, it computes the     performance of the models, with\u00a0models\u00a0and\u00a0testdata\u00a0as parameters:</li> </ul> <p>def compute_performance(models: Dict[str,np.array], \\</p> <p>testdata, is_local: bool) -&gt; float:</p> <p># Convert np arrays to a CNN</p> <p>net = \\</p> <p>Converter.cvtr().convert_dict_nparray_to_nn(models)</p> <p>correct, total = 0, 0</p> <p>with torch.no_grad():</p> <p>for data in DataManger.dm().testloader:</p> <p>images, labels = data</p> <p>_, predicted = torch.max(net(images).data, 1)</p> <p>total += labels.size(0)</p> <p>correct += (predicted == labels).sum().item()</p> <p>acc = float(correct) / total</p> <p>return acc</p> <p>CopyExplain</p> <ul> <li>The\u00a0judge_termination\u00a0and\u00a0prep_test_data\u00a0functions are the same as     the functions of the minimal examples.</li> </ul> <p>Integration of client libraries into the IC example</p> <ul> <li> <p>Now, everything\u00a0is ready\u00a0to start the IC algorithm, and all the code     to integrate the preceding functions is the same as that used in the     previous\u00a0Integration of client libraries into your local ML     engine\u00a0section.</p> </li> <li> <p>Please look into the\u00a0classification_engine.py\u00a0file to make sure the     code is the\u00a0same, except for the\u00a0part that shows the actual number     of data samples that we are sending. This way, by just rewriting the     preceding functions, you will be able to easily connect your own     local ML application to the FL system that we have discussed here.</p> </li> <li> <p>Please refer to the\u00a0Running image classification and its     analysis\u00a0section,\u00a0Running the Federated Learning System and     Analyzing the Results, to check the results of running the code     discussed in this section.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>Here, we discussed FL client-side implementation. There are three     basic but important functionalities when participating in the FL     process, receiving global models sent from an aggregator with a push     or polling mechanism, and sending local models to an aggregator     after the local training process.</p> </li> <li> <p>In order to effectively implement the client-side ML engines that     cooperate with the FL aggregator, understanding the client state is     important. The client states include a state waiting for the global     model, a state indicating that local training is happening, a state     showing the readiness to send local models, and a state for having     the updated global models.</p> </li> <li> <p>We also discussed the design philosophy of FL client-side libraries,     where the core functions need to be effectively packaged to provide     user-friendly interfaces for ML developers and engineers.</p> </li> <li> <p>Last but not least, we learned how to actually use the FL client     libraries to integrate a local ML engine into an FL system, where we     used the minimal dummy example and IC example to understand the     integration process itself.</p> </li> <li> <p>In the next section, we will actually run the code that was     introduced in this and previous sections so that we can dig into     what is happening with the models, which are aggregated with a     minimal example as well as an IC example.</p> </li> </ul> <p>Running the Federated Learning System and Analyzing the Results</p> <ul> <li> <p>Here, you will run the\u00a0federated learning\u00a0(FL) system that     has been discussed in previous sections and analyze the system     behaviors and the outcomes of the aggregated models.</p> </li> <li> <p>We will start by explaining the configuration of the FL system     components in order to run the systems properly.</p> </li> <li> <p>Basically, after installing the simple FL system provided by our     GitHub sample, you first need to pick up the server machines or     instances to run the database and aggregator modules.</p> </li> <li> <p>Then, you can run agents to connect to the aggregator that is     already running. The IP address of the aggregator needs to be     correctly set up in each agent-side configuration.</p> </li> <li> <p>Also, there is a simulation mode so that you can run all the     components on the same machine or laptop to just test the     functionality of the FL system. After successfully running all the     modules of the FL system, you will be able to see the data folder     and a database created under the path that you set up in the     database server as well as on the agent side.</p> </li> <li> <p>You will be able to check both the local and global models, trained     and aggregated, so that you can download the recent or     best-performing models from the data folders.</p> </li> <li> <p>In addition, you can also see examples of running the FL system on a     minimal engine and image classification. By reviewing the outcomes     of the generated models and the performance data, you can understand     the aggregation algorithms as well as the actual interaction of the     models between an aggregator and agents.</p> </li> <li> <p>Here, we will cover the following main topics:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Configuring and running the FL system</p> <ul> <li> <p>Understanding what happens when the minimal example runs</p> </li> <li> <p>Running image classification and analyzing the results</p> </li> </ul> <p>Technical requirements</p> <ul> <li>All the code files introduced Here can be found on GitHub     (https://github.com/keshavaspanda/simple-fl).</li> </ul> <p>Configuring and running the FL system</p> <ul> <li>Configuring the FL system\u00a0and installing its\u00a0environment are simple     enough to do. Follow the instructions in the next subsections.</li> </ul> <p>Installing the FL environment</p> <ul> <li>First, to run the FL system\u00a0discussed in the previous section, clone     the following repository to the machines that you want to run FL on     using the following command:</li> </ul> <p>git clone https://github.com/keshavaspanda/simple-fl</p> <p>CopyExplain</p> <ul> <li> <p>Once done with the cloning process, change the directory to     the\u00a0simple-fl\u00a0folder in the command line. The simulation run can be     carried out using just one machine or using multiple systems. In     order to run the FL process on one or multiple machines that include     the FL server (aggregator), FL client (agent), and database server,     you should create a\u00a0conda\u00a0virtual environment and activate it.</p> </li> <li> <p>To create a\u00a0conda\u00a0environment in macOS, you will need to type the     following command:</p> </li> </ul> <p>conda env create -n federatedenv -f ./setups/federatedenv.yaml</p> <p>CopyExplain</p> <ul> <li>If you're using a Linux machine, you can create     the\u00a0conda\u00a0environment by using the following command:</li> </ul> <p>conda env create -n federatedenv -f ./setups/federatedenv_linux.yaml</p> <p>CopyExplain</p> <ul> <li> <p>Then, activate the\u00a0conda\u00a0environment\u00a0federatedenv\u00a0when you run the     code. For your information,     the\u00a0federatedenv.yaml\u00a0and\u00a0federatedenv_linux.yaml\u00a0files can be found     in the\u00a0setups\u00a0folder of the\u00a0simple-fl\u00a0GitHub repository and include     the libraries that are used in the code examples throughout this     book.</p> </li> <li> <p>As noted in the\u00a0README\u00a0file of the GitHub repo, there are mainly     three components to run: the database server, aggregator, and     agent(s). If you want to conduct a simulation within one machine,     you can just install a\u00a0conda\u00a0environment (federatedenv)\u00a0on that     machine.</p> </li> <li> <p>If you want to create a distributed\u00a0environment, you need to install     the\u00a0conda\u00a0environment on all the machines you want to use, such as     the database server on a cloud instance, the aggregator server on a     cloud instance, and the local client machine.</p> </li> <li> <p>Now that the installation process for the entire FL process is     ready, let's move on to configuring the FL system with configuration     files.</p> </li> </ul> <p>Configuring the FL system with JSON files for each component</p> <ul> <li>First, edit the configuration\u00a0JSON files in the\u00a0setups\u00a0folder of the     provided GitHub repository. These JSON files are read by a database     server, aggregator, and agents to configure their initial setups.     Again, the configuration details are explained as follows.</li> </ul> <p>config_db.json</p> <ul> <li>The\u00a0config_db.json\u00a0file deals with configuring a database server.     Use the following information to properly\u00a0operate the server:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   db_ip: The database server's IP address (for example,\u00a0localhost). If     you want to run the database server on a cloud\u00a0instance, such as     an\u00a0Amazon Web Services\u00a0(AWS) EC2 instance, you can specify     the private IP address of the instance.</p> <ul> <li> <p>db_socket: The socket number used between the database and     aggregator (for example,\u00a09017).</p> </li> <li> <p>db_name: The name of the SQLite database (for example,\u00a0sample_data).</p> </li> <li> <p>db_data_path: The path to the SQLite database (for example,\u00a0./db).</p> </li> <li> <p>db_model_path: The\u00a0path to the directory to save all\u00a0Machine     Learning\u00a0(ML) models (for example,\u00a0./db/models).</p> </li> </ul> <p>config_aggregator.json</p> <ul> <li>The\u00a0config_aggregator.json\u00a0file deals with configuring an aggregator     in the FL server. Use the following\u00a0information to properly operate     the aggregator:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   aggr_ip: The aggregator's IP address (for example,\u00a0localhost). If     you want to run the aggregator server on a cloud instance, such as     an AWS EC2 instance, you can specify the private IP address of the     instance.</p> <ul> <li> <p>db_ip: The database server's IP address (for example,\u00a0localhost). If     you want to connect to the database server hosted on a different     cloud instance, you can specify the public IP address of the     database instance. If you host the database server on the same cloud     instance as the aggregator's instance, you can specify the same     private IP address of the instance.</p> </li> <li> <p>reg_socket: The socket number used by agents to connect to an     aggregator for the first time (for example,\u00a08765).</p> </li> <li> <p>recv_socket: The socket number used to upload local models or poll     to an aggregator from an agent. Agents will learn this socket     information by communicating with an aggregator (for example,\u00a07890).</p> </li> <li> <p>exch_socket: The socket number used to send global models back to an     agent from an aggregator when a push method is used. Agents will     learn this socket information by communicating with an aggregator     (for example,\u00a04321).</p> </li> <li> <p>db_socket: The socket number used between the database and an     aggregator (for example,\u00a09017).</p> </li> <li> <p>round_interval: The period of time after which an agent checks     whether there are enough models to start an aggregation step (unit:     seconds; for example,\u00a05).</p> </li> <li> <p>aggregation_threshold: The percentage of collected local models     required to start an aggregation step (for example,\u00a00.85).</p> </li> <li> <p>polling: The flag to\u00a0specify whether to use a polling method or not.     If the flag is\u00a01, use the polling method; if the flag is\u00a00, use a     push method. This value needs to be the same between the aggregator     and agent.</p> </li> </ul> <p>config_agent.json</p> <ul> <li>The\u00a0config_agent.json\u00a0file deals with configuring an agent in the FL     client. Use the following information to properly operate the agent:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   aggr_ip: The aggregator\u00a0server's IP address (for     example,\u00a0localhost). If you want to connect to the aggregator server     hosted on a cloud instance, such as an AWS EC2 instance, you can     specify the public IP address of the aggregator instance.</p> <ul> <li> <p>reg_socket: The socket number used by agents to join an aggregator     for the first time (for example,\u00a08765).</p> </li> <li> <p>model_path: The path to a local director in the agent machine to     save local and global models and some state information (for     example,\u00a0./data/agents).</p> </li> <li> <p>local_model_file_name: The filename to save local models in the     agent machine (for example,\u00a0lms.binaryfile).</p> </li> <li> <p>global_model_file_name: The filename to save local models in the     agent machine (for example,\u00a0gms.binaryfile).</p> </li> <li> <p>state_file_name: The filename to store the agent state in the agent     machine (for example,\u00a0state).</p> </li> <li> <p>init_weights_flag:\u00a01\u00a0if the weights are initialized with certain     values,\u00a00\u00a0otherwise, where weights are initialized with zeros.</p> </li> <li> <p>polling: The flag to specify whether to use a polling method or not.     If the flag is\u00a01, use the polling method; if the flag is\u00a00, use a     push method. This value needs to be\u00a0the same between the aggregator     and agent.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Now, the FL systems can be configured using the configuration files     explained in this section. Next, you will run the database and     aggregator on the FL server side.</p> <p>Running the database and aggregator on the FL server</p> <ul> <li>In this section, you will\u00a0configure the database and aggregator on     the FL server side. Then, you will edit the configuration files in     the\u00a0setups\u00a0folder of the\u00a0simple-fl\u00a0GitHub repo. After that, you will     run\u00a0pseudo_db\u00a0first, and\u00a0then\u00a0server_th, as follows:</li> </ul> <p>python -m fl_main.pseudodb.pseudo_db</p> <p>python -m fl_main.aggregator.server_th</p> <p>CopyExplain</p> <p>Important note</p> <ul> <li> <p>If the database server and aggregator server are running on     different machines, you will need to specify the IP address of the     database server or instance of the aggregator. The IP address of the     database server can be modified in the\u00a0config_aggregator.json\u00a0file     in the\u00a0setups\u00a0folder.</p> </li> <li> <p>Also, if both the database and aggregator instances are running in     public cloud environments, the IP address of the configuration files     of those servers needs to be the private IP address.</p> </li> <li> <p>Agents need to connect to the aggregator using the public IP address     and the connecting socket (port number) needs to be open to accept     inbound messages.</p> </li> <li> <p>After you start the database and aggregator servers, you will see a     message such as the following in the console:</p> </li> </ul> <p># Database-side Console Example</p> <p>INFO:root:--- Pseudo DB Started ---</p> <p>CopyExplain</p> <ul> <li>On the aggregator\u00a0side of the console, you will see something like     the following:</li> </ul> <p># Aggregator-side Console Example</p> <p>INFO:root:--- Aggregator Started ---</p> <p>CopyExplain</p> <ul> <li> <p>Behind this aggregator\u00a0server, the model synthesis module is running     every 5 seconds, where it starts checking whether the number of     collected local models is more than the number that the aggregation     threshold defines.</p> </li> <li> <p>We have now run the database and aggregator modules and are ready to     run a minimal example with the FL client.</p> </li> </ul> <p>Running a minimal example with the FL client</p> <ul> <li> <p>In the previous\u00a0section, we talked about the integration of local ML     engines into the FL system. Here, using a minimal sample that does     not have actual training data, we will try to run the FL systems     that have been discussed. This minimal example can be used as a     template when implementing any locally distributed ML engine.</p> </li> <li> <p>Before running the minimal example, you should check whether the     database and aggregator servers are running already. Then, run the     following command:</p> </li> </ul> <p>python -m examples.minimal.minimal_MLEngine</p> <p>CopyExplain</p> <ul> <li> <p>In this case, only one agent with a minimal ML engine is connected.     Thus, the aggregation happens every time this default agent uploads     the local model.</p> </li> <li> <p>Note that if the aggregator server is running on a different     machine, you will need to specify the public IP address of the     aggregator server or instance. The IP address of the aggregator can     be modified in the\u00a0config_agent.json\u00a0file in the\u00a0setups\u00a0folder. We     also recommend setting the\u00a0polling\u00a0flag to\u00a01\u00a0when running the     aggregator and database in a cloud instance.</p> </li> </ul> <p>Figure 6.1\u00a0shows an example\u00a0of the console screen when running a database server:</p> <p>{width=\"6.268055555555556in\" height=\"4.277777777777778in\"}</p> <p>Figure 6.1 --\u00a0Example of a database-side console</p> <p>Figure 6.2\u00a0shows an example of the console screen when running an aggregator:</p> <p>{width=\"6.268055555555556in\" height=\"4.104861111111111in\"}</p> <p>Figure 6.2 -- Example of an aggregator-side console</p> <p>Figure 6.3\u00a0shows an\u00a0example of the console screen when running an agent:</p> <p>{width=\"6.268055555555556in\" height=\"4.260416666666667in\"}</p> <p>Figure 6.3 -- Example of an agent-side console</p> <ul> <li> <p>Now we know how to\u00a0run all the FL components: a database,     aggregator, and agent.</p> </li> <li> <p>In the next section, we will examine how outputs are generated by     running the FL system.</p> </li> </ul> <p>Data and database folders</p> <ul> <li> <p>After running the FL system, you\u00a0will notice that the database     folder and data folder are created under the locations that you     specified in the config files of the database and agent.</p> </li> <li> <p>For example, the\u00a0db\u00a0folder is created under\u00a0db_data_path, written in     the\u00a0config_db.json\u00a0file. In the database folder, you will find the     SQLite database, such as\u00a0model_data12345.db, where the metadata of     local and cluster global models is stored, as well as     a\u00a0models\u00a0folder that contains all the actual local models uploaded     by the agents and global models created by the aggregator.</p> </li> <li> <p>Figure 6.4\u00a0shows the SQLite database and ML model files in a     binary file format stored in the\u00a0db\u00a0folder created by running the     minimal example code:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"2.6215277777777777in\"}</p> <p>Figure 6.4 -- The SQLite database and ML model files in a binary file format stored in the db folder</p> <ul> <li>The\u00a0data\u00a0folder is created under an agent device at the location of     the\u00a0model_path, a string value\u00a0defined in\u00a0config_agent.json. In the     example run of the minimal example, the following files are created     under the\u00a0data/agents/default-agent\u00a0folder:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   lms.binaryfile: A binary file containing a local model created by     the agent</p> <ul> <li> <p>gms.binaryfile: A binary file containing a global model created by     the aggregator sent back to the agent</p> </li> <li> <p>state: A file that has an integer value that indicates the state of     the client itself</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Figure 6.5\u00a0shows the structure of the agent-side data, which     includes global and local ML models represented with a binary file     format, as well as the file reflecting the FL client state:</p> <p>{width=\"6.268055555555556in\" height=\"1.9270833333333333in\"}</p> <p>Figure 6.5 -- Data of the agents including global and local ML models with a binary file format as well as the client state</p> <ul> <li>Now we understand where the key data, such as global and local     models, is stored. Next, we will take a closer\u00a0look at the database     using SQLite.</li> </ul> <p>Databases with SQLite</p> <ul> <li>The database\u00a0created in the\u00a0db\u00a0folder can be viewed using any tool     to show the SQLite database that can open files with     the\u00a0***.db\u00a0format. The database tables are defined in the     following sections.</li> </ul> <p>Local models in a database</p> <ul> <li>Figure 6.6\u00a0shows sample database\u00a0entries related to uploaded local     models where each entry lists the local model ID, the time that the     model was generated, the ID of the agent that uploaded the local     model, round information, performance metrics, and the number of     data samples:</li> </ul> <p>{width=\"6.268055555555556in\" height=\"1.5618055555555554in\"}</p> <p>Figure 6.6 -- Sample database entries related to uploaded local models</p> <p>Cluster models in a database</p> <ul> <li>Figure 6.7\u00a0shows sample database entries related to uploaded     cluster models where each entry lists the cluster model ID, the time     that the model was created, the ID of the aggregator that created     this cluster model, round information, and the number of data     samples:</li> </ul> <p>{width=\"6.268055555555556in\" height=\"1.7402777777777778in\"}</p> <p>Figure 6.7 -- Sample database entries related to uploaded cluster models</p> <ul> <li>Now we have learned\u00a0how to configure and run the FL system with a     minimal example and how to examine the results. In the next section,     you will learn about the behavior of the FL system and what happens     when the minimal example is run.</li> </ul> <p>Understanding what happens when the minimal example runs</p> <ul> <li>Understanding the behavior of the entire FL system step by step will     help you design applications with FL enabled and further enhance the     FL system itself. Let us first look into what happens when we run     just one agent by printing some procedures of the agent and     aggregator modules.</li> </ul> <p>Running just one minimal agent</p> <ul> <li>Let's run the minimal\u00a0agent after running the database and     aggregator servers and see what happens. When the agent is started     with the minimal ML engine, you will see the following messages in     the agent console:</li> </ul> <p># Agent-side Console Example</p> <p>INFO:root:--- This is a minimal example ---</p> <p>INFO:root:--- Agent initialized ---</p> <p>INFO:root:--- Your IP is xxx.xxx.1.101 ---</p> <p>CopyExplain</p> <ul> <li>When the agent initializes the model to be used for FL, it shows     this message, and if you look at the\u00a0state\u00a0file, it has entered     the\u00a0sending\u00a0state, which will trigger sending models to the     aggregator when the FL client is started:</li> </ul> <p># Agent-side Console Example</p> <p>INFO:root:--- Model template generated ---</p> <p>INFO:root:--- Local (Initial/Trained) Models saved ---</p> <p>INFO:root:--- Client State is now sending ---</p> <p>CopyExplain</p> <ul> <li>Then, after the client is started with the\u00a0start_fl_client\u00a0function,     the participation message is\u00a0sent to the aggregator. Here is the     participation message sent to the aggregator:</li> </ul> <p>[</p> <p>\\&lt;AgentMsgType.participate: 0&gt;, # Agent Message Type</p> <p>\\'A89fd1c2d9*****\\', # Agent ID</p> <p>\\'047b18ddac*****\\',\u00a0\u00a0\u00a0\u00a0# Model ID</p> <p>{</p> <p>\\'model1\\': array([[1, 2, 3], [4, 5, 6]]),</p> <p>\\'model2\\': array([[1, 2], [3, 4]])</p> <p>}, # ML Models</p> <p>True,\u00a0\u00a0\u00a0\u00a0# Init weights flag</p> <p>False, # Simulation flag</p> <p>0, # Exch Port</p> <p>1645141807.846751, # Generated Time of the models</p> <p>{\\'accuracy\\': 0.0, \\'num_samples\\': 1}, # Meta information</p> <p>\\'xxx.xxx.1.101\\' # Agent\\'s IP Address</p> <p>]</p> <p>CopyExplain</p> <ul> <li> <p>The participation message to the aggregator includes the message     type, agent ID, model ID, ML model with NumPy, initialization     weights flag, simulation flag, exchange port number, time the models     were generated, and meta information such as performance metrics and     the agent's IP address.</p> </li> <li> <p>The agent receives the\u00a0welcome message from an aggregator confirming     the connection of this agent, which also includes the following     information:</p> </li> </ul> <p># Agent-side Console Example</p> <p>INFO:root:--- Init Response: [</p> <p>\\&lt;AggMsgType.welcome: 0&gt;, # Message Type</p> <p>\\'4e2da*****\\', # Aggregator ID</p> <p>\\'23487*****\\', # Model ID</p> <p>{\\'model1\\': array([[1, 2, 3], [4, 5, 6]]),</p> <p>\\'model2\\': array([[1, 2], [3, 4]])}, # Global Models</p> <p>1, # FL Round</p> <p>\\'A89fd1c2d9*****\\', # Agent ID</p> <p>\\'7890\\', # exch_socket number</p> <p>\\'4321\\' # recv_socket number</p> <p>] ---</p> <p>CopyExplain</p> <ul> <li>On the aggregator side, after this agent sends a participation     message to the aggregator, the aggregator confirms the participation     and pushes this initial model to the database:</li> </ul> <p># Aggregator-side Console Example</p> <p>INFO:root:--- Participate Message Received ---</p> <p>INFO:root:--- Model Formats initialized, model names: [\\'model1\\', \\'model2\\'] ---</p> <p>INFO:root:--- Models pushed to DB: Response [\\'confirmation\\'] ---</p> <p>INFO:root:---\u00a0\u00a0Global Models Sent to A89fd1c2d9***** ---</p> <p>INFO:root:--- Aggregation Threshold (Number of agents needed for aggregation): 1 ---</p> <p>INFO:root:--- Number of collected local models: 0 ---</p> <p>INFO:root:--- Waiting for more local models to be collected ---</p> <p>CopyExplain</p> <ul> <li>In the database server-side\u00a0console, you can also check that the     local model is sent from the aggregator and the model is saved in     the database:</li> </ul> <p># DB-side Console Example</p> <p>INFO:root:Request Arrived</p> <p>INFO:root:--- Model pushed: ModelType.local ---</p> <p>INFO:root:--- Local Models are saved ---</p> <p>CopyExplain</p> <ul> <li>After the aggregator sends the global model back to the agent, the     agent receives and saves it and changes the client state     from\u00a0waiting_gm\u00a0to\u00a0gm_ready, indicating the global model is ready     for retraining locally:</li> </ul> <p># Agent-side Console Example</p> <p>INFO:root:--- Global Model Received ---</p> <p>INFO:root:--- Global Models Saved ---</p> <p>INFO:root:--- Client State is now gm_ready ---</p> <p>CopyExplain</p> <ul> <li>Here is the message sent to the agent from an aggregator, including     the global model. The contents of the message include the message     type, aggregator ID, cluster model ID, FL round, and ML models with     NumPy:</li> </ul> <p>[</p> <p>\\&lt;AggMsgType.sending_gm_models: 1&gt;, # Message Type</p> <p>\\'8c6c946472*****\\', # Aggregator ID</p> <p>\\'ab633380f6*****\\', # Global Model ID</p> <p>1, # FL Round Info</p> <p>{\u00a0\u00a0\u00a0\u00a0</p> <p>\\'model1\\': array([[1., 2., 3.],[4., 5., 6.]]),</p> <p>\\'model2\\': array([[1., 2.],[3., 4.]])</p> <p>} # ML models</p> <p>]</p> <p>CopyExplain</p> <ul> <li>Then, the agent reads the\u00a0global models to proceed with using them     for local training and changes the client state to\u00a0training:</li> </ul> <p># Agent-side Console Example</p> <p>INFO:root:--- Global Models read by Agent ---</p> <p>INFO:root:--- Client State is now training ---</p> <p>INFO:root:--- Training ---</p> <p>INFO:root:--- Training is happening ---</p> <p>INFO:root:--- Training is happening ---</p> <p>INFO:root:--- Training Done ---</p> <p>INFO:root:--- Local (Initial/Trained) Models saved ---</p> <p>INFO:root:--- Client State is now sending ---</p> <p>INFO:root:--- Local Models Sent ---</p> <p>INFO:root:--- Client State is now waiting_gm ---</p> <p>INFO:root:--- Polling to see if there is any update (shown only when polling) ---</p> <p>INFO:root:--- Global Model Received ---</p> <p>INFO:root:--- The global models saved ---</p> <p>CopyExplain</p> <ul> <li> <p>After the preceding local training process, the agent proceeds     with\u00a0sending\u00a0the trained local models to the aggregator and changes     the client state to\u00a0waiting_gm, which means it waits for the global     model with the polling mechanism.</p> </li> <li> <p>Here is the message\u00a0sent to the aggregator as a trained local model     message. The contents of the message include message type, agent ID,     model ID, ML models, generated time of the models, and metadata such     as performance data:</p> </li> </ul> <p>[</p> <p>\\&lt;AgentMsgType.update: 1&gt;, # Agent\\'s Message Type</p> <p>\\'a1031a737f*****\\', # Agent ID</p> <p>\\'e89ccc5dc9*****\\', # Model ID</p> <p>{</p> <p>\\'model1\\': array([[1, 2, 3],[4, 5, 6]]),</p> <p>\\'model2\\': array([[1, 2],[3, 4]])</p> <p>}, # ML Models</p> <p>1645142806.761495, # Generated Time of the models</p> <p>{\\'accuracy\\': 0.5, \\'num_samples\\': 1} # Meta information</p> <p>]</p> <p>CopyExplain</p> <ul> <li>Then, in the aggregator, after the local model is pushed to the     database, it shows the change in the buffer, that the number of     collected local models is up to 1 from 0, thus indicating that     enough local models are collected to start the aggregation:</li> </ul> <p># Aggregator-side Console Example</p> <p>INFO:root:--- Models pushed to DB: Response [\\'confirmation\\'] ---</p> <p>INFO:root:--- Local Model Received ---</p> <p>INFO:root:--- Aggregation Threshold (Number of agents needed for aggregation): 1 ---</p> <p>INFO:root:--- Number of collected local models: 1 ---</p> <p>INFO:root:--- Enough local models are collected. Aggregation will start. ---</p> <p>CopyExplain</p> <ul> <li>Then, aggregation for round 1 happens and the cluster global models     are formed, pushed to the database, and\u00a0sent to the agent once the     polling message arrives from the agent. The aggregator can also push     the message back to the agent via a push method:</li> </ul> <p># Aggregator-side Console Example</p> <p>INFO:root:Round 1</p> <p>INFO:root:Current agents: [{\\'agent_name\\': \\'default_agent\\', \\'agent_id\\': \\'A89fd1c2d9*****\\', \\'agent_ip\\': \\'xxx.xxx.1.101\\', \\'socket\\': 7890}]</p> <p>INFO:root:--- Cluster models are formed ---</p> <p>INFO:root:--- Models pushed to DB: Response [\\'confirmation\\'] ---</p> <p>INFO:root:--- Global Models Sent to A89fd1c2d9***** ---</p> <p>CopyExplain</p> <ul> <li>On the database server side, the cluster global model is received     and pushed to the database:</li> </ul> <p># DB-side Console Example</p> <p>INFO:root:Request Arrived</p> <p>INFO:root:--- Model pushed: ModelType.cluster ---</p> <p>INFO:root:--- Cluster Models are saved ---</p> <p>CopyExplain</p> <ul> <li> <p>This process in this section is repeated after cluster models are     generated and saved for the upcoming FL round and the round of FL     proceeds with this interaction mechanism.</p> </li> <li> <p>If you look at both the local and cluster global models, they are as     follows:</p> </li> </ul> <p>{</p> <p>\\'model1\\': array([[1, 2, 3],[4, 5, 6]]),</p> <p>\\'model2\\': array([[1, 2],[3, 4]])</p> <p>}</p> <p>CopyExplain</p> <ul> <li> <p>This means only one fixed model is used all the time even if     aggregation happens, so the global model is exactly the same as the     initial one as the dummy training process is used here.</p> </li> <li> <p>We will now look into the\u00a0results when running two minimal agents in     the next section.</p> </li> </ul> <p>Running two minimal agents</p> <ul> <li> <p>With the database and aggregator\u00a0servers running, you can run many     agents using the\u00a0minimal_MLEngine.py\u00a0file in     the\u00a0simple-fl/examples/minimal\u00a0folder.</p> </li> <li> <p>You should run the two individual agents from different local     machines by specifying the IP address of the aggregator to connect     those agents with the minimal ML example.</p> </li> <li> <p>You can also run multiple agents from the same machine for     simulation purposes by specifying the different port numbers for the     individual agents.</p> </li> <li> <p>In the code provided in the\u00a0simple-fl\u00a0repository on GitHub, you can     run the multiple agents by using the following command:</p> </li> </ul> <p>python -m examples.minimal.minimal_MLEngine [simulation_flag] [gm_recv_port] [agent_name]</p> <p>CopyExplain</p> <ul> <li> <p>To conduct the simulation,\u00a0simulation_flag\u00a0should be set     to\u00a01.\u00a0gm_recv_port\u00a0is the port number to receive the global models     from the aggregator. The agent will be notified of the port number     by the aggregator through the response of a participation message.     Also,\u00a0agent_name\u00a0is the name of the local agent and the directory     name storing the state and model files. This needs to be unique for     every agent.</p> </li> <li> <p>For instance, you can run the first and second agents with the     following commands:</p> </li> </ul> <p># First agent</p> <p>python -m examples.minimal.minimal_MLEngine 1 50001 a1</p> <p># Second agent</p> <p>python -m examples.minimal.minimal_MLEngine 1 50002 a2</p> <p>CopyExplain</p> <ul> <li> <p>You can edit the configuration JSON files in the\u00a0setups\u00a0folder if     needed. In this case,\u00a0agg_threshold\u00a0is set to\u00a01.</p> </li> <li> <p>When you run the simulation in the database server running a minimal     example with multiple agents, the console screen will look similar     to that in\u00a0Figure 6.1.</p> </li> <li> <p>Figure 6.8\u00a0shows the console screen of a simulation in the     aggregator server running a minimal example using dummy ML models:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.104861111111111in\"}</p> <p>Figure 6.8 -- Example of an aggregator-side console running a minimal example connecting two agents</p> <ul> <li>Figure 6.9\u00a0shows the\u00a0console screen of a simulation in one of the     agents running a minimal example using dummy ML models:</li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.509027777777778in\"}</p> <p>Figure 6.9 -- Example of agent 1's console running a minimal example using dummy ML models</p> <ul> <li>Figure 6.10\u00a0shows the\u00a0console screen of a simulation in another     agent running a minimal example using dummy ML models:</li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.522222222222222in\"}</p> <p>Figure 6.10 -- Example of agent 2's console running a minimal example using dummy ML models</p> <ul> <li>Now we know how to run the\u00a0minimal example with two agents. In order     to further look into the FL procedure using this example, we will     answer the following questions:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Has aggregation been done correctly for the simple cases?</p> <ul> <li> <p>Has the\u00a0FedAvg\u00a0algorithm been applied correctly?</p> </li> <li> <p>Does aggregation threshold work with connected agents?</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   After running and connecting the two agents, the aggregator will     wait to receive two models from the two connected agents, as     follows:</p> <p># Aggregator-side Console Example</p> <p>INFO:root:--- Aggregation Threshold (Number of agents needed for aggregation): 2 ---</p> <p>INFO:root:--- Number of collected local models: 0 ---</p> <p>INFO:root:--- Waiting for more local models to be collected ---</p> <p>CopyExplain</p> <ul> <li> <p>In this case, the aggregation threshold is set to\u00a01.0\u00a0in     the\u00a0config_aggregator.json\u00a0file in the\u00a0setups\u00a0folder, so the     aggregator needs to collect all the models from connected agents,     meaning it needs to receive local ML models from all the agents that     are connected to the aggregator.</p> </li> <li> <p>Then, it receives one model\u00a0from one of the agents and the number of     collected local models is increased to 1. However, as the aggregator     is still missing one local model, it does not start aggregation yet:</p> </li> </ul> <p># Aggregator-side Console Example</p> <p>INFO:root:--- Local Model Received ---</p> <p>INFO:root:--- Aggregation Threshold (Number of agents needed for aggregation): 2 ---</p> <p>INFO:root:--- Number of collected local models: 1 ---</p> <p>INFO:root:--- Waiting for more local models to be collected ---</p> <p>CopyExplain</p> <ul> <li> <p>On the agent side, after the local models are sent to the     aggregator, it will wait until the cluster global model to be     created in the aggregator and sent back to the agent. In this way,     you can synchronize the FL process at the agent side and automate     the local training procedure when the global model is sent back to     the agent and ready for retraining.</p> </li> <li> <p>After the aggregator receives another local model, enough models are     collected to start the aggregation process:</p> </li> </ul> <p># Aggregator-side Console Example</p> <p>INFO:root:--- Local Model Received ---</p> <p>INFO:root:--- Aggregation Threshold (Number of agents needed for aggregation): 2 ---</p> <p>INFO:root:--- Number of collected local models: 2 ---</p> <p>INFO:root:--- Enough local models are collected. Aggregation will start. ---</p> <p>CopyExplain</p> <ul> <li>It will finally start the aggregation for the first round, as     follows:</li> </ul> <p># Aggregator-side Console Example</p> <p>INFO:root:Round 1</p> <p>INFO:root:Current agents: [{\\'agent_name\\': \\'a1\\', \\'agent_id\\': \\'1f503*****\\', \\'agent_ip\\': \\'xxx.xxx.1.101\\', \\'socket\\': 50001}, {\\'agent_name\\': \\'a2\\', \\'agent_id\\': \\'70de8*****\\', \\'agent_ip\\': \\'xxx.xxx.1.101\\', \\'socket\\': 50002}]</p> <p>INFO:root:--- Cluster models are formed ---</p> <p>INFO:root:--- Models pushed to DB: Response [\\'confirmation\\'] ---</p> <p>INFO:root:--- Global Models Sent to 1f503***** ---</p> <p>INFO:root:--- Global Models Sent to 70de8***** ---</p> <p>CopyExplain</p> <p>Here, let's look at the agent-side\u00a0ML models that are locally trained:</p> <p># Agent 1\\'s Console Example</p> <p>INFO:root:--- Training ---</p> <p>INFO:root:--- Training is happening ---</p> <p>INFO:root:--- Training Done ---</p> <p>Trained models: {\\'model1\\': array([[1, 2, 3],</p> <p>[4, 5, 6]]), \\'model2\\': array([[1, 2],</p> <p>[3, 4]])}</p> <p>INFO:root:--- Local (Initial/Trained) Models saved ---</p> <p>CopyExplain</p> <ul> <li>Also, let's look at another agent's ML models that are locally     trained:</li> </ul> <p># Agent 2\\'s Console Example</p> <p>INFO:root:--- Training ---</p> <p>INFO:root:--- Training is happening ---</p> <p>INFO:root:--- Training Done ---</p> <p>Trained models: {\\'model1\\': array([[3, 4, 5],</p> <p>[6, 7, 8]]), \\'model2\\': array([[3, 4],</p> <p>[5, 6]])}</p> <p>INFO:root:--- Local (Initial/Trained) Models saved ---</p> <p>CopyExplain</p> <ul> <li> <p>As in the models sent to the aggregator from agents 1 and 2,     if\u00a0FedAvg\u00a0is correctly applied, the global model should be the     averaged value of these two models. In this case, the number of data     samples is the\u00a0same for both agents 1 and 2, so the global model     should just be an average of the two models.</p> </li> <li> <p>So, let's look at the global models that are generated in the     aggregator:</p> </li> </ul> <p># Agent 1 and 2\\'s Console Example</p> <p>Global Models: {\\'model1\\': array([[2., 3., 4.],</p> <p>[5., 6., 7.]]), \\'model2\\': array([[2., 3.],</p> <p>[4., 5.]])}</p> <p>CopyExplain</p> <ul> <li> <p>The received model is the average of the two local models and thus     averaging has been correctly conducted.</p> </li> <li> <p>The database and data folders are created in     the\u00a0model_path\u00a0specified in the agent configuration file. You can     look at the database values with an SQLite viewer application and     look for some models based on the model ID.</p> </li> <li> <p>Now that we understand what's happening with minimal example runs,     in the next section, we will run a real ML application using an     image classification model using a\u00a0Convolutional Neural     Network\u00a0(CNN).</p> </li> </ul> <p>Running image classification and analyzing the results</p> <ul> <li> <p>This example demonstrates the\u00a0use of this FL framework for     image\u00a0classification tasks. We will use a famous image dataset,     CIFAR-10     (URL:\u00a0[https://www.cs.toronto.edu/\\~kriz/cifar.html]{.underline}),     to show how an ML model grows through the FL process over time.</p> </li> <li> <p>However, this example is only given for the purposes of using the FL     system we have discussed so far and is not focused on maximizing the     performance of the image classification task.</p> </li> </ul> <p>Preparing the CIFAR-10 dataset</p> <ul> <li>The following is the\u00a0information required related to the dataset     size, the training and test data, the number of classes, and the     image size:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Dataset size: 60,000 images</p> <ul> <li> <p>Training data: 50,000 images</p> </li> <li> <p>Test data: 10,000 images</p> </li> <li> <p>Number of classes: 10     (airplane,\u00a0automobile,\u00a0bird,\u00a0cat,\u00a0deer,\u00a0dog,\u00a0frog,\u00a0horse,\u00a0ship,     and\u00a0truck)</p> </li> <li> <p>Each class has 6,000 images</p> </li> <li> <p>Image size: 32x32 pixels, in color</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Figure 6.11\u00a0shows a collection of sample pictures of 10 different     classes in the dataset with 10 random images for each:</p> <p>{width=\"6.268055555555556in\" height=\"4.816666666666666in\"}</p> <p>Figure 6.11 -- The classes in the dataset as well as 10 random images for each category (the images are adapted from https://www.cs.toronto.edu/\\~kriz/cifar.html)</p> <ul> <li>Now that the dataset is\u00a0prepared, we will look into a CNN model used     for the FL process.</li> </ul> <p>The ML model used for FL with\u00a0image classification</p> <ul> <li>Here is the\u00a0description of the ML model architecture of the CNN     model used in this image classification example. To learn more about     what the CNN is, you can find many useful study resources, such     as\u00a0[https://cs231n.github.io/convolutional-networks/]{.underline}:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Conv2D</p> <ul> <li> <p>MaxPool2D (maximum pooling)</p> </li> <li> <p>Conv2D</p> </li> <li> <p>3 fully-connected layers</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The script to define the CNN model is already designed and can be     found in\u00a0cnn.py\u00a0in\u00a0examples/image_classification\u00a0in     the\u00a0simple-fl\u00a0repository on GitHub.</p> <ul> <li>Next, we will run the image classification application with the FL     system.</li> </ul> <p>How to run\u00a0the image classification example with CNN</p> <ul> <li>As mentioned in the installation\u00a0steps at the beginning of This     section, we first install\u00a0the necessary libraries with\u00a0federatedenv,     and then install\u00a0torch\u00a0and\u00a0torchvision\u00a0after that:</li> </ul> <p>pip install torch</p> <p>pip install torchvision</p> <p>CopyExplain</p> <ul> <li> <p>You can configure many settings through the JSON config files in     the\u00a0setups\u00a0folder of the\u00a0simple-fl\u00a0repo of GitHub. For more details,     you can read the general description of the config files in     our\u00a0setups\u00a0documentation     ([https://github.comkeshavaspandat/simple-fl/tree/master/setups]{.underline}).</p> </li> <li> <p>First, you can run two agents. You can increase the number of agents     running on the same device by specifying the appropriate port     numbers.</p> </li> <li> <p>As you already know, the first thing you can do is run the database     and aggregator:</p> </li> </ul> <p># FL server side</p> <p>python -m fl_main.pseudodb.pseudo_db</p> <p>python -m fl_main.aggregator.server_th</p> <p>CopyExplain</p> <ul> <li>Then, start the first and second agents to run the image     classification example:</li> </ul> <p># First agent</p> <p>python -m examples.image_classification.classification</p> <p>_engine 1 50001 a1</p> <p># Second agent</p> <p>python -m examples.image_classification.classification</p> <p>_engine 1 50002 a2</p> <p>CopyExplain</p> <ul> <li> <p>To simulate the actual FL\u00a0scenarios, the amount of training data     accessible from each agent can be limited to a specific number. This     should be specified with the\u00a0num_training_data\u00a0variable     in\u00a0classification_engine.py. By default, it uses 8,000 images (2,000     batches) for each round.</p> </li> <li> <p>Now that we can run the two agents to test the FL process using CNN     models, let us look further into the results by running the image     classification example.</p> </li> </ul> <p>Evaluation of running the image classification with CNN</p> <ul> <li> <p>The performance\u00a0data (the accuracy of each local model cluster     model) is stored in our database. You can access the     corresponding\u00a0.db\u00a0file to see the performance history.</p> </li> <li> <p>The\u00a0DataManager\u00a0instance (defined in\u00a0ic_training.py) has a function     to return one batch of images and their labels (get_random_images).     You can use this function to show the actual labels and the     predicted labels by the trained CNN on specific images.</p> </li> <li> <p>Figure 6.12\u00a0shows a plot of the learning performance from our     experimental runs on our side; the results may look different when     you run it with your own settings:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.204861111111111in\"}</p> <p>Figure 6.12 -- Plot of the learning performance from the experimental runs for FL using CNN for image classification</p> <ul> <li>Again, as we only\u00a0use two agents here, the results just look     slightly different. However, with the proper hyperparameter     settings, data amount, and the number of agents, you will be able to     carry out an FL evaluation that produces meaningful results, which     we would like you to explore on your own, as the focus here is just     how to connect the actual ML models to this FL environment.</li> </ul> <p>Running five agents</p> <ul> <li> <p>You can easily run five\u00a0agents for the image classification     application by just specifying different port numbers and agent     names in the terminal.</p> </li> <li> <p>The results look similar to what we discussed in the previous     section except the real ML models are connected (in this case, the     ML model being aggregated is CNN).</p> </li> <li> <p>After running the five agents, the data and database folders look     like in\u00a0Figure 6.13:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.6479166666666667in\"}</p> <p>Figure 6.13 -- Results to be stored in each folder with the agent's unique name</p> <ul> <li>Figure 6.14\u00a0shows the uploaded local\u00a0models in the database with     information about the local model ID, the time the models were     generated, the ID of the agent that uploaded the local model,     performance metrics, and round information:</li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.133333333333334in\"}</p> <p>Figure 6.14 -- Information about the local models in the database\u00a0</p> <ul> <li> <p>If you look at the database in\u00a0Figure 6.14, there are five models     collected by the five agents with local performance data.</p> </li> <li> <p>For each round, those five local models are aggregated to produce a     cluster global model, as in the\u00a0cluster_models\u00a0table in the     database, as shown in\u00a0Figure 6.15.</p> </li> <li> <p>The database storing cluster models has information about the     cluster model ID, the time the models were\u00a0generated, the ID of the     aggregator that created the cluster model, and round information:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"1.6520833333333333in\"}</p> <p>Figure 6.15 -- Information about the cluster models in the database</p> <ul> <li>In this way, you can connect as many agents as possible. It is up to     you to optimize the settings of the local ML algorithms to obtain     the best-performing federated models out of the FL system.</li> </ul> <p>Summary</p> <ul> <li> <p>Here, we\u00a0discussed the execution of FL systems in detail and how the     system will behave according to the interactions between the     aggregator and agents.</p> </li> <li> <p>The step-by-step explanation of the FL system behavior based on the     outcomes of the console examples guides you to understand the     aggregation process of the\u00a0FedAvg\u00a0algorithm.</p> </li> <li> <p>Furthermore, the image classification example showed how CNN models     are connected to the FL system and how the FL process increases the     accuracy through aggregation, although this was not optimized to     maximize the training results but simplified to validate the     integration using CNN.</p> </li> <li> <p>With what you have learned Here, you will be able to design your own     FL applications integrating the principles and framework introduced     in this book, and furthermore, will be able to assess the FL     behavior on your own to see whether the whole flow of the FL process     and model aggregation is happening correctly and consistently.</p> </li> <li> <p>In the next section, we will cover a variety of model aggregation     methods and show how FL works well with those aggregation     algorithms.</p> </li> </ul> <p>Model Aggregation</p> <ul> <li> <p>In the\u00a0Model aggregation basics\u00a0section of\u00a0section 3,\u00a0Workings     of the Federated Learning System, we introduced the concept of     aggregation within the\u00a0federated learning\u00a0(FL) process at a     high level.</p> </li> <li> <p>Recall that aggregation is the means by which an FL approach uses     the models trained locally by each agent to produce a model with     strong global performance.</p> </li> <li> <p>It is clear to see that the strength and robustness of the     aggregation method employed are directly correlated to the resulting     performance of the end\u00a0global model.</p> </li> <li> <p>As a result, choosing the appropriate aggregation method based on     the local datasets, agents, and FL system hierarchy is key to     achieving good performance with FL. In fact, the focal point of many     publications in this field is providing mathematically backed     convergence guarantees for these methods in a variety of\u00a0theoretical     scenarios.</p> </li> <li> <p>The goal of This section is to cover some of the research that has     been done on aggregation methods and their convergence in both ideal     and non-ideal cases, tying these methods to their strengths in the     different scenarios that arise in the practical applications of FL.</p> </li> <li> <p>After reading the section, you should be able to understand how     different characterizations of an FL scenario call for different     aggregation methods, and you should have an idea of how these     algorithms can actually\u00a0be implemented.</p> </li> <li> <p>Here, we will cover the\u00a0following topics:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Revisiting aggregation</p> <ul> <li> <p>Understanding FedAvg</p> </li> <li> <p>Modifying aggregation for\u00a0non-ideal cases</p> </li> </ul> <p>Technical requirements</p> <ul> <li> <p>The Python algorithm implementations presented in this work can all     be found in the\u00a0ch7\u00a0folder, which is     located\u00a0at\u00a0https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7.</p> </li> <li> <p>For the pure aggregation algorithms, auxiliary code is included to     display example output from preset local parameters. The aggregation     methods that modify the local training process require an FL system     in order to operate -- for these, full implementations using STADLE     are included.</p> </li> <li> <p>Also, the pure aggregation algorithms can be directly tested with     STADLE by configuring the aggregation method. Information on how to     run the examples can be found in the associated\u00a0README\u00a0files.</p> </li> <li> <p>The installation of the\u00a0stadle-client\u00a0package through\u00a0pip\u00a0is     necessary to run the full FL process examples. The following command     can be used to perform\u00a0this installation:</p> </li> </ul> <p>pip install stadle-client</p> <p>CopyExplain</p> <ul> <li>Using a virtual environment is recommended to isolate the specific     package versions installed with\u00a0stadle-client\u00a0from other     installations on\u00a0the system.</li> </ul> <p>Revisiting aggregation</p> <ul> <li>To solidly\u00a0contextualize aggregation within FL, first, we describe     the components of a system that are necessary for FL to\u00a0be applied:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   A set of computational agents that perform the local training     portion\u00a0of FL.</p> <ul> <li> <p>Each agent possesses a local dataset (static or dynamic), of which     no portion can be communicated to another agent under the     strictest\u00a0FL scenario.</p> </li> <li> <p>Each agent possesses a parameterized model that can be trained on     the local dataset, a process that produces the local optima     parameter set for\u00a0the model.</p> </li> <li> <p>A parameter server, or aggregator, which receives the locally     trained models at each iteration from the agents and sends back the     resulting model produced by the aggregation method chosen to\u00a0be     used.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Every FL communication round can then be broken down into the     following\u00a0two phases:</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The\u00a0local training phase, where agents train their local models on     their local datasets for some number\u00a0of iterations</p> <ul> <li>The\u00a0aggregation phase, where the agents send the resulting trained     local models from the previous phase to the aggregator and receive     the aggregated model for use as the starting model in the local     training phase of the\u00a0next round.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   So, what exactly does it mean for an agent to send a locally trained     model during the aggregation phase?</p> <ul> <li> <p>The general approach is to use the\u00a0parameter sets\u00a0that define the     local models, allowing for some degree of generalization across all     models that can be parameterized in such a way. However, a second     approach focuses on sending the\u00a0local gradients\u00a0accumulated during     the local training when using a gradient-based optimization approach     to the aggregator, with the agents updating their models using the     received aggregate gradient at the end of the round.</p> </li> <li> <p>While this approach restricts usage to models with gradient-based     local training methods, the prevalence of such methods when\u00a0training     deep learning models has led to a subset of aggregation methods     based on gradient aggregation.</p> </li> <li> <p>Here, we choose to frame model aggregation through the lens of     the\u00a0FedAvg algorithm.</p> </li> </ul> <p>Understanding FedAvg</p> <ul> <li> <p>Earlier, the aggregation algorithm\u00a0known as FedAvg was introduced to     help clarify the general structure and represent the more abstract     concepts discussed earlier with a specific example.</p> </li> <li> <p>FedAvg was used for two reasons: simplicity in the underlying     algorithm, and generalizability across more model types than     gradient-based approaches. It also benefits from extensive     references by researchers, with performance analysis in different     theoretical scenarios using FedAvg as a baseline when proposing new     aggregation methods.</p> </li> <li> <p>This focus in the research community can most likely be attributed     to the fact that the original FedAvg paper was published by the team     working at Google that first brought exposure to the concept and     benefits of FL. For further reading, this paper can be     found\u00a0at\u00a0https://arxiv.org/abs/1602.05629?context=cs.</p> </li> <li> <p>FedAvg is predated by an aggregation approach known as\u00a0Federated     Stochastic Gradient Descent\u00a0(FedSGD). FedSGD can be viewed as     the gradient aggregation\u00a0analog of the model parameter averaged     performed by FedAvg.</p> </li> <li> <p>In addition, the concept of averaging model parameters was examined     prior to FedAvg for parallelized SGD approaches, outside of the     context of FL. Essentially, the analysis of these parallelized SGD     approaches mirrors the\u00a0Independently and Identically     Distributed\u00a0(IID) case of FedAvg -- this concept will be     discussed\u00a0later in the section.</p> </li> <li> <p>Regardless, the simplicity, generalizability, and popularity of     FedAvg make it a good base to delve deeper into, contextualizing the     need for the numerous aggregation approaches discussed in later     sections that have built upon or, otherwise, improved\u00a0on FedAvg.</p> </li> <li> <p>Previously, FedAvg was only presented as an algorithm that takes     models\u00a0{width=\"1.4427088801399826in\"     height=\"0.23811679790026247in\"}with a respective local dataset size     of\u00a0{width=\"1.3452384076990376in\"     height=\"0.20833333333333334in\"}, where the sum equals\u00a0N\u00a0and     returns:</p> </li> </ul> <p>{width=\"2.026042213473316in\" height=\"0.9540912073490814in\"}</p> <p>As shown in the\u00a0Aggregating local models\u00a0section of\u00a0section 4,\u00a0Federated Learning Server Implementation with Python,\u00a0simple-fl\u00a0uses the\u00a0following function to compute a weighted average of the buffered models (models sent from clients during the current round) based on the amount of data used to locally train\u00a0each model:</p> <p>def _average_aggregate(self,</p> <p>buffer: List[np.array],</p> <p>num_samples: List[int]) -&gt; np.array:</p> <p>\\\"\\\"\\\"</p> <p>Given a list of models, compute the average model (FedAvg).</p> <p>This function provides a primitive mathematical operation.</p> <p>:param buffer: List[np.array] - A list of models to be aggregated</p> <p>:return: np.array - The aggregated models</p> <p>\\\"\\\"\\\"</p> <p>denominator = sum(num_samples)</p> <p># weighted average</p> <p>model = float(num_samples[0]) / denominator * buffer[0]</p> <p>for i in range(1, len(buffer)):</p> <p>model += float(num_samples[i]) / denominator * buffer[i]</p> <p>return model</p> <p>CopyExplain</p> <ul> <li> <p>The original algorithm does not differ too greatly from this     portrayal. The high-level steps of the algorithm are\u00a0as follows:</p> </li> <li> <p>The server randomly samples\u00a0K * C\u00a0clients, where\u00a0K\u00a0is the total     number of clients and\u00a0C\u00a0is a parameter between 0\u00a0and 1.</p> </li> <li> <p>The selected\u00a0K * C\u00a0clients receive the most recent aggregate     model and begin to train the model on their\u00a0local data.</p> </li> <li> <p>Each client\u00a0sends its locally trained model back to the server after     some desired amount of training\u00a0is completed.</p> </li> <li> <p>The server computes the parameter-wise arithmetic mean of the     received models to compute the newest\u00a0aggregate model.</p> </li> <li> <p>Parallels can be immediately drawn between this formal     representation and our presentation of the FL process,     with\u00a0ClientUpdate\u00a0performing local training for an agent and the     server performing aggregation using the same weighted averaging     algorithm.</p> </li> <li> <p>One important point is the sampling of a subset of clients to     perform the local training and model transmission during each round,     allowing for client subsampling parameterized by C.</p> </li> <li> <p>This parameter is included to experimentally determine the     convergence rates of various client set sizes -- in an ideal     scenario, this will be set\u00a0to\u00a01.</p> </li> <li> <p>As previously mentioned, FedAvg is the ideal FL scenario that     essentially mirrors an approach to parallelized stochastic\u00a0gradient     descent. In\u00a0parallelized SGD\u00a0(pSGD), the goal is to leverage     hardware parallelization (for example, running on multiple cores in     parallel) in order to speed up SGD convergence on a specific machine     learning task.</p> </li> <li> <p>One approach for this task is for each core to train a base model on     some subset of the data in parallel for some number of iterations,     then aggregate the partially trained models and use the aggregated     models as the next base for training.</p> </li> <li> <p>In this case, if the cores are considered to be agents in an FL     scenario, the parallelized SGD approach is the same as FedAvg in an     ideal scenario.</p> </li> <li> <p>This means all of the convergence guarantees and respective analyses     that were done for pSGD can be directly applied to FedAvg, assuming     the ideal FL scenario. From this prior work, it has, therefore, been     shown that FedAvg demonstrates strong\u00a0convergence rates.</p> </li> <li> <p>After all this praise for FedAvg, it is only natural to question why     more complex aggregation methods are even necessary.</p> </li> <li> <p>Recall that the phrase \"ideal FL scenario\" was used several times     when discussing FedAvg convergence. The unfortunate reality is that     most practical FL\u00a0applications will fail to meet one or more of the     conditions stipulated by\u00a0that phrase.</p> </li> <li> <p>The ideal FL scenario can be broken down into three\u00a0main conditions:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The local datasets used for training are IID (the datasets are     independently drawn from the same\u00a0data distribution).</p> <ul> <li> <p>The computational agents are relatively homogeneous in\u00a0computational     power.</p> </li> <li> <p>All agents can be assumed to\u00a0be non-adversarial.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   At a high level, it is clear why these qualities would be desirable     in an FL scenario. To understand, in greater detail, why these three     conditions are necessary, the performance of\u00a0FedAvg in the absence     of each condition will be examined in the\u00a0upcoming subsections.</p> <p>Dataset distributions</p> <ul> <li> <p>To examine\u00a0FedAvg in the non-IID case, first, it is important     to\u00a0define what exactly is being referred to by the distribution of a     dataset. In classification problems, the data distribution often     refers to the distribution of the true classes associated with each     data point. For example, consider the MNIST dataset, where each     image is a handwritten digit from 0 to 9.</p> </li> <li> <p>If a uniform random sample of 1,000 images was to be taken from the     dataset, the expected number of images from each class would be the     same -- this could be considered a uniform data distribution.</p> </li> <li> <p>Alternatively, a sample with 910 images of the digit 0 along with 10     images of the other digits would be a heavily skewed\u00a0data     distribution.</p> </li> <li> <p>To generalize outside of classification tasks, this definition can     be extended to refer to the distribution of\u00a0features\u00a0present     across the dataset.</p> </li> <li> <p>These features could be manually crafted and provided to the model     (such as linear regression), or they could be extracted from the raw     data as part of the model pipeline (such as deep CNN models).</p> </li> <li> <p>For classification problems, the class distribution is generally     contained within the feature distribution, due to the implicit     belief that the features are sufficient for correctly predicting the     class.</p> </li> <li> <p>The benefit of looking at feature distributions is the data-centric     focus on features (versus the task-centric focus on classes),     allowing for generalization across machine\u00a0learning tasks.</p> </li> <li> <p>However, in the context of experimental analysis, the ability to     easily construct non-IID samples from a dataset makes classification     tasks ideal for testing the robustness of FedAvg and different     aggregation methods within an FL context.</p> </li> <li> <p>To examine FedAvg in this section, consider\u00a0a toy FL scenario where     each agent trains a CNN on\u00a0data samples taken from the MNIST dataset     described earlier. There are two main cases, which are\u00a0detailed     next.</p> </li> </ul> <p>IID case</p> <ul> <li> <p>The convergence\u00a0of the models can be represented\u00a0through the use of     the model parameter space. The parameter space of a model     with\u00a0n\u00a0parameters can be thought of as an\u00a0n-dimensional     Euclidean space, where each parameter corresponds to one dimension     in the space.</p> </li> <li> <p>Consider an initialized model; the initial parameters of this model     can then be represented as a\u00a0point\u00a0in the parameter space.</p> </li> <li> <p>As local training and aggregation occur, this representative point     will move in the parameter space, with the end goal being     convergence to a point in the space corresponding to a local optimum     of the loss or error function\u00a0being minimized.</p> </li> <li> <p>One key point of these functions is the dependence on the data used     during the local training process -- when the datasets across the     agents are IID, there is a general tendency for the optima of the     respective loss/error functions to be relatively close in the     parameter space.</p> </li> <li> <p>Consider a trivial case where the datasets are IID and all models     are initialized with the same parameters. As shown in the\u00a0Model     aggregation basics\u00a0section of\u00a0section 3,\u00a0</p> </li> <li> <p>Workings of the Federated Learning System, a simplified version of     the parameter space can\u00a0be depicted:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.855555555555555in\"}</p> <p>Figure 7.1 -- Models with the same initialization and IID datasets</p> <ul> <li> <p>Observe how both\u00a0models start at the same point (purple x) and\u00a0move     toward the same optima (purple dot), resulting in aggregate models     close to the optima shared by\u00a0both models.</p> </li> <li> <p>Due to the resulting similarity of the error/loss functions across     the agents, the models tend to converge toward the same or similar     optima in the space during training.</p> </li> <li> <p>This means that the change in the models after each aggregation step     is relatively small, resulting in the convergence rates mirroring     the single local model case.</p> </li> <li> <p>If the underlying data distribution is representative of the true     data distribution (for example, uniform across the 10 different     digits for MNIST), the resulting aggregated model will     demonstrate\u00a0strong performance.</p> </li> <li> <p>Next, consider the generalized IID case where each model     is\u00a0initialized separately:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.747222222222222in\"}</p> <p>Figure 7.2 -- Models with different initializations and IID datasets</p> <ul> <li> <p>In this scenario, observe\u00a0how both models start at different\u00a0points     (bold/dotted\u00a0x) and initially move toward different optima,     producing a poor first model. However, after the first aggregation,     both models start at the same point and move toward the same optima,     resulting in similar convergence to the\u00a0first case.</p> </li> <li> <p>It should be clear that this reduces to the previous case after the     first aggregation step since each model starts the second round with     the resulting aggregated parameters. As a result, the convergence     properties previously stated can be extended to the general case\u00a0of     FedAvg\u00a0with IID\u00a0local datasets.</p> </li> </ul> <p>Non-IID Case</p> <ul> <li> <p>The key property\u00a0in the IID local dataset case that\u00a0allows for     convergence speeds mirroring the single model case is the similarity     of the local optima of the loss/error functions, due to their     construction from similar data distributions. In the non-IID case,     similarity in the optima is generally no\u00a0longer observed.</p> </li> <li> <p>Using the MNIST example, let's consider an FL scenario with two     agents such that the first agent only has images with digits 0 to 4     and the second agent only has images with digits 5 to 9; that is,     the datasets are not IID. These datasets would essentially lead to     two completely different five-class classification tasks at the     local training level, as opposed to the original 10-class     classification problem -- this will result in completely different     parameter space optima between the first agent and the second agent.</p> </li> <li> <p>Consider the simplified representation of this parameter space as     follows, with both models having the\u00a0same initialization:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.8597222222222225in\"}</p> <p>Figure 7.3 -- Models with different initializations and non-IID datasets</p> <ul> <li> <p>Now that optima are no longer shared (triangles/squares representing     optima for bold/dotted\u00a0model, respectively), even     repeated\u00a0aggregations cannot create an aggregate model close to     optima of either model.</p> </li> <li> <p>The models diverge, or drift, during each local training phase due     to the different target optima in\u00a0each round.</p> </li> <li> <p>Only a small subset of the optima will be shared between the     loss/error functions of both agents.</p> </li> <li> <p>As a result, there is a high probability that each model will move     toward optima that are not shared during local training, leading the     models to drift apart in the parameter space. Each aggregation step     will then pull the models toward the wrong optima, reverting the     progress made during local training and hampering convergence.</p> </li> <li> <p>Note that just taking the average of optima from different agents is     very unlikely to be near optima from any of the agents in the     parameter space, so in this case, the result of continued     aggregation is generally a model that performs poorly across the     whole dataset.</p> </li> <li> <p>Convergence to a shared optimum might eventually occur due to     stochasticity observed during local training, inducing movement of     the aggregate model in the parameter space, but this does not have     theoretical guarantees and will be far slower than convergence in     the IID case when it\u00a0does occur.</p> </li> </ul> <p>Important note</p> <ul> <li> <p>This MNIST example is a theoretical extreme of non-IID datasets. In     practice, non-IID datasets might refer to different skews in the     data distributions across agents (for example, twice as many images     with digits 0--4 versus 5--9, and vice versa).</p> </li> <li> <p>The severity of the difference is correlated to the performance of     FedAvg, so adequate performance can still be reached in less severe     cases.</p> </li> <li> <p>However, in these cases, the performance of FedAvg will generally     always be inferior to the analogous centralized training task where     a single model is trained on all of the local datasets at once --     the theoretically optimal model achievable\u00a0by FL.</p> </li> <li> <p>While this section\u00a0focused on the statistical basis for\u00a0the issues     that arise from non-IID datasets, the next section examines a far     more direct problem that can arise -- especially when deploying     at\u00a0larger scales.</p> </li> </ul> <p>Computational power distributions</p> <ul> <li> <p>An unstated\u00a0assumption of the agents\u00a0participating in FL is that     each agent is capable of performing local training if given infinite     time.</p> </li> <li> <p>Agents with limited computational power (memory and speed) might     take significantly more time than other agents to finish local     training, or they might require techniques such as quantization to     support the model and training process.</p> </li> <li> <p>However, agents that cannot complete local training during some     rounds will trivially prevent convergence by stalling the\u00a0FL     process.</p> </li> <li> <p>Generally, convergence bounds and experimental results focus on the     number of communication rounds required to reach some level of     performance.</p> </li> <li> <p>Under this metric and the aforementioned assumption, convergence is     completely independent of the computational power afforded to each     agent, since computational power only affects the actual time     necessary to complete one round.</p> </li> <li> <p>However, convergence speed in practical applications is measured by     the actual time taken, not the number of completed communication     rounds -- this means that the time to complete each round is as     important as the number of rounds.</p> </li> <li> <p>This metric of the total time taken is where na\u00efve FedAvg     demonstrates poor performance when heterogeneous computation power     is observed in the agents participating\u00a0in FL</p> </li> <li> <p>Specifically, the time to complete each round is bottlenecked by the     local training time of the slowest agent participating in the round;     this is because aggregation is trivially fast compared to training     in most cases and must wait for all agents to complete local     training.</p> </li> <li> <p>When all agents are participating in the round, this bottleneck     becomes the slowest overall agent. In the homogeneous computational     power case, the difference in local training time between the     fastest agent and the slowest agent will be relatively     insignificant. In the heterogeneous case, a single straggler will     greatly reduce the convergence time of FedAvg and lead to     significant idle time in the faster agents waiting to receive     the\u00a0aggregated model.</p> </li> <li> <p>Two modifications\u00a0to FedAvg with full agent participation\u00a0might     initially seem to address this problem; however, both have drawbacks     that lead to\u00a0suboptimal performance:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   One approach is to rely on agent subsampling in each round, leading     to the probability of the straggler effect occurring in each round     depending on the number of agents and the sample size taken in each     round. This can be sufficient in cases with only a few straggling     agents, but it becomes proportionately worse as this number     increases and does not completely eliminate the problem from     occurring. In addition, small sample sizes lose out on the     robustness benefits from aggregation over\u00a0more agents.</p> <ul> <li>A second approach is to allow all agents to begin local training at     the beginning of each round and prematurely begin aggregation after     some number of models have been received. This method has the     benefit of being able to completely eliminate the straggler effect     without greatly restricting the number of agents participating in     aggregation during each round. However, it results in the slowest     agents never participating in aggregation over all rounds,     essentially reducing the number of active agents and potentially     limiting the variety of data used during training. In addition, the     agents that are too slow to participate in aggregation will have     done computational work for\u00a0no benefit.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   It is clear that some local adjustment based on available     computational power is necessary for aggregation to be performed     efficiently, regardless of the end aggregation method applied to the     received models at the end of\u00a0each round.</p> <ul> <li>Both the non-IID case and the heterogeneous computation power case     focus on properties of an FL system that are generally easy to     observe and under some level of administrative\u00a0control. The next     case we present deviates\u00a0from this by challenging a key assumption     when considering practical\u00a0FL systems.</li> </ul> <p>Protecting against adversarial agents</p> <ul> <li> <p>So far, it has been\u00a0assumed that every agent\u00a0participating in an FL     scenario always acts in the desired way; that is, actively and     correctly training the received model locally and participating in     model transmission to/from the aggregator.</p> </li> <li> <p>This is easily achieved in a research setting, where the federated     setting is simulated and the agents are singularly controlled;     however, this assumption of agents behaving correctly does not     always hold\u00a0in practice.</p> </li> <li> <p>One example that does not involve targeted malicious intent is an     error in the model weights being transmitted by an agent to the     aggregator.</p> </li> <li> <p>This can happen when the dataset used by an agent is flawed or the     training algorithm is incorrectly implemented (the corruption of the     parameter data during transmission is also possible). In the worst     case, this can essentially lead to the parameters of one or many     models being statistically equivalent to random noise.</p> </li> <li> <p>When the L2 norm (the extension of vector magnitude     for\u00a0n-dimensional tensors) of the random noise is not     significantly greater than that of the valid models,</p> </li> <li> <p>FedAvg will suffer performance loss that is proportional to the     ratio of faulty agents to all agents -- which is relatively     acceptable when this ratio is small. However, even a single faulty     agent can induce a near-random aggregate model if the norm of the     agent's noise is significantly high.</p> </li> <li> <p>This is due to the nature of the arithmetic mean being performed     internally during the\u00a0FedAvg aggregation.</p> </li> <li> <p>The problem becomes worse when agents can be controlled by malicious     adversaries.</p> </li> <li> <p>A single malicious agent with sufficient information is capable of     producing any desired model after aggregation through large     modifications to the parameters of the model it submits.</p> </li> <li> <p>Even without direct knowledge of the model parameters and associated     weights of the other agents, a malicious agent can leverage     relatively small changes between the local models and the aggregate     model in later rounds to use the previous aggregate model as an     estimate of the expected local\u00a0model parameters.</p> </li> <li> <p>Therefore, FedAvg offers little to no robustness against both random     and controlled adversarial agents in an FL setting. While one     potential means of mitigation would be to separately monitor the     agents and prevent adversarial agents from transmitting models,     significant damage to the convergence of the final model might have     already occurred in the time necessary to identify\u00a0such agents.</p> </li> <li> <p>It should now be clear that FedAvg trades robustness in these     non-ideal cases for simplicity in the calculation. Unfortunately,     this robustness is a key consideration for practical\u00a0applications of     FL due to the lack of\u00a0control compared to the research setting.</p> </li> <li> <p>The next section focuses on methods of achieving robustness against     the three non-ideal cases presented in\u00a0this section.</p> </li> </ul> <p>Modifying aggregation for non-ideal cases</p> <ul> <li> <p>In practical FL\u00a0applications, at least one of the aforementioned     assumptions that constitute an ideal FL scenario generally does not     hold; therefore, the usage of alternative aggregation methods might     be necessary to best perform FL.</p> </li> <li> <p>The goal of this section is to cover examples of aggregation methods     that target heterogeneous computational power, adversarial agents,     and non-IID datasets, in order\u00a0of difficulty.</p> </li> </ul> <p>Handling heterogeneous computational power</p> <ul> <li> <p>As mentioned\u00a0earlier, the\u00a0ideal aggregation approach, in this case,     consistently avoids the straggler effect while maximizing the number     of agents participating in FL and allowing all agents to contribute     to some extent, regardless of computational power differences.</p> </li> <li> <p>Agents become stragglers during a round when their local training     takes significantly more time than the majority of the agents.</p> </li> <li> <p>Therefore, effectively addressing this problem actually requires     some level of adaptability at the agent level in the local     training\u00a0process, based\u00a0on the computational power available to\u00a0each     agent.</p> </li> </ul> <p>Manual adjustment</p> <ul> <li> <p>One straightforward\u00a0way of accomplishing this is to change the     number of local training iterations based on the time necessary for     each iteration. In other words, the local training time is fixed and     each agent performs as many iterations as possible within this time,     as opposed to performing a fixed number of iterations.</p> </li> <li> <p>This trivially eliminates the straggler problem but might result in     poor performance if a large amount of local training time must be     allocated for the slow agents to meaningfully contribute due to the     model drift from faster agents potentially performing too many local     training iterations.</p> </li> <li> <p>This can be mitigated by setting a maximum number of local training     iterations. However, a careful balance in the allocated local     training must be found to have enough time for slow agents to     produce adequate models while preventing faster agents from sitting     idle after reaching the maximum number of iterations.</p> </li> <li> <p>It is also unclear how such a threshold could be preemptively     determined to achieve optimal performance instead of relying on     experimental results to search for the\u00a0best configuration.</p> </li> </ul> <p>Automatic adjustment -- FedProx</p> <ul> <li> <p>An aggregation\u00a0method known\u00a0as FedProx follows this same methodology     of dynamically adjusting the local training processes for each agent     based on computational power, while also revising the termination     condition for local training to aid in the theoretical analysis of     convergence.</p> </li> <li> <p>Specifically, the fixed number of local training iterations is     replaced by a termination condition for the training loop that     accommodates agents with varying levels of\u00a0computational power.</p> </li> <li> <p>The underlying concept for this termination condition is the     \u03b3-inexact solution, which is satisfied when the magnitude of the     gradient at the \u03b3-inexact optima is less than \u03b3 times the magnitude     of the gradient at the beginning of local training. Intuitively, \u03b3     is a value between 0 and 1, with values closer to 0 leading to more     local training iterations due to the stricter termination condition.</p> </li> <li> <p>Therefore, \u03b3 allows for the parameterization of an     agent's\u00a0computational power.</p> </li> <li> <p>One potential problem with the termination condition approach is the     divergence of the locally trained model from the aggregate model     after many iterations of local training resulting from a strict     condition.</p> </li> <li> <p>To combat this, FedProx adds a proximal term to the objective     function being minimized equal to\u00a0the following:</p> </li> </ul> <p>{width=\"1.1302088801399826in\" height=\"0.40687554680664917in\"}</p> <p>Here,\u00a0{width=\"0.5311986001749781in\" height=\"0.26833770778652666in\"}\u00a0represents the received aggregate\u00a0model weights.</p> <ul> <li> <p>The proximal term penalizes differences between the current weights     and the aggregated model weights, restricting the aforementioned     local model divergence with the strength parameterized by \u03bc.</p> </li> <li> <p>From these two concepts, FedProx allows for a variable number of     iterations proportional to the computational power to be performed     by each agent without requiring manually tuned iteration counts for     each agent or a set amount of allocated training time.</p> </li> <li> <p>Because of the addition of the proximal term, FedProx requires     gradient-based optimization methods to be employed in order to work     -- more information\u00a0on the underlying\u00a0theory and comparison to     FedAvg can be found in the original paper (which     is\u00a0at\u00a0https://arxiv.org/abs/1812.06127).</p> </li> </ul> <p>Implementing FedProx</p> <ul> <li> <p>Because the\u00a0modifications\u00a0made by FedProx to FedAvg are all on the     client side, the actual implementation of FedProx consists entirely     of modifications to the local training framework.</p> </li> <li> <p>Specifically, FedProx involves a new termination condition for local     training and the addition of a constraining term to the local loss     function. Therefore, it is helpful to use an example of the local     training code to frame exactly how FedProx can\u00a0be integrated.</p> </li> <li> <p>Let's consider the following generic training code\u00a0using PyTorch:</p> </li> </ul> <p>agg_model = ... # Get aggregate model -- abstracted out of example</p> <p>model.load_state_dict(agg_model.state_dict())</p> <p>for epoch in range(num_epochs):</p> <p>for batch_idx, (inputs, targets) in enumerate(trainloader):</p> <p>inputs, targets = inputs.to(device), targets.to(device)</p> <p>optimizer.zero_grad()</p> <p>outputs = model(inputs)</p> <p>loss = criterion(outputs, targets)</p> <p>loss.backward()</p> <p>optimizer.step()</p> <p>CopyExplain</p> <ul> <li> <p>Let this be the code that performs\u00a0num_epochs\u00a0epochs of training on     the local dataset using the received aggregate model for each round.</p> </li> <li> <p>The first necessary modification for FedProx is to\u00a0replace the     fixed\u00a0number of epochs with a dynamic termination condition,     checking whether a \u03b3-inexact solution has been found with the     aggregated model as the initial model. To do this, the total     gradient over the entire training dataset for the aggregate model     and the current local model must be stored -- this can be     performed\u00a0as follows:</p> </li> </ul> <p>agg_model = ... # Get aggregated model from aggregator</p> <p>model.load_state_dict(agg_model.state_dict())</p> <p>agg_grad = None</p> <p>curr_grad = None</p> <p>gamma = 0.9</p> <p>mu = 0.001</p> <p>CopyExplain</p> <ul> <li> <p>Values for the two FedProx parameters,\u00a0gamma\u00a0and\u00a0mu, are set, and     variables to store the gradients of both the aggregate model and the     latest local model\u00a0are defined.</p> </li> <li> <p>We then define the \u03b3-inexact new termination condition for local     training using these\u00a0gradient variables:</p> </li> </ul> <p>def gamma_inexact_solution_found(curr_grad, agg_grad, gamma):</p> <p>if (curr_grad is None):</p> <p>return False</p> <p>return curr_grad.norm(p=2) \\&lt; gamma * agg_grad.norm(p=2)</p> <p>CopyExplain</p> <ul> <li>This condition is now checked before each training loop iteration to     determine when to stop local training. The\u00a0total_grad\u00a0variable is     created to store the cumulative gradients that\u00a0were created\u00a0from     each minibatch\u00a0during backpropagation:</li> </ul> <p>model.train()</p> <p>while (not gamma_inexact_solution_found(curr_grad, agg_grad, gamma)):</p> <p>total_grad = torch.cat([torch.zeros_like(param.data.flatten()) for param in model.parameters()])</p> <p>for batch_idx, (inputs, targets) in enumerate(trainloader):</p> <p>inputs, targets = inputs.to(device), targets.to(device)</p> <p>optimizer.zero_grad()</p> <p>outputs = model(inputs)</p> <p>loss = criterion(outputs, targets)</p> <p>CopyExplain</p> <ul> <li>To compute the proximal term, the weights of both the aggregate     model and the latest local model are computed. From these weights,     the proximal term is computed and added to the\u00a0loss term:</li> </ul> <p>curr_weights = torch.cat([param.data.flatten() for param in model.parameters()])</p> <p>agg_weights = torch.cat([param.data.flatten() for param in agg_model.parameters()])</p> <p>prox_term = mu * torch.norm(curr_weights - agg_weights, p=2)**2</p> <p>loss += prox_term</p> <p>CopyExplain</p> <ul> <li>The gradients are computed and added to the cumulative sum     stored\u00a0in\u00a0total_grad:</li> </ul> <p>loss.backward()</p> <p>grad = torch.cat([param.grad.flatten() for param in model.parameters()])</p> <p>total_grad += grad</p> <p>optimizer.step()</p> <p>CopyExplain</p> <ul> <li>Finally, we\u00a0update\u00a0agg_grad\u00a0(if the\u00a0gradients were computed with the     aggregate weights) and\u00a0curr_grad\u00a0after the current local training     iteration\u00a0is completed:</li> </ul> <p>if (agg_grad == None):</p> <p>agg_grad = total_grad</p> <p>curr_grad = total_grad</p> <p>CopyExplain</p> <ul> <li> <p>These modifications allow for FedProx to be implemented on top of     FedAvg. The full FL example using FedProx can be     found\u00a0at\u00a0https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7/agg_fl_examples/cifar_fedprox_example.</p> </li> <li> <p>An auxiliary approach to handle the heterogeneous computational     power scenario that helps with computational efficiency when only     mild heterogeneity is observed is the idea of\u00a0compensation in     aggregation.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Consider the case where aggregation occurs once the number of     received models surpasses some threshold (generally, this is less     than the number of participating agents). Using this threshold     allows the straggler effect to be mitigated; however, the work done     by slower agents ends up being discarded each round, leading     to\u00a0training inefficiency.</p> <ul> <li> <p>The core idea of compensation is to allow for the local training     that is done by a slower agent in one round to instead be included     in the model aggregation of a subsequent round. The age of the model     is compensated for in this subsequent round by multiplying the     weight used for the weighted average and a penalizing term during     aggregation. By doing so, slower agents can be given two or three     times as much training time as that used by the faster agents while     avoiding the straggler effect.</p> </li> <li> <p>Mild heterogeneity is required in order to prevent cases where     slower agents require too much extra time for training. This is due     to the associated penalty given to the model after many rounds have     passed; it will be severe enough to effectively lead to no     contribution and reduce aggregation without compensation -- this is     necessary to prevent models that are too old from hampering the     convergence of the\u00a0aggregate model.</p> </li> <li> <p>Finally, we examine methods that help to address the third non-ideal     property, where some subset of\u00a0agents are\u00a0controlled by an adversary     or are, otherwise, behaving in an\u00a0undesirable way.</p> </li> </ul> <p>Adversarial agents</p> <ul> <li> <p>In the previous\u00a0section, it was shown that the\u00a0core problem with     FedAvg in the presence of adversarial agents was the lack of     robustness to outliers in the underlying arithmetic mean used during     aggregation.</p> </li> <li> <p>This naturally raises the question of whether this mean can be     estimated in such a manner that does offer such robustness. The     answer is the class of robust mean estimators.</p> </li> <li> <p>There are many such estimators that offer varying trade-offs between     robustness, distance from the true arithmetic mean,     and\u00a0computational efficiency.</p> </li> <li> <p>For use as a base for the implementation of the following     aggregation methods, consider the following general\u00a0aggregation     function:</p> </li> </ul> <p>def aggregate(parameter_vectors):</p> <p># Perform some form of aggregation</p> <p>return aggregated_parameter_vector</p> <p>CopyExplain</p> <ul> <li> <p>This function takes a list of parameter vectors and returns the     resulting aggregated\u00a0parameter vector.</p> </li> <li> <p>Now we will examine three example implementations of robust\u00a0mean     estimators.</p> </li> </ul> <p>Aggregation using the geometric median</p> <ul> <li> <p>The geometric\u00a0median of a sample\u00a0is the point minimizing the sum of     L1 distances\u00a0between itself and the sample. This is conceptually     similar to the arithmetic mean, which is the point minimizing the     sum of L2 distances between itself and the sample.</p> </li> <li> <p>The use of L1 distances allows for greater robustness to outliers;     in fact, an arbitrary point can only be induced in the geometric     median if at least half of the points are from adversarial agents.</p> </li> <li> <p>However, the geometric median cannot be directly computed, instead     relying on numerical approximations or iterative algorithms\u00a0to     compute.</p> </li> <li> <p>To compute the geometric mean iteratively, Weiszfeld's algorithm can     be used\u00a0as follows:</p> </li> </ul> <p>def geometric_median_aggregate(parameter_vectors, epsilon):</p> <p>vector_shape = parameter_vectors[0].shape</p> <p>vector_buffer = list(v.flatten() for v in parameter_vectors)</p> <p>prev_median = np.zeros(vector_buffer[0].shape)</p> <p>delta = np.inf</p> <p>vector_matrix = np.vstack(vector_buffer)</p> <p>while (delta &gt; epsilon):</p> <p>dists = np.sqrt(np.sum((vector_matrix - prev_median[np.newaxis, :])**2, axis=1))</p> <p>curr_median = np.sum(vector_matrix / dists[:, np.newaxis], axis=0) / np.sum(1 / dists)</p> <p>delta = np.linalg.norm(curr_median - prev_median)</p> <p>prev_median = curr_median</p> <p>return prev_median.reshape(vector_shape)</p> <p>CopyExplain</p> <ul> <li>This algorithm uses the fact that the geometric median of a set of     points is the point that minimizes the sum of Euclidean distances     over the set, performing a form of weighted least\u00a0squares     with\u00a0weights\u00a0inversely proportional to the Euclidean distance     between the point and the current median estimate at\u00a0each iteration.</li> </ul> <p>Aggregation using the coordinate-wise median</p> <ul> <li> <p>The\u00a0coordinate-wise median is\u00a0constructed by taking the\u00a0median of     each coordinate across the sample, as the name suggests.</p> </li> <li> <p>This median can be directly computed, unlike the geometric median,     and intuitively offers similar robustness to outliers due to the     properties of the median in univariate statistics.</p> </li> <li> <p>However, it is unclear whether the resulting model displays any     theoretical similarities to the arithmetic mean in regard to     performance on the dataset\u00a0and convergence.</p> </li> <li> <p>NumPy makes the implementation of this function quite simple,\u00a0as     follows:</p> </li> </ul> <p>def coordinate_median_aggregate(parameter_vectors):</p> <p>**\u00a0\u00a0\u00a0\u00a0return np.median(parameter_vectors, axis=0)**</p> <p>CopyExplain</p> <ul> <li>It is clear that the coordinate-wise median is far more     computationally efficient to compute than the geometric median,     trading off theoretical guarantees\u00a0for speed.</li> </ul> <p>Aggregation using the Krum algorithm</p> <ul> <li> <p>An alternative\u00a0approach is to isolate\u00a0outlier\u00a0points from     adversarial agents prior to aggregation. The most well-known example     of this approach is\u00a0the\u00a0Krum algorithm, where distance-based     scoring is performed prior to aggregation as a means of     locating\u00a0outlier points.</p> </li> <li> <p>Specifically, the Krum algorithm first computes the pairwise L2     distance between each point -- these distances are then used to     compute a score for each point equal to the sum of     the\u00a0n-f-2\u00a0smallest L2 distances (f\u00a0is a parameter that is set).</p> </li> <li> <p>Then, Krum outputs the received point with the lowest score,     effectively returning the point with a minimal total L2 distance     with\u00a0f\u00a0outlier points that are ignored. Alternatively, the scoring     approach used by Krum can be used to trim outlier points prior to     the computation of the arithmetic mean.</p> </li> <li> <p>In both cases, for sufficiently large\u00a0n\u00a0and\u00a02f+2 \\&lt; n,     convergence rates similar to those of FedAvg in the non-adversarial     case are achieved. More information on the Krum algorithm can be     found in the original paper, which is     located\u00a0at\u00a0https://papers.nips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html.</p> </li> <li> <p>The Krum\u00a0algorithm can be used to\u00a0perform\u00a0aggregation\u00a0as follows:</p> </li> </ul> <p>def krum_aggregate(parameter_vectors, f, use_mean=False):</p> <p>num_vectors = len(parameter_vectors)</p> <p>filtered_size = max(1, num_vectors-f-2)</p> <p>scores = np.zeros(num_vectors)</p> <p>for i in range(num_vectors):</p> <p>distances = np.zeros(num_vectors)</p> <p>for j in range(num_vectors):</p> <p>distances[j] = np.linalg.norm(parameter_vectors[i] - parameter_vectors[j])</p> <p>scores[i] = np.sum(np.sort(distances)[:filtered_size])</p> <p>if (use_mean):</p> <p>idx = np.argsort(scores)[:filtered_size]</p> <p>return np.mean(np.stack(parameter_vectors)[idx], axis=0)</p> <p>else:</p> <p>idx = np.argmin(scores)</p> <p>return parameter_vectors[idx]</p> <p>CopyExplain</p> <ul> <li>Note that a flag has been included to determine which of the two     Krum aggregation approaches (single selection versus trimmed mean)     should be used. Vectorizing the distance computation is possible,     but the iterative approach was preferred due to the expectation\u00a0of     large parameter\u00a0vectors\u00a0and smaller\u00a0agent counts.</li> </ul> <p>Non-IID datasets</p> <ul> <li> <p>The\u00a0theoretical\u00a0underpinning granted to FL by working with IID     datasets plays a significant role in allowing performant aggregate     models to be achieved through FL. At a high level, this can be     explained by the discrepancy between the learning done by models in     different datasets.</p> </li> <li> <p>No theoretical guarantees can be made for the convergence of such     models when dataset-agnostic aggregation methods are applied --     unless constraints on the non-IID nature of the datasets are     applied.</p> </li> <li> <p>The key hindering factor is the high probability of local models     moving toward non-shared optima in the parameter space, leading to     consistent drift between the local models and the aggregate model     after each local\u00a0training phase.</p> </li> <li> <p>There are methods that attempt to restrict the modifications made to     the aggregate model based on the local machine learning task,     relying on the overparameterization of deep learning models to find     relatively disjointed parameter subsets to optimize the aggregate     model of each task.</p> </li> <li> <p>One such aggregation\u00a0approach is\u00a0FedCurv, which uses the Fisher     information matrix of the previous aggregate model to act as a     regulator for auxiliary parameter modifications during local     training.</p> </li> <li> <p>However, the robustness of this approach for extreme non-IID cases     in practical applications likely needs to be tested further     to\u00a0ensure\u00a0acceptable performance.</p> </li> </ul> <p>Implementing FedCurv</p> <ul> <li> <p>The implementation\u00a0of FedCurv involves two key\u00a0modifications to the     standard FedAvg approach.</p> </li> <li> <p>First, the local loss function must be modified to include the     regularization term incorporating the aggregated Fisher information     from the previous round.</p> </li> <li> <p>Second, the Fisher information matrix of the parameters must be     calculated and aggregated correctly for use in the\u00a0next round.</p> </li> <li> <p>The local training example code, as shown in the\u00a0Implementing     FedProx\u00a0section, will be used again to demonstrate an     implementation of FedCurv.</p> </li> <li> <p>Earlier, we saw that a model conversion layer allows for     framework-agnostic model representations to be operated on by the     aggregator.</p> </li> <li> <p>Previously, these representations only contained the respective     parameters from the original models; however, this agnostic     representation actually allows for any desired parameter to be     aggregated, even those only loosely tied to the true model     parameters.</p> </li> <li> <p>This means that the secondary parameters can be bundled and sent     with the local model, aggregated, and then separated from the     aggregate model in the\u00a0next round.</p> </li> <li> <p>In FedCurv, there are two sets of parameters that must be computed     locally and aggregated for use in the next round; therefore, it can     be assumed that these parameters are sent with the local model after     training and separated from the aggregate model before training, for     the sake of brevity in the example code (the implementation of this     functionality is straightforward).</p> </li> <li> <p>As a result, the two key modifications for FedCurv, as mentioned     earlier, can be simplified down into computing the Fisher     information parameters after locally training the model and     computing the regularization term with the received aggregate     Fisher\u00a0information parameters.</p> </li> <li> <p>The Fisher information matrix refers to the covariance of the     gradient of the log-likelihood function of a model with respect to     its parameters, often empirically evaluated over the data present.</p> </li> <li> <p>FedCurv only utilizes the diagonal entries of this matrix, the     variances between the gradient parameters, and their expected     values\u00a0of zero.</p> </li> <li> <p>At a high level, this variance term can be considered an estimate of     how influential the parameter is in changing the performance of the     model on the data.</p> </li> <li> <p>This information is essential for preventing the modification of     parameters key to good performance on one dataset during the local     training of other agents -- the underlying idea\u00a0behind FedCurv.</p> </li> <li> <p>Relaxing the measure of model performance from the gradient of the     log-likelihood to the gradient of any objective function allows for     the direct use of the gradient terms computed during backpropagation     when computing the variance terms for models using gradient-based     optimization methods, such as deep learning models.</p> </li> <li> <p>Specifically, the variance term of a parameter is equal to the     square of its respective gradient term, allowing for the terms to be     directly computed from the net gradients calculated during\u00a0local     training.</p> </li> <li> <p>First, we create two variables to store the agent's most recent     Fisher information parameters and the received aggregate Fisher     information parameters, which are used to determine the Fisher     information from the other agents. The value of the lambda parameter     of FedCurv is fixed, and\u00a0total_grad\u00a0is initialized as a container     for the cumulative\u00a0gradient\u00a0from each\u00a0training loop:</p> </li> </ul> <p>agg_model = ... # Get aggregated model from aggregator</p> <p>model.load_state_dict(agg_model.state_dict())</p> <p>fisher_info_params = ... # Initialize at start, then maintain to store past round parameters</p> <p>agg_fisher_info_params = ... # Separate aggregate Fisher information parameters from aggregate model parameters</p> <p># Only consider other agents, and convert to PyTorch tensor</p> <p>agg_fisher_info_params = {k:torch.tensor(agg_fisher_info_params[k] - fisher_info_params[k]) for k in fisher_info_params.keys()}</p> <p># Scaling parameter for FedCurv regularization term</p> <p>fedcurv_lambda = 1.0</p> <p>total_grad = {i:torch.zeros_like(param.data) for i,param in enumerate(model.parameters())}</p> <p>CopyExplain</p> <ul> <li>Then, we compute the FedCurv regularization term from the model     weights and the aggregate Fisher information parameters. This term     is weighted by lambda and added to the loss term before     computing\u00a0the gradients:</li> </ul> <p>model.train()</p> <p>for epoch in range(num_epochs):</p> <p>for batch_idx, (inputs, targets) in enumerate(trainloader):</p> <p>inputs, targets = inputs.to(device), targets.to(device)</p> <p>optimizer.zero_grad()</p> <p>outputs = model(inputs)</p> <p>loss = criterion(outputs, targets)</p> <p>for i,param in enumerate(model.parameters()):</p> <p># Factor out regularization term to use saved fisher info parameters</p> <p>reg_term = (param.data ** 2) * agg_fisher_info_params[f\\'fedcurv_u_{i}\\']</p> <p>reg_term += 2 * param.data * agg_fisher_info_params[f\\'fedcurv_v_{i}\\']</p> <p>reg_term += (agg_fisher_info_params[f\\'fedcurv_v_{i}\\'] ** 2) / agg_fisher_info_params[f\\'fedcurv_u_{i}\\']</p> <p>loss += fedcurv_lambda * reg_term.sum()</p> <p>CopyExplain</p> <ul> <li>The\u00a0gradients are\u00a0then computed and stored in\u00a0total_grad\u00a0before     updating the\u00a0model weights:</li> </ul> <p>loss.backward()</p> <p>for i,param in enumerate(model.parameters()):</p> <p>total_grad[i] += param.grad</p> <p>optimizer.step()</p> <p>CopyExplain</p> <ul> <li>Finally, we compute and store the agent's most recent Fisher     information parameters for use in the\u00a0next round:</li> </ul> <p>for i,param in enumerate(model.parameters()):</p> <p>fisher_info_params[f\\'fedcurv_u_{i}\\'] = (total_grad[i] ** 2).numpy()</p> <p>fisher_info_params[f\\'fedcurv_v_{i}\\'] = ((total_grad[i] ** 2) * param.data).numpy()</p> <p>CopyExplain</p> <ul> <li>Therefore, framework-agnostic\u00a0aggregation can\u00a0be used to implement     FedCurv on top of FedAvg. The full FL example using FedCurv can be     found\u00a0at\u00a0https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7/agg_fl_examples/cifar_fedcurv_example.</li> </ul> <p>Data-sharing approach</p> <ul> <li> <p>To make further\u00a0progress, changes to external aspects of\u00a0the FL     scenario are necessary. For example, let's assume that the data     privacy restriction is loosened, such that small subsets of the     local datasets from each agent can be shared with the other agents.</p> </li> <li> <p>This data-sharing approach allows for homogeneity in the local data     distributions proportional to the amount of shared data to be     achieved, at the expense of the key stationary data property of FL     that makes it desirable in many privacy-oriented applications.</p> </li> <li> <p>Thus, data-sharing approaches are generally unsuitable for the     majority\u00a0of applications.</p> </li> </ul> <p>Personalization through fine-tuning</p> <ul> <li> <p>It is clear that\u00a0producing a single model that demonstrates strong     performance across the local datasets is not easy when the datasets     are IID. However, what would happen if the single model restriction     was removed from the FL process?</p> </li> <li> <p>If the goal is to produce local models that perform well on the same     edge devices where training is conducted, removing the single model     restriction allows for the use of different local models that have     been trained on the exact data distributions where inference     is\u00a0being applied.</p> </li> <li> <p>This concept is\u00a0called\u00a0personalization, in which agents use     versions of the aggregate model tuned for the local data     distribution to achieve strong performance. The key point of this     approach is to balance the local performance of the locally trained     model with the global performance and the resulting robustness of     the aggregate model received in each round.</p> </li> <li> <p>One method of accomplishing this is for each agent to maintain their     local models across the rounds, updating the local model with the     weighted average of the previous local model and the received     aggregate model during\u00a0each round.</p> </li> <li> <p>Alternatively, consider a relaxation that allows for multiple     aggregate models to be produced in each round. In cases where the     local data distributions can be clustered into just a few separated     groups, distribution-aware aggregation would allow for the selective     application of aggregation methods to groups of models belonging to     the same\u00a0distribution cluster.</p> </li> <li> <p>One example of this approach is the\u00a0Performance-Based Neighbor     Selection\u00a0(PENS) algorithm, where\u00a0agents receive locally     trained models from other agents and test them on their\u00a0own local     dataset during the first phase. Using the assumption that models     trained on similar datasets will perform better than models trained     on different datasets, the agents then determine the set of other     agents with similar data distributions, allowing for aggregation to     only be performed with similar agents in the\u00a0second phase.</p> </li> <li> <p>A second approach\u00a0is to\u00a0add an intermediate aggregation\u00a0step     between the local models and the global aggregate model called a     cluster model. By leveraging knowledge about the agent data     distributions or through a dynamic allocation method, agents with     similar data distributions can be assigned to a cluster aggregator,     which is then known to produce a strong model due to its agents     having\u00a0IID datasets.</p> </li> <li> <p>Balancing the performance of the cluster models with the robustness     of global aggregation leads to the concept of the semi-global model,     in which subsamples of the cluster models can be selected     (potentially based on data distribution) to create a smaller set of     partially global aggregate models that maintain performance and     robustness.</p> </li> <li> <p>Therefore, the cluster and semi-global model approach is beneficial     for both aggregation and achieving a fully distributed\u00a0FL system.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>The goal of This section was to provide a conceptual overview of the     current knowledge of aggregation, the key theoretical step in FL     that allows for the disjoint training done by each agent to be     pooled together with minimal transmission required.</p> </li> <li> <p>FedAvg is a simple, yet surprisingly powerful aggregation algorithm     that performs well in an ideal FL scenario. This scenario is     achieved when training is done across IID datasets using machines     with similar levels of computational power and no adversarial or     otherwise incorrectly\u00a0performing agents.</p> </li> <li> <p>Unfortunately, these conditions are often not met when deploying an     FL system in the real world.</p> </li> <li> <p>To address these cases, we introduced and implemented modified     aggregation approaches: FedProx, FedCurv, and three different robust     mean estimators.</p> </li> <li> <p>After reading This section, you should have a solid understanding of     the considerations that must be taken into account for practical FL     applications, and you should be able to integrate the aforementioned     algorithms into\u00a0these applications.</p> </li> <li> <p>In the next section, we will do a deep dive into some of the     existing FL frameworks with several toy examples to demonstrate the     functionalities provided\u00a0by each.</p> </li> </ul> <p>Introducing Existing Federated Learning Frameworks</p> <ul> <li> <p>The objective of This section is to introduce\u00a0existing\u00a0federated     learning\u00a0(FL) frameworks and platforms, applying each to     federated learning scenarios\u00a0involving toy\u00a0machine     learning\u00a0(ML) problems.</p> </li> <li> <p>The platforms focused on Here are Flower, TensorFlow Federated,     OpenFL, IBM FL, and STADLE -- the idea behind this selection was to     help you by covering a breadth of existing FL platforms.</p> </li> <li> <p>By the end of This section, you should have a basic understanding of     how to use each platform for FL, and you should be able to choose a     platform based on its associated strengths and weaknesses for an\u00a0FL     application.</p> </li> <li> <p>Here, we will cover the\u00a0following topics:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Introduction to existing\u00a0FL frameworks</p> <ul> <li> <p>Implementations of an example NLP FL task on movie review dataset,     using\u00a0existing frameworks</p> </li> <li> <p>Implementations of example computer vision FL task with non-IID     datasets, using\u00a0existing frameworks</p> </li> </ul> <p>Technical requirements</p> <ul> <li> <p>You can find the supplemental code files for This section in the     book's\u00a0GitHub repository:</p> </li> <li> <p>https://github.com/PacktPublishing/Federated-Learning-with-Python</p> </li> <li> <p>Each implementation example Here was run on an x64 machine     running\u00a0Ubuntu 20.04.</p> </li> <li> <p>The implementation of the training code for the NLP example requires     the following libraries\u00a0to run:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Python 3 (version \u2265 3.8)</p> <ul> <li> <p>NumPy</p> </li> <li> <p>TensorFlow (version \u2265\u00a02.9.1)</p> </li> <li> <p>TensorFlow Hub (pip\u00a0install tensorflow-hub)</p> </li> <li> <p>TensorFlow Datasets (pip\u00a0install tensorflow-datasets)</p> </li> <li> <p>TensorFlow Text (pip\u00a0install tensorflow-text)</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Using a GPU with the appropriate TensorFlow installation is     recommended to save training time for the NLP example, due to the     size of\u00a0the model.</p> <ul> <li>The implementation of the training code for     the\u00a0non-IID\u00a0(non-independent and identical distribution)     computer vision example requires the following libraries\u00a0to run:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Python 3 (version \u2265 3.8)</p> <ul> <li> <p>NumPy</p> </li> <li> <p>PyTorch (version \u2265\u00a01.9)</p> </li> <li> <p>Torchvision (version \u2265 0.10.0, tied to\u00a0PyTorch version)</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The installation instructions for each FL framework are listed in     the\u00a0following subsections.</p> <p>TensorFlow Federated</p> <ul> <li>You can\u00a0install the following libraries to\u00a0use TFF:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   tensorflow_federated\u00a0(using the\u00a0pip     install\u00a0tensorflow_federated\u00a0command)</p> <ul> <li>nest_asyncio\u00a0(using the\u00a0pip install\u00a0nest_asyncio\u00a0command)</li> </ul> <p>OpenFL</p> <ul> <li> <p>You can install\u00a0OpenFL using\u00a0pip\u00a0install openfl.</p> </li> <li> <p>Alternatively, you can build from source with the\u00a0following     commands:</p> </li> </ul> <p>git clone https://github.com/intel/openfl.git</p> <p>cd openfl</p> <p>pip install .</p> <p>CopyExplain</p> <p>IBM FL</p> <ul> <li>Installing the\u00a0locally hosted version of IBM FL requires the wheel     installation file located in the code repository. To perform this     installation, run the\u00a0following commands:</li> </ul> <p>git clone https://github.com/IBM/federated-learning-lib.git</p> <p>cd federated-learning-lib</p> <p>pip install federated_learning_lib-*-py3-none-any.whl</p> <p>CopyExplain</p> <p>Flower</p> <p>= You can install\u00a0Flower using the\u00a0pip install\u00a0flwr\u00a0command.</p> <p>STADLE</p> <ul> <li>You can install\u00a0the STADLE client-side library using the\u00a0pip     install\u00a0stadle-client\u00a0command.</li> </ul> <p>Introduction to FL frameworks</p> <ul> <li>First, we\u00a0introduce the FL frameworks and platforms to be used in     the subsequent implementation-focused sections.</li> </ul> <p>Flower</p> <ul> <li> <p>Flower ([https://flower.dev/]{.underline})     is\u00a0an open source and ML framework-agnostic\u00a0FL framework\u00a0that aims     to be accessible to users. Flower follows a standard client-server     architecture, in which the clients are set up to receive the model     parameters from the server, train on local data, and send the new     local model parameters back to\u00a0the server.</p> </li> <li> <p>The high-level orchestration of the federated learning process is     dictated by what Flower calls strategies, used by the server for     aspects such as client selection and\u00a0parameter aggregation.</p> </li> <li> <p>Flower\u00a0uses\u00a0Remote Procedure Calls\u00a0(RPCs) in order to     perform said orchestration through client-side execution from     messages sent by the server. The extensibility of the framework     allows researchers to experiment with novel approaches such as new     aggregation algorithms and communication methods (such as\u00a0model     compression).</p> </li> </ul> <p>TensorFlow Federated (TFF)</p> <ul> <li> <p>TFF     ([https://www.tensorflow.org/federated]{.underline})     is\u00a0an open source FL/computation\u00a0framework\u00a0built on top of     TensorFlow that aims to allow researchers to easily simulate     federated learning with existing TensorFlow/Keras models and     training pipelines.</p> </li> <li> <p>It consists of the Federated Core layer, which allows for the     implementation of general federated computations, and the Federated     Learning layer, which is built on top and provides interfaces     for\u00a0FL-specific processes.</p> </li> <li> <p>TFF focuses on single-machine local simulations of FL, using     wrappers to create TFF-specific datasets, models, and federated     computations (core client and server computation performed during     the FL process) from the standard TensorFlow equivalents.</p> </li> <li> <p>The focus on building everything from general federated computations     allows researchers to implement each step as desired, allowing     experimentation to\u00a0be supported.</p> </li> </ul> <p>OpenFL</p> <ul> <li> <p>OpenFL     ([https://github.com/intel/openfl]{.underline})     is\u00a0an open source FL\u00a0framework\u00a0developed by Intel, focused on     allowing cross-silo privacy-preserving ML to be performed.</p> </li> <li> <p>OpenFL allows for two different workflows depending on the desired     lifespan of the federation (where federation refers to the entire\u00a0FL     system).</p> </li> <li> <p>In the aggregator-based workflow, a single experiment and associated     federated learning plan are sent from the aggregator to the     participating\u00a0collaborators\u00a0(agents) to be run as the local     training step of the FL process---the federation is stopped after     the experiment is complete. In the director-based workflow,     long-lived components are instead used to allow for experiments to     be run on demand.</p> </li> <li> <p>The following diagram depicts the\u00a0architecture and users for the     director-based workflow:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.473611111111111in\"}</p> <p>Figure 8.1 -- Architecture of director-based workflow (adapted from https://openfl.readthedocs.io/en/latest/source/openfl/components.html)</p> <ul> <li> <p>Director Manager\u00a0oversees the running of experiments, working     with long-lived\u00a0Envoy\u00a0components residing on the collaborator     nodes to manage the short-lived components (collaborators +     aggregator) for each experiment.</p> </li> <li> <p>In targeting the cross-silo data scenario, OpenFL applies a unique     focus on managing data shards, including cases where data     representations differ\u00a0across silos.</p> </li> </ul> <p>IBM FL</p> <ul> <li> <p>IBM FL is a\u00a0framework that also focuses on enterprise FL. It follows     a\u00a0straightforward aggregator-party design, where some number of     parties with local data collaborate with other parties by sending     incremental model training results to the aggregator and working     with the produced aggregate models (following standard client-server     FL architecture).</p> </li> <li> <p>IBM FL has official support for a number of fusion (aggregation)     algorithms and certain fairness techniques aimed at combating     bias---the details of these algorithms can be found at the     repository located at https://github.com/IBM/federated-learning-lib.</p> </li> <li> <p>One specific goal of IBM FL is to be highly extensible,     allowing\u00a0users to easily make necessary modifications if specific     features are desired. It also supports a Jupyter-Notebook-based     dashboard to aid in orchestrating\u00a0FL experiments.</p> </li> </ul> <p>STADLE</p> <ul> <li> <p>Unlike the\u00a0previous\u00a0frameworks, STADLE     ([https://stadle.ai/]{.underline}) is     an\u00a0ML-framework-agnostic FL and distributed learning SaaS platform     that aims to allow for the seamless integration of FL into     production-ready applications and ML pipelines.</p> </li> <li> <p>The goal of STADLE is to minimize the amount of FL-specific code     necessary for integration, making FL accessible to newcomers while     still providing flexibility to those looking to experiment.</p> </li> <li> <p>With the STADLE SaaS platform, users of varying technical abilities     can collaborate on FL projects at all scales.</p> </li> <li> <p>Performance tracking and model management functionalities allow     users to produce validated federated models with strong performance,     while an intuitive configuration panel allows for detailed control     over the federated learning process.</p> </li> <li> <p>STADLE uses a two-level component hierarchy that allows for multiple     aggregators to operate in parallel, scaling to match demand. The     following figure depicts the\u00a0high-level architecture:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.026388888888889in\"}</p> <p>Figure 8.2 -- STADLE multi-aggregator architecture</p> <ul> <li>Development of STADLE clients is streamlined with\u00a0pip\u00a0installation     and an easy-to-understand\u00a0configuration file, with several examples     made publicly\u00a0available for use as a reference on the different ways     STADLE can be integrated into existing\u00a0ML code.</li> </ul> <p>PySyft</p> <ul> <li> <p>While\u00a0PySyft     ([https://github.com/OpenMined/PySyft]{.underline})     implementations\u00a0are not included in this\u00a0section due to ongoing     changes in the codebase, it is still a major player in the     privacy-preserving deep learning space.</p> </li> <li> <p>The core principle behind PySyft is to allow for the ability to     perform computations over data stored on a machine without direct     access to said data ever being given.</p> </li> <li> <p>This is accomplished by adding an intermediate layer between the     user and the data location that sends computation requests to     participating worker machines, returning the computed result to the     user while maintaining the privacy of the data stored and used by     each worker to perform\u00a0the computation.</p> </li> <li> <p>This general capability directly extends itself to FL, reworking     each step of a normal deep learning training flow to be a     computation over the model parameters and data stored at each worker     (agent) participating in FL.</p> </li> <li> <p>To accomplish this, PySyft utilizes hooks that encapsulate the     standard PyTorch/TensorFlow libraries, modifying the requisite     internal functions in order to allow model training and testing to     be supported as PySyft\u00a0privacy-preserving computations.</p> </li> <li> <p>Now that the high-level ideas behind the FL frameworks have been     explained, we move to the implementation-level details for their     practical usage in two example scenarios. First, we\u00a0look at how to     modify the existing centralized training code for\u00a0an NLP model     to\u00a0use FL.</p> </li> </ul> <p>Example -- the federated training of an NLP model</p> <ul> <li> <p>The first ML\u00a0problem that will be converted into an FL scenario     through each of the aforementioned FL frameworks will be a     classification problem within the domain of NLP.</p> </li> <li> <p>At a high level, NLP refers to the intersection of computational     linguistics and ML with an overarching goal of allowing computers to     achieve some level of\u00a0understanding\u00a0from human language -- the     details of this understanding vary widely based on the specific     problem being targeted.</p> </li> <li> <p>For this example, we will be performing sentiment analysis on movie     reviews, classifying them as positive or negative. The dataset we     will be using is the SST-2 dataset     (https://nlp.stanford.edu/sentiment/), containing movie reviews in a     string format and the associated binary labels 0/1 representing     negative and positive\u00a0sentiment, respectively.</p> </li> <li> <p>The model we will use to perform binary classification is a     pretrained BERT model with a custom classification head.</p> </li> <li> <p>The BERT model allows us to encode a sentence into a     high-dimensional numerical vector, which can then be passed to the     classification head to output the binary label prediction; more     information on the BERT model can be found at     https://huggingface.co/blog/bert-101.</p> </li> <li> <p>We choose to use a pretrained model that has already learned how to     produce general encodings for sentences after a significant amount     of training, as opposed to performing said training from scratch.     This allows us to focus training on the classification head to     fine-tune the model on the SST-2 dataset, saving time     while\u00a0maintaining performance.</p> </li> <li> <p>We will now go through the local (centralized) training code that     will be used as a base when showing how to use each of the FL     frameworks, starting with the Keras model definition\u00a0and\u00a0dataset     loader.</p> </li> </ul> <p>Defining the sentiment analysis model</p> <ul> <li> <p>The\u00a0SSTModel\u00a0object\u00a0defined\u00a0in the\u00a0sst_model.py\u00a0file is the Keras     model we will be using for this example.</p> </li> <li> <p>First, we import the\u00a0requisite libraries:</p> </li> </ul> <p>import tensorflow as tf</p> <p>from tensorflow import keras</p> <p>from keras import layers</p> <p>import tensorflow_text</p> <p>import tensorflow_hub as hub</p> <p>import tensorflow_datasets as tfds</p> <p>CopyExplain</p> <ul> <li> <p>TensorFlow Hub is used to easily download the pretrained BERT     weights into a Keras layer. TensorFlow Text is used when loading in     the BERT weights from TensorFlow Hub. TensorFlow Datasets will allow     us to download and cache the\u00a0SST-2 dataset.</p> </li> <li> <p>Next, we define the model and initialize the model\u00a0layer objects:</p> </li> </ul> <p>class SSTModel(keras.Model):</p> <p>def __init__(self):</p> <p>super(SSTModel, self).__init__()</p> <p>self.preprocessor = hub.KerasLayer(\\\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\\\")</p> <p>self.small_bert = hub.KerasLayer(\\\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\\\")</p> <p>self.small_bert.trainable = False</p> <p>self.fc1 = layers.Dense(512, activation=\\'relu\\')</p> <p>self.fc2 = layers.Dense(64, activation=\\'relu\\')</p> <p>self.fc3 = layers.Dense(1, activation=\\'sigmoid\\')</p> <p>CopyExplain</p> <ul> <li> <p>The\u00a0preprocessor\u00a0object takes the raw sentence input batches and     converts them into the format used by the BERT model.</p> </li> <li> <p>We load the preprocessor and BERT layers from TensorFlow Hub, then     initialize the dense layers that make up the classification head. We     use the sigmoid activation function at the end to squash the output     into the interval (0,1), allowing for comparison with the\u00a0true     labels.</p> </li> <li> <p>We can then\u00a0define the forward pass of\u00a0the model:</p> </li> </ul> <p>def call(self, inputs):</p> <p>input_dict = self.preprocessor(inputs)</p> <p>bert_output = self.small_bert(input_dict)[\\'pooled_output\\']</p> <p>output = self.fc1(keras.activations.relu(bert_output, alpha=0.2))</p> <p>scores = self.fc3(self.fc2(output))</p> <p>return scores</p> <p>CopyExplain</p> <ul> <li>We apply leaky ReLU to the BERT output to add non-linearity before     passing the output to the\u00a0classification\u00a0head layers.</li> </ul> <p>Creating the data loader</p> <ul> <li>We also\u00a0implement a function\u00a0to load in the SST-2 dataset using the     TensorFlow Datasets library. First, the training data is loaded and     converted into a NumPy array for use\u00a0during training:</li> </ul> <p>def load_sst_data(client_idx=None, num_clients=1):</p> <p>x_train = []</p> <p>y_train = []</p> <p>for d in tfds.load(name=\\\"glue/sst2\\\", split=\\\"train\\\"):</p> <p>x_train.append(d[\\'sentence\\'].numpy())</p> <p>y_train.append(d[\\'label\\'].numpy())</p> <p>x_train = np.array(x_train)</p> <p>y_train = np.array(y_train)</p> <p>CopyExplain</p> <ul> <li>We load the test data in a\u00a0similar manner:</li> </ul> <p>x_test = []</p> <p>y_test = []</p> <p>for d in tfds.load(name=\\\"glue/sst2\\\", split=\\\"validation\\\"):</p> <p>x_test.append(d[\\'sentence\\'].numpy())</p> <p>y_test.append(d[\\'label\\'].numpy())</p> <p>x_test = np.array(x_test)</p> <p>y_test = np.array(y_test)</p> <p>CopyExplain</p> <ul> <li>If\u00a0client_idx\u00a0and\u00a0num_clients\u00a0are specified, we return the     respective partition of the training\u00a0dataset -- this will be\u00a0used     for\u00a0performing FL:</li> </ul> <p>if (client_idx is not None):</p> <p>shard_size = int(x_train.size / num_clients)</p> <p>x_train = x_train[client_idx*shard_size:(client_idx+1)*shard_size]</p> <p>y_train = x_train[client_idx*shard_size:(client_idx+1)*shard_size]</p> <p>return (x_train, y_train), (x_test, y_test)</p> <p>CopyExplain</p> <ul> <li>Next, we examine the code to perform local training,     located\u00a0in\u00a0local_training.py.</li> </ul> <p>Training the model</p> <ul> <li>We first\u00a0import\u00a0the\u00a0requisite libraries:</li> </ul> <p>import tensorflow as tf</p> <p>from tensorflow import keras</p> <p>from sst_model import SSTModel, load_sst_data</p> <p>CopyExplain</p> <ul> <li>We can then use the previously defined dataset loader (without     splitting) to load in the train and\u00a0test splits:</li> </ul> <p>(x_train,y_train), (x_test,y_test) = load_sst_data()</p> <p>CopyExplain</p> <ul> <li>We can now compile the model and\u00a0begin training:</li> </ul> <p>model.compile(</p> <p>optimizer = keras.optimizers.Adam(learning_rate=0.0005, amsgrad=False),</p> <p>loss = keras.losses.BinaryCrossentropy(),</p> <p>metrics = [keras.metrics.BinaryAccuracy()]</p> <p>)</p> <p>model.fit(x_train, y_train, batch_size=64, epochs=3)</p> <p>CopyExplain</p> <ul> <li>Finally, we evaluate the model on the\u00a0test split:</li> </ul> <p>_, acc = model.evaluate(x_test, y_test, batch_size=64)</p> <p>print(f\\\"Accuracy of model on test set: {(100*acc):.2f}%\\\")</p> <p>CopyExplain</p> <ul> <li> <p>The model should reach around 82% test accuracy after three     epochs\u00a0of training.</p> </li> <li> <p>Now that we have gone through the local training code, we can     examine how the code can be modified to use FL with each of the     aforementioned\u00a0FL frameworks.</p> </li> </ul> <p>Adopting an FL training approach</p> <ul> <li> <p>To demonstrate\u00a0how FL can be\u00a0applied to the SST model training     scenario, we have to first split the original SST-2 dataset into     disjoint subsets representing the local datasets in an FL     application. To keep things simple, we will examine the case of     three agents each training on separate thirds of the dataset.</p> </li> <li> <p>For now, these subsets are randomly sampled without replacement from     the dataset -- in the next section,\u00a0Federated training of an image     classification model on non-IID data, we examine the case where the     local datasets are created from a biased sampling of the original     dataset. Instead of locally training for three epochs, we will     perform three rounds of FL with each local training phase training     for one epoch on the local data.</p> </li> <li> <p>FedAvg will be used to aggregate the locally trained models at the     end of each round. After these three rounds, the aforementioned     validation metrics will be computed using the final aggregate model,     allowing for comparisons to be drawn between the local training     cases and the\u00a0FL case.</p> </li> </ul> <p>Integrating TensorFlow Federated for SST-2</p> <ul> <li> <p>As previously\u00a0mentioned, the\u00a0TensorFlow Federated\u00a0(TFF)     framework was built on top of the TensorFlow\u00a0and Keras deep     learning\u00a0libraries. The model implementation was done using Keras;     as a result, the integration of TFF into the local training code     is\u00a0relatively straightforward.</p> </li> <li> <p>The first step is to add the TFF-specific imports and FL-specific     parameters prior to loading\u00a0the dataset:</p> </li> </ul> <p>import nest_asyncio</p> <p>nest_asyncio.apply()</p> <p>import tensorflow_federated as tff</p> <p>NUM_CLIENTS = 3</p> <p>NUM_ROUNDS = 3</p> <p>CopyExplain</p> <ul> <li>TFF allows us to simulate some number of agents by passing the     appropriate number of datasets (local datasets) to the FL process.     To split the SST-2 dataset into thirds after preprocessing, we can     use the\u00a0following code:</li> </ul> <p>client_datasets = [load_sst_data(idx, NUM_CLIENTS)[0] for idx in range(NUM_CLIENTS)]</p> <p>CopyExplain</p> <ul> <li>Next, we have to wrap the Keras model using a TFF API function to     easily create the respective\u00a0tff.learning.Model\u00a0object. We create a     function that initializes the SST model and passes it along with the     input spec (information on the size of each data element) to this     API function, returning the result -- TFF will use this function     internally to create\u00a0the\u00a0model\u00a0during the\u00a0FL process:</li> </ul> <p>def sst_model_fn():</p> <p>sst_model = SSTModel()</p> <p>sst_model.build(input_shape=(None,64))</p> <p>return tff.learning.from_keras_model(</p> <p>sst_model,</p> <p>input_spec=tf.TensorSpec(shape=(None), dtype=tf.string),</p> <p>loss=keras.metrics.BinaryCrossentropy()</p> <p>)</p> <p>CopyExplain</p> <ul> <li>The TFF FedAvg process can then be created, using     the\u00a0sst_model_fn\u00a0function along with the optimizers used to update     the local models and the aggregate model. Using a learning rate of     1.0 for the server optimizer function allows for the new aggregate     model to replace the old one at the end of each round (as opposed to     computing a weighted average of the old and\u00a0new models):</li> </ul> <p>fed_avg_process = tff.learning.algorithms.build_unweighted_fed_avg(</p> <p>model_fn = sst_model_fn,</p> <p>client_optimizer_fn = lambda: keras.optimizers.Adam(learning_rate=0.001),</p> <p>server_optimizer_fn = lambda: keras.optimizers.SGD(learning_rate=1.0)</p> <p>)</p> <p>CopyExplain</p> <ul> <li>Finally, we initialize and run the federated learning process for 10     rounds. Each\u00a0fed_avg_process.next()\u00a0call simulates one round by     performing local training with three models on the client datasets     followed by aggregation using FedAvg. The resulting state after the     first round is passed to the next call as the starting FL state     for\u00a0the round:</li> </ul> <p>state = fed_avg_process.initialize()</p> <p>for round in range(NUM_ROUNDS):</p> <p>state = fed_avg_process.next(state, client_datasets).state</p> <p>CopyExplain</p> <ul> <li>After the FL process is completed, we convert the final     aggregate\u00a0tff.learning.Model\u00a0object back into the original Keras     model format in order to compute\u00a0the\u00a0validation\u00a0metrics:</li> </ul> <p>fed_weights = fed_avg_process.get_model_weights(state)</p> <p>fed_sst_model = SSTModel()</p> <p>fed_sst_model.build(input_shape=(None, 64))</p> <p>fed_sst_model.compile(</p> <p>optimizer = keras.optimizers.Adam(learning_rate=0.005, amsgrad=False),</p> <p>loss = keras.losses.BinaryCrossentropy(),</p> <p>metrics = [keras.metrics.BinaryAccuracy()]</p> <p>)</p> <p>fed_weights.assign_weights_to(fed_sst_model)</p> <p>_, (x_test, y_test) = load_sst_data()</p> <p>_, acc = fed_sst_model.evaluate(x_test, y_test, batch_size=64)</p> <p>print(f\\\"Accuracy of federated model on test set: {(100*acc):.2f}%\\\")</p> <p>CopyExplain</p> <ul> <li> <p>The final accuracy of the aggregate model should be\u00a0around 82%.</p> </li> <li> <p>From this, it\u00a0should be clear that the TFF FedAvg results are nearly     identical to those of the local\u00a0training scenario.</p> </li> </ul> <p>Integrating OpenFL for SST-2</p> <ul> <li> <p>Recall that\u00a0OpenFL supports two different\u00a0workflows: the     aggregator-based workflow and the director-based workflow. This     example will use the director-based workflow, involving long-living     components that can conduct FL task requests as they come in.\u00a0This     was chosen due to the desirability of having a persistent FL setup     for deploying multiple projects; however, both workflows conduct the     same core FL process and thus demonstrate\u00a0similar performance.</p> </li> <li> <p>To help with model serialization in this case, we only aggregate the     classification head weights, reconstructing the full model at     runtime during training and validation (TensorFlow Hub caches the     downloaded layers, so the download process only occurs once).\u00a0We     include the following functions in\u00a0sst_model.py\u00a0to aid with\u00a0this     modification:</p> </li> </ul> <p>def get_sst_full(preprocessor, bert, classification_head):</p> <p>sst_input = keras.Input(shape=(), batch_size=64, dtype=tf.string)</p> <p>scores = classification_head(bert(preprocessor(sst_input))[\\'pooled_output\\'])</p> <p>return keras.Model(inputs=sst_input, outputs=scores, name=\\'sst_model\\')</p> <p>def get_classification_head():</p> <p>classification_head = keras.Sequential([</p> <p>layers.Dense(512, activation=\\'relu\\', input_shape=(768,)),</p> <p>layers.Dense(64, activation=\\'relu\\', input_shape=(512,)),</p> <p>layers.Dense(1, activation=\\'sigmoid\\', input_shape=(64,))</p> <p>])</p> <p>return classification_head</p> <p>CopyExplain</p> <ul> <li> <p>Because OpenFL focuses on addressing the data silo case, the     creation of the local datasets from the SST-2 data is slightly more     involved than the TFF case. The objects needed to create the dataset     will be implemented in a separate file\u00a0named\u00a0sst_fl_dataset.py.</p> </li> <li> <p>First, we include the necessary imports. The two OpenFL-specific     objects we import are the\u00a0ShardDescriptor\u00a0object, which handles the     dataset loading and sharding, and the\u00a0DataInterface\u00a0object,     which\u00a0handles\u00a0access to\u00a0the datasets:</p> </li> </ul> <p>from openfl.interface.interactive_api.shard_descriptor import ShardDescriptor</p> <p>from openfl.interface.interactive_api.experiment import DataInterface</p> <p>import tensorflow as tf</p> <p>from sst_model import load_sst_data</p> <p>CopyExplain</p> <p>Implementing ShardDescriptor</p> <ul> <li>We first\u00a0implement the\u00a0SSTShardDescriptor\u00a0class. When\u00a0this shard     descriptor is created, we save the\u00a0rank\u00a0(client number)     and\u00a0worldsize\u00a0(total number of clients) values, then load the     training and\u00a0validation datasets:</li> </ul> <p>class SSTShardDescriptor(ShardDescriptor):</p> <p>def __init__(</p> <p>self,</p> <p>rank_worldsize: str = \\'1, 1\\',</p> <p>**kwargs</p> <p>):</p> <p>self.rank, self.worldsize = tuple(int(num) for num in rank_worldsize.split(\\',\\'))</p> <p>(x_train,y_train), (x_test,y_test) = load_sst_data(self.rank-1, self.worldsize)</p> <p>self.data_by_type = {</p> <p>\\'train\\': tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(64),</p> <p>\\'val\\': tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)</p> <p>}</p> <p>CopyExplain</p> <ul> <li>We implement the\u00a0ShardDescriptor\u00a0class functions to get the     available dataset types (training\u00a0and validation in this case) and     the respective dataset/shard based on the rank of\u00a0the client:</li> </ul> <p>def get_shard_dataset_types(self):</p> <p>return list(self.data_by_type)</p> <p>def get_dataset(self, dataset_type=\\'train\\'):</p> <p>if dataset_type not in self.data_by_type:</p> <p>raise Exception(f\\'Wrong dataset type: {dataset_type}\\')</p> <p>return self.data_by_type[dataset_type]</p> <p>CopyExplain</p> <ul> <li>We also specify the properties of the specific dataset being used.     Note that the sample shape is set to\u00a01. The preprocessor layer of     the\u00a0SSTModel\u00a0allows us to pass in strings as input, which are     treated as input vectors of type\u00a0tf.string\u00a0and\u00a0length\u00a01:</li> </ul> <p>@property</p> <p>def sample_shape(self):</p> <p>return [\\\"1\\\"]</p> <p>@property</p> <p>def target_shape(self):</p> <p>return [\\\"1\\\"]</p> <p>@property</p> <p>def dataset_description(self) -&gt; str:</p> <p>return (f\\'SST dataset, shard number {self.rank}\\'</p> <p>f\\' out of {self.worldsize}\\')</p> <p>CopyExplain</p> <ul> <li>With\u00a0this, the\u00a0SSTShardDescriptor\u00a0implementation\u00a0is completed.</li> </ul> <p>Implementing DataInterface</p> <ul> <li>Next, we\u00a0implement the\u00a0SSTFedDataset\u00a0class\u00a0as a subclass     of\u00a0DataInterface. This is done by implementing the shard descriptor     getter and setter methods, with the setter method preparing the data     to be provided to the training/validation\u00a0FL tasks:</li> </ul> <p>class SSTFedDataset(DataInterface):</p> <p>def __init__(self, **kwargs):</p> <p>super().__init__(**kwargs)</p> <p>@property</p> <p>def shard_descriptor(self):</p> <p>return self._shard_descriptor</p> <p>@shard_descriptor.setter</p> <p>def shard_descriptor(self, shard_descriptor):</p> <p>self._shard_descriptor = shard_descriptor</p> <p>self.train_set = shard_descriptor.get_dataset(\\'train\\')</p> <p>self.valid_set = shard_descriptor.get_dataset(\\'val\\')</p> <p>CopyExplain</p> <ul> <li>We also implement\u00a0the API functions to grant dataset access and     dataset size information (used\u00a0during aggregation):</li> </ul> <p>def get_train_loader(self):</p> <p>return self.train_set</p> <p>def get_valid_loader(self):</p> <p>return self.valid_set</p> <p>def get_train_data_size(self):</p> <p>return len(self.train_set) * 64</p> <p>def get_valid_data_size(self):</p> <p>return len(self.valid_set) * 64</p> <p>CopyExplain</p> <ul> <li>With this, the\u00a0local SST-2 datasets can be constructed\u00a0and used.</li> </ul> <p>Creating FLExperiment</p> <ul> <li>We now focus on\u00a0the actual implementation\u00a0of the FL process within a     new file,\u00a0fl_sim.py. First, we import the necessary libraries --     from OpenFL, we import\u00a0the following:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   TaskInterface: Allows us to define our FL training and validation     tasks for the model; the registered tasks are what the director     instructs each envoy\u00a0to conduct</p> <ul> <li> <p>ModelInterface: Allows us to convert our Keras model into the format     used by OpenFL in the\u00a0registered tasks</p> </li> <li> <p>Federation: Manages information relating to the connection with\u00a0the     director</p> </li> <li> <p>FLExperiment: Uses the\u00a0TaskInterface,\u00a0ModelInterface,     and\u00a0Federation\u00a0objects to conduct the\u00a0FL process</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The requisite\u00a0imports are done\u00a0as follows:</p> <p>import tensorflow as tf</p> <p>from tensorflow import keras</p> <p>import tensorflow_hub as hub</p> <p>from openfl.interface.interactive_api.experiment import TaskInterface</p> <p>from openfl.interface.interactive_api.experiment import ModelInterface</p> <p>from openfl.interface.interactive_api.experiment import FLExperiment</p> <p>from openfl.interface.interactive_api.federation import Federation</p> <p>from sst_model import get_classification_head, get_sst_full</p> <p>from sst_fl_dataset import SSTFedDataset</p> <p>CopyExplain</p> <ul> <li>Next, we create the\u00a0Federation\u00a0object using the     default\u00a0director\u00a0connection information:</li> </ul> <p>client_id = \\'api\\'</p> <p>director_node_fqdn = \\'localhost\\'</p> <p>director_port = 50051</p> <p>federation = Federation(</p> <p>client_id=client_id,</p> <p>director_node_fqdn=director_node_fqdn,</p> <p>director_port=director_port,</p> <p>tls=False</p> <p>)</p> <p>CopyExplain</p> <ul> <li>We then initialize\u00a0the model with the associated optimizer and loss     function -- these objects are used by the OpenFL\u00a0KerasAdapter\u00a0to     create the\u00a0ModelInterface\u00a0object. We call the model on a dummy Keras     input in order to initialize all of the weights before passing the     model\u00a0to\u00a0ModelInterface:</li> </ul> <p>classification_head = get_classification_head()</p> <p>optimizer = keras.optimizers.Adam(learning_rate=0.005, amsgrad=False)</p> <p>loss = keras.losses.BinaryCrossentropy()</p> <p>framework_adapter = \\'openfl.plugins.frameworks_adapters.keras_adapter.FrameworkAdapterPlugin\\'</p> <p>MI = ModelInterface(model=classification_head, optimizer=optimizer, framework_plugin=framework_adapter)</p> <p>CopyExplain</p> <ul> <li>Next, we\u00a0create a\u00a0TaskInterface\u00a0object and use it to register the     training task. Note that including the optimizer in the decorator     function of a task will result in the training dataset being passed     to the task; otherwise, the validation dataset will be passed to\u00a0the     task:</li> </ul> <p>TI = TaskInterface()</p> <p>\\@TI.register_fl_task(model=\\'model\\', data_loader=\\'train_data\\', device=\\'device\\', optimizer=\\'optimizer\\')</p> <p>def train(model, train_data, optimizer, device):</p> <p>preprocessor = hub.KerasLayer(\\\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\\\")</p> <p>small_bert = hub.KerasLayer(\\\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\\\")</p> <p>small_bert.trainable = False</p> <p>full_model = get_sst_full(preprocessor, small_bert, model)</p> <p>full_model.compile(loss=loss, optimizer=optimizer)</p> <p>history = full_model.fit(train_data, epochs=1)</p> <p>return {\\'train_loss\\':history.history[\\'loss\\'][0]}</p> <p>CopyExplain</p> <ul> <li>Similarly, we register the validation task using     the\u00a0TaskInterface\u00a0object. Note that we can collect the metrics     generated\u00a0by\u00a0the\u00a0evaluate\u00a0function and return the values as a means     of\u00a0tracking performance:</li> </ul> <p>\\@TI.register_fl_task(model=\\'model\\', data_loader=\\'val_data\\', device=\\'device\\')</p> <p>def validate(model, val_data, device):</p> <p>preprocessor = hub.KerasLayer(\\\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\\\")</p> <p>small_bert = hub.KerasLayer(\\\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\\\")</p> <p>small_bert.trainable = False</p> <p>full_model = get_sst_full(preprocessor, small_bert, model)</p> <p>full_model.compile(loss=loss, optimizer=optimizer)</p> <p>loss, acc = full_model.evaluate(val_data, batch_size=64)</p> <p>return {\\'val_acc\\':acc, \\'val_loss\\':loss,}</p> <p>CopyExplain</p> <ul> <li>We can now load in the dataset using the\u00a0SSTFedDataset\u00a0class     implemented earlier and create and start\u00a0a new\u00a0FLExperiment\u00a0using     the created\u00a0ModelInterface,\u00a0TaskInterface,     and\u00a0SSTFedDatasets\u00a0objects:</li> </ul> <p>fed_dataset = SSTFedDataset()</p> <p>fl_experiment = FLExperiment(federation=federation, experiment_name=\\'sst_experiment\\')</p> <p>fl_experiment.start(</p> <p>model_provider=MI,</p> <p>task_keeper=TI,</p> <p>data_loader=fed_dataset,</p> <p>rounds_to_train=3,</p> <p>opt_treatment=\\'CONTINUE_LOCAL\\'</p> <p>)</p> <p>CopyExplain</p> <p>Defining the configuration files</p> <ul> <li>The last step is to create the configuration files used     by\u00a0director\u00a0and\u00a0envoys\u00a0in order to actually load the data and start     the FL process. First, we create\u00a0director_config\u00a0containing     the\u00a0following information:</li> </ul> <p>settings:</p> <p>listen_host: localhost</p> <p>listen_port: 50051</p> <p>sample_shape: [\\\"1\\\"]</p> <p>target_shape: [\\\"1\\\"]</p> <p>CopyExplain</p> <ul> <li> <p>This is saved\u00a0in\u00a0director/director_config.yaml.</p> </li> <li> <p>We then create the three\u00a0envoy\u00a0configuration files. The first file     (envoy_config_1.yaml) contains\u00a0the following:</p> </li> </ul> <p>params:</p> <p>cuda_devices: []</p> <p>optional_plugin_components: {}</p> <p>shard_descriptor:</p> <p>template: sst_fl_dataset.SSTShardDescriptor</p> <p>params:</p> <p>rank_worldsize: 1, 3</p> <p>CopyExplain</p> <ul> <li>The second and third\u00a0envoy\u00a0config files are the same, except with     the values\u00a0rank_worldsize: 2, 3\u00a0and\u00a0rank_worldsize: 3, 3,     respectively. These config files, alongside all of the code files,     are stored in the experiment directory. The directory structure     should look like\u00a0the following:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   -director</p> <pre><code>-   director_config.yaml\n</code></pre> <ul> <li> <p>experiment</p> <ul> <li> <p>envoy_config_1.yaml</p> </li> <li> <p>envoy_config_2.yaml</p> </li> <li> <p>envoy_config_3.yaml</p> </li> <li> <p>sst_fl_dataset.py</p> </li> <li> <p>sst_model.py</p> </li> <li> <p>fl_sim.py (file with\u00a0FLExperiment creation)</p> </li> </ul> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   With everything set up, we can now perform FL\u00a0with OpenFL.</p> <ul> <li> <p>Running the OpenFL example</p> </li> <li> <p>First, start the\u00a0director by running the following command from     within the\u00a0director\u00a0folder (make sure OpenFL is installed in     the\u00a0working environment):</p> </li> </ul> <p>fx director start --disable-tls -c director_config.yaml</p> <p>CopyExplain</p> <ul> <li>Next, run the following commands in separate terminals from     the\u00a0experiment directory:</li> </ul> <p>fx envoy start -n envoy_1 ---disable-tls --envoy-config-path envoy_config_1.yaml -dh localhost -dp 50051</p> <p>fx envoy start -n envoy_2 ---disable-tls --envoy-config-path envoy_config_2.yaml -dh localhost -dp 50051</p> <p>fx envoy start -n envoy_3 ---disable-tls --envoy-config-path envoy_config_3.yaml -dh localhost -dp 50051</p> <p>CopyExplain</p> <ul> <li>Finally, start\u00a0FLExperiment\u00a0by running the\u00a0fl_sim.py\u00a0script. After     the three rounds are\u00a0completed, the aggregate model should achieve a     validation accuracy of around 82%. Once again, the performance is     nearly identical to the local\u00a0training scenario.</li> </ul> <p>Integrating IBM FL for SST-2</p> <ul> <li>IBM FL uses a\u00a0saved version\u00a0of the model when performing FL. The     following code (create_saved_model.py) initializes a model (calling     the model on a dummy input to initialize the parameters) and then     saves the model in the Keras\u00a0SavedModel\u00a0format for IBM FL\u00a0to use:</li> </ul> <p>import tensorflow as tf</p> <p>from tensorflow import keras</p> <p>from sst_model import SSTModel</p> <p>sst_model = SSTModel()</p> <p>optimizer = keras.optimizers.Adam(learning_rate=0.005, amsgrad=False)</p> <p>loss = keras.losses.BinaryCrossentropy(),</p> <p>sst_model.compile(loss=loss, optimizer=optimizer)</p> <p>sst_input = keras.Input(shape=(), dtype=tf.string)</p> <p>sst_model(sst_input)</p> <p>sst_model.save(\\'sst_model_save_dir\\')</p> <p>CopyExplain</p> <ul> <li>Run this once to save the model into the folder     named\u00a0sst_model_save_dir\u00a0-- we will point\u00a0IBM FL\u00a0to load in the     model saved in\u00a0this directory.</li> </ul> <p>Creating DataHandler</p> <ul> <li>Next, we create a\u00a0subclass of the IBM FL\u00a0DataHandler\u00a0class\u00a0in charge     of providing the training and validation data to the model -- this     subclass will load, preprocess, and store the SST datasets as class     attributes. We first import the\u00a0necessary libraries:</li> </ul> <p>from ibmfl.data.data_handler import DataHandler</p> <p>import tensorflow as tf</p> <p>from sst_model import load_sst_data</p> <p>CopyExplain</p> <ul> <li>The\u00a0init\u00a0function of this class loads the data info parameters,     which are then used to load the correct SST-2\u00a0data partition:</li> </ul> <p>class SSTDataHandler(DataHandler):</p> <p>def __init__(self, data_config=None):</p> <p>super().__init__()</p> <p>if (data_config is not None):</p> <p>if (\\'client_id\\' in data_config):</p> <p>self.client_id = int(data_config[\\'client_id\\'])</p> <p>if (\\'num_clients\\' in data_config):</p> <p>self.num_clients = int(data_config[\\'num_clients\\'])</p> <p>train_data, val_data = load_sst_data(self.client_id-1, self.num_clients)</p> <p>self.train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(64)</p> <p>self.val_dataset = tf.data.Dataset.from_tensor_slices(val_data).batch(64)</p> <p>CopyExplain</p> <ul> <li>We also\u00a0implement the API function that returns the\u00a0loaded datasets     for use\u00a0during training/validation:</li> </ul> <p>def get_data(self):</p> <p>return self.train_dataset, self.val_dataset</p> <p>CopyExplain</p> <p>Defining the configuration files</p> <ul> <li>The next step\u00a0is to create the configuration JSON files used when     starting the aggregator and initializing the parties.\u00a0The     aggregation config first specifies the connection information it     will use to communicate with\u00a0the parties:</li> </ul> <p>{</p> <p>\\\"connection\\\": {</p> <p>\\\"info\\\": {</p> <p>\\\"ip\\\": \\\"127.0.0.1\\\",</p> <p>\\\"port\\\": 5000,</p> <p>\\\"tls_config\\\": {</p> <p>\\\"enable\\\": \\\"false\\\"</p> <p>}</p> <p>},</p> <p>\\\"name\\\": \\\"FlaskConnection\\\",</p> <p>\\\"path\\\": \\\"ibmfl.connection.flask_connection\\\",</p> <p>\\\"sync\\\": \\\"False\\\"</p> <p>},</p> <p>CopyExplain</p> <ul> <li>Next, we specify the\u00a0fusion handler used\u00a0for aggregation:</li> </ul> <p>\\\"fusion\\\": {</p> <p>\\\"name\\\": \\\"IterAvgFusionHandler\\\",</p> <p>\\\"path\\\": \\\"ibmfl.aggregator.fusion.iter_avg_fusion_handler\\\"</p> <p>},</p> <p>CopyExplain</p> <ul> <li>We also specify the hyperparameters related to both local training     and aggregation.\u00a0perc_quorum\u00a0refers to the percentage of parties     that must participate before aggregation\u00a0can begin:</li> </ul> <p>\\\"hyperparams\\\": {</p> <p>\\\"global\\\": {</p> <p>\\\"max_timeout\\\": 10800,</p> <p>\\\"num_parties\\\": 1,</p> <p>\\\"perc_quorum\\\": 1,</p> <p>\\\"rounds\\\": 3</p> <p>},</p> <p>\\\"local\\\": {</p> <p>\\\"optimizer\\\": {</p> <p>\\\"lr\\\": 0.0005</p> <p>},</p> <p>\\\"training\\\": {</p> <p>\\\"epochs\\\": 1</p> <p>}</p> <p>}</p> <p>},</p> <p>CopyExplain</p> <ul> <li>Finally, we specify the IBM FL protocol handler\u00a0to use:</li> </ul> <p>\\\"protocol_handler\\\": {</p> <p>\\\"name\\\": \\\"ProtoHandler\\\",</p> <p>\\\"path\\\": \\\"ibmfl.aggregator.protohandler.proto_handler\\\"</p> <p>}</p> <p>}</p> <p>CopyExplain</p> <ul> <li>This configuration is saved\u00a0in\u00a0agg_config.json.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   We also create\u00a0the base party configuration file used to conduct FL     with the local data. We first specify the connection information of     the aggregator and\u00a0the party:</p> <p>{</p> <p>\\\"aggregator\\\":</p> <p>{</p> <p>\\\"ip\\\": \\\"127.0.0.1\\\",</p> <p>\\\"port\\\": 5000</p> <p>},</p> <p>\\\"connection\\\": {</p> <p>\\\"info\\\": {</p> <p>\\\"ip\\\": \\\"127.0.0.1\\\",</p> <p>\\\"port\\\": 8085,</p> <p>\\\"id\\\": \\\"party\\\",</p> <p>\\\"tls_config\\\": {</p> <p>\\\"enable\\\": \\\"false\\\"</p> <p>}</p> <p>},</p> <p>\\\"name\\\": \\\"FlaskConnection\\\",</p> <p>\\\"path\\\": \\\"ibmfl.connection.flask_connection\\\",</p> <p>\\\"sync\\\": \\\"false\\\"</p> <p>},</p> <p>CopyExplain</p> <ul> <li>We then specify the data handler and the local training handler to     use -- this component trains the SST model\u00a0using the model     information and the\u00a0local data:</li> </ul> <p>\\\"data\\\": {</p> <p>\\\"info\\\": {</p> <p>\\\"client_id\\\": 0,</p> <p>\\\"num_clients\\\": 3</p> <p>},</p> <p>\\\"name\\\": \\\"SSTDataHandler\\\",</p> <p>\\\"path\\\": \\\"sst_data_handler\\\"</p> <p>},</p> <p>\\\"local_training\\\": {</p> <p>\\\"name\\\": \\\"LocalTrainingHandler\\\",</p> <p>\\\"path\\\": \\\"ibmfl.party.training.local_training_handler\\\"</p> <p>},</p> <p>CopyExplain</p> <ul> <li>The model format and information is then specified -- this is where     we point to the saved model\u00a0created earlier:</li> </ul> <p>\\\"model\\\": {</p> <p>\\\"name\\\": \\\"TensorFlowFLModel\\\",</p> <p>\\\"path\\\": \\\"ibmfl.model.tensorflow_fl_model\\\",</p> <p>\\\"spec\\\": {</p> <p>\\\"model-name\\\": \\\"sst_model\\\",</p> <p>\\\"model_definition\\\": \\\"sst_model_save_dir\\\"</p> <p>}</p> <p>},</p> <p>CopyExplain</p> <ul> <li>Finally, we specify\u00a0the\u00a0protocol handler:</li> </ul> <p>\\\"protocol_handler\\\": {</p> <p>\\\"name\\\": \\\"PartyProtocolHandler\\\",</p> <p>\\\"path\\\": \\\"ibmfl.party.party_protocol_handler\\\"</p> <p>}</p> <p>}</p> <p>CopyExplain</p> <p>Creating IBM FL party</p> <ul> <li>With this, all that\u00a0is left is the code that starts each party,     saved in\u00a0fl_sim.py. We first import the\u00a0necessary libraries:</li> </ul> <p>import argparse</p> <p>import json</p> <p>from ibmfl.party.party import Party</p> <p>CopyExplain</p> <ul> <li>We include an\u00a0argparse\u00a0argument that allows for the party number to     be specified -- this is used to modify the base party configuration     file in order to allow for distinct parties to be started from     the\u00a0same file:</li> </ul> <p>parser = argparse.ArgumentParser()</p> <p>parser.add_argument(\\\"party_id\\\", type=int)</p> <p>args = parser.parse_args()</p> <p>party_id = args.party_id</p> <p>with open(\\'party_config.json\\') as cfg_file:</p> <p>party_config = json.load(cfg_file)</p> <p>party_config[\\'connection\\'][\\'info\\'][\\'port\\'] += party_id</p> <p>party_config[\\'connection\\'][\\'info\\'][\\'id\\'] += f\\'_{party_id}\\'</p> <p>party_config[\\'data\\'][\\'info\\'][\\'client_id\\'] = party_id</p> <p>CopyExplain</p> <ul> <li>Finally, we create and start a new\u00a0Party\u00a0object with the     modified\u00a0configuration information:</li> </ul> <p>party = Party(config_dict=party_config)</p> <p>party.start()</p> <p>party.register_party()</p> <p>CopyExplain</p> <ul> <li>With this, we\u00a0can now begin performing FL using\u00a0IBM FL.</li> </ul> <p>Running the IBM FL example</p> <ul> <li>First, start\u00a0aggregator\u00a0by\u00a0running the\u00a0following command:</li> </ul> <p>python -m ibmfl.aggregator.aggregator agg_config.json</p> <p>CopyExplain</p> <ul> <li>After the aggregator is finished setting up, type\u00a0START\u00a0and     press\u00a0Enter\u00a0key to open the aggregator to receive incoming     connections. You can then start three parties using the following     commands in\u00a0separate terminals:</li> </ul> <p>python fl_sim.py 1</p> <p>python fl_sim.py 2</p> <p>python fl_sim.py 3</p> <p>CopyExplain</p> <ul> <li>Finally, type\u00a0TRAIN\u00a0into the aggregator window and press\u00a0Enter\u00a0key     to begin the FL process. When\u00a0three rounds are completed, you can     type\u00a0SAVE\u00a0into the same window to save the latest\u00a0aggregate model.</li> </ul> <p>Integrating Flower for SST-2</p> <ul> <li> <p>The two main\u00a0Flower\u00a0components that must be incorporated on top of     the existing local training code are the client and strategy     subclass implementations.</p> </li> <li> <p>The client subclass implementation allows us to interface with     Flower, with API functions that allow for model parameters to be     passed between the clients and the server.</p> </li> <li> <p>The strategy subclass implementation allows us to specify the     details of the aggregation approach performed by\u00a0the server.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   We begin by writing the code to implement and start a client (stored     in\u00a0fl_sim.py). First, the necessary libraries\u00a0are imported:</p> <p>import argparse</p> <p>import tensorflow as tf</p> <p>from tensorflow import keras</p> <p>from sst_model import SSTModel, load_sst_data</p> <p>import flwr as fl</p> <p>CopyExplain</p> <ul> <li>We add a command-line argument specifying the client ID in order to     allow for the same client script to be reused for all\u00a0three agents:</li> </ul> <p>parser = argparse.ArgumentParser()</p> <p>parser.add_argument(\\\"client_id\\\", type=int)</p> <p>args = parser.parse_args()</p> <p>client_id = args.client_id</p> <p>NUM_CLIENTS = 3</p> <p>CopyExplain</p> <p>We then load in the\u00a0SST-2 datasets:</p> <p>(x_train,y_train), (x_test,y_test) = load_sst_data(client_id-1, NUM_CLIENTS)</p> <p>CopyExplain</p> <ul> <li>Note that we use the client ID to get the respective shard from     the\u00a0training dataset.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Next, we create the model and the associated optimizer and loss     objects, making sure to call the model\u00a0on a dummy\u00a0input to     initialize\u00a0the weights:</p> <p>sst_model = SSTModel()</p> <p>sst_model.compile(</p> <p>optimizer = keras.optimizers.Adam(learning_rate=0.005, amsgrad=False),</p> <p>loss = keras.losses.BinaryCrossentropy(),</p> <p>metrics = [keras.metrics.BinaryAccuracy()]</p> <p>)</p> <p>sst_input = keras.Input(shape=(), dtype=tf.string)</p> <p>sst_model(sst_input)</p> <p>CopyExplain</p> <p>Implementing the Flower client</p> <ul> <li>We can now\u00a0implement the Flower client object that will pass model     parameters to and from the server. To implement a client subclass,     we have to define\u00a0three functions:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   get_parameters(self, config): Returns the model\u00a0parameter values</p> <ul> <li> <p>fit(self, parameters, config): Sets the weights of the local model     to the received parameters, performs local training, and returns the     new model parameters alongside the dataset size and\u00a0training metrics</p> </li> <li> <p>evaluate(self, parameters, config): Sets the weights of the local     model to the received parameters, then evaluates the model on     validation/test data and returns the\u00a0performance metrics</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Using\u00a0fl.client.NumPyClient\u00a0as the superclass allows us to take     advantage of the Keras model\u00a0get_weights\u00a0and\u00a0set_weights\u00a0functions     that convert the model parameters into\u00a0lists of\u00a0NumPy arrays:</p> <p>class SSTClient(fl.client.NumPyClient):</p> <p>def get_parameters(self, config):</p> <p>return sst_model.get_weights()</p> <p>def fit(self, parameters, config):</p> <p>sst_model.set_weights(parameters)</p> <p>history = sst_model.fit(x_train, y_train, epochs=1)</p> <p>return sst_model.get_weights(), len(x_train),</p> <p>CopyExplain</p> <ul> <li>The\u00a0evaluate\u00a0function is\u00a0also defined:</li> </ul> <p>def evaluate(self, parameters, config):</p> <p>sst_model.set_weights(parameters)</p> <p>loss, acc = sst_model.evaluate(x_test, y_test, batch_size=64)</p> <p>return loss, len(x_train), {\\'val_acc\\':acc, \\'val_loss\\':loss}</p> <p>CopyExplain</p> <ul> <li>With this client implementation, we can finally start the client     using the default connection information with the\u00a0following line:</li> </ul> <p>fl.client.start_numpy_client(server_address=\\\"[::]:8080\\\", client=SSTClient())</p> <p>CopyExplain</p> <ul> <li>Creating the Flower server</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The final step\u00a0before running Flower is to create the script     (server.py) that will start the Flower server. We begin with the     necessary imports and the\u00a0MAX_ROUNDS\u00a0parameter:</p> <p>import flwr as fl</p> <p>import tensorflow as tf</p> <p>from tensorflow import keras</p> <p>from sst_model import SSTModel</p> <p>MAX_ROUNDS = 3</p> <p>CopyExplain</p> <ul> <li>Because we want to save the model after performing federated     learning, we create a subclass of the flower FedAvg strategy and add     a final step that saves the model at the last round during     the\u00a0aggregation phase:</li> </ul> <p>class SaveKerasModelStrategy(fl.server.strategy.FedAvg):</p> <p>def aggregate_fit(self, server_round, results, failures):</p> <p>agg_weights = super().aggregate_fit(server_round, results, failures)</p> <p>if (server_round == MAX_ROUNDS):</p> <p>sst_model = SSTModel()</p> <p>sst_input = keras.Input(shape=(), dtype=tf.string)</p> <p>sst_model(sst_input)</p> <p>sst_model.set_weights(fl.common.parameters_to_ndarrays(agg_weights[0]))</p> <p>sst_model.save(\\'final_agg_sst_model\\')</p> <p>return agg_weights</p> <p>CopyExplain</p> <ul> <li>With this strategy, we can run the following line to start the     server (passing the\u00a0MAX_ROUNDS\u00a0parameter through     the\u00a0config\u00a0argument):</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   fl.server.start_server(strategy=SaveKerasModelStrategy(),     config=fl.server.ServerConfig(num_rounds=MAX_ROUNDS))</p> <p>CopyExplain</p> <ul> <li>We can now\u00a0start the server and clients, allowing for FL to be     performed\u00a0using Flower.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Running the Flower example</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   To start the\u00a0server, first run the\u00a0server.py\u00a0script.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Each of the three clients can then be started by running the     following commands in separate\u00a0terminal windows:</p> <p>python fl_sim.py 1</p> <p>python fl_sim.py 2</p> <p>python fl_sim.py 3</p> <p>CopyExplain</p> <ul> <li>The final aggregate model after FL will be saved in     the\u00a0final_agg_sst_model\u00a0directory as a\u00a0SavedModel\u00a0object.</li> </ul> <p>Integrating STADLE for SST-2</p> <ul> <li> <p>STADLE differs\u00a0from the\u00a0previously examined FL frameworks by     providing a cloud-based platform (STADLE Ops) to handle the     deployment of aggregators and management of the FL process.</p> </li> <li> <p>Because the deployment of the server side can be done through the     platform, the client-side implementation is all that needs to be     implemented for performing FL with STADLE.</p> </li> <li> <p>This integration is done by creating a client object that     occasionally sends the local model and returns the aggregate model     from the previous round. To do this, we need to create the agent     configuration file and modify the local training code to     interface\u00a0with STADLE.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   First, we create the configuration file for the agent\u00a0as follows:</p> <p>{</p> <p>\\\"model_path\\\": \\\"./data/agent\\\",</p> <p>\\\"aggr_ip\\\": \\\"localhost\\\",</p> <p>\\\"reg_port\\\": \\\"8765\\\",</p> <p>\\\"token\\\": \\\"stadle12345\\\",</p> <p>\\\"base_model\\\": {</p> <p>\\\"model_fn\\\": \\\"SSTModel\\\",</p> <p>\\\"model_fn_src\\\": \\\"sst_model\\\",</p> <p>\\\"model_format\\\": \\\"Keras\\\",</p> <p>\\\"model_name\\\": \\\"Keras-SST-Model\\\"</p> <p>}</p> <p>}</p> <p>CopyExplain</p> <ul> <li>Information on these parameters can be found at     https://stadle-documentation.readthedocs.io/en/latest/documentation.html#configuration-of-agent.     Note that the aggregator IP and registration port values listed here     are placeholders and will be modified when connecting to the     STADLE\u00a0Ops platform.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Next, we\u00a0modify the local training\u00a0code to work with STADLE. We     first import the\u00a0requisite libraries:</p> <p>import argparse</p> <p>import tensorflow as tf</p> <p>from tensorflow import keras</p> <p>from sst_model import SSTModel, load_sst_data</p> <p>from stadle import BasicClient</p> <p>CopyExplain</p> <ul> <li>Once again, we add a command-line argument to specify which     partition of the training data the agent\u00a0should receive:</li> </ul> <p>parser = argparse.ArgumentParser()</p> <p>parser.add_argument(\\\"client_id\\\", type=int)</p> <p>args = parser.parse_args()</p> <p>client_id = args.client_id</p> <p>NUM_CLIENTS = 3</p> <p>(x_train,y_train), (x_test,y_test) = load_sst_data(client_id-1, NUM_CLIENTS)</p> <p>CopyExplain</p> <ul> <li>Next, we instantiate a\u00a0BasicClient\u00a0object -- this is the STADLE     client component that handles communication between the local     training process and the aggregators on the server side. We use the     configuration file defined earlier to create\u00a0this client:</li> </ul> <p>stadle_client = BasicClient(config_file=\\\"config_agent.json\\\", agent_name=f\\\"sst_agent_{client_id}\\\")</p> <p>CopyExplain</p> <ul> <li>Finally, we implement the FL training loop. In each round, the     client gets the aggregate model from the previous round (starting     with the base model) and trains it further on the local data before     sending it back to the aggregator through\u00a0the client:</li> </ul> <p>for round in range(3):</p> <p>sst_model = stadle_client.wait_for_sg_model()</p> <p>history = sst_model.fit(x_train, y_train, epochs=1)</p> <p>loss = history.history[\\'loss\\'][0]</p> <p>stadle_client.send_trained_model(sst_model, {\\'loss_training\\': loss})</p> <p>stadle_client.disconnect()</p> <p>CopyExplain</p> <ul> <li>The\u00a0wait_for_sg_model\u00a0function returns the latest aggregate model     from the server, and the\u00a0send_trained_model\u00a0function sends the     locally trained model with the desired performance metrics to the     server. More information on these integration steps can be     found\u00a0at\u00a0https://stadle-documentation.readthedocs.io/en/latest/usage.html#client-side-stadle-integration.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Now that\u00a0the client\u00a0side has been implemented, we can use the STADLE     Ops platform to start an aggregator and start an\u00a0FL process.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Creating a STADLE Ops project</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   First, go to stadle.ai\u00a0and create a new account. Once you are logged     in, you should be directed to the project information page in\u00a0STADLE     Ops:</p> <p>{width=\"6.268055555555556in\" height=\"2.0034722222222223in\"}</p> <p>Figure 8.3 -- Project information page in STADLE Ops</p> <ul> <li>Click on\u00a0Create New Project, then fill in the project     information and click\u00a0Create Project. The project information     page should have changed to show\u00a0the following:</li> </ul> <p>{width=\"6.268055555555556in\" height=\"1.8298611111111112in\"}</p> <p>Figure 8.4 -- New project added to the project information page</p> <ul> <li>Click on the plus icon under\u00a0Initiate Aggregator\u00a0to start a new     aggregator for the project, then click\u00a0OK\u00a0on the confirmation     prompt. You can now navigate to the\u00a0Dashboard\u00a0page on the left     side, resulting in a page that looks like\u00a0the following:</li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.50625in\"}</p> <p>Figure 8.5 -- Dashboard page of STADLE Ops</p> <ul> <li>Replace the\u00a0aggr_ip\u00a0and\u00a0reg_port\u00a0placeholder parameter values in     the\u00a0config_agent.json\u00a0file with\u00a0the values under\u00a0IP Address to     Connect\u00a0and\u00a0Port to\u00a0Connect, respectively.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   With this, we are now ready to begin the FL\u00a0training process.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Running the STADLE example</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The first\u00a0step is to send the base model object to the server,     allowing it to in turn distribute the model to the training     agents.\u00a0This is done with the\u00a0following command:</p> <p>stadle upload_model --config_path config_agent.json</p> <p>CopyExplain</p> <ul> <li>Once the command successfully runs, the\u00a0Base Model Info\u00a0section     on the STADLE Ops dashboard should update to show the model     information. We can now start the three agents by running     the\u00a0following commands:</li> </ul> <p>python fl_sim.py 1</p> <p>python fl_sim.py 2</p> <p>python fl_sim.py 3</p> <p>CopyExplain</p> <ul> <li>After three rounds, the agents will terminate and the final     aggregate model will be displayed in the project dashboard,     available for download in the Keras SavedModel format.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The user guide located     at\u00a0[https://stadle.ai/user_guide/guide]{.underline}\u00a0is     recommended for more information on the various functionalities of     the STADLE\u00a0Ops platform.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Evaluating the resulting aggregate models produced by each FL     framework results in the same conclusion---the performance of the     aggregate model essentially matches that of the centralized training     model.</p> <ul> <li>As explained in the\u00a0Dataset distributions\u00a0section of\u00a0section     7,\u00a0Model Aggregation, this is generally the expected result. The     natural question to\u00a0ask is how the performance is affected when the     local datasets are not IID---this is the focal point of the\u00a0next     section.</li> </ul> <p>Example -- the federated training of an image classification model on non-IID data</p> <ul> <li>In the previous\u00a0example, we\u00a0examined how a centralized deep learning     problem could be converted into an FL analog by training multiple     clients on disjoint subsets of the original training dataset     (the\u00a0local datasets) in an FL process. One key point of this local     dataset creation was that the subsets were created by random     sampling, leading to local datasets that were all IID under the same     distribution as the original dataset.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   As a result, the similar performance of FedAvg compared to the local     training scenario was expected -- each client's model essentially     had the same set of local minima to move toward during training,     making all local training beneficial for the\u00a0global objective.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Recall that in\u00a0section     7,\u00a0Model     Aggregation, we explored how FedAvg was susceptible to the     divergence in training objectives induced by severely non-IID local     datasets.</p> <ul> <li> <p>To explore the performance of FedAvg on varying non-IID severities,     this example trains the VGG-16 model (a simple deep-learning-based     image classification model) on constructed non-IID local datasets     sampled from the CIFAR-10 dataset (located     at\u00a0[https://www.cs.toronto.edu/\\~kriz/cifar.html]{.underline}).</p> </li> <li> <p>CIFAR-10 is a well-known simple image classification dataset     containing 60,000 images separated into 10 different classes; the     goal of models trained on CIFAR-10 is to correctly predict the class     associated with an input image.</p> </li> <li> <p>The relatively low complexity and ubiquity as a benchmark dataset     make CIFAR-10 ideal for exploring the response of FedAvg to\u00a0non-IID     data.</p> </li> </ul> <p>Important note</p> <ul> <li> <p>To avoid including redundant code samples, this section focuses on     the key lines of code that allow FL to be performed on PyTorch     models using non-IID local datasets.</p> </li> <li> <p>It is recommended that you go through the examples within     the\u00a0Example -- the federated training of an NLP model\u00a0section Here     prior to reading this section in order to understand the core     components needed for each FL framework.</p> </li> <li> <p>The implementations for this example can be found in full at this     book's GitHub repository     ([https://github.com/PacktPublishing/Federated-Learning-with-Python]{.underline}\u00a0tree/main/ch8/cv_code),     for use as\u00a0a reference.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The key point of this example is determining how the non-IID     datasets should be constructed.\u00a0We will change the class label     distributions of each local dataset by changing the number of images     of each class included in the training dataset.\u00a0</p> <ul> <li>For example, a dataset skewed toward cars and birds might have 5,000     images of cars, 5,000 images of birds, and 500 images for every     other class. By creating three disjointed subsets of\u00a0the 10 classes     and\u00a0constructing local datasets skewed toward these classes, we     produce three local datasets with non-IID severity proportional to     the number of images included from the classes\u00a0not selected.</li> </ul> <p>Skewing the CIFAR-10 dataset</p> <ul> <li>We first map\u00a0the three class subsets to client IDs, and set the     proportion of images to be taken from the original dataset for     selected classes (sel_count) and the other\u00a0classes (del_count):</li> </ul> <p>classes = (\\'airplane\\', \\'automobile\\', \\'bird\\', \\'cat\\', \\'deer\\',</p> <p>\\'dog\\', \\'frog\\', \\'horse\\', \\'ship\\', \\'truck\\')</p> <p>class_id_map = {</p> <p>1: classes[:3],</p> <p>2: classes[3:6],</p> <p>3: classes[6:]</p> <p>}</p> <p>sel_count = 1.0, def_count = 0.2</p> <p>CopyExplain</p> <ul> <li>We then sample the appropriate number of images from the original     dataset, using the indices of the images in the dataset to construct     the skewed\u00a0CIFAR-10 subset:</li> </ul> <p>class_counts = int(def_count * 5000) * np.ones(len(classes))</p> <p>for c in classes:</p> <p>if c in class_rank_map[self.rank]:</p> <p>class_counts[trainset.class_to_idx[c]] = int(sel_count * 5000)</p> <p>class_counts_ref = np.copy(class_counts)</p> <p>imbalanced_idx = []</p> <p>for i,img in enumerate(trainset):</p> <p>c = img[1]</p> <p>if (class_counts[c] &gt; 0):</p> <p>imbalanced_idx.append(i)</p> <p>class_counts[c] -= 1</p> <p>trainset = torch.utils.data.Subset(trainset, imbalanced_idx)</p> <p>CopyExplain</p> <ul> <li>The skewed\u00a0trainset is then used to create the     skewed\u00a0trainloader\u00a0for local training. When we refer to biasing the     training data going forward, this is the code that\u00a0is run.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   We will now demonstrate how to use different FL frameworks to run     this non-IID FL process. Please refer to the installation     instructions and framework-specific implementations in the previous     section,\u00a0Example -- the federated training of an NLP model, for     the\u00a0explanations of the basics omitted in\u00a0this section.</p> <p>Integrating OpenFL for CIFAR-10</p> <ul> <li>Similar to\u00a0the Keras\u00a0NLP example, we first create     the\u00a0ShardDescriptor\u00a0and\u00a0DataInterface\u00a0subclasses for the non-IID     CIFAR-10 datasets in\u00a0cifar_fl_dataset.py. Only a few changes need to     be made in order to accommodate the\u00a0new dataset.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   First, we modify the\u00a0self.data_by_type\u00a0dictionary to instead store     the modified\u00a0CIFAR datasets:</p> <p>train_dataset, val_dataset = self.load_cifar_data()</p> <p>self.data_by_type = {</p> <p>\\'train\\': train_dataset,</p> <p>\\'val\\': val_dataset</p> <p>}</p> <p>CopyExplain</p> <ul> <li>The\u00a0load_cifar_data\u00a0function loads in the training and test data     using\u00a0torchvision, then biases the training data based on the rank     passed to\u00a0the object.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Because the dimensions of a data element are now known (the size of     CIFAR-10 image), we also modify the shape properties with\u00a0fixed     values:</p> <p>@property</p> <p>def sample_shape(self):</p> <p>return [\\\"32\\\", \\\"32\\\"]</p> <p>@property</p> <p>def target_shape(self):</p> <p>return [\\\"10\\\"]</p> <p>CopyExplain</p> <ul> <li>We then implement the\u00a0CifarFedDataset\u00a0subclass of     the\u00a0DataInterface\u00a0class. No significant modifications are needed for     this implementation; thus, we can now use the biased CIFAR-10     dataset\u00a0with OpenFL.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   We now move to the actual FL process implementation (fl_sim.py). One     key difference is the framework adapter that must be used to create     the\u00a0ModelInterface\u00a0object from a\u00a0PyTorch model:</p> <p>model = vgg16()</p> <p>optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)</p> <p>criterion = nn.CrossEntropyLoss()</p> <p>framework_adapter = \\'openfl.plugins.frameworks_adapters.pytorch_adapter.FrameworkAdapterPlugin\\'</p> <p>MI = ModelInterface(model=model, optimizer=optimizer, framework_plugin=framework_adapter)</p> <p>CopyExplain</p> <ul> <li>The only other major change is modifying the train and validation     functions passed to the\u00a0TaskInterface\u00a0object to mirror the PyTorch     implementations of these functions from the local\u00a0training code.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The\u00a0last\u00a0step is to create the configuration files used by the     director and envoys. The only necessary change in the director     config is the updated\u00a0sample_shape\u00a0and\u00a0target_shape\u00a0for the\u00a0CIFAR-10     data:</p> <p>settings:</p> <p>listen_host: localhost</p> <p>listen_port: 50051</p> <p>sample_shape: [\\\"32\\\",\\\"32\\\"]</p> <p>target_shape: [\\\"10\\\"]</p> <p>CopyExplain</p> <ul> <li>This is saved\u00a0in\u00a0director/director_config.yaml.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The envoy configuration files require no changes outside of updating     the object and filenames -- the directory structure should look     like\u00a0the following:</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   director</p> <pre><code>-   director_config.yaml\n</code></pre> <ul> <li> <p>experiment</p> <ul> <li> <p>envoy_config_1.yaml</p> </li> <li> <p>envoy_config_2.yaml</p> </li> <li> <p>envoy_config_3.yaml</p> </li> <li> <p>cifar_fl_dataset.py</p> </li> <li> <p>fl_sim.py</p> </li> </ul> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   You can\u00a0refer\u00a0to\u00a0Running the OpenFL example\u00a0in the\u00a0Integrating     OpenFL for SST-2\u00a0section to run\u00a0this example.</p> <p>Integrating IBM FL for CIFAR-10</p> <ul> <li>Recall\u00a0that IBM FL\u00a0requires a saved version of the model used during     training. We first run the following code     in\u00a0create_saved_model.py\u00a0to create the saved VGG-16\u00a0PyTorch model:</li> </ul> <p>import torch</p> <p>from torchvision.models import vgg16</p> <p>model = vgg16()</p> <p>torch.save(model, \\'saved_vgg_model.pt\\')</p> <p>CopyExplain</p> <ul> <li>Next, we create the\u00a0DataHandler\u00a0subclass for the skewed CIFAR-10     datasets. The only core change is the modification of     the\u00a0load_and_preprocess_data\u00a0function to instead load in the     CIFAR-10 data and bias the\u00a0training set.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The next step is to create the configuration JSON files used when     starting the aggregator and initializing the parties. No significant     changes to the aggregator config (agg_config.json) are necessary,     and the only core change in the party config is the modification of     the model information to work\u00a0with PyTorch:</p> <p>\\\"model\\\": {</p> <p>\\\"name\\\": \\\"PytorchFLModel\\\",</p> <p>\\\"path\\\": \\\"ibmfl.model.pytorch_fl_model\\\",</p> <p>\\\"spec\\\": {</p> <p>\\\"model-name\\\": \\\"vgg_model\\\",</p> <p>\\\"model_definition\\\": \\\"saved_vgg_model.pt\\\",</p> <p>\\\"optimizer\\\": \\\"optim.SGD\\\",</p> <p>\\\"criterion\\\": \\\"nn.CrossEntropyLoss\\\"</p> <p>}</p> <p>},</p> <p>CopyExplain</p> <ul> <li>The code in\u00a0fl_sim.py\u00a0responsible for starting up the parties can     essentially remain unmodified due to the extensive use of     the\u00a0configuration files.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   You\u00a0can\u00a0refer to\u00a0Running the IBM FL example\u00a0in the\u00a0Integrating     IBM FL for SST-2\u00a0section to run\u00a0this example.</p> <p>Integrating Flower for CIFAR-10</p> <ul> <li>After\u00a0loading in the CIFAR-10 data and biasing the training data,     the\u00a0core change needed for the Flower implementation is     the\u00a0NumPyClient\u00a0subclass. Unlike the Keras example,     the\u00a0get_parameters\u00a0and\u00a0set_parameters\u00a0methods rely on the PyTorch     model state dictionaries and are a bit\u00a0more involved:</li> </ul> <p>class CifarClient(fl.client.NumPyClient):</p> <p>def get_parameters(self, config):</p> <p>return [val.numpy() for _, val in model.state_dict().items()]</p> <p>def set_parameters(self, parameters):</p> <p>params_dict = zip(model.state_dict().keys(), parameters)</p> <p>state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})</p> <p>model.load_state_dict(state_dict)</p> <p>CopyExplain</p> <ul> <li>We modify the\u00a0fit\u00a0function to mirror the training code in the local     training example and modify the evaluate function to similarly     mirror the local training evaluation code. Note that we     call\u00a0self.set_parameters(parameters)\u00a0in order to update the local     model instance with the most\u00a0recent weights.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   We also set the\u00a0grpc_max_message_length\u00a0parameter to 1 GB when     starting the Flower client and server to accommodate the larger     VGG16 model size. The client initialization function is now\u00a0the     following:</p> <p>fl.client.start_numpy_client(</p> <p>server_address=\\\"[::]:8080\\\",</p> <p>client=CifarClient(),</p> <p>grpc_max_message_length=1024**3</p> <p>)</p> <p>CopyExplain</p> <ul> <li>Finally, we\u00a0modify\u00a0the aggregator code in\u00a0server.py\u00a0-- the custom     strategy we used previously to save the aggregate model at the end     of the last round needs to be modified to work with\u00a0PyTorch models:</li> </ul> <p>if (server_round == MAX_ROUNDS):</p> <p>vgg_model = vgg16()</p> <p>np_weights = fl.common.parameters_to_ndarrays(agg_weights[0])</p> <p>params_dict = zip(vgg_model.state_dict().keys(), np_weights)</p> <p>state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})</p> <p>torch.save(state_dict, \\\"final_agg_vgg_model.pt\\\")</p> <p>CopyExplain</p> <ul> <li>With this strategy, we can run the following line to start the     server (adding the\u00a0grpc_max_message_length\u00a0parameter here\u00a0as well):</li> </ul> <p>fl.server.start_server(</p> <p>strategy=SavePyTorchModelStrategy(),</p> <p>config=fl.server.ServerConfig(num_rounds=MAX_ROUNDS),</p> <p>grpc_max_message_length=1024**3</p> <p>)</p> <p>CopyExplain</p> <ul> <li>Refer to\u00a0Running the Flower example\u00a0in the\u00a0Integrating Flower for     SST-2\u00a0section to run\u00a0this\u00a0example.</li> </ul> <p>Integrating STADLE for CIFAR-10</p> <ul> <li>We first\u00a0modify\u00a0the\u00a0config_agent.json\u00a0config file to use the VGG16     model from the\u00a0torchvision\u00a0library:</li> </ul> <p>{</p> <p>\\\"model_path\\\": \\\"./data/agent\\\",</p> <p>\\\"aggr_ip\\\": \\\"localhost\\\",</p> <p>\\\"reg_port\\\": \\\"8765\\\",</p> <p>\\\"token\\\": \\\"stadle12345\\\",</p> <p>\\\"base_model\\\": {</p> <p>\\\"model_fn\\\": \\\"vgg16\\\",</p> <p>\\\"model_fn_src\\\": \\\"torchvision.models\\\",</p> <p>\\\"model_format\\\": \\\"PyTorch\\\",</p> <p>\\\"model_name\\\": \\\"PyTorch-VGG-Model\\\"</p> <p>}</p> <p>}</p> <p>CopyExplain</p> <ul> <li>To integrate STADLE into the local training code, we initialize     the\u00a0BasicClient\u00a0object and modify the training loop to send the     local model every two local training epochs and wait for the     new\u00a0aggregate model:</li> </ul> <p>stadle_client = BasicClient(config_file=\\\"config_agent.json\\\")</p> <p>for epoch in range(num_epochs):</p> <p>state_dict = stadle_client.wait_for_sg_model().state_dict()</p> <p>model.load_state_dict(state_dict)</p> <p># Normal training code...</p> <p>if (epoch % 2 == 0):</p> <p>stadle_client.send_trained_model(model)</p> <p>CopyExplain</p> <p>Note</p> <ul> <li> <p>The code located     at\u00a0[https://github.com/PacktPublishing/Federated-Learning-with-Python]{.underline}\u00a0contains     the full implementation of this integration example for reference.</p> </li> <li> <p>To start an aggregator and perform FL with the CIFAR-10 STADLE     example, please refer to\u00a0Creating a STADLE Ops     project\u00a0and\u00a0Running the STADLE example\u00a0in the\u00a0Integrating STADLE     for\u00a0SST-2\u00a0subsection.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Testing\u00a0different\u00a0levels of bias in the constructed local datasets     should lead to the same conclusion stated in the\u00a0Dataset     distributions\u00a0section of\u00a0section     7,\u00a0Model     Aggregation\u00a0for non-IID cases---as the non-IID severity increases,     the convergence speed and model performance decrease.</p> <ul> <li> <p>The goal of this section was to build off of the understanding of     each FL framework from the SST-2 example, highlighting the key     changes necessary to work with a PyTorch model on a modified     dataset.</p> </li> <li> <p>Using this section alongside the code examples     in\u00a0[https://github.com/PacktPublishing/Federated-Learning-with-Python]{.underline}\u00a0should     help in understanding this\u00a0example integration.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>Here, we covered several FL frameworks through the context of two     different examples.</p> </li> <li> <p>From the first example, you learned how a traditional centralized ML     problem can be converted into the analogous FL scenario by     separating the data into disjointed subsets.</p> </li> <li> <p>It is now clear that random sampling leads to local datasets that     are IID, allowing FedAvg to reach the same level of performance as     the centralized equivalent with any of the FL frameworks.</p> </li> <li> <p>In the second example, you learned one of the many ways a group of     datasets can be non-IID (different class label distributions) and     observed how different severities of non-IID datasets affect the     performance of FedAvg. We encourage you to explore how alternative     aggregation methods can improve on FedAvg in\u00a0these cases.</p> </li> <li> <p>Both examples also should have given you a solid understanding of     the general trends when working with different FL frameworks; while     the specific implementation-level details may change (due to the     rapidly changing field), the core concepts and implementation     details will remain fundamentals.</p> </li> <li> <p>In the next section, we continue our transition to the business     application side of FL by taking a look at several case studies     involving the application of FL to\u00a0specific domains.</p> </li> </ul> <p>Case\u00a0Studies with Key Use Cases of Federated Learning Applications</p> <ul> <li> <p>Federated learning\u00a0(FL) has\u00a0met with a variety of AI     applications so far in various contexts and integration has been     explored with trials and errors in those fields. One of the most     popular areas has been in the medical and healthcare fields where     the concept of privacy-preserving AI naturally fits with the current     needs and challenges of healthcare AI.</p> </li> <li> <p>FL has also been applied to the financial services industry, edge     computing devices, and the\u00a0Internet of Things\u00a0(IoT),     through\u00a0which FL has been shown to have significant benefits in     quite a few applications, which will resolve many important\u00a0social     problems.</p> </li> <li> <p>Here, we will be discussing some of the major use cases of FL in     different fields. It is our hope that by the end of This section,     you'll be familiar with some of the applications of FL in different     industries.</p> </li> <li> <p>We\\'ll start by exploring the use of FL in the healthcare and     financial industries before making the transition to the edge     computing and IoT sectors. Finally, we will conclude the section by     discussing the intersection of FL and distributed learning for\u00a0big     data.</p> </li> <li> <p>Here, we will cover the\u00a0following topics:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Applying FL to the\u00a0healthcare sector</p> <ul> <li> <p>Applying FL to the\u00a0financial sector</p> </li> <li> <p>FL meets\u00a0edge computing</p> </li> <li> <p>Moving towards the Internet\u00a0of Intelligence</p> </li> <li> <p>Applying FL to distributed learning for\u00a0big data</p> </li> </ul> <p>Applying FL to the healthcare sector</p> <ul> <li> <p>FL\u00a0used in healthcare is a topic\u00a0that gained quite a lot of     attention in the last couple of years. Healthcare advances can have     an enormous effect on our lives.</p> </li> <li> <p>However, several challenges make these advances perhaps more     difficult than in other domains. Let's begin by discussing some of     the common challenges that exist and are preventing the further     development of AI\u00a0in healthcare.</p> </li> </ul> <p>Challenges in healthcare</p> <ul> <li> <p>One of the\u00a0primary challenges is data accessibility.\u00a0</p> </li> <li> <p>Data accessibility\u00a0is not an issue unique to healthcare. It is a     huge problem across the AI industry and will only become a greater     challenge as time goes on.</p> </li> <li> <p>It is a core problem in the development of AI in healthcare and we     will touch briefly on some of the reasons why it is an issue here.     We will also continue to revisit this major hurdle, addressing     problems and solutions from many different angles and applications.</p> </li> <li> <p>This strategy will allow you to understand the many different     aspects, complexity, and drivers of\u00a0the problem.</p> </li> <li> <p>The data accessibility problem has\u00a0many components:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Privacy regulations: The\u00a0main bottleneck of the data     accessibility issue is centered around the privacy regulations put     in place to protect personal data; these regulations are absolutely     necessary and should be in place. Rather than circumventing these     regulations, we'll be discussing how we can work in parallel with     them, keeping these valuable regulations in place, and at the same     time, making use of the intelligence that this data can provide. You     can think of this as a best-of-both-worlds scenario. Several     important privacy regulations are discussed in the\u00a0Data privacy as     a bottleneck\u00a0section of\u00a0section     1,\u00a0Challenges     in Big Data and\u00a0Traditional AI.</p> <ul> <li> <p>Lack of data/need for real data: Few areas hold as much promise     for providing a positive societal impact as healthcare does. Yet,     the healthcare industry has fallen far behind in capitalizing on all     of the many benefits that AI has to offer. One reason for this is     that for AI and ML models to learn effectively, they need large     amounts of data. We'll discuss more on the need for large amounts of     data throughout This section. This is the limiting factor for AI. In     healthcare, there are many regulations in place that prevent these     models from utilizing the data in any way, and\u00a0rightfully so.</p> </li> <li> <p>Many data types from many places: As we'll discuss further,     there are many different data types from many different places. Data     can be in the form of text, video, images, or speech, which is     stored in many different places. Aside from the ability to access     data from many different locations, which is a\u00a0major challenge on     its own, these\u00a0institutions store data in various formats\u00a0as well.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   As mentioned, some of the earliest work in the application of FL has     been in the healthcare space. Within this large field, there are     several ways FL can help solve problems. Let's take a look at just a     few of the areas where FL has great potential to transform\u00a0the     healthcare system. Some of these areas include medical imaging, drug     discovery, and\u00a0Electronic Health\u00a0Records\u00a0(EHRs).</p> <ul> <li>Let's start by taking a closer look at how FL is being used     in\u00a0medical\u00a0imaging.</li> </ul> <p>Medical imaging</p> <ul> <li> <p>There is\u00a0a lot of optimism surrounding FL in the medical imaging     space as discussed in\u00a0A Comprehensive Analysis of Recent Deep and     Federated-Learning-Based Methodologies for Brain Tumor Diagnosis,     which is listed in the\u00a0Further reading\u00a0section Here.</p> </li> <li> <p>These high expectations are, in part, due to some of the challenges     that need to be addressed and the capability of FL to overcome these     hurdles. One of these challenges is needing large amounts\u00a0of data.</p> </li> <li> <p>Large amounts of medical imaging data are created every day as the     medical imaging industry continues to develop better equipment,     procedures, and facilities. The exponential growth of this data is a     huge opportunity for healthcare providers to develop better ML     models and increase the quality\u00a0of healthcare.</p> </li> <li> <p>Another reason\u00a0for the optimism around FL having a positive impact     on medical imaging is the already\u00a0proven success of\u00a0machine     learning\u00a0(ML) -- more     specifically,\u00a0deep\u00a0learning\u00a0(DL).</p> </li> <li> <p>Let's take a brief look at DL to understand better why it is so     important when dealing with large amounts of data. DL is a subspace     of ML encompassed by the AI umbrella.</p> </li> <li> <p>DL is different from ML in that it uses several layers of what are     known as neural networks. Several books have been written on DL and     neural networks as a singular subject, so we won't attempt to     explain these in greater detail in this book. For more in-depth     coverage of DL and neural networks,\u00a0Advanced Deep Learning with     Python is a great book to read.</p> </li> <li> <p>For our general discussion, we'll provide a very\u00a0basic explanation.</p> </li> <li> <p>The following\u00a0figure,\u00a0Figure 9.1, shows a simple example of a     neural network being used to help classify types of brain tumors     using\u00a0medical imaging:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.765277777777778in\"}</p> <p>Figure 9.1 -- Tumor classification with a neural network</p> <ul> <li> <p>On the left-hand side of the figure, we have an image of a brain     that has one of two types of tumors. The image is broken down into     numbers that represent the image pixels.</p> </li> <li> <p>These numbers are added to the neural network. The hidden layers     utilize different weights for these numbers and produce different     outputs through activation functions. Finally, we can see the two     output layers. In this case, there are two possible outputs. One     is\u00a0Tumor is Type 1\u00a0and the other is\u00a0Tumor is\u00a0Type 2.</p> </li> </ul> <p>Single location data is limited</p> <ul> <li>As you can see, DL models require a lot of data to train. Generally,     a single data repository has only a small amount of data, limiting     the ability of any model to\u00a0generalize well.</li> </ul> <p>Possible solutions to data accessibility challenges</p> <ul> <li> <p>One\u00a0solution is to utilize privacy-preserving FL, which can make use     of all the data available in multiple centers while keeping     sensitive\u00a0data private.</p> </li> <li> <p>FL enables the deployment of large-scale ML models trained in     different data centers without sharing\u00a0sensitive data.</p> </li> <li> <p>In FL, rather than moving the data to the model to be trained, we     move the model to the data and only bring back the intelligence     gathered from the data, referred to as\u00a0Intelligence from     Data\u00a0(IfD), discussed\u00a0later in the\u00a0Potential of IfD\u00a0section     in\u00a0This section.</p> </li> </ul> <p>Example use case -- ML in hospitals</p> <ul> <li> <p>Let's\u00a0walk through an example of how FL could be applied to medical     imaging data. This example is actually what was done in an     international challenge focused on brain tumors. The goal here is to     segment the tumor using\u00a0MRI scans.</p> </li> <li> <p>For this example, we're going to use three hospitals. We will label     them Hospital A, Hospital B, and Hospital C. Each hospital has     anonymized private medical imaging data that is stored locally. Each     hospital begins with a learning model, which you can see in     the\u00a0following diagram:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"5.235416666666667in\"}</p> <p>Figure 9.2 -- Hospitals sharing ML models for FL</p> <ul> <li> <p>Each hospital runs the model locally; this creates what is referred     to as a local model. It's important to note that each one of the     hospital's local models will be different at this point. Each of     them has only trained on the data that resides at\u00a0their hospital.</p> </li> <li> <p>The\u00a0intelligence from training these three local models is sent to a     centralized server in the form of model parameters. The server     gathers the local models and combines them to create a global model.     This global model, a combination of the intelligence from all three     local models, is then sent back to each hospital and is again     trained locally only on that\u00a0hospital's data.</p> </li> <li> <p>Again, only the intelligence from these models is sent back to the     server for aggregation. This process is repeated until the model has     learned all it can (known\u00a0as convergence).</p> </li> <li> <p>Utilizing\u00a0FL, you can train models that perform as if all the data     came from a single location even when data resides at different     locations. As you can see, the implementation of privacy-preserving     methods such as this one has the power to revolutionize the field\u00a0of     medicine.</p> </li> <li> <p>Let's now take a look at how FL can improve the drug\u00a0discovery     space.</p> </li> </ul> <p>Drug discovery</p> <ul> <li>Data has\u00a0become somewhat of a new currency in our modern world. For     pharmaceutical companies, especially, the use of this data to     provide\u00a0personalized medicine\u00a0has become a major focus. In the     years ahead, companies that can make use of more data will be far     more competitive. This will be one of the defining strategies for     the future success of\u00a0any organization.</li> </ul> <p>Precision medicine</p> <ul> <li>Personalized medicine, also\u00a0known as\u00a0precision medicine, relies     heavily on large amounts of\u00a0real-world data\u00a0to make this possible.     In addition, ML algorithms are needed to process and analyze this     data in order to extract meaningful insights. As we will discuss,     accessing significant amounts of real data is currently very     difficult, if\u00a0not impossible.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   In\u00a0Figure 9.3, we show some of the areas where precision medicine     will have an\u00a0immediate impact:</p> <p>{width=\"6.268055555555556in\" height=\"6.09375in\"}</p> <p>Figure 9.3 -- Precision medicine impacting many areas</p> <ul> <li>As you can see from\u00a0Figure 9.3, precision\u00a0medicine covers a vast     area of different fields and disciplines, such as oncology,     wellness, diagnostics, research, and\u00a0health monitoring.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Currently, many AI solutions that are deployed in the healthcare     systems fail due to being created with small datasets that only     represent a fraction of the patient population. Researchers and     developers can validate and improve models using data from many     sources\u00a0without ownership.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   To access this data so that it can be processed and analyzed, a new     approach is needed. This is where FL comes in. As we've covered     throughout this book, FL provides access to the intelligence needed,     and not the\u00a0data itself.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Precision medicine\u00a0is a model that proposes that rather than     utilizing a\u00a0one-size-fits-all\u00a0approach, it should customize     healthcare to individuals for better results in terms of drug     effectiveness and cancer treatment outcomes, among other things. To     do this, precision medicine relies heavily on large amounts of     real-world data. Accessing large amounts of real-world data is the     first hurdle that must be overcome to realize precision medicine\u00a0at     scale.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Let's look at the current approach to see how FL can provide     the\u00a0needed answer.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Shown in\u00a0Figure 9.4\u00a0is the current common approach to ML system     implementation. Here, the data is transmitted to a central server,     where all of the data is gathered together, and algorithms are then     trained on this aggregated data. This is known as the\u00a0bringing the     data to the\u00a0model\u00a0approach:</p> <p>{width=\"5.467361111111111in\" height=\"4.206944444444445in\"}</p> <p>Figure 9.4 -- Precision medicine now</p> <ul> <li>It's easy to imagine the immense cost of moving data from many     hospitals into one centralized place in this fashion. Processing     data in this way also compromises data security and makes regulatory     compliance difficult, if\u00a0not impossible.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   As shown in\u00a0Figure 9.5, the FL approach is quite different.     Instead of\u00a0bringing the data to the model, the\u00a0model is moved to     the data. This means that the ML model trains on local data and     only sends the intelligence to the FL server for aggregation, thus     allowing the various local models to benefit from updates to     the\u00a0global model:</p> <p>{width=\"5.177777777777778in\" height=\"3.7395833333333335in\"}</p> <p>Figure 9.5 -- Precision medicine with FL</p> <ul> <li>The FL\u00a0approach allows for efficient model transfers and data     security while\u00a0being compliance-friendly.</li> </ul> <p>Potential of IfD</p> <ul> <li> <p>Utilizing\u00a0FL to gain access to real-world data has huge potential to     improve all of the clinical research stages. Accessing this kind of     data allows us to utilize the intelligence gathered and IfD can     dramatically accelerate the processes and steps in drug discovery.</p> </li> <li> <p>One important idea to keep in mind when discussing how FL works is     that the training data never leaves\u00a0the device.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Figure 9.6\u00a0describes the process of extracting the ML model from     training data through the\u00a0training process:</p> <p>{width=\"6.268055555555556in\" height=\"1.8090277777777777in\"}</p> <p>Figure 9.6 -- An IfD diagram</p> <ul> <li>As you\u00a0can see in\u00a0Figure 9.6, the data is used locally to train     the ML model. In FL, the model is located on the device itself,     where it is trained, and only the model weights are sent for     aggregation -- so only the intelligence from the data, not the\u00a0data     itself.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The ability to gather IfD collected from multiple sources, as well     as a wide range of data types, including video, text, speech, and     other sensory data, can help improve enrollment processes, both in     terms of the speed of doing so and finding the right match for the     research and development\u00a0of treatments.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   This is especially important for rare diseases and cancers. When     identifications are made in this way, subjects can be notified     of\u00a0trial opportunities.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Federated access to data and the collection of IfD can open up     access to a substantial amount of data worldwide. This allows for     aggregated data repositories to mine a sufficient number of patients     fulfilling the protocol criteria. Potentially, this could allow all     participants in a trial to receive the actual drug and not\u00a0the     placebo.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Ultimately, precision medicine at scale is not possible without     robust AI and robust AI can only be trained with large amounts of     real-world data. Using FL could allow for improved outcome     measurements. In the coming years, a federated approach has the     potential to drive advancements and innovation in the discovery of     new medical treatments in new ways that have not been\u00a0possible     before.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   In the following figure,\u00a0Figure 9.7, we show a generalized view of     how FL\u00a0collects IfD:</p> <p>{width=\"6.268055555555556in\" height=\"4.642361111111111in\"}</p> <p>Figure 9.7 -- A generalized view of how FL collects IfD</p> <ul> <li> <p>As\u00a0shown in\u00a0Figure 9.7\u00a0here, all the data remains isolated within     each organization and is not transferred to the\u00a0federated server.</p> </li> <li> <p>Let's now move forward and discuss an FL application\u00a0with EHRs.</p> </li> </ul> <p>EHRs</p> <ul> <li>An EHR is a\u00a0collection of health information that is systematically     and digitally stored. These records are designed to be shared     with\u00a0healthcare providers\u00a0(HCPs) when\u00a0appropriate. According     to\u00a0HealthIT.gov\u00a0statistics, as of 2017, 86% of office-based     physicians have adopted EHRs in the\u00a0United States.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Figure 9.8\u00a0depicts the\u00a0EHRs\u00a0collected from various sources, such     as hospitals and insurers. In\u00a0this figure, we use the     terms\u00a0Electronic Medical Records\u00a0(EMRs) and\u00a0Personal     Health\u00a0Records\u00a0(PHRs):</p> <p>{width=\"6.268055555555556in\" height=\"4.810416666666667in\"}</p> <p>Figure 9.8 -- EHRs</p> <ul> <li>This adoption of EHRs has laid the groundwork for beneficial     collaboration between healthcare organizations. As we've discussed     throughout the book, the ability to access more real-world data     allows AI models trained on this data to be much more robust\u00a0and     effective.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Although this foundation has been put in place, there are still many     challenges to the traditional ML approach in terms of sharing EHR     data across multiple institutions.</p> <ul> <li> <p>These include privacy concerns and regulations as well as data     standardization.</p> </li> <li> <p>One of the\u00a0major problems is the storage of this data in\u00a0Central     Data Repositories\u00a0(CDRs), as shown in\u00a0Figure 9.9, where     various forms of local data are stored to be trained to create an\u00a0ML     model.</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"2.834722222222222in\"}</p> <p>Figure 9.9 -- A centralized data mining method</p> <ul> <li>This CDR approach is not ideal because of the data isolation     problem, which is discussed in the\u00a0following section.</li> </ul> <p>The data isolation problem</p> <ul> <li>The use of CDRs for\u00a0data storage brings many problems. Some examples     include things such as\u00a0data leakage,\u00a0hefty regulations, and     a\u00a0high cost\u00a0to set up\u00a0and maintain.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The effect that data storage methods have on the quality and     useability of the data itself is just as important. ML models     trained on a single center data usually cannot generalize well when     compared to data that has been gathered from\u00a0multiple locations.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   FL allows model training collaboration across multiple     organizations, resulting in the production of superior model     performance without violating data\u00a0privacy regulations.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Let's take a look at an example of an FL application\u00a0with EHRs.</p> <p>Representation learning in EHRs</p> <ul> <li>Researchers have\u00a0applied FL to representation learning in EHRs as     mentioned in the\u00a0Further reading\u00a0list such as\u00a0Two-stage federated     phenotyping and patient representation learning.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   They used FL with\u00a0natural language processing\u00a0(NLP)     for\u00a0phenotyping and representation learning with patient data. In     the first stage, a representation of patient data is created based     on some medical records from several hospitals without sharing the     raw data itself.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The representation that has been learned is not limited to any     specific medical task. In the second stage, an ML model for specific     phenotyping work is trained in a federated manner using the related     features derived from the\u00a0learned representations.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   FL has been showcased as an effective alternative to current     methodologies in the advancement of AI developments in healthcare,     whether in drug discovery, medical imaging, or the\u00a0analysis\u00a0of EHRs.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Let's move on to the financial sector as another promising use case     related to FL in the\u00a0following section.</p> <p>Applying FL to the financial sector</p> <ul> <li> <p>In the US alone, financial\u00a0services firms spend billions of dollars     every year on compliance to combat laundering, yet the current     system is so ineffective that less than 1% of money laundering     activities are thwarted. In fact, it's estimated that firms spend     roughly 100 times more money than they are able to recover from this     criminal activity.</p> </li> <li> <p>Only a small percentage of transactions\u00a0are caught by\u00a0anti-money     laundering\u00a0(AML) systems, and an even smaller\u00a0percentage of     those alerts are eventually reported in\u00a0suspicious activity     reports\u00a0(SARs), as required\u00a0by the\u00a0Bank Secrecy     Act\u00a0(BSA)\u00a0of 1970.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Conservatively speaking, the value of information coming from a     network of banks is many times higher than the information any one     bank has. This is because you can see not just where the money came     from, but also where\u00a0it went.</p> <p>Anti-Money Laundering (AML)</p> <ul> <li>The current AML\u00a0system needs major improvements, with several     challenges that need to be overcome. Many privacy regulations are in     place to protect personal financial data. These regulations vary     from institution to institution and region\u00a0to region.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   One solution is a collaboration between financial institutions. This     collaboration would allow these institutions to share intelligence     gathered from their own data in a mutually beneficial way. Rather     than moving data between collaborators, the ML models themselves     would be deployed locally at each institution.</p> <ul> <li>This would allow only the IfD to be shared and benefit each     collaborator that could utilize the intelligence gathered. As we've     discussed, FL has\u00a0this capability.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   In FL, the models themselves move from institution to institution.     While doing so, the models adjust parameters and become more     intelligent as they learn from more data. This is different from     these two alternatives\u00a0to FL:</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   One option is to collect the data from the collaborating financial     institutions into one central repository. However, this approach     isn't possible due to customer\u00a0privacy regulations.</p> <ul> <li>Another approach is to share some kind of identifier as necessary.     However, again due to privacy laws and regulations, this is not     possible and could only be used as part of an\u00a0investigation process.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   To be able\u00a0to use FL, we need to be able to ensure that the privacy     and security of data remain intact. To do so, we must ensure that     the models are being trained in a financial institution without     privacy violations. Lastly, we need to ensure that all communication     related to model parameter updates sent to the federated server is     safe\u00a0and secure.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   FL can improve the effectiveness, efficiency, and fairness of the     global BSA and AML regimes. In the following section, we will look     at using FL in the context of transaction monitoring implementation     across\u00a0AML disciplines.</p> <p>Proposed solutions to the existing AML approach</p> <ul> <li>The\u00a0development of FL approaches across AML disciplines includes the     essential topic of customer onboarding and it may help to use     non-traditional information to verify the identities of\u00a0potential     customers.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   A new approach is needed that can leverage sophisticated technology     to increase the awareness and efficiency of risk\u00a0detection systems.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   This approach should enable\u00a0the following:</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Enable firms and regulators to learn from each other without sharing     sensitive or\u00a0protected data</p> <ul> <li> <p>Enhance the ability of firms to identify accurately real risks and     reduce unfounded\u00a0risk reporting</p> </li> <li> <p>Improve the risk-reward calculi of firms when making decisions about     whether to serve\u00a0specific markets</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The AML framework serves a vital national interest in preventing any     intent of harm, whether through terrorism, money laundering, fraud,     or human trafficking, using the global financial system for     those\u00a0illegal purposes.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Although there has been enormous investment in and attention to     money laundering risk detection systems, the system is broken. Firms     invest considerable resources to satisfy AML requirements but get     little feedback on the quality of their\u00a0risk reporting.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Two key\u00a0factors have driven firms out of operating in\u00a0specific     markets:</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The first is the costs of risk rating activity that occurs within     the money\u00a0services industry</p> <ul> <li>The second key factor is the regulatory risks and reputational     impact for financial firms connected to illicit\u00a0financial activities</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   As a result, globally, correspondent banking relationships have     fallen by 25%. Since 2009, 25% of banks in emerging markets have     reported correspondent\u00a0banking losses.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   In\u00a0Figure 9.10, transaction reporting is depicted and each     institution's database of risk patterns reflects its experience     with\u00a0illicit activities:</p> <p>{width=\"6.268055555555556in\" height=\"4.373611111111111in\"}</p> <p>Figure 9.10 -- Institutional reporting of suspected illicit activity</p> <ul> <li>Institutions won't necessarily know about patterns their competitors     are picking up or what the government knows about which transactions     flagged are suspicious or genuine. Firms get little timely feedback     on the accuracy of the reports they submit. The result is that firms     lack the most vital information for improving their risk detection     capabilities: timely information about\u00a0confirmed problems.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   So, imagine\u00a0transaction monitoring that is more effective,     efficient, and fair in this scenario. The bank acts as a     utility-like hub.</p> <ul> <li> <p>Powerful computers combined with smart algorithms could be deployed     to evaluate data at different institutions.</p> </li> <li> <p>The ML model that has learned the risk patterns would then move     between the participating firms to pick up the patterns and learn     from the risk at each institution. All of this could be done without     sharing sensitive or protected data.</p> </li> <li> <p>This is depicted in\u00a0Figure 9.11:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.429861111111111in\"}</p> <p>Figure 9.11 -- The data and intelligence flow within a banking ecosystem</p> <ul> <li> <p>In the\u00a0FL approach, the bank creates a classification algorithm that     trains on each participating firm's data.</p> </li> <li> <p>The bank develops a key model and model parameters that reflect     insights from all participating firms and the data in the     government's possession.</p> </li> <li> <p>The bank distributes the key model and model parameters to the     participating firms while the data stays in each institution. These     distributed models adopt the risk patterns in those firms by     learning from their local data and then sending them back to\u00a0the     bank.</p> </li> </ul> <p>Demo of FL in the AML space</p> <ul> <li> <p>The\u00a0researchers at TieSet, Inc. have\u00a0conducted an experiment of     applying FL to the AML space over STADLE, using some synthetic     transaction data generated by the PaySim mobile money simulator     ([https://www.kaggle.com/ealaxi/paysim1]{.underline}).</p> </li> <li> <p>They have used supervised learning with logistic regression where     model features include time, amount, and the new and old balance of     the original account and the destination account.</p> </li> <li> <p>The dataset has 636,2620 transactions (8,213 fraud transactions and     635,4407 valid transactions), which are split into 10 separate\u00a0local     agents.</p> </li> <li> <p>Figure 9.12\u00a0is the\u00a0outcome of applying FL to AML\u00a0where the     precision score and F1 score are plotted at each round of training.     In the figure, the thicker line is the performance of the aggregated     model, and the thin lines are the results of individual agents     training separately only using\u00a0local data:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.0034722222222223in\"}</p> <p>Figure 9.12 -- Outcome of applying FL to AML (thicker line: aggregated model, thin lines: individual agent training separately)</p> <ul> <li> <p>As in\u00a0Figure 9.12, the aggregated model performs in a quite stable     manner, constantly achieving more than 90% in terms of precision and     F1 score. FL could reduce the fraud transactions down from the total     fraud transactions of $1,241,770,000 to $65,780,000, meaning only     5.3% of fraud transactions\u00a0are missing.</p> </li> <li> <p>Let's conclude this section by looking at a list of benefits that FL     provides for\u00a0risk detection.</p> </li> </ul> <p>Benefits of FL for risk detection systems</p> <ul> <li>There are\u00a0several benefits in financial sectors for applying FL to     risk detection systems\u00a0as follows:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   FL can create a larger dataset of risk detection algorithms to\u00a0train     on</p> <ul> <li> <p>Improved accuracy of illicit\u00a0activity detection</p> </li> <li> <p>Provides a way for organizations\u00a0to collaborate</p> </li> <li> <p>Firms can enter\u00a0new markets</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   The ability to identify money laundering tactics within your own     financial institution is limited by the data you have access to.     Data sharing restrictions make collaboration difficult, if not     impossible.</p> <ul> <li> <p>The solutions and advantages that FL brings to the finance industry     are numerous. Additional advantages include better operational     efficiency and better allocation of human capital.</p> </li> <li> <p>There is no limit to the application of FL, especially in the     financial space, due to the ability to extract intelligence     from\u00a0client data.</p> </li> <li> <p>We'll now be switching gears a bit as we move on to discussing the     use of FL in several emerging technologies in the\u00a0following section.</p> </li> </ul> <p>FL meets edge computing</p> <ul> <li> <p>The section in this\u00a0section is a\u00a0mixture of different areas, some of     which are emergent technologies.</p> </li> <li> <p>These areas are all very interconnected, as we will cover. Many of     these technologies depend on one another to overcome their own     challenges and limitations.</p> </li> <li> <p>Combining these technologies alongside FL is an especially potent     combination of technology that is sure to be key to advancements and     innovation in the\u00a0coming years.</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   In the decade ahead, we will see several changes brought about by     the expansion of edge computing capabilities, IoT, and 5G     connectivity.</p> <ul> <li> <p>We will see an exponential increase in both the amount of data and     the speed at which it is transmitted.</p> </li> <li> <p>We will continue to see more privacy regulations put in place to     protect private user data and an explosion in the automation     and\u00a0analytics areas.</p> </li> </ul> <p>Edge computing with IoT over 5G</p> <ul> <li> <p>The foundation for\u00a0realizing the full potential of smart devices is     only possible if these devices are able to connect with a     much-improved network, such as 5G.</p> </li> <li> <p>In fact, by the end of 2023, it is expected that there will be 1.3     billion subscribers to 5G services worldwide. Alongside edge     computing, 5G networks are essential for IoT connectivity. Combining     these technologies will help pave the way for\u00a0smart devices.</p> </li> <li> <p>Figure 9.13\u00a0depicts a variety of things, with edge computing     capability connected to the cloud and data centers within an\u00a0IoT     framework:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.565277777777778in\"}</p> <p>Figure 9.13 -- Edge computing and the Internet</p> <ul> <li> <p>Many of these\u00a0IoT devices, however, lack adequate security     capabilities. In\u00a0addition to laws and regulations such as     the\u00a0General Data Protection Regulation\u00a0(GDPR), we can also     expect additional policies to be implemented to protect user data.     Essentially\u00a0the need for a solution that extracts IfD will continue     to build as more\u00a0time passes.</p> </li> <li> <p>Let's take a look at an example of FL applied to\u00a0edge computing.</p> </li> </ul> <p>Edge FL example -- object detection</p> <ul> <li>Edge computing\u00a0is an architecture that uses distributed computing to     bring computation and data storage as close to the sources of data     as possible. Ideally, this should reduce latency and save bandwidth.     Here's an example of how FL can be utilized with different types     of\u00a0edge devices.</li> </ul> <p>Technical settings</p> <ul> <li> <p>In this example, three\u00a0devices were used to demonstrate object     detection with FL using edge devices.</p> </li> <li> <p>One was an EIS200 edge microserver that ran on Nvidia's Jetson with     an Ubuntu OS.</p> </li> <li> <p>The second device was a Raspberry Pi, using Raspberry Pi OS, and the     third device was simply a regular PC, whose OS was Ubuntu as well.     These machines individually trained an object detection model     with\u00a0unique datasets.</p> </li> </ul> <p>How to do it</p> <ul> <li> <p>The EIS200 trained on pictures of fish, meat, and tomatoes with the     labels\u00a0fish,\u00a0meat, and\u00a0vegetable. The Raspberry Pi trained on     pictures of fish, meat, and eggplants. Evidently, here, tomatoes     were replaced with eggplants.</p> </li> <li> <p>The labels, however, remained the same -- fish, meat, and vegetable.     Likewise, the regular PC trained on pictures of fish, meat, and     leeks, still with the labels fish, meat,\u00a0and vegetable.</p> </li> <li> <p>As you would expect, each environment had biased data containing     different vegetables -- as in, tomatoes, eggplants, and leeks --     with an identical label, vegetable, for all\u00a0of them.</p> </li> </ul> <p>How it works</p> <ul> <li> <p>First, the model was\u00a0trained with pictures of tomatoes by EIS200. As     you would expect, only tomatoes were correctly labeled as     vegetables, whereas eggplants and leeks\u00a0were mislabeled.</p> </li> <li> <p>In the same manner, the Raspberry Pi's model trained with pictures     of eggplants only identified eggplants correctly. One of the two     leeks was labeled as a vegetable as well, but the other one was     identified as fish. As expected, the regular PC's model only     identified leeks\u00a0as vegetables.</p> </li> <li> <p>None of the\u00a0three agents could label all three vegetables correctly,     as we would have anticipated. Next, they were connected to an FL     platform called STADLE, developed by\u00a0TieSet, Inc:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.2736111111111112in\"}</p> <p>Figure 9.14 -- Demo of detecting tomatoes, eggplants, and leeks using FL where the distribution of the datasets is different on each machine</p> <ul> <li> <p>The STADLE aggregator was run as an instance in AWS. Here, again,     each environment had a uniquely biased dataset containing only one     type of vegetable.</p> </li> <li> <p>Connected with the STADLE platform, each agent trained with local     data. After several training epochs, the weights of the models were     sent from the agents to the aggregator.</p> </li> <li> <p>Those weights were then aggregated and sent back to the agents to     continue training. The\u00a0repetition of this aggregation cycle     generated\u00a0unbiased weights.</p> </li> </ul> <p>Examining the results</p> <ul> <li>The FL model\u00a0was able to detect and label all three types of     vegetables correctly as in\u00a0Figure 9.15. This is a straightforward     example of the power of FL in terms of\u00a0bias elimination:</li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.8965277777777776in\"}</p> <p>Figure 9.15 -- Results of the demo using three edge devices</p> <ul> <li> <p>As mentioned previously, all of the model training took place at the     local storage of the edge device itself.</p> </li> <li> <p>The model trained on the local data and then only sent the parameter     weights to the federated server for aggregation.</p> </li> <li> <p>The federated server averaged the model. If you recall from earlier     sections, this is called FedAvg.</p> </li> <li> <p>The federated server then sent back the improved and updated model     to the edge device. So, again, only IfD is collected, not the\u00a0data     itself.</p> </li> <li> <p>Now, let's look at another edge example in\u00a0the automotive sector in     the\u00a0following section.</p> </li> </ul> <p>Making autonomous driving happen with FL</p> <ul> <li> <p>Edge computing\u00a0with ML gains significant interest in AI industries     at scale, especially in the automotive field. Use cases such as     autonomous driving require low latency and real-time responses to     operate correctly. Therefore, FL becomes one of the best solutions     for the automotive field in terms of distributed data processing\u00a0and     training.</p> </li> <li> <p>Offloading computation and storage to edge IoT devices makes the     cloud systems for managing autonomous driving applications much     smaller and cheaper. That's the most powerful benefit of moving on     to the FL paradigm from central\u00a0cloud-based ML.</p> </li> <li> <p>Modern cars already have edge devices with complex computing     capabilities.\u00a0Advanced Driver Assistance Systems\u00a0(ADASs) are     the essential functions for autonomous cars where\u00a0calculations     happen onboard. They also require significant\u00a0computation power.</p> </li> <li> <p>The model is trained and prepared using regular, costly training     systems within on-premises servers or in the cloud even if the     prediction happens in the\u00a0autonomous vehicle.</p> </li> <li> <p>The training process will become more computationally expensive and     slower if the data becomes bigger and will require significant     storage\u00a0as well.</p> </li> <li> <p>FL needs to be used to avoid those issues because updated ML models     are passed between the\u00a0vehicles and the server where the car stores     the user driving patterns and streaming images from the onboard     camera. FL, again, can work in accordance with user consent and     adherence to privacy and\u00a0regional regulations.</p> </li> <li> <p>Figure 9.16\u00a0is about decentralized FL with multiple aggregators to     improve ADASs for safe driving, conducted as a real use case by     TieSet, Inc. with its\u00a0technological partner:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"4.0777777777777775in\"}</p> <p>Figure 9.16 -- Decentralized FL with multiple aggregators to improve ADASs for safe driving</p> <ul> <li> <p>The\u00a0ADAS integrated into the STADLE of TieSet, Inc., tailored to     provide comfort and personal safety measures, especially to senior     citizens, delivered optimized steering controlling assistance for     car products.</p> </li> <li> <p>Via advanced computer vision and\u00a0Reinforcement Learning\u00a0(RL)     technologies, they achieved a\u00a0design that provides prompt danger     situation awareness and intelligently learns about the     best-personalized\u00a0driving strategies.</p> </li> <li> <p>While personalization is a principal focus of the design,     substantial privacy issues come with personal data usage. The FL     framework enabled by the STADLE platform provided a realistic     solution to overcome this barrier.</p> </li> <li> <p>The architecture, presenting a collaborative form of ML training     distributed among edge users via AI intelligence exchanges, avoids     data transferal and ensures data privacy.</p> </li> <li> <p>Furthermore, the aggregated models can cope with various risky and     unpredictable situations beyond the personal experience\u00a0of drivers.</p> </li> <li> <p>During the\u00a0proof of concept using real cars, they successfully     demonstrated that the designed RL model could efficiently generate     the desirable steering strategy customized for the drivers using     STADLE's\u00a0aggregation framework.</p> </li> <li> <p>In the following section, we will talk about how FL could be applied     to the\u00a0robotics domain.</p> </li> </ul> <p>Applying FL to robotics</p> <ul> <li> <p>In\u00a0robotics systems and applications, ML\u00a0has already become an     integral and essential part of completing necessary tasks. Computer     vision has evolved to make robotics systems perform very well for     many tasks, such as image segmentation and object detection and     classification as well as NLP and signal processing tasks.</p> </li> <li> <p>ML can handle many robotics tasks, including perception, path     planning, sensor fusion, and grasping detected objects     in\u00a0manufacturing settings.</p> </li> <li> <p>However, ML in robotics also has many challenges. The first is the     training time. Even when the amount of data is enough to train the     ML models to solve the aforementioned problems, it takes weeks or     months to train an authentic robotics system.</p> </li> <li> <p>Equally, if the data is not sufficient, it can restrict the ML model     performance significantly. Often, data privacy and accessibility     become an issue for collecting enough data to train the ML models     for\u00a0the robots.</p> </li> <li> <p>That is why the FL framework is considered an essential solution to     the domain of robotics.</p> </li> <li> <p>Researchers at TieSet, Inc. developed a system and methods that     allow robotic manipulators and tools to share their manipulation     skills (including reaching, pick-and-place, holding, and grasping)     for objects of various types and shapes with other robots, as well     as use the skills of other robots to improve and expand\u00a0their own.</p> </li> <li> <p>This system covers the methods to create a general manipulation     model for robots that continuously improves by crowdsourcing skills     from various robotic agents while keeping the data private.</p> </li> <li> <p>They propose a new architecture where multiple AI-powered robotic     agents collaboratively train a global manipulation model by     submitting their models to an aggregator. This communication enables     each agent to utilize the training results of the agents by     receiving an optimally updated\u00a0global model.</p> </li> <li> <p>Figure 9.17\u00a0is the architecture showing how the federated     crowdsourced global manipulation framework for\u00a0robotics works:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"5.684027777777778in\"}</p> <p>Figure 9.17 -- Architecture of the federated crowdsourced global manipulation framework for robotics</p> <ul> <li> <p>Based on the\u00a0architecture of the preceding figure, in\u00a0the simulation     settings, they prepare five robotic arms for the individual tasks of     grabbing boxes, balls, ducks, and teddies.</p> </li> <li> <p>Using the STADLE platform by TieSet, Inc., which can conduct     asynchronous FL, the ML models from those arms are aggregated     continuously. In the end, the federated robotics ML model can grab     all these objects, whether boxes, balls, ducks, or teddies, with a     higher performance (an 80% success rate) in grabbing those objects,     as seen in\u00a0Figure 9.18:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.0875in\"}</p> <p>Figure 9.18 -- Arm robots can cross-train with different tasks for accuracy and efficiency</p> <ul> <li> <p>The FL based on\u00a0STADLE could significantly reduce\u00a0the time taken to     train robots and ML for production lines using computer vision.     Federated performance is much better than training individual robots     and the learning time is much faster than when training\u00a0individual     robots.</p> </li> <li> <p>In the following section, we will talk about AI at scale, where     learning should constantly happen even with numerous devices with     connected environments and the IoT should evolve into the     Internet\u00a0of Intelligence.</p> </li> </ul> <p>Moving toward the Internet of Intelligence</p> <ul> <li> <p>In this section, we will\u00a0talk about why FL is quite important in the     context of the latest development of scalable technologies, such as     the IoT and 5G.</p> </li> <li> <p>As in the previous section, the areas in which AI needs to keep     learning at scale include autonomous driving, retail systems, energy     management, robotics, and manufacturing, all of which generate a     huge amount of data on the edge side, and most of the data needs to     be fully learned to generate performant\u00a0ML models.</p> </li> <li> <p>Following this trend, let us look into the world of the Internet of     Intelligence, in which learning can happen on the edge side to cope     with dynamic environments and numerous devices connected to\u00a0the     Internet.</p> </li> </ul> <p>Introducing the IoFT</p> <ul> <li> <p>The IoT involves intelligent\u00a0and connected systems. They are     intelligent because the information is shared and intelligence is     extracted and used for some purpose -- for example, prediction or     control of a device. They are often connected to the cloud and are     able to collect data from\u00a0many endpoints.</p> </li> <li> <p>Figure 9.19\u00a0shows the current IoT system with more and more     data\u00a0over time:</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.9027777777777777in\"}</p> <p>Figure 9.19 -- A current IoT system</p> <ul> <li> <p>As shown in\u00a0Figure 9.19, in the current IoT flow, large amounts of     data must be uploaded and stored in\u00a0the cloud.</p> </li> <li> <p>The models train for specific purposes, such as predictive     maintenance and text prediction. Finally, the trained models are     sent back to the\u00a0edge devices.</p> </li> <li> <p>As you can see, there are several issues with the\u00a0current approach:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   A large amount of storage space\u00a0is needed</p> <ul> <li> <p>Latency is affected due to the amount\u00a0of data</p> </li> <li> <p>Privacy issues due to the movement\u00a0of data</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   To overcome these issues, FL plays an\u00a0important role.</p> <ul> <li> <p>The\u00a0Internet of Federated Things\u00a0(IoFT) is an\u00a0idea     originally presented by researchers at the University of Michigan,     whose paper\u00a0The Internet of Federated Things\u00a0(IoFT) is listed in     the\u00a0Further reading\u00a0section of This section. The IoFT is an     extended framework combining IoT with the concept of FL.</p> </li> <li> <p>As the computational power on the edge side has improved     significantly, AI chips have been penetrating the market rapidly.     Even smartphones have a really strong computing capability these     days and small but powerful computers are often attached to     most\u00a0edge devices.</p> </li> <li> <p>Therefore, the ML model training process is brought down to the edge     due to the increased computational capability of edge devices, and     the IoT's functionality of sending data to the server can be used to     transmit ML models to the cloud. This is also a very effective     approach to protecting private data on edge devices, such as\u00a0mobile     phones.</p> </li> <li> <p>Let's take a look at an example of the IoFT shown in\u00a0Figure 9.20.</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"5.33125in\"}</p> <p>Figure 9.20 -- An example of the IoFT</p> <ul> <li>Potential applications of the IoFT include distributed     manufacturing, traffic intersection control, and energy control, to     name\u00a0a few.</li> </ul> <p>Understanding the role of FL in Web 3.0</p> <ul> <li> <p>FL can be\u00a0integrated into Web 3.0 technologies to accelerate the     adoption of the Internet of Intelligence.</p> </li> <li> <p>The intelligence represented by ML models could be the property of     particular individuals or industries. At the same time, it could be     considered a public asset if it is something that could contribute     to the entire learning process of that ML model for people     worldwide.</p> </li> <li> <p>Whether private intellectual property or public assets, by utilizing     Web 3.0 technology, intelligence can be managed and evolved in a     decentralized manner.</p> </li> <li> <p>Therefore, more and more people will receive the benefits of     intelligence that people have collaboratively trained, which leads     to the true innovation of our entire society in various domains and     with\u00a0various applications.</p> </li> </ul> <p>Applying FL to distributed learning for big data</p> <ul> <li> <p>In this section, we will discuss how FL can be applied to     distributed learning in the context of\u00a0big data.</p> </li> <li> <p>FL for\u00a0big data may not be related to privacy-related issues so much     because the data needed for intelligence purposes is already     possessed. Therefore, it may be more applicable to efficient     learning for big data and improving training time significantly, as     well as reducing the costs of using huge servers, computation,\u00a0and     storage.</p> </li> <li> <p>There are several ways to conduct distributed learning on big data,     such as building a specific end-to-end ML stack applied to different     types of servers, such as parameter servers, or utilizing certain ML     schemes on top of big data platforms such as Hadoop and Spark.</p> </li> <li> <p>There are also some other platforms, such as GraphLab and Pregel.     You can use any libraries, and methods such as stochastic proximal     descent and coordinate descent with low-level utilities\u00a0for ML.</p> </li> <li> <p>These frameworks can support the parallel training of ML models     computationally, but will not be able to assign the data source to     different machines to train them locally in a distributed way,     especially when the training environments are dispersed over the     Internet.</p> </li> <li> <p>With FL, you can simply aggregate what different distributed     machines learn just by synchronizing the federation of the models,     but you do need to develop a well-designed platform to coordinate     the continuous operation of distributed learning, with proper model     repository and versioning approaches\u00a0as well.</p> </li> <li> <p>An example of conducting distributed learning on big data is     depicted in\u00a0Figure 9.21.</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.6145833333333335in\"}</p> <p>Figure 9.21 -- Distributed learning integration into big data</p> <ul> <li> <p>In the\u00a0example in\u00a0Figure 9.21, the data source, which is typically     very large, is sharded into multiple data sources to be dispersed     even into different machines or instances that are available for     training.</p> </li> <li> <p>Within an FL framework, trained models from distributed environments     are all aggregated. The trained and aggregated model then goes to     the processes of\u00a0ML Operations\u00a0(ML Ops) for\u00a0performance     validation and\u00a0continuous monitoring with\u00a0Model     Operations\u00a0(Model Ops).</p> </li> <li> <p>Another layer on top of the preceding scenario can be to combine the     insights from the other data sources. In this case, the FL can     elegantly combine the insights from the other data sources and     nicely coordinate the integration of the other forms of intelligence     directly created in the distributed environments.</p> </li> <li> <p>This way, you can also create the hybrid model of centralized ML and     distributed ML\u00a0as well.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>Here, we discussed many of the challenges facing different     industries in terms of AI advancements. The majority of the     challenges are related in some way to data accessibility. Issues     such as data privacy regulations, lack of real data, and data     transmission costs are all unique and challenging problems that we     expect to see FL continue to\u00a0help solve.</p> </li> <li> <p>Here, you learned about the use cases of the areas in which the FL     is playing a more and more important role, such as healthcare,     financial, edge, and IoT domains. The adherence to privacy that FL     offers is particularly important for the healthcare and financial     sectors, while FL can add significant value in terms of scalability     and learning efficiency to lots of edge AI and IoT scenarios. You     also learned how to apply FL to distributed learning for big data to     reduce training time\u00a0and costs.</p> </li> <li> <p>In the next and final section, we will wrap up this work by     discussing the very exciting future trends and developments in which     FL is expected to play a key role in the\u00a0coming decade.</p> </li> </ul> <p>Future Trends and Developments</p> <ul> <li> <p>Intelligence will drive the next generation of technologies, not big     data. Big data systems have some issues, as discussed in\u00a0section     1,\u00a0Challenges     in Big Data and Traditional AI, and the world is gradually     transitioning from the data-centric era to the intelligence-centric     generation.\u00a0Federated learning\u00a0(FL) will play a core role in     wisdom-driven technologies. Thus, the time is now to welcome the     world of collective intelligence.</p> </li> <li> <p>Here, we will talk about the direction of future AI technologies     that are driven by the paradigm shift happening with FL. For many AI     fields, such as privacy-sensitive areas and fields requiring     scalability in\u00a0machine learning\u00a0(ML), the benefits and     potential of FL are already significant, mainly because of the     privacy-preserving and distributed learning aspects that FL     naturally supports with its design. You will then learn about the     different types of FL as well as the latest development efforts in     that area, as seen in the split and swarm learning techniques, which     can be considered as evolutional frameworks enhancing FL.</p> </li> <li> <p>In addition, FL creates a new concept of an\u00a0Internet of     Intelligence, where people and computers exchange their wisdom     instead of just data themselves. The Internet of Intelligence for     everyone is further accelerated by blockchain technologies as well.     This Internet of Intelligence can then form a newly defined concept     of\u00a0collective intelligence\u00a0that drives another innovation,     from\u00a0data-centric\u00a0approaches     to\u00a0intelligence-centric\u00a0or\u00a0model-centric\u00a0approaches.</p> </li> <li> <p>Finally, we will share a collective vision in which FL plays a key     role in collaboratively creating intelligence learned by many people     and machines around the world.</p> </li> <li> <p>Here, we will cover the following topics:</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Looking at future AI trends</p> <ul> <li> <p>Ongoing research and developments in FL</p> </li> <li> <p>Journeying on to collective intelligence</p> </li> </ul> <p>Looking at future AI trends</p> <ul> <li>The majority of industry leaders are now aware of the limitations of     centralized ML as discussed in the next section.</li> </ul> <p>The limitation of centralized ML</p> <ul> <li> <p>When looking\u00a0at the future of AI, it is important to first know the     fact that many companies today are struggling to extract     intelligence and obtain insight from the data they possess.</p> </li> <li> <p>More than half of the data that organizations and companies have     collected is usually not used. Traditional approaches to machine     learning and data science need data to be organized and consolidated     into data lakes and stores in advance of analyzing and training ML     models.</p> </li> <li> <p>You need to duplicate and move the data, which will result in delays     in realizing and delivering the value of the intelligence extracted     from the data, together with certain operational risks and     complexities.</p> </li> <li> <p>In addition, most of the data generated by enterprise companies will     be created and processed outside a traditional centralized data     center or cloud. It is becoming increasingly unrealistic and     inefficient to process data for generating insight in a centralized     manner.</p> </li> <li> <p>Furthermore, based on some market reports out there, most of the     largest global organizations and companies will explore FL at least     once to create much more accurate, secure, and sustainable models     environmentally.</p> </li> <li> <p>That being said, quite a few industries and markets are gradually     becoming aware of the importance of a distributed and FL paradigm,     because they are facing the unavoidable issues and limitations of     the current centralized AI training with big data, as discussed     in\u00a0section     1,\u00a0Challenges     in Big Data and Traditional AI.</p> </li> <li> <p>FL brings the model to the data where the training process resides     instead of bringing the data to the model. Thus, FL is considered to     be the future of data science and ML.</p> </li> <li> <p>In the next section, let's summarize the points of why FL is     beneficial to those companies, especially enterprises that have been     facing the aforementioned issues.</p> </li> </ul> <p>Revisiting the benefits of FL</p> <ul> <li>In this section, we will summarize the benefits of FL that have been     introduced throughout this book.</li> </ul> <p>Increased model accuracy and generalizability</p> <ul> <li>FL realizes collaborative\u00a0and distributed learning that can improve     the performance of ML models, by training on dispersed datasets     locally to continuously incorporate the learning into a global     model. This way, more accurate and generalized ML models can be     produced.</li> </ul> <p>Further privacy and security</p> <ul> <li>FL provides privacy and security advantages because it won't require     private and raw data by its design and security mechanisms, as we     discussed previously in\u00a0section     2,\u00a0What     Is Federated Learning?\u00a0and\u00a0section     9,\u00a0Case     Studies with Key Use Cases of Federated Learning Applications.     Thus, FL\u00a0reduces the potential risk\u00a0of data misuse, leakage, or     exposure\u00a0to sensitive information. FL is also compliant with many     privacy regulations, such as\u00a0General Data Protection     Regulation\u00a0(GDPR),\u00a0California Consumer Privacy     Act\u00a0(CCPA), and\u00a0Health Insurance Portability and     Accountability Act\u00a0(HIPAA).</li> </ul> <p>Improved speed and efficiency</p> <ul> <li> <p>FL is also known to realize high computation efficiency, which can     accelerate the deployment and testing of ML models as well as     decrease communication and computational latency.</p> </li> <li> <p>Due to the decentralized nature of FL, the delay for model delivery     and update is minimized, which leads to a prediction by the global     model in near real time. Real-time delivery and updates of     intelligence are really valuable for time-sensitive ML applications.</p> </li> <li> <p>FL also helps reduce bandwidth and energy consumption by overcoming     system heterogeneity and unbalanced data distribution, which leads     to minimizing data storage and transfer costs that can also     significantly contribute to reducing the environmental impact.</p> </li> </ul> <p>Toward distributed learning for further privacy and training efficiency</p> <ul> <li> <p>Currently, AI is trained\u00a0on huge computational servers,     usually\u00a0happening on big machines in big data companies.</p> </li> <li> <p>As seen in the era of the supercomputer, which can process a huge     amount of data and tasks within one machine or one cluster of     machines, the evolutionary process in technology starts from a     central location and gradually transitions to distributed     environments.</p> </li> <li> <p>The same thing is exactly about to happen in AI. Now, the data lake     concept is popular to organize and train ML models in one place, but     ML already requires distributed learning frameworks.</p> </li> <li> <p>FL is a great way to distribute a training process over multiple     nodes. As shown in many research reports, most data is not fully     used to extract insights into ML models.</p> </li> <li> <p>There are some companies and projects that are trying to use FL as a     powerful distributed learning\u00a0technique, such as the\u00a0platforms     provided by Devron ([devron.ai]{.underline}),     FedML ([fedml.ai]{.underline}), and STADLE     ([stadle.ai]{.underline}).</p> </li> <li> <p>These platforms are already\u00a0resolving the issues discussed in\u00a0The     limitation of centralized AI\u00a0section and have shown a drastic     improvement in the ML process in various use cases, as stated in     the\u00a0Revisiting the benefits of FL\u00a0section.</p> </li> <li> <p>Based on the AI trends that we have\u00a0discussed, let's look into the     ongoing research\u00a0and developments related to FL that cutting-edge     companies are conducting now in the next section.</p> </li> </ul> <p>Ongoing research and developments in FL</p> <ul> <li>We now talk about the ongoing research\u00a0and development projects\u00a0that     are being taken place both in academia and industries around the     world. Let's start with the different types and approaches of FL,     and move on to ongoing efforts to further enhance the FL framework.</li> </ul> <p>Exploring various FL types and approaches</p> <ul> <li>In this work, we have visited\u00a0the most basic algorithms and design     concepts of an FL system. In the real world, we need to dig a bit     deeper into what types of FL frameworks are available to extract the     best performance out of those algorithms. Depending on the data     scenario and use cases, we have several approaches in FL, as     follows:</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Horizontal FL and vertical FL</p> <ul> <li> <p>Centralized FL and decentralized FL</p> </li> <li> <p>Cross-silo FL and cross-device FL</p> </li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Now, let's look at each type of FL in the following sections.</p> <p>Horizontal FL and vertical FL</p> <ul> <li> <p>Horizontal FL\u00a0uses datasets with the same feature\u00a0space or     schema across all distributed devices     ([https://www.arxiv-vanity.com/papers/1902.04885/]{.underline}).     This actually means that datasets\u00a0share the same columns with     different rows. Most existing FL projects are based on horizontal     FL. Datasets and training processes with horizontal FL are     straightforward because the datasets are formed identically, with     different data\u00a0distributions and inputs\u00a0to be learned. Horizontal FL     is also called homogeneous or sample-based FL.</p> </li> <li> <p>Vertical FL\u00a0is applied to the cases where different\u00a0datasets     share the same sample ID space\u00a0but differ in feature space. You can     check out this paper (https://arxiv.org/pdf/2202.04309) for further     information about vertical FL. Relating these different databases     through FL can be challenging, especially if the unique ID for the     data is different. The key idea of vertical FL is to improve an ML     model by using distributed datasets with a diverse set of     attributes. Therefore, vertical FL\u00a0can handle the partitioned     data\u00a0vertically with different attributes in the same sample space.     Vertical FL is also called heterogeneous or feature-based FL.</p> </li> </ul> <p>Centralized FL and decentralized FL</p> <ul> <li>Centralized FL\u00a0is currently the most common approach\u00a0and most of     the platforms employ this framework. It uses a centralized server to     collect and aggregate the different ML models, with distributed     training across all local data sources. In this book, we focused on     a centralized FL approach, with a scenario where local training     agents communicate the learning results to a centralized FL server     to create a global model.</li> </ul> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Decentralized FL, on the other hand, does not use a     centralized\u00a0server to aggregate ML models. It requires individual ML     models trained over local data sources to be communicated among     themselves without a master node. In this case, model weights are     transferred from each individual dataset to the others for further     training. It could potentially be susceptible to model poisoning if     an untrusted party could access the intelligence, and this is a     common problem derived from peer-to-peer frameworks as well.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Cross-silo FL and cross-device FL</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Cross-silo FL\u00a0is the case where ML models are trained\u00a0on data     distributed across any functional, organizational, and regulatory     barriers. In this case, big data is usually stored in a larger size     of storage, with training computing capabilities such as cloud     virtual machines. In the cross-silo FL case, the number of     silos/training environments is relatively small, so not so many     agents are needed in the FL process.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   Cross-device FL\u00a0is the case where models need to be trained\u00a0at     scale, often within edge devices, such as mobile phones,\u00a0Internet     of Things\u00a0(IoT) devices, Raspberry Pi-type\u00a0environments, and     so on. In this case, a huge number of devices are connected for the     aggregation of ML models. In the cross-device FL case, the     limitation basically lies in the low computing power of those edge     devices. The framework also needs to handle a number of disconnected     and inactive devices to conduct a consistent and continuous FL     process. The training process and its data volume should be limited     too.</p> <p><pre><code>&lt;!-- --&gt;\n</code></pre> -   That concludes the different types of FL that can be applied to a     variety of scenarios in ML applications. There are new techniques     that try to enhance the FL framework to evolve into the next     generation of AI technologies with FL. Let's look into several     advanced approaches in the next section.</p> <p>Understanding enhanced distributed learning frameworks with FL</p> <ul> <li>There are ongoing efforts\u00a0to further enhance FL or distributed     learning frameworks.</li> </ul> <p>Split learning</p> <ul> <li> <p>Split learning, developed in the MIT Media Lab, is an     emerging\u00a0distributed learning technique that enables partitioning ML     models into multiple sections, trains those partitioned ML models at     distributed clients, and aggregates them at the end. Split learning     does not have to share the data either, so it is considered a     privacy-preserving AI as well.</p> </li> <li> <p>The overall framework is similar to the FL. However, there is a     difference in that the neural network is partitioned into multiple     sections that will be trained on distributed clients.</p> </li> <li> <p>The trained weights of the section of the neural network are then     transferred to the server and clients.</p> </li> <li> <p>The weights of those multiple sections are continuously trained in     the next training sessions.</p> </li> <li> <p>Therefore, no raw and private data is shared among the distributed     clients, and only the weights of each section are sent to the next     client.</p> </li> <li> <p>Especially,\u00a0SplitFed\u00a0([https://arxiv.org/abs/2004.12088]{.underline})     is another advanced technique\u00a0that combines split learning\u00a0and FL.     SplitFed splits the deep neural network architecture between the FL     clients and servers to realize a higher level of privacy than FL. It     offers better efficiency than split learning based on the parallel     learning paradigm of FL.</p> </li> </ul> <p>Swarm learning</p> <ul> <li> <p>Swarm learning\u00a0is a decentralized ML solution built\u00a0on     blockchain technology, particularly designed to enable enterprise     industries to take advantage of the power of distributed data, which     results in protecting data privacy and security.</p> </li> <li> <p>This can be achieved by individual nodes sharing parameters of ML     models derived from the local data.</p> </li> <li> <p>Parameters shared from the distributed clients are merged into a     global model. The difference from the normal FL is that the merge     process is not performed by a central server.</p> </li> <li> <p>The distributed nodes and clients choose a temporary leader to     perform the merge. That is why swarm learning is truly     decentralized, also providing greater fault tolerance and     resiliency.</p> </li> <li> <p>The distributed agents have the collective intelligence of a network     without sharing local data into one node.</p> </li> <li> <p>Swarm learning builds\u00a0on top of blockchain. Blockchain provides the     decentralized control, scalability, and fault-tolerance aspects to     work beyond the restrictions of a single enterprise.</p> </li> <li> <p>At the same time, blockchain introduces a tamperproof cryptocurrency     framework, and the participants can use the framework to monetize     their contributions.</p> </li> </ul> <p>BAFFLE</p> <ul> <li> <p>In addition, there is a framework called\u00a0BAFFLE\u00a0that stands     for\u00a0Blockchain Based Aggregator Free Federated     Learning\u00a0(https://arxiv.org/abs/1909.07452). BAFFLE is also     an\u00a0aggregator-free, blockchain-driven FL framework\u00a0that is     inherently decentralized.</p> </li> <li> <p>BAFFLE utilizes\u00a0Smart Contracts\u00a0(SCs) from the blockchain     framework to coordinate\u00a0round management, as well as model     aggregation and updating tasks of FL. Using BAFFLE boosts     computational performance.</p> </li> <li> <p>The global model is also decomposed into many sets of chunks,     directly handled by the SC.</p> </li> <li> <p>Now that we have learned about the latest research and developments     in the FL field, in the next section, let's look at a more visionary     aspect of the AI, science, and technologies of collective     intelligence.</p> </li> </ul> <p>Journeying on to collective intelligence</p> <ul> <li> <p>Big data has been a game changer\u00a0for the AI movement. While the     amount of data generated at the edge and by people will increase     exponentially, intelligence derived from that data benefits society.     Therefore, the big data era will gradually pass the baton to the     collective intelligence era, empowered by FL, in which people will     collaboratively create a wisdom-driven world.</p> </li> <li> <p>Let's start by defining an intelligence-centric era where the     concept of collective intelligence is realized based on FL.</p> </li> </ul> <p>Intelligence-centric era with collective intelligence</p> <ul> <li> <p>Collective Intelligence\u00a0(CI) is the concept of a large group     of single entities acting together in ways\u00a0that seem intelligent. CI     is an emergent phenomenon where groups of people process information     to achieve insights that are not understandable by just individual     members alone.</p> </li> <li> <p>Recently, Thomas Malone, the head of the MIT Center for Collective     Intelligence, and the person\u00a0who initially coined the     phrase\u00a0collective intelligence, broadened the definition of     CI:\u00a0\"CI is something that can emerge from a group that includes     people and computers. CI is a very general property, and superminds     can arise in many kinds of systems, although the systems I've mostly     talked about are those that involve people and     computers\"\u00a0(Reference:\u00a0[https://www2.deloitte.com/xe/en/insights/focus/technology-and-the-future-of-work/human-and-machine-collaboration.html]{.underline}).</p> </li> <li> <p>We are now welcoming the new perspective of CI in technologies     empowered by FL.</p> </li> <li> <p>Data, in the current world of technology, is a great source to     extract intelligence. Dispersed datasets around the world can be     converted into a collection of intelligence represented by AI     technologies. The current trend, as mentioned, is big data, so big     data companies are leading not only the technology industries but     also the entire economy of the world as well. The future is moving     in a CI direction.</p> </li> <li> <p>The vision of CI is even clearer with the emergence of sophisticated     ML algorithms, including deep learning, as the intelligence     represented by ML models can extract intelligence from people,     computers, or any devices that generate meaningful data.</p> </li> <li> <p>Why does FL promote the idea of CI? The nature of FL is to collect a     set of distributed intelligence to be enhanced by an aggregating     mechanism as discussed in this book. This itself enables a data-less     platform that does not require collecting data from people or     devices directly.</p> </li> <li> <p>With the big data issues discussed throughout the book, we have     steered clear of focusing on data-centric platforms. However, it is     also true that learning big data is very much critical and     inevitable to really create systems and applications that are truly     valuable and deliver real value in many domains of the world. That     is why the big data field is still the most prosperous industry,     even if it is facing significant challenges represented by privacy     regulations, security, data silos, and so on.</p> </li> <li> <p>Now is the time to further develop and disseminate the technologies     such as FL that can accelerate the era of CI by fundamentally     resolving the issues of big data. This way, we can realize a new era     of technologies, truly driven by CI that has been backed up by an     authentic mathematical basis.</p> </li> <li> <p>As mentioned,\u00a0data-centric\u00a0platforms are the current trend. So     many\u00a0data and auto ML vendors can support and automate the processes     of creating ML-based intelligence by organizing data and learning     procedures to do so.     An\u00a0intelligence-centric\u00a0or\u00a0model-centric\u00a0platform should be the     next wave of technology\u00a0in which people can share and enhance     intelligence\u00a0that they generate on their own.</p> </li> <li> <p>With FL, we can even realize crowd-sourced learning, where people     can collaboratively and continuously enhance the quality and     performance of ML models.</p> </li> <li> <p>Thus, FL is a critical and essential part of the     intelligence-centric platform to truly achieve a     wisdom-driven\u00a0world.</p> </li> </ul> <p>Internet of Intelligence</p> <ul> <li> <p>The IoT evolved into the\u00a0Internet of Everything. However,     what\u00a0is the essential information\u00a0that people\u00a0want? Is it just\u00a0big     data? Or intelligence derived from data? With 5G technologies, a lot     of data can be transferred over the Internet at a much higher speed,     partially resolving the latency issues in many AI applications. FL     can exchange less information than raw data but still needs to     transfer ML models over the Internet.</p> </li> <li> <p>While lots of research projects are minimizing communications     latency in FL, in the future, information related to intelligence     will be another entity often exchanged over the web. There\u00a0will be a     model repository such as\u00a0Model Zoo\u00a0everywhere, and crowdsourced     learning empowered by FL will be more common to create better     intelligence over the Internet with people worldwide     collaboratively.</p> </li> <li> <p>This paradigm shift is not just in the AI field itself but also in     the wide range of information technologies. As we'll discuss in the     next sections, this\u00a0Internet of Intelligence\u00a0movement will be     the basis of crowdsourced learning and CI, and will help make     intelligence\u00a0available\u00a0to as many people as possible in the coming     years.</p> </li> </ul> <p>Crowdsourced learning with FL</p> <ul> <li> <p>The\u00a0collection of intelligence\u00a0performed by FL naturally\u00a0makes it     a strong fit\u00a0for moving toward CI. The same thing is applied to a     scenario where people can collectively contribute a training process     to global ML models.</p> </li> <li> <p>High-performing ML models in areas such as computer vision and     natural language processing have been trained by certain big data     companies, often spending a huge amount of money, including hundreds     of millions of dollars.</p> </li> <li> <p>Is there any way to collectively train an ML model that will     probably be beneficial for a wide range of people in general? With     the advanced framework of FL, that is possible.</p> </li> <li> <p>FL provides an authentic way to manage the aggregation of multiple     trained models from various distributed agents. In this case, the     distributed agents themselves may be people worldwide, where each     individual user and trainer of the ML model has their own unique     datasets that are not available to anybody else because of data     privacy, silos, and so on.</p> </li> <li> <p>This way of utilizing CI is often called\u00a0crowdsourced learning.     However, traditional crowdsourced learning\u00a0is conducted in a much     more limited way, just based on facilitating and recruiting data     annotators at a large scale.</p> </li> <li> <p>With this new paradigm with FL, users on the CI platform can access     and download ML models that they are interested in and retrain them     if necessary to absorb learning in their own environments. Then,     with the framework to share the trained ML models by those users, an     advanced aggregation framework of FL could pick up the appropriate     models to be federated and make the global model perform better,     adopting diverse data that can be only accessible to the users.</p> </li> <li> <p>This way, intelligence by ML is becoming more available to many     individuals in general, not just to specific companies that have a     significant amount of data and budgets to train an authentic ML     model. In other words, without an FL framework, collaborative     learning is difficult and tricky and almost impossible to even     automate. This openness of the ML models will move the entire     technological world to the next level, and a lot more     applications\u00a0will become feasible, with truly powerful intelligence     that is trained\u00a0by enthusiasts to make the world better.</p> </li> </ul> <p>Summary</p> <ul> <li> <p>In this final section of the book, we discussed fascinating future     trends and developments in which FL is expected to play a crucial     role in the coming decade. In the future, FL is     a\u00a0must-to-have\u00a0technology from a\u00a0nice-to-have\u00a0framework for most     enterprises and application providers, because of the inevitable     privacy regulations and technology trends requiring scalability with     so many users.</p> </li> <li> <p>As we discussed, future technologies will be empowered by the     concept of the Internet of Intelligence, by which people and     computers mainly exchange their wisdom altogether to create a more     intelligent society and world. Finally, the data-centric     technologies will gradually evolve into intelligence-centric     technologies because of the current collaborative learning trend     with CI, which makes people pay significant attention to FL-related     technologies, whose foundations are discussed throughout this book.</p> </li> <li> <p>This book was written at the dawn of a new age in advancements made     possible by AI. There are many uncertainties and many more     challenges ahead. We have made great strides in utilizing the big     data playbook in the last couple of decades, and we have now     outgrown those methods and must adopt new ways of doing things, new     technologies, and new ideas to forge ahead. As long as we capture     the current moment and invest in new technologies such as FL, we     will have a bright future ahead of us.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/","title":"Spanda Bootcamp Day 4   Very Raw","text":"<p>Table of contents</p> <p>Preface 1</p> <p>1 Introduction 3</p> <p>1.1 The Role of LLMs in NLP 3</p> <p>1.2 The Importance of Question Answering over PDFs 5</p> <p>1.3 The Retrieval-Augmented Generation Approach 6</p> <p>1.4 A Brief History of LLMs 7</p> <p>1.4.1 Foundation Models 8</p> <p>1.4.2 Reinforcement Learning from Human Feedback 10</p> <p>1.4.3 GAN 11</p> <p>1.4.4 Applications 11</p> <p>1.4.5 Prompt Learning 14</p> <p>2 Retrieval Augmented Generation (RAG) 17</p> <p>2.1 The Limitation of Generative AI 17</p> <p>2.1.1 Example: Transforming Customer Support with RAG 18</p> <p>2.1.2 How RAG Transforms Customer Support 19</p> <p>2.2 Introducing Retrieval Augmented Generation (RAG) 19</p> <p>2.2.1 Key Concepts and Components 20</p> <p>2.2.2 How It Improves Question Answering 21</p> <p>2.3 RAG Architecture 22</p> <p>2.4 Building the Retrieval System 24</p> <p>2.4.1 Choosing a Retrieval Model 24</p> <p>2.5 Embeddings and Vector Databases for Retrieval in RAG 25</p> <p>2.5.1 Vector Embeddings: An Overview 25</p> <p>2.5.2 Vector Databases and Their Role in Enhancing Retrieval 26</p> <p>Tableofcontents</p> <p>2.6 RAG Data Ingestion Pipeline . . . . . . . . . . . . . . . . . . . . 28</p> <p>2.7 Challenges of Retrieval-Augmented Generation . . . . . . . . . 28</p> <p>2.7.1 Data Quality and Relevance . . . . . . . . . . . . . . . . 29</p> <p>2.7.2 Integration Complexity . . . . . . . . . . . . . . . . . . 29</p> <p>2.7.3 Scalability . . . . . . . . . . . . . . . . . . . . . . . . . . 29</p> <p>2.7.4 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . 30</p> <p>2.7.5 Domain Adaptation . . . . . . . . . . . . . . . . . . . . . 30</p> <ol> <li> <p>RAG Pipeline Implementation 31</p> <ol> <li> <p>Preprocessing PDF documents . . . . . . . . . . . . . . . . . .     . 31</p> <ol> <li> <p>PDF Text Extraction . . . . . . . . . . . . . . . . . . . .     31</p> </li> <li> <p>Handling Multiple Pages . . . . . . . . . . . . . . . . . .     32</p> </li> <li> <p>Text Cleanup and Normalization . . . . . . . . . . . . . 32</p> </li> <li> <p>Language Detection . . . . . . . . . . . . . . . . . . . .     32</p> </li> </ol> </li> <li> <p>Data Ingestion Pipeline Implementation . . . . . . . . . . . . .     . 33</p> </li> <li> <p>Generation Component Implementation . . . . . . . . . . . . . 41</p> </li> <li> <p>Impact of Text Splitting on Retrieval Augmented Generation</p> </li> </ol> </li> </ol> <p>(RAG) Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45</p> <ol> <li> <p>Splitting by Character . . . . . . . . . . . . . . . . . . . 45</p> </li> <li> <p>Splitting by Token . . . . . . . . . . . . . . . . . . . . . 46</p> </li> <li> <p>Finding the Right Balance . . . . . . . . . . . . . . . . . 46</p> </li> <li> <p>Hybrid Approaches . . . . . . . . . . . . . . . . . . . . . 47</p> </li> </ol> <p><pre><code>&lt;!-- --&gt;\n</code></pre> 5.  Impact of Metadata in the Vector Database on Retrieval Aug-</p> <p>mented Generation (RAG) . . . . . . . . . . . . . . . . . . . . . 47</p> <ol> <li> <p>Contextual Clues . . . . . . . . . . . . . . . . . . . . . . 48</p> </li> <li> <p>Improved Document Retrieval . . . . . . . . . . . . . . . 48</p> </li> <li> <p>Contextual Response Generation . . . . . . . . . . . . . 49</p> </li> <li> <p>User Experience and Trust . . . . . . . . . . . . . . . . . 50</p> </li> </ol> <p><pre><code>&lt;!-- --&gt;\n</code></pre> 4.  From Simple to Advanced RAG 51</p> <pre><code>1.  Introduction . . . . . . . . . . . . . . . . . . . . . . . . . .\n    . . . 51\n</code></pre> <p>Retrieval Challenges . . . . . . . . . . . . . . . . . . . . . . . . . 52</p> <p>Generation Challenges . . . . . . . . . . . . . . . . . . . . . . . 52</p> <p>What Can be done . . . . . . . . . . . . . . . . . . . . . . . . . . 52</p> <p>Tableofcontents</p> <ol> <li> <p>Optimul Chunk Size for Efficient Retrieval . . . . . . . . . . . .     53</p> <ol> <li> <p>Balance Between Context and Efficiency . . . . . . . . . 53</p> </li> <li> <p>Additional Resources for RAG Evaluation . . . . . . . . 58</p> </li> </ol> </li> <li> <p>Retrieval Chunks vs. Synthesis Chunks . . . . . . . . . . . . . . 59</p> <ol> <li>Embed References to Text Chunks . . . . . . . . . . . . 60</li> </ol> </li> </ol> <p>Step 1: Read the PDF file . . . . . . . . . . . . . . . . . . . . . . 62</p> <p>Step 2: Create Document Summary Index . . . . . . . . . . . . . 63</p> <p>Step 3: Retrieve and Generate Response using Document Sum-</p> <p>mary Index . . . . . . . . . . . . . . . . . . . . . . . . . 63</p> <ol> <li>Expand sentence-level context window . . . . . . . . . . 68</li> </ol> <p>Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 69</p> <ol> <li> <p>Lost in the Middle Problem . . . . . . . . . . . . . . . . 72</p> </li> <li> <p>Embedding Optimization . . . . . . . . . . . . . . . . . . 77</p> </li> </ol> <p><pre><code>&lt;!-- --&gt;\n</code></pre> 4.  Rethinking Retrieval Methods for Heterogeneous Document</p> <p>Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80</p> <ol> <li>How metadata can help . . . . . . . . . . . . . . . . . . 81</li> </ol> <p><pre><code>&lt;!-- --&gt;\n</code></pre> 5.  Hybrid Document Retrieval . . . . . . . . . . . . . . . . . . . . .     88</p> <ol> <li>Query Rewriting for Retrieval-Augmented Large Language</li> </ol> <p>Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91</p> <ol> <li>Leveraging Large Language Models (LLMs) for Query</li> </ol> <p>Rewriting in RAGs . . . . . . . . . . . . . . . . . . . . . 95</p> <ol> <li> <p>Query Routing in RAG . . . . . . . . . . . . . . . . . . . . . . .     98</p> </li> <li> <p>Leveraging User History to Enhance RAG Performance . . . . . 103</p> <ol> <li> <p>Challenge . . . . . . . . . . . . . . . . . . . . . . . . . .     106</p> </li> <li> <p>How User History Enhances RAG Performance . . . . . 106</p> </li> <li> <p>How Memory/User History Works . . . . . . . . . . . . 107</p> </li> </ol> </li> </ol> <p><pre><code>&lt;!-- --&gt;\n</code></pre> 5.  Observability Tools for RAG 111</p> <pre><code>1.  Weights &amp; Biases Integration with LlamaIndex . . . . . . . . . .\n    113\n\n2.  Phoenix Integration with LlamaIndex . . . . . . . . . . . . . .\n    . 113\n\n3.  HoneyHive Integration with LlamaIndex . . . . . . . . . . . . .\n    116\n</code></pre> <ol> <li> <p>Ending Note 119</p> <ol> <li>Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . .     . 119</li> </ol> </li> </ol> <p>Tableofcontents</p> <p>References 121</p> <p>List of Figures</p> <ol> <li> <p>Examples of AIGC in image generation. Image source . . . . . . 8</p> </li> <li> <p>Overview of AIGC model types. Image source . . . . . . . . . . 9</p> </li> <li> <p>Categories of pre-trained LLMs. Image source . . . . . . . . . . 10</p> </li> <li> <p>Categories of vision generative models. Image source . . . . . . 12</p> </li> <li> <p>Knowlege Graph for Applications. Image source . . . . . . . . . 13     1.6 Emerging RAG &amp; Prompt Engineering Architecture for LLMs.</p> </li> </ol> <p>Image source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15</p> <ol> <li> <p>RAG architechture . . . . . . . . . . . . . . . . . . . . . . . . .     . 23</p> </li> <li> <p>Vector databases comparison. Image source . . . . . . . . . . . . 27</p> </li> <li> <p>RAG data ingestion pipeline . . . . . . . . . . . . . . . . . . . .     28</p> </li> </ol> <p><pre><code>&lt;!-- --&gt;\n</code></pre> 1.  Load pdf files . . . . . . . . . . . . . . . . . . . . . . . . . . .     . . 34</p> <ol> <li> <p>Langchain data loader . . . . . . . . . . . . . . . . . . . . . . .     . 35</p> </li> <li> <p>Langchain data loader output . . . . . . . . . . . . . . . . . . . .     35</p> </li> <li> <p>Langchain text split method . . . . . . . . . . . . . . . . . . . .     36</p> </li> <li> <p>Various vector databases. Image source . . . . . . . . . . . . . .     37</p> </li> <li> <p>Qdrant vector database setup via Langchain . . . . . . . . . . . 38</p> </li> <li> <p>Question answering example with output . . . . . . . . . . . . . 39</p> </li> <li> <p>The entire code for retrieval component . . . . . . . . . . . . . .     40</p> </li> <li> <p>RAG pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . .     . 41</p> </li> <li> <p>Response generation using Langchain chain . . . . . . . . . . . 43</p> </li> <li> <p>Using load_qa_with_sources_chain chain for response gen-</p> </li> </ol> <p>eration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43</p> <ol> <li>The usage of RetrievalQA chain . . . . . . . . . . . . . . . . . 44</li> </ol> <p>ListofFigures</p> <ol> <li>Code snippet for using Langchain RetrievalQAWith-</li> </ol> <p>SourcesChain for response generation . . . . . . . . . . . . . . 44</p> <ol> <li> <p>data preparation for reponse evaluation . . . . . . . . . . . . . .     55</p> </li> <li> <p>Define criteria for evaluation . . . . . . . . . . . . . . . . . . .     . 56</p> </li> <li> <p>Define a function to perform evaluation . . . . . . . . . . . . . .     57</p> </li> <li> <p>Run the evaluation function with different parameters . . . . . . 58</p> </li> <li> <p>Databricks evaluation experiment setup. Image source . . . . . 59</p> </li> <li> <p>Document summary index . . . . . . . . . . . . . . . . . . . . . 61</p> </li> <li> <p>Read a list of documents from each page of the pdf . . . . . . . 62</p> </li> <li> <p>Build a document summary index . . . . . . . . . . . . . . . . . 64</p> </li> <li> <p>Example of a document summary . . . . . . . . . . . . . . . . . 65</p> </li> <li> <p>High-level query execution approach (default approach) . . . . 66</p> </li> <li> <p>LLM based retrieval approach . . . . . . . . . . . . . . . . . . .     67</p> </li> <li> <p>Embedding based retrieval . . . . . . . . . . . . . . . . . . . . .     68</p> </li> <li> <p>Expanding the sentence level context, so LLM has a bigger con-</p> </li> </ol> <p>text to use to generate the response . . . . . . . . . . . . . . . . 69</p> <ol> <li> <p>Basic setup for sentence window implementation . . . . . . . . 70</p> </li> <li> <p>Build the sentence index, and run the query . . . . . . . . . . . 71</p> </li> <li> <p>Output of the window response . . . . . . . . . . . . . . . . . . 72</p> </li> <li> <p>Original sentence that was retrieved for each node, as well as</p> </li> </ol> <p>the actual window of sentences . . . . . . . . . . . . . . . . . . 73</p> <ol> <li>Accuracy of the RAG based on the postions of the retrieved doc-</li> </ol> <p>uments. Image source . . . . . . . . . . . . . . . . . . . . . . . . 74</p> <ol> <li>Comparing LLM models with various context size and the im-</li> </ol> <p>pact of changing the position of relevant documents . . . . . . . 75</p> <ol> <li> <p>Pseudocode of a function to solve lost in the middle problem . . 76</p> </li> <li> <p>Langchain approach for solving lost in the middle problem . . . 76</p> </li> <li> <p>Models by average English MTEB score (y) vs speed (x) vs em-</p> </li> </ol> <p>bedding size (circle size). Image source . . . . . . . . . . . . . . 78</p> <ol> <li> <p>Linear Identifiability. Image source . . . . . . . . . . . . . . . .     80</p> </li> <li> <p>How metadata filtering can improve retrieval process . . . . . . 82</p> </li> <li> <p>Read the files and update the metadata property . . . . . . . . . 83</p> </li> <li> <p>Output example of metadata for text chunks . . . . . . . . . . . 83</p> </li> </ol> <p>ListofFigures</p> <ol> <li> <p>Insert text chunks into the vector database and perform retrieval 84</p> </li> <li> <p>Metadata filtering in LlamaIndex for document retrieval . . . . . 85</p> </li> <li> <p>Define text node and metadata for auto retrieval . . . . . . . . .     86</p> </li> <li> <p>Define VectorIndexAutoRetriever retriever and VectorStoreInfo,</p> </li> </ol> <p>which contains a structured description of the vector store col-</p> <p>lection and the metadata filters it supports. . . . . . . . . . . . . 87</p> <ol> <li> <p>Hybrid retrieval pipeline . . . . . . . . . . . . . . . . . . . . .     . 89</p> </li> <li> <p>Load documents and initialize document store . . . . . . . . . . 92</p> </li> <li> <p>Define keyword and embedding based retrievers . . . . . . . . . 93</p> </li> <li> <p>Create end-to-end pipeline and run the retrievers . . . . . . . . 94</p> </li> <li> <p>Query re-writing using LLMs. LLM can expand the query or</p> </li> </ol> <p>create multiple sub-queries. . . . . . . . . . . . . . . . . . . . . . 95</p> <ol> <li> <p>Query router architecture . . . . . . . . . . . . . . . . . . . . .     . 99</p> </li> <li> <p>Using a zero-shot classifier to categorize and route user queries     101</p> </li> <li> <p>Using a LLM to categorize and route user queries . . . . . . . . 102</p> </li> <li> <p>Query routing example in LlamaIndex. First we load documents</p> </li> </ol> <p>and create different indecies. . . . . . . . . . . . . . . . . . . . . 104</p> <ol> <li>Define QueryEngine and RouterQueryEngine objects, and run</li> </ol> <p>the engine for user queries. . . . . . . . . . . . . . . . . . . . . . 105</p> <ol> <li>A basic key-value implementation of memory for RAG. . . . . . 108</li> </ol> <p><pre><code>&lt;!-- --&gt;\n</code></pre> 1.  General pattern for integrating observability tools into LlamaIn-</p> <p>dex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113</p> <ol> <li> <p>W&amp;B integration with LlamaIndex . . . . . . . . . . . . . . . . .     114</p> </li> <li> <p>W&amp;B logs at different steps . . . . . . . . . . . . . . . . . . . .     . 114</p> </li> <li> <p>W&amp;B dashboard . . . . . . . . . . . . . . . . . . . . . . . . . . .     115</p> </li> <li> <p>Phoenix integration with LlamaIndex RAG applications . . . . . 115</p> </li> <li> <p>Phoenix UI that shows traces of queries in real time. . . . . . . .     116</p> </li> <li> <p>HoneyHive integration with LlamaIndex . . . . . . . . . . . . . 117</p> </li> <li> <p>HoneyHive dashboard . . . . . . . . . . . . . . . . . . . . . . . .     117</p> </li> </ol> <p>1</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#1-introduction","title":"1 Introduction","text":"<p>In this chapter, we will lay the foundation for building a chat-to-PDF app using Large Language Models (LLMs) with a focus on the Retrieval-Augmented Generation approach. We'll explore the fundamental concepts and technologies that underpin this project.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#11-the-role-of-llms-in-nlp","title":"1.1 The Role of LLMs in NLP","text":"<p>Large Language Models (LLMs) play a crucial role in Natural Language Processing (NLP). These models have revolutionized the field of NLP by their ability to understand and generate human-like text. With advances in deep learning and neural networks, LLMs have become valuable assets in various NLP tasks, including language translation, text summarization, and chatbot development.</p> <p>One of the key strengths of LLMs lies in their capacity to learn from vast amounts of text data. By training on massive datasets, LLMs can capture complex linguistic patterns and generate coherent and contextually appropriate responses. This enables them to produce high-quality outputs that are indistinguishable from human-generated text.</p> <p>LLMs are trained using a two-step process: pre-training and fine-tuning. During pre-training, models are exposed to a large corpus of text data and learn to predict the next word in a sentence. This helps them develop a strong understanding of language structure and semantics. In the fine-tuning phase, the models are further trained on task-specific data to adapt their knowledge to specific domains or tasks. The versatility and effectiveness of LLMs make them a powerful tool in advancing the field of NLP. They have not only improved the performance of existing NLP systems but have also opened up new possibilities for developing innovative applications. With continued research and development, LLMs are expected to further push the boundaries of what is possible in natural language understanding and generation.</p> <p>Large Language Models (LLMs) represent a breakthrough in NLP, allowing machines to understand and generate human-like text at an unprecedented level of accuracy and fluency. Some of the key roles of LLMs in NLP include:</p> <ol> <li> <p>Natural Language Understanding (NLU): LLMs can comprehend the     nuances of human language, making them adept at tasks such as     sentiment analysis, entity recognition, and language translation.</p> </li> <li> <p>Text Generation: LLMs excel at generating coherent and     contextually relevant text. This capability is invaluable for     content generation, chatbots, and automated writing.</p> </li> <li> <p>Question Answering: LLMs are particularly powerful in question     answering tasks. They can read a given text and provide accurate     answers to questions posed in natural language.</p> </li> <li> <p>Summarization: LLMs can summarize lengthy documents or articles,     distilling the most important information into a concise form.</p> </li> <li> <p>Conversational AI: They serve as the backbone of conversational     AI systems, enabling chatbots and virtual assistants to engage in     meaningful and context-aware conversations.</p> </li> <li> <p>Information Retrieval: LLMs can be used to retrieve relevant     information from vast corpora of text, which is crucial for     applications like search engines and document retrieval.</p> </li> <li> <p>Customization: LLMs can be fine-tuned for specific tasks or     domains, making them adaptable to a wide range of applications.</p> </li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#12-the-importance-of-question-answering-over-pdfs","title":"1.2 The Importance of Question Answering over PDFs","text":"<p>Question answering over PDF documents addresses a critical need in information retrieval and document processing. Here, we'll explore why it is important and how LLMs can play a pivotal role:</p> <p>The Importance of Question Answering over PDFs:</p> <ol> <li> <p>Document Accessibility: PDF is a widely used format for storing     and sharing documents. However, extracting information from PDFs,     especially in response to specific questions, can be challenging for     users. Question answering over PDFs enhances document accessibility.</p> </li> <li> <p>Efficient Information Retrieval: For researchers, students, and     professionals, finding answers within lengthy PDF documents can be     time-consuming. Question-answering systems streamline this process,     enabling users to quickly locate the information they need.</p> </li> <li> <p>Enhanced User Experience: In various domains, including legal,     medical, and educational, users often need precise answers from PDF     documents. Implementing question answering improves the user     experience by providing direct and accurate responses.</p> </li> <li> <p>Automation and Productivity: By automating the process of     extracting answers from PDFs, organizations can save time and     resources. This automation can be particularly beneficial in     scenarios where large volumes of documents need to be processed.</p> </li> <li> <p>Scalability: As the volume of digital documents continues to     grow, scalable solutions for question answering over PDFs become     increasingly important. LLMs can handle large datasets and diverse     document types.</p> </li> </ol> <p>In various industries, there is a growing demand for efficient information retrieval from extensive collections of PDF documents. Take, for example, a legal firm or department collaborating with the Federal Trade Commission (FTC) to process updated information about legal cases and proceedings. Their task often involves processing a substantial volume of documents, sifting through them, and extracting relevant case information---a labour intensive process.</p> <p>Background: Every year the FTC brings hundreds of cases against individuals and companies for violating consumer protection and competition laws that the agency enforces. These cases can involve fraud, scams, identity theft, false advertising, privacy violations, anti-competitive behaviour and more.</p> <p>The advent of the Retrieval-Augmented Generation (RAG) approach marks a new era in question and answering that promises to revolutionize workflows within these industries.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#13-the-retrieval-augmented-generation-approach","title":"1.3 The Retrieval-Augmented Generation Approach","text":"<p>The Retrieval-Augmented Generation approach is a cutting-edge technique that combines the strengths of information retrieval and text generation. Let's explore this approach in detail:</p> <p>The Retrieval-Augmented Generation Approach:</p> <p>The Retrieval-Augmented Generation approach combines two fundamental components, retrieval and generation, to create a powerful system for question answering and content generation. Here's an overview of this approach:</p> <ol> <li> <p>Retrieval Component: This part of the system is responsible for     searching and retrieving relevant information from a database of     documents. It uses techniques such as indexing, ranking, and query     expansion to find the most pertinent documents.</p> </li> <li> <p>Generation Component: Once the relevant documents are retrieved,     the generation component takes over. It uses LLMs to process the     retrieved information and generate coherent and contextually     accurate responses to user queries.</p> </li> <li> <p>Benefits: The key advantage of this approach is its ability to     provide answers based on existing knowledge (retrieval) while also     generating contextually rich responses (generation). It combines the     strengths of both worlds to deliver high-quality answers.</p> </li> <li> <p>Use Cases: Retrieval-Augmented Generation is particularly useful     for question answering over large document collections, where     traditional search engines may fall short in providing concise and     informative answers.</p> </li> <li> <p>Fine-Tuning: Successful implementation of this approach often     involves fine-tuning LLMs on domain-specific data to improve the     quality of generated responses.</p> </li> </ol> <p>By understanding the role of LLMs in NLP, the importance of question answering over PDFs, and the principles behind the Retrieval-Augmented Generation approach, you have now laid the groundwork for building your chat-to-PDF app using these advanced technologies. In the following chapters, we will delve deeper into the technical aspects and practical implementation of this innovative solution.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#14-a-brief-history-of-llms","title":"1.4 A Brief History of LLMs","text":"<p>Lately, ChatGPT, as well as DALL-E-2 and Codex, have been getting a lot of attention. This has sparked curiosity in many who want to know more about what's behind their impressive performance. ChatGPT and other Generative AI (GAI) technologies fall into a category called Artificial Intelligence Generated Content (AIGC). This means they're all about using AI models to create content like images, music, and written language. The whole idea behind AIGC is to make creating conetent faster and easier.</p> <p>AIGC is achieved by extracting and understanding intent information from instructions provided by human, and generating the content according to its knowledge and the intent information. In recent years, large-scale models have become increasingly important in AIGC as they provide better intent extraction and thus, improved generation results.</p> <p>With more data and bigger models, these AI systems can make things that look and sound quite realistic and high-quality. The following shows an example of text prompting that generates images according to the instructions, leveraging the OpenAI DALL-E-2 model.</p> <p>{width=\"4.857113954505687in\" height=\"1.3914140419947507in\"}</p> <p>Figure 1.1: Examples of AIGC in image generation. Image source</p> <p>In the realm of Generative AI (GAI), models can typically be divided into two categories: unimodal models and multimodal models. Unimodal models operate by taking instructions from the same type of data as the content they generate, while multimodal models are capable of receiving instructions from one type of data and generating content in another type. The following figure illustrates these two categories of models.</p> <p>These models have found applications across diverse industries, such as art and design, marketing, and education. It's evident that in the foreseeable future, AIGC will remain a prominent and continually evolving research area with artificial intelligence.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#141-foundation-models","title":"1.4.1 Foundation Models","text":"<p>Speaking of LLMs and GenAI, we cannot overlook the significant role played by Transformer models.</p> <p>{width=\"4.857123797025372in\" height=\"1.893879046369204in\"}</p> <p>Figure 1.2: Overview of AIGC model types. Image source</p> <p>Transformer is the backbone architecture for many state-of-the-art models, such as GPT, DALL-E, Codex, and so on.</p> <p>Transformer started out to address the limitations of traditional models like RNNs when dealing with variable-length sequences and context. The heart of the Transformer is its self-attention mechanism, allowing the model to focus on different parts of an input sequence. It comprises an encoder and a decoder. The encoder processes the input sequence to create hidden representations, while the decoder generates an output sequence. Each encoder and decoder layer includes multi-head attention and feed-forward neural networks. Multi-head attention, a key component, assigns weights to tokens based on relevance, enhancing the model's performance in various NLP tasks. The Transformer's inherent parallelizability minimizes inductive biases, making it ideal for large-scale pretraining and adaptability to different downstream tasks.</p> <p>Transformer architecture has dominated natural language processing, with two main types of pre-trained language models based on training tasks: masked language modeling (e.g., BERT) and autoregressive language modeling (e.g., GPT3). Masked language models predict masked tokens within a sentence, while autoregressive models focus on predicting the next token given previous ones, making them more suitable for generative tasks. RoBERTa and XL-Net are classic examples of masked language models and have further improved upon the BERT architecture with additional training data and techniques.</p> <p>{width=\"4.857099737532808in\" height=\"1.7512346894138233in\"}</p> <p>Figure 1.3: Categories of pre-trained LLMs. Image source</p> <p>In this graph, you can see two types of information flow indicated by lines: the black line represents bidirectional information flow, while the gray line represents left-to-right information flow. There are three main model categories:</p> <ol> <li> <p>Encoder models like BERT, which are trained with context-aware     objectives.</p> </li> <li> <p>Decoder models like GPT, which are trained with autoregressive     objectives.</p> </li> <li> <p>Encoder-decoder models like T5 and BART, which merge both     approaches. These models use context-aware structures as encoders     and left-to-right structures as decoders.</p> </li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#142-reinforcement-learning-from-human-feedback","title":"1.4.2 Reinforcement Learning from Human Feedback","text":"<p>To improve AI-generated content (AIGC) alignment with user intent, i.e., considerations in usefulness and truthfulness, reinforcement learning from human feedback (RLHF) has been applied in models like Sparrow, InstructGPT, and ChatGPT.</p> <p>The RLHF pipeline involves three steps: pre-training, reward learning, and finetuning with reinforcement learning. In reward learning, human feedback on diverse responses is used to create reward scalars. Fine-tuning is done through reinforcement learning with Proximal Policy Optimization (PPO), aiming to maximize the learned reward.</p> <p>However, the field lacks benchmarks and resources for RL, which is seen as a challenge. But this is changing day-by day. For example, an open-source library called RL4LMs was introduced to address this gap. Claude, a dialogue agent, uses Constitutional AI, where the reward model is learned via RL from AI feedback. The focus is on reducing harmful outputs, with guidance from a set of principles provided by humans. See more about the topic of Constitutional AI in one of our blog post here.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#143-gan","title":"1.4.3 GAN","text":"<p>Generative Adversarial Networks (GANs) are widely used for image generation. GANs consist of a generator and a discriminator. The generator creates new data, while the discriminator decides if the input is real or not.</p> <p>The design of the generator and discriminator influences GAN training and performance. Various GAN variants have been developed, including LAPGAN, DCGAN, Progressive GAN, SAGAN, BigGAN, StyleGAN, and methods addressing mode collapse like D2GAN and GMAN.</p> <p>The following graph illustrates some of the categories of vision generative models.</p> <p>Although GAN models are not the focus of our book, they are essential in powering multi-modality applications such as the diffusion models.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#144-applications","title":"1.4.4 Applications","text":"<p>Chatbots are probably one of the most popular applications for LLMs.</p> <p>{width=\"4.857048337707787in\" height=\"2.4782884951881017in\"}</p> <p>Figure 1.4: Categories of vision generative models. Image source</p> <p>Chatbots are computer programs that mimic human conversation through textbased interfaces. They use language models to understand and respond to user input. Chatbots have various use cases, like customer support and answering common questions. Our \"chat with your PDF documents\" is a up-and-coming use case!</p> <p>Other notable examples include Xiaoice, developed by Microsoft, which expresses empathy, and Google's Meena, an advanced chatbot. Microsoft's Bing now incorporates ChatGPT, opening up new possibilities for chatbot development.</p> <p>This graph illustrates the relationships among current research areas, applications, and related companies. Research areas are denoted by dark blue circles, applications by light blue circles, and companies by green circles.</p> <p>In addition, we have previously written about chatbots and now they are part of history, but still worth reviewing:</p> <ul> <li>Blog post: What Does A Chatbot Look Like Under the Hood?</li> </ul> <p>{width=\"4.857147856517935in\" height=\"3.0453543307086615in\"}</p> <p>Figure 1.5: Knowlege Graph for Applications. Image source</p> <ul> <li> <p>Blogpost: What Is Behind the Scene of A Chatbot NLU?</p> </li> <li> <p>Blogpost: What More Can You Do with Chatbots?</p> </li> </ul> <p>Of course, chatbots are not the only application. There are vast possibilities in arts and design, music generation, education technology, coding and beyond your imagination doesn't need to stop here.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#145-prompt-learning","title":"1.4.5 Prompt Learning","text":"<p>Prompt learning is a new concept in language models. Instead of predicting\ud835\udc65 \ud835\udc65^\u2032^ \ud835\udc43(\ud835\udc66|\ud835\udc65^\u2032^) \ud835\udc66</p> <p>given , it aims to find a template that predicts .</p> <p>Normally, prompt learning will freeze the language model and directly perform few-shot or zero- shot learning on it. This enables the language models to be pre-trained on large amount of raw text data and be adapted to new domains without tuning it again. Hence, prompt learning could help save much time and efforts.</p> <p>Traditionally, prompt learning involves prompting the model with a task, and it can be done in two stages: prompt engineering and answer engineering.</p> <p>Prompt engineering: This involves creating prompts, which can be either discrete (manually designed) or continuous (added to input embeddings) to convey task-specific information.</p> <p>Answer engineering: After reformulating the task, the generated answer must be mapped to the correct answer space.</p> <p>Besides single-prompt, multi-prompt methods combine multiple prompts for better predictions, and prompt augmentation basically beefs up the prompt to generate better results.</p> <p>Moreover, in-context learning, a subset of prompt learning, has gained popularity. It enhances model performance by incorporating a pre-trained language model and supplying input-label pairs and task-specific instructions to improve alignment with the task.</p> <p>Overall, in the dynamic landscape of language models, tooling and applications, the graph below illustrates to the evolution of language model engineering. With increasing flexibility along the x-axis and rising complexity along the yaxis, this graph offers a bird's-eye view of the choices and challenges faced by developers, researchers and companies.</p> <p>{width=\"4.857174103237095in\" height=\"2.8068908573928257in\"}</p> <p>Figure 1.6: Emerging RAG &amp; Prompt Engineering Architecture for LLMs. Image</p> <p>source</p> <p>In the top-right corner, you can see the complex, yet powerful tools like OpenAI, Cohere, and Anthropic (to-be-added), which have pushed the boundaries of what language models can achieve. Along the diagonal, the evolution of prompt engineering is displayed, from static prompts to templates, prompt chaining, RAG pipelines, autonomous agents, and prompt tuning. On the more flexible side, options like Haystack and LangChain have excelled, presenting broader horizons for those seeking to harness the versatility of language models.</p> <p>This graph serves as a snapshot of the ever-evolving landscape of toolings in the realm of language model and prompt engineering today, providing a roadmap for those navigating the exciting possibilities and complexities of this field. It is likely going to be changing every day, reflecting the continuous innovation and dynamism in the space.</p> <p>In the next Chapter we'll turn our focus to more details of Retrieval Augmented Generation (RAG) pipelines. We will break down their key components, architecture, and the key steps involved in building an efficient retrieval system.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#2-retrieval-augmented-generation-rag","title":"2 Retrieval Augmented Generation (RAG)","text":"<p>Generative AI, a subset of artificial intelligence, has revolutionized the field of text generation. It has paved the way for machines to generate human-like text, offering a myriad of benefits in various applications. From content creation and chatbots to language translation and natural language understanding, generative AI has proven to be a powerful tool in the world of natural language processing. However, it is essential to recognize that despite its remarkable capabilities, generative AI systems have limitations, one of which is their reliance on the data they have been trained on to generate responses.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#21-the-limitation-of-generative-ai","title":"2.1 The Limitation of Generative AI","text":"<p>Generative AI models, such as GPT3 (Generative Pre-trained Transformer), have been trained on vast datasets containing text from the internet. While this training process equips them with a broad understanding of language and context, it also introduces limitations. These models can only generate text that aligns with the patterns and information present in their training data. As a result, their responses may not always be accurate or contextually relevant, especially when dealing with niche topics or recent developments that may not be adequately represented in their training data.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#211-example-transforming-customer-support-with-rag","title":"2.1.1 Example: Transforming Customer Support with RAG","text":"<p>Imagine you're the owner of a thriving e-commerce platform, selling a wide range of products from electronics to fashion. You've recently integrated a chatbot to assist your customers with inquiries, but you're starting to see its limitations. Let's explore how Retrieval-Augmented Generation (RAG) can help overcome these limitations and enhance the customer support experience.</p> <p>Limitations of a Traditional Large Language Model (LLM)</p> <p>Your existing chatbot is built around a traditional Large Language Model (LLM). While it's knowledgeable about general product information, your customers are increasingly seeking more specific and real-time assistance. Here are some challenges you've encountered:</p> <p>Product Availability: Customers often inquire about the availability of specific items, especially during sales or promotions. The LLM can provide information based on its training data, but it doesn't have access to real-time inventory data.</p> <p>Shipping and Delivery: Customers frequently ask about shipping times, tracking information, and potential delays. The LLM can provide standard shipping policies, but it can't offer real-time updates on the status of an individual order.</p> <p>Product Reviews: Shoppers want to know about recent product reviews and ratings to make informed decisions. The LLM lacks access to the latest customer reviews and sentiment analysis.</p> <p>Promotions and Discounts: Customers seek information about ongoing promotions, discounts, and special offers. The LLM can only provide details based on the data it was trained on, missing out on time-sensitive deals.</p> <p>2.2IntroducingRetrievalAugmentedGeneration(RAG)</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#212-how-rag-transforms-customer-support","title":"2.1.2 How RAG Transforms Customer Support","text":"<p>Now, let's introduce RAG into your e-commerce customer support system:</p> <p>Retrieval of Real-Time Data: With RAG, your chatbot can connect to your ecommerce platform's databases and data warehouses in real-time. It can retrieve the latest information about product availability, stock levels, and shipping status.</p> <p>Incorporating User Reviews: RAG can scrape and analyze customer reviews and ratings from your website, social media, and other sources. It can then generate responses that include recent reviews, helping customers make informed choices.</p> <p>Dynamic Promotions: RAG can access your promotion database and provide up-to-the-minute details about ongoing discounts, flash sales, and limited-time offers. It can even suggest personalized promotions based on a user's browsing history.</p> <p>Order Tracking: RAG can query your logistics system to provide customers with real-time tracking information for their orders. It can also proactively notify customers of any delays or issues.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#22-introducing-retrieval-augmented-generation-rag","title":"2.2 Introducing Retrieval Augmented Generation (RAG)","text":"<p>To address the limitation of generative AI, researchers and engineers have developed innovative approaches, one of which is the Retrieval Augmented Generation (RAG) approach. RAG initially caught the interest of generative AI developers following the release of a seminal paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" (Lewis et al. 2020) at Facebook AI Research. RAG combines the strengths of generative AI with retrieval techniques to enhance the quality and relevance of generated text. Unlike traditional generative models that rely solely on their internal knowledge, RAG incorporates an additional step where it retrieves information from external sources, such as databases, documents, or the web, before generating a response. This integration of retrieval mechanisms empowers RAG to access up-to-date information and context, making it particularly valuable for applications where accurate and current information is critical.</p> <p>In this chapter, we will delve deeper into the Retrieval Augmented Generation (RAG) approach, exploring its architecture, advantages, and real-world applications. By doing so, we will gain a better understanding of how RAG represents a significant step forward in improving the capabilities of generative AI and overcoming the limitations posed by reliance on static training data. Understanding the key concepts and components of this approach is essential for building an effective chat-to-PDF app.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#221-key-concepts-and-components","title":"2.2.1 Key Concepts and Components","text":"<p>To grasp the essence of Retrieval-Augmented Generation, let's explore its key concepts and components:</p> <ol> <li> <p>Retrieval Component: The retrieval component is responsible for     searching and selecting relevant information from a database or     corpus of documents. This component utilizes techniques like     document indexing, query expansion, and ranking to identify the most     suitable documents based on the user's query.</p> </li> <li> <p>Generation Component: Once the relevant documents are retrieved,     the generation component takes over. It leverages Large Language     Models (LLMs) such as GPT-3 to process the retrieved information and     generate coherent and contextually accurate responses. This     component is responsible for converting retrieved facts into     human-readable answers.</p> </li> <li> <p>Interaction Loop: Retrieval-Augmented Generation often involves     an interaction loop between the retrieval and generation components.     The</p> </li> </ol> <p>2.2IntroducingRetrievalAugmentedGeneration(RAG)</p> <p>initial retrieval may not always return the perfect answer, so the generation component can refine and enhance the response iteratively by referring back to the retrieval results.</p> <ol> <li> <p>Fine-Tuning: Successful implementation of this approach often     requires fine-tuning LLMs on domain-specific data. Fine-tuning     adapts the model to understand and generate content relevant to the     specific knowledge domain, improving the quality of responses.</p> </li> <li> <p>Latent Space Representations: Retrieval models often convert     documents and queries into latent space representations, making it     easier to compare and rank documents based on their relevance to a     query. These representations are crucial for efficient retrieval.</p> </li> <li> <p>Attention Mechanisms: Both the retrieval and generation     components typically employ attention mechanisms. Attention     mechanisms help the model focus on the most relevant parts of the     input documents and queries, improving the accuracy of responses.</p> </li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#222-how-it-improves-question-answering","title":"2.2.2 How It Improves Question Answering","text":"<p>The Retrieval-Augmented Generation approach offers several advantages for question answering:</p> <ol> <li> <p>Access to a Wide Knowledge Base: By integrating retrieval, the     system can access a vast knowledge base, including large document     collections. This enables the model to provide answers that may not     be present in its pre-training data, making it highly informative.</p> </li> <li> <p>Contextual Understanding: The generation component uses the     context provided by the retrieval results to generate answers that     are not only factually accurate but also contextually relevant. This     contextual understanding leads to more coherent and precise     responses.</p> </li> <li> <p>Iterative Refinement: The interaction loop between retrieval and     generation allows the system to iteratively refine its responses. If     the initial response is incomplete or incorrect, the generation     component can make further inquiries or clarifications based on the     retrieval results, leading to improved answers.</p> </li> <li> <p>Adaptability to Diverse Queries: Retrieval-Augmented Generation     can handle a wide range of user queries, including complex and     multifaceted questions. It excels in scenarios where simple     keyword-based search engines may fall short.</p> </li> <li> <p>Fine-Tuning for Specific Domains: By fine-tuning the model on     domain-specific data, you can tailor it to excel in particular     knowledge domains. This makes it a valuable tool for specialized     question answering tasks, such as legal or medical consultations.</p> </li> </ol> <p>In summary, Retrieval-Augmented Generation is a dynamic approach that combines the strengths of retrieval and generation to provide accurate, contextually relevant, and informative answers to user queries. Understanding its key components and advantages is essential as we move forward in building our chatto-PDF app, which will leverage this approach to enhance question answering over PDF documents.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#23-rag-architecture","title":"2.3 RAG Architecture","text":"<p>At its core, RAG is a framework that synergizes two vital components:</p> <p>Retrieval Model: This component specializes in searching and retrieving relevant information from extensive datasets, such as documents, articles, or databases. It identifies passages or documents that contain information related to a user's query.</p> <p>Generation Model: On the other hand, the generation model excels in crafting coherent and contextually rich responses to user queries. It's often based on large language models (LLMs) like GPT-3, which can generate human-like text.</p> <p>Figure 2.1 shows the RAG architecture.</p> <p>2.3RAGArchitecture</p> <p>{width=\"5.08923665791776in\" height=\"3.4895833333333335in\"}</p> <p>Figure 2.1: RAG architechture</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#24-building-the-retrieval-system","title":"2.4 Building the Retrieval System","text":"<p>In this section, we will focus on building the retrieval system, a critical component of the chat-to-PDF app that enables the extraction of relevant information from PDF documents. This section is essential for implementing the RetrievalAugmented Generation approach effectively.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#241-choosing-a-retrieval-model","title":"2.4.1 Choosing a Retrieval Model","text":"<p>Choosing the right retrieval model is a crucial decision when building your chat-to-PDF app. Retrieval models determine how efficiently and accurately the system can find and rank relevant documents in response to user queries. Here are some considerations when selecting a retrieval model:</p> <ul> <li> <p>TF-IDF (Term Frequency-Inverse Document Frequency): TF-IDF is a     classical retrieval model that calculates the importance of terms in     a document relative to a corpus. It's simple to implement and     effective for certain tasks.</p> </li> <li> <p>BM25: BM25 is an improved version of TF-IDF that accounts for     document length and term saturation. It's often more effective in     modern retrieval tasks.</p> </li> <li> <p>Vector Space Models: These models represent documents and     queries as vectors in a high-dimensional space. Cosine similarity or     other distance metrics are used to rank documents. Implementations     like Latent Semantic Analysis (LSA) and Word Embeddings (e.g.,     Word2Vec) can be used.</p> </li> <li> <p>Neural Ranking Models: Modern neural models, such as BERT-based     models, are increasingly popular for retrieval tasks due to their     ability to capture complex semantic relationships. They can be     fine-tuned for specific tasks and domains.</p> </li> <li> <p>Hybrid Models: Combining multiple retrieval models, such as a     combination of TF-IDF and neural models, can offer the benefits of     both approaches.</p> </li> </ul> <p>2.5EmbeddingsandVectorDatabasesforRetrievalinRAG</p> <ul> <li> <p>Domain and Data Size: Consider the specific requirements of your     chatto-PDF app. Some retrieval models may be more suitable for small     or specialized document collections, while others excel in handling     large, diverse corpora.</p> </li> <li> <p>Scalability: Ensure that the chosen retrieval model can scale to     meet the needs of your application, especially if you anticipate     handling a substantial volume of PDF documents.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#25-embeddings-and-vector-databases-for-retrieval-in-rag","title":"2.5 Embeddings and Vector Databases for Retrieval in RAG","text":"<p>In addition to selecting an appropriate retrieval model, leveraging embeddings and vector databases can significantly enhance the performance and efficiency of the retrieval component within your chat-to-PDF app. Vector embeddings are a fundamental concept in modern information retrieval and natural language processing. They transform textual data into numerical vectors, enabling computers to understand and manipulate text data in a mathematical, geometric space. These embeddings capture semantic and contextual relationships between words, documents, or other textual entities, making them highly valuable in various applications, including the retrieval component of Retrieval Augmented Generation (RAG).</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#251-vector-embeddings-an-overview","title":"2.5.1 Vector Embeddings: An Overview","text":"<p>Vector embeddings represent words, phrases, sentences, or even entire documents as points in a high-dimensional vector space. The key idea is to map each textual element into a vector in such a way that semantically similar elements are located close to each other in this space, while dissimilar elements are further apart. This geometric representation facilitates similarity calculations, clustering, and other operations.</p> <p>Examples of Vector Embeddings:</p> <ol> <li> <p>Word Embeddings (Word2Vec, GloVe): Word embeddings represent     individual words as vectors. For example, \"king\" and \"queen\" may be     represented as vectors that are close together in the vector space     because they share similar semantic properties.</p> </li> <li> <p>Document Embeddings (Doc2Vec, BERT): Document embeddings map     entire documents (such as PDFs) into vectors. Two documents     discussing similar topics will have embeddings that are close in the     vector space.</p> </li> </ol> <p>There are abundent of tutorials and resources that can help you learn more about vector embeddings. Here are some resources that can help you get started:</p> <ul> <li> <p>Vector Embeddings Explained</p> </li> <li> <p>Google Vector embeddings</p> </li> <li> <p>What are vector embeddings</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#252-vector-databases-and-their-role-in-enhancing-retrieval","title":"2.5.2 Vector Databases and Their Role in Enhancing Retrieval","text":"<p>Vector databases, also known as similarity search engines or vector index databases, play a crucial role in the retrieval component of RAG by efficiently storing and retrieving these vector embeddings. They are specialized databases designed for retrieving vectors based on similarity, making them well-suited for scenarios where similarity between data points needs to be calculated quickly and accurately.</p> <p>How Vector Databases Enhance Retrieval in RAG:</p> <ol> <li>Fast Retrieval: Vector databases employ indexing structures     optimized for similarity search. They use algorithms like     approximate nearest neighbor (ANN) search to quickly locate the most     similar vectors, even in large datasets containing numerous     documents.</li> </ol> <p>2.5EmbeddingsandVectorDatabasesforRetrievalinRAG</p> <ol> <li> <p>Scalability: Vector databases can efficiently scale as the     corpus of documents grows. This ensures that retrieval performance     remains consistent, regardless of the dataset's size.</p> </li> <li> <p>Advanced Similarity Scoring: These databases offer a range of     similarity metrics, such as cosine similarity or Jaccard index,     allowing you to fine-tune the relevance ranking of retrieved     documents based on your specific requirements.</p> </li> <li> <p>Integration with Retrieval Models: Vector databases can be     seamlessly integrated into your retrieval system. They complement     retrieval models like TF-IDF, BM25, or neural ranking models by     providing an efficient means of candidate document selection based     on vector similarity.</p> </li> </ol> <p>All of these factors has resulted in numerous new vector databases. Selecting and depending on one of these databases can have long-lasting consequences and dependencies within your system. Ideally, we opt for a vector database that exhibits strong scalability, all while maintaining cost-efficiency and minimizing latency. Some of these vector databases are: Qdrant, Weaviate, Pinecone, pgvector, Milvus, and Chroma.</p> <p>{width=\"4.8567322834645665in\" height=\"2.1673359580052494in\"}</p> <p>Figure 2.2: Vector databases comparison. Image source</p> <ol> <li>RAG Data Ingestion Pipeline</li> </ol> <p>Before your Chat-to-PDF app can effectively retrieve information from a vector database, it's imperative to preprocess the PDF documents and create a structured and searchable index for the preprocessed data. This searchable index serves as the cornerstone of your application, akin to a meticulously organized library catalog. It empowers your system to swiftly and accurately locate relevant information within PDF documents, enhancing the efficiency and precision of the retrieval process.</p> <p>Figure 2.3 illustrates the RAG data ingestion pipeline. in Chapter 3, we will fully discuss how to prepare, index, and store the documents in a vector database.</p> <p>{width=\"4.857162073490814in\" height=\"1.6858803587051618in\"}</p> <p>Figure 2.3: RAG data ingestion pipeline</p> <ol> <li>Challenges of Retrieval-Augmented Generation</li> </ol> <p>The adoption of Retrieval-Augmented Generation (RAG) represents a significant advancement in natural language processing and information retrieval. However, like any complex AI system, RAG presents a set of challenges that must be addressed to fully harness its potential. In this section, we explore some of the key challenges associated with RAG.</p> <p>2.7ChallengesofRetrieval-AugmentedGeneration</p> <p>2.7.1 Data Quality and Relevance</p> <p>RAG heavily relies on the availability of high-quality and relevant data for both retrieval and generation tasks. Challenges in this area include:</p> <ul> <li> <p>Noisy Data: Incomplete, outdated, or inaccurate data sources can     lead to retrieval of irrelevant information, impacting the quality     of generated responses.</p> </li> <li> <p>Bias and Fairness: Biases present in training data may lead to     biased retrieval and generation, perpetuating stereotypes or     misinformation.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#272-integration-complexity","title":"2.7.2 Integration Complexity","text":"<p>Integrating retrieval and generation components seamlessly is non-trivial, as it involves bridging different architectures and models. Challenges include:</p> <ul> <li> <p>Model Compatibility: Ensuring that the retrieval and generation     models work harmoniously, especially when combining traditional     methods (e.g., TF-IDF) with neural models (e.g., GPT-3).</p> </li> <li> <p>Latency and Efficiency: Balancing the need for real-time     responsiveness with the computational resources required for     retrieval and generation.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#273-scalability","title":"2.7.3 Scalability","text":"<p>Scaling RAG systems to handle large volumes of data and user requests can be challenging:</p> <ul> <li> <p>Indexing Efficiency: As the document corpus grows, maintaining     an efficient and up-to-date index becomes crucial for retrieval     speed.</p> </li> <li> <p>Model Scaling: Deploying large-scale neural models for both     retrieval and generation may require substantial computational     resources.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#274-evaluation-metrics","title":"2.7.4 Evaluation Metrics","text":"<p>Evaluating the performance of RAG systems presents difficulties:</p> <ul> <li> <p>Lack of Gold Standards: In some cases, there may be no clear     gold standard for evaluating the relevance and quality of retrieved     documents.</p> </li> <li> <p>Diverse User Needs: Users have diverse information needs, making     it challenging to develop universal evaluation metrics.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#275-domain-adaptation","title":"2.7.5 Domain Adaptation","text":"<p>Adapting RAG systems to specific domains or industries can be complex:</p> <ul> <li> <p>Domain-Specific Knowledge: Incorporating domain-specific     knowledge and jargon into retrieval and generation.</p> </li> <li> <p>Training Data Availability: The availability of domain-specific     training data for fine-tuning models.</p> </li> </ul> <p>Addressing these challenges is essential to unlock the full potential of RAG in various applications, from question answering to content generation. As research and development in this field continue, finding innovative solutions to these challenges will be critical for building robust and reliable RAG systems that deliver accurate, relevant, and trustworthy information to users.</p> <p>As we conclude our exploration of the foundations and the retrieval component of Retrieval-Augmented Generation (RAG) systems in this Chapter, we now turn our attention to the practical implementation of RAG pipelines in Chapter 3. In this next chapter, we'll delve into the nitty-gritty details of how these systems come to life, starting with the preprocessing of PDF documents and the data ingestion pipeline. We'll also discuss the generation components that make RAG systems work. Further, Chapter 3 explores the impact of text splitting methods on RAG quality and the crucial role of metadata in enhancing the overall RAG experience.</p> <ol> <li>RAG Pipeline Implementation</li> </ol> <p>3.1 Preprocessing PDF documents</p> <p>Before we can harness the power of Large Language Models (LLMs) and particularly RAG method for question answering over PDF documents, it's essential to prepare our data. PDFs, while a common format for documents, pose unique challenges for text extraction and analysis. In this section, we'll explore the critical steps involved in preprocessing PDF documents to make them suitable for our Chat-to-PDF app. These steps are not only essential for PDFs but are also applicable to other types of files. However, our primary focus is on PDF documents due to their prevalence in various industries and applications.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#311-pdf-text-extraction","title":"3.1.1 PDF Text Extraction","text":"<p>PDFs may contain a mix of text, images, tables, and other elements. To enable text-based analysis and question answering, we need to extract the textual content from PDFs. Here's how you can accomplish this:</p> <ul> <li> <p>TextExtraction Tools: Explore available tools and libraries like     PyPDF2, pdf2txt, or PDFMiner to extract text from PDF files     programmatically.</p> </li> <li> <p>Handling Scanned Documents: If your PDFs contain scanned images     instead of selectable text, you may need Optical Character     Recognition (OCR) software to convert images into machine-readable     text.</p> </li> <li> <p>Quality Control: Check the quality of extracted text and perform     any necessary cleanup, such as removing extraneous characters or     fixing formatting issues.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#312-handling-multiple-pages","title":"3.1.2 Handling Multiple Pages","text":"<p>PDF documents can span multiple pages, and maintaining context across pages is crucial for question answering. Here's how you can address this challenge:</p> <ul> <li> <p>Page Segmentation: Segment the document into logical units, such     as paragraphs or sections, to ensure that context is preserved.</p> </li> <li> <p>Metadata Extraction: Extract metadata such as document titles,     authors, page numbers, and creation dates. This metadata can aid in     improving searchability and answering user queries.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#313-text-cleanup-and-normalization","title":"3.1.3 Text Cleanup and Normalization","text":"<p>PDFs may introduce artifacts or inconsistencies that can affect the quality of the extracted text. To ensure the accuracy of question answering, perform text cleanup and normalization:</p> <ul> <li> <p>Whitespace and Punctuation: Remove or replace excessive     whitespace and special characters to enhance text readability.</p> </li> <li> <p>Formatting Removal: Eliminate unnecessary formatting, such as     font styles, sizes, and colors, which may not be relevant for     question answering.</p> </li> <li> <p>Spellchecking: Check and correct spelling errors that might     occur during the extraction process.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#314-language-detection","title":"3.1.4 Language Detection","text":"<p>If your PDF documents include text in multiple languages, it is a good idea to implement language detection algorithms to identify the language used in each section. This information can be useful when selecting appropriate LLM models for question answering.</p> <p>3.2 Data Ingestion Pipeline Implementation</p> <p>As it has been depicted in Figure 2.3 the first step of the data ingestion pipeline is extracting and spliting text from the pdf documents. There are several packages for this goal including:</p> <ul> <li> <p>PyPDF2</p> </li> <li> <p>pdfminer.six</p> </li> <li> <p>unstructured</p> </li> </ul> <p>Additionally, there are data loaders hub like llamahub that contains tens of data loaders for reading and connecting a wide variety data sources to a Large Language Model (LLM).</p> <p>Finally, there are packages like llamaindex, and langchain. These are frameworks that faciliates developing applications powered by LLMs. Therefore, they have implemented many of these data loaders including extracting and spliting text from pdf files.</p> <p>Step 1: Install necessary libraries</p> <p>pip install llama-index pip install langchain</p> <p>Step 2: Load the pdf file and extract the text from it</p> <p>Code below will iterate over the pages of the pdf file, extract the text and add it to the documents list object, see Figure 3.1.</p> <p>{width=\"4.857147856517935in\" height=\"2.36549321959755in\"}</p> <p>Figure 3.1: Load pdf files</p> <p>Now every page has become a separate document that we can later embed (vectorize) and store in the vector database. However, some pages could be very lengthy and other ones could be very short as page length varies. This could signaficantly impact the quality of the document search and retrieval.</p> <p>Additionally, LLMs have a limited context window (token limit), i.e. they can handle certain number of tokens (a token roughly equals to a word). Therefore, we instead first concatenate all the pages into a long text document and then split that document into smaller reletively equal size chunks. We then embed each chunk of text and insert into the vector database.</p> <p>Nevertheless, since we are going to use llamaindex and langchain frameworks for the RAG pipeline, Let's utilize the features and functions these frameworks offer. They have data loaders and splitters that we can use to read and split pdf files. You can see the code in Figure 3.2.</p> <p>pdf_content[0] contains the entire content of pdf, and has s special structure. It is a Document object with some properties including page_content and metadata. page_content is the textual content and metadata contains some meta-</p> <p>{width=\"4.8570811461067365in\" height=\"1.9640212160979877in\"}</p> <p>Figure 3.2: Langchain data loader</p> <p>data about the pdf. Here's the partial output the Document object of our pdf in Figure 3.3.</p> <p>{width=\"4.8570406824146986in\" height=\"1.0407939632545933in\"}</p> <p>Figure 3.3: Langchain data loader output</p> <p>A Document object is a generic class for storing a piece of unstructured text and its associated metadata. See here for more information. Step 3: Split the text into smaller chunks</p> <p>There are several different text splitters. For more information see langchain API, or llamaIndex documentation. Two common ones are:</p> <p>i.  CharacterTextSplitter: Split text based on a certain number     characters.</p> <p>ii. TokenTextSplitter: Split text to tokens using model tokenizer.</p> <p>The following code in Figure 3.4 chunks the pdf content into sizes no greater than 1000, with a bit of overlap to allow for some continued context.</p> <p>{width=\"4.856896325459317in\" height=\"1.5922495625546806in\"}</p> <p>Figure 3.4: Langchain text split method</p> <p>Here's the number of chunks created from splitting the pdf file.</p> <p># Print the number of chunks (list of Document objects)</p> <p>print(len(docs))</p> <p># 30</p> <p>Step 4: Embed and store the documents in the vector database</p> <p>In this step we need to convert chunks of text into embedding vectors. There are plenty of embedding models we can use including OpenAI models, Huggingface models, and Cohere models. You can even define your own custom embedding model. Selecting an embedding model depnds on several factors:</p> <ul> <li> <p>Cost: Providers such as OpenAI or Cohere charge for embeddings,     albeit it's cheap, when you scale to thusands of pdf files, it will     become prohibitive.</p> </li> <li> <p>Latency and speed: Hosting an embedding model on your server     reduce the latency, whereas using vendors' API increases the     latency.</p> </li> <li> <p>Convenience: Using your own embedding model needs more compute     resource and maintainance whereas using vendors APIs like OpenAI     gives you a hassle-free experience.</p> </li> </ul> <p>Similar to having several choices for embedding models, there are so many options for choosing a vector database, which is out the scope of this book.</p> <p>Figure 3.5 shows some of the most popular vector database vendors and some of the features of their hosting. This blog fully examines these vector databases from different perspective.</p> <p>{width=\"4.856864610673666in\" height=\"3.24830927384077in\"}</p> <p>Figure 3.5: Various vector databases. Image source</p> <p>We are going to use OpenAI models, particularly text-embedding-ada-002 for embedding. Furthermore, we choose Qdrant as our vector database. It's open source, fast, very flexible, and offers a free clould-based tier.</p> <p>We first install the openai and qdrant package.</p> <p>pip install openai pip install qdrant-client</p> <p>We also require an API key that we can get it from here.</p> <p>If we set OPENAI_API_KEY environment variable to our API key, we can easily call the functions that needs it without getting any error. Otherwise we can pass the API key parameter to functions requiring it. Figure 3.6 shows how to do it.</p> <p>{width=\"4.856896325459317in\" height=\"3.0135192475940507in\"}</p> <p>Figure 3.6: Qdrant vector database setup via Langchain</p> <p>Please note that there are several different ways to achieve the same goal</p> <p>(embedding and storing in the vector database). You can use Qdrant client library directly instead of using the langchain wrapper for it. Also, you can first create embeddings separately and then store them in the Qdrant vector database. Here, we embedded the documents and stored them all by calling Qdrant.from_documents().</p> <p>In addition, you can use Qdrant cloud vector database to store the embeddings and use their REST API to interact with it, unlike this example where the index is stored locally in the /tmp/local_qdrant directory. This approach is suitable for testing and POC (Proof-Of-Concept), not for production environment.</p> <p>We can try and see how we can search and retrieve relevant documents from the vector database. For instance, let's see what the answer to the question \"what is knearest neighbor?\". See the output in Figure 3.7.</p> <p>{width=\"4.856896325459317in\" height=\"2.7089610673665794in\"}</p> <p>Figure 3.7: Question answering example with output</p> <p>Awesome! The retrieved answer seems quite relevant.</p> <p>The entire code is displayed in Figure 3.8.</p> <p>{width=\"4.856896325459317in\" height=\"4.43478893263342in\"}</p> <p>Figure 3.8: The entire code for retrieval component</p> <p>3.3GenerationComponentImplementation</p> <p>3.3 Generation Component Implementation</p> <p>Figure 3.9 illustrates a simplified version of the RAG pipeline we saw in Chapter 2. So far our Retrieval component of the RAG is implemented. In the next section we will implement the Generation component.</p> <p>{width=\"4.85706583552056in\" height=\"2.0912357830271215in\"}</p> <p>Figure 3.9: RAG pipeline</p> <p>The steps for generating a response for a user's question are:</p> <ul> <li> <p>Step 1: Embed the user's query using the same model used for     embedding documents</p> </li> <li> <p>Step 2: Pass the query embedding to vector database, search and     retrieve the top-k documents (i.e. context) from the vector database</p> </li> <li> <p>Step 3: Create a \"prompt\" and include the user's query and     context in it</p> </li> <li> <p>Step 4: Call the LLM and pass the the prompt</p> </li> <li> <p>Step 5: Get the generated response from LLM and display it to     the user</p> </li> </ul> <p>Again, we can follow each step one by one, or utilize the features langchain or llamaIndex provide. We are going to use langchain in this case.</p> <p>Langchain includes several kinds of built-in question-answering chains. A chain in LangChain refers to a sequence of calls to components, which can include other chains or external tools. In order to create a question answering chain, we use:</p> <ol> <li> <p>load_qa_chain: load_qa_chain() is a function in Langchain that     loads a pre-configured question answering chain. It takes in a     language model like OpenAI, a chain type (e.g. \"stuff\" for     extracting answers from text), and optionally a prompt template and     memory object. The function returns a QuestionAnsweringChain     instance that is ready to take in documents and questions to     generate answers.</p> </li> <li> <p>load_qa_with_sources_chain: This is very similar to     load_qa_chain except it contains sources/metadata along with the     returned response.</p> </li> <li> <p>RetrievalQA: RetrievalQA is a class in Langchain that creates a     question answering chain using retrieval. It combines a retriever,     prompt template, and LLM together into an end-to-end QA pipeline.     The prompt template formats the question and retrieved documents     into a prompt for the LLM. This chain retrieves relevant documents     from a vector database for a given query, and then generates an     answer using those documents.</p> </li> <li> <p>RetrievalQAWithSourcesChain: It is a variant of RetrievalQA that     returns relevant source documents used to generate the answer. This     chain returns an AnswerWithSources object containing the answer     string and a list of source IDs.</p> </li> </ol> <p>Here's the code demonstraing the implementation, Figure 3.10:</p> <p>Figure 3.11 shows how to use load_qa_with_sources_chain:</p> <p>Similarly, if we use RetrievalQA, we will have Figure 3.12:</p> <p>And here's the code when we use RetrievalQAWithSourcesChain, Figure 3.13:</p> <p>As you can see, it's fairly straight forward to implement RAG (or say a prototype RAG application) using frameworks like langchain or llamaIndex. However, when it comes to deploying RAG to production and scaling the system, it becomes notoriously challenging. There are a lot of nuances that will affect</p> <p>3.3GenerationComponentImplementation</p> <p>{width=\"4.856896325459317in\" height=\"2.607442038495188in\"}</p> <p>Figure 3.10: Response generation using Langchain chain</p> <p>{width=\"4.856896325459317in\" height=\"2.302884951881015in\"}</p> <p>Figure 3.11: Using load_qa_with_sources_chain chain for response generation</p> <p>{width=\"4.856896325459317in\" height=\"1.9983267716535433in\"}</p> <p>Figure 3.12: The usage of RetrievalQA chain</p> <p>{width=\"4.856896325459317in\" height=\"2.0998458005249345in\"}</p> <p>Figure 3.13: Code snippet for using Langchain RetrievalQAWith-</p> <p>SourcesChain for response generation</p> <p>3.4ImpactofTextSplittingonRetrievalAugmentedGeneration(RAG)Quality</p> <p>the quality of the RAG, and we need to take them into consideration. We will discuss some of the main challenges and how to address them in the next few sections.</p> <p>3.4 Impact of Text Splitting on Retrieval Augmented</p> <p>Generation (RAG) Quality</p> <p>In the context of building a Chat-to-PDF app using Large Language Models (LLMs), one critical aspect that significantly influences the quality of your Retrieval Augmented Generation (RAG) system is how you split text from PDF documents. Text splitting can be done at two levels: splitting by character and splitting by token. The choice you make between these methods can have a profound impact on the effectiveness of your RAG system. Let's delve into the implications of each approach.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#341-splitting-by-character","title":"3.4.1 Splitting by Character","text":"<p>Advantages:</p> <p>Fine-Grained Context: Splitting text by character retains the finest granularity of context within a document. Each character becomes a unit of input, allowing the model to capture minute details.</p> <p>Challenges:</p> <ul> <li> <p>Long Sequences: PDF documents often contain long paragraphs or     sentences. Splitting by character can result in extremely long input     sequences, which may surpass the model's maximum token limit, making     it challenging to process and generate responses.</p> </li> <li> <p>Token Limitations: Most LLMs, such as GPT-3, have token limits,     often around 4,000 tokens. If a document exceeds this limit, you'll     need to truncate or omit sections, potentially losing valuable     context.</p> </li> <li> <p>Increased Inference Time: Longer sequences require more     inference time, which can lead to slower response times and     increased computational costs.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#342-splitting-by-token","title":"3.4.2 Splitting by Token","text":"<p>Advantages:</p> <ul> <li> <p>Token Efficiency: Splitting text by token ensures that each     input sequence remains within the model's token limit, allowing for     efficient processing.</p> </li> <li> <p>Balanced Context: Each token represents a meaningful unit,     striking a balance between granularity and manageability.</p> </li> <li> <p>Scalability: Splitting by token accommodates documents of     varying lengths, making the system more scalable and adaptable.</p> </li> </ul> <p>Challenges:</p> <ul> <li>Contextual Information: Token-based splitting may not capture     extremely fine-grained context, potentially missing nuances present     in character-level splitting.</li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#343-finding-the-right-balance","title":"3.4.3 Finding the Right Balance","text":"<p>The choice between character-level and token-level splitting is not always straightforward and may depend on several factors:</p> <p>3.5ImpactofMetadataintheVectorDatabaseonRetrievalAugmentedGeneration(RAG)</p> <ul> <li> <p>Document Types: Consider the types of PDF documents in your     collection. Technical manuals with precise details may benefit from     characterlevel splitting, while general documents could work well     with token-level splitting.</p> </li> <li> <p>Model Limitations: Take into account the token limits of your     chosen LLM. If the model's limit is a significant constraint,     token-level splitting becomes a necessity.</p> </li> <li> <p>User Experience: Assess the trade-off between detailed context     and response time. Character-level splitting might provide richer     context but at the cost of slower responses.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#344-hybrid-approaches","title":"3.4.4 Hybrid Approaches","text":"<p>In practice, you can also explore hybrid approaches to text splitting. For instance, you might use token-level splitting for most of the document and switch to character-level splitting when a specific question requires fine-grained context.</p> <p>The impact of text splitting on RAG quality cannot be overstated. It's a critical design consideration that requires a balance between capturing detailed context and ensuring system efficiency. Carefully assess the nature of your PDF documents, the capabilities of your chosen LLM, and user expectations to determine the most suitable text splitting strategy for your Chat-to-PDF app. Regular testing and user feedback can help refine this choice and optimize the overall quality of your RAG system.</p> <p>3.5 Impact of Metadata in the Vector Database on Retrieval Augmented Generation (RAG)</p> <p>The inclusion of metadata about the data stored in the vector database is another factor that can significantly enhance the quality and effectiveness of your</p> <p>Retrieval Augmented Generation (RAG) system. Metadata provides valuable contextual information about the PDF documents, making it easier for the RAG model to retrieve relevant documents and generate accurate responses. Here, we explore the ways in which metadata can enhance your RAG system.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#351-contextual-clues","title":"3.5.1 Contextual Clues","text":"<p>Metadata acts as contextual clues that help the RAG model better understand the content and context of each PDF document. Typical metadata includes information such as:</p> <ul> <li> <p>Document Title: The title often provides a high-level summary of     the document's content.</p> </li> <li> <p>Author: Knowing the author can offer insights into the     document's perspective and expertise.</p> </li> <li> <p>Keywords and Tags: Keywords and tags can highlight the main     topics or themes of the document.</p> </li> <li> <p>Publication Date: The date of publication provides a temporal     context, which is crucial for understanding the relevance of the     document.</p> </li> <li> <p>Document Type: Differentiating between research papers, user     manuals, and other types of documents can aid in tailoring responses     appropriately.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#352-improved-document-retrieval","title":"3.5.2 Improved Document Retrieval","text":"<p>With metadata available in the vector database, the retrieval component of your RAG system can become more precise and efficient. Here's how metadata impacts document retrieval:</p> <ul> <li>Relevance Ranking: Metadata, such as document titles, keywords,     and tags, can be used to rank the documents based on relevance to a     user query. Documents with metadata matching the query can be given     higher priority during retrieval. For example, if a user asks a     question related</li> </ul> <p>3.5ImpactofMetadataintheVectorDatabaseonRetrievalAugmentedGeneration(RAG)</p> <p>to \"machine learning,\" documents with \"machine learning\" in their keywords or tags might be given priority during retrieval.</p> <ul> <li> <p>Filtering: Metadata can be used to filter out irrelevant     documents early in the retrieval process, reducing the computational     load and improving response times. For instance, if a user asks     about \"biology,\" documents with metadata indicating they are     engineering manuals can be excluded from consideration.</p> </li> <li> <p>Enhanced Query Understanding: Metadata provides additional     context for the user's query, allowing the RAG model to better     understand the user's intent and retrieve documents that align with     that intent. For example, if the metadata includes the publication     date, the RAG model can consider the temporal context when     retrieving documents.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#353-contextual-response-generation","title":"3.5.3 Contextual Response Generation","text":"<p>Metadata can also play a crucial role in the generation component of your RAG system. Here's how metadata impacts response generation:</p> <ul> <li> <p>Context Integration: Metadata can be incorporated into the     response generation process to provide more contextually relevant     answers. For example, including the publication date when answering     a historical question.</p> </li> <li> <p>Customization: Metadata can enable response customization. For     instance, the tone and style of responses can be adjusted based on     the author's information.</p> </li> <li> <p>Enhanced Summarization: Metadata can aid in the summarization of     retrieved documents, allowing the RAG model to provide concise and     informative responses. For instance, if the metadata includes the     document type as \"research paper,\" the RAG system can generate a     summary that highlights the key findings or contributions of the     paper.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#354-user-experience-and-trust","title":"3.5.4 User Experience and Trust","text":"<p>Including metadata in the RAG system not only enhances its technical capabilities but also improves the overall user experience. Users are more likely to trust and find value in a system that provides contextually relevant responses. Metadata can help build this trust by demonstrating that the system understands and respects the nuances of the user's queries.</p> <p>Overall, incorporating metadata about data in the vector database of your Chatto-PDF app's RAG system can significantly elevate its performance and user experience. Metadata acts as a bridge between the user's queries and the content of PDF documents, facilitating more accurate retrieval and generation of responses.</p> <p>As we conclude our exploration of the nuts and bolts of RAG pipelines in this Chapter, it's time to move on to more complex topics. In Chapter 4, we'll take a deep dive and try to address some of the retrieval and generation challenges that come with implementing advanced RAG systems.</p> <p>We'll discuss the optimal chunk size for efficient retrieval, consider the balance between context and efficiency, and introduce additional resources for evaluating RAG performance. Furthermore, we'll explore retrieval chunks versus synthesis chunks and ways to embed references to text chunks for better understanding.</p> <p>We'll also investigate how to rethink retrieval methods for heterogeneous document corpora, delve into hybrid document retrieval, and examine the role of query rewriting in enhancing RAG capabilities.</p> <ol> <li>From Simple to Advanced RAG</li> </ol> <p>4.1 Introduction</p> <p>As we traverse the path from development to production, the world of Retrieval Augmented Generation (RAG) applications unveils its potential to transform the way we interact with vast collections of information. In the preceding chapters, we've laid the groundwork for building RAG systems that can answer questions, provide insights, and deliver valuable content. However, the journey is far from over.</p> <p>Please note that LlamaIndex framework has been used for several of the code implementations in this chapter. This framework also contains a lot of advanced tutorials about RAG that inspired the content of this chapter.</p> <p>The transition from a well-crafted RAG system in the development environment to a real-world production application is a monumental step, one that demands careful consideration of a myriad of factors and deals with limitations of existing approaches. These considerations ensure your RAG application operates seamlessly in a real-world production environment. There are so many challenges and consideration for building production-ready RAG. Nevertheless, we will discuss some of the primary ones.</p> <p>RAG pipeline consists of two components: i) Retrieval and, ii) response generation (synthesis). Each component has its own challenges.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#retrieval-challenges","title":"Retrieval Challenges","text":"<ul> <li> <p>Low precision: When retrieving the top-k chunks, there's a risk     of including irrelevant content, potentially leading to issues like     hallucination and generating inaccurate responses.</p> </li> <li> <p>Low recall: In certain cases, even when all the relevant chunks     are retrieved, the text chunks might lack the necessary global     context beyond the retrieved chunks to generate a coherent response.</p> </li> <li> <p>Obsolete information: Ensuring the data remains up-to-date is     critical to avoid reliance on obsolete information. Regular updates     are essential to maintain data relevance and accuracy.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#generation-challenges","title":"Generation Challenges","text":"<ul> <li> <p>Hallucination: The model generates responses that include     information not present in the context, potentially leading to     inaccuracies or fictional details.</p> </li> <li> <p>Irrelevance: The model produces answers that do not directly     address the user's question, resulting in responses that lack     relevance or coherence.</p> </li> <li> <p>Bias: The model generates answers that contai n harmful or     offensive content, potentially reflecting biases and undermining     user trust and safety.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#what-can-be-done","title":"What Can be done","text":"<p>In order to effectively tackle the challenges in Retrieval Augmented Generation (RAG) systems, several key strategies can be employed:</p> <ol> <li> <p>Data Augmentation: Can we enrich our dataset by storing     supplementary information beyond raw text chunks, such as metadata     or structured data, to provide richer context for retrieval and     generation processes?</p> </li> <li> <p>Optimized Embeddings: Can we refine and enhance our embedding     representations to capture context more effectively, improve the     relevance of retrieved information, and enable more coherent     response generation?</p> </li> <li> <p>Advanced Retrieval Techniques: Can we go beyond basic top-k     embedding lookup and implement advanced retrieval methods, such as     semantic search, or hybrid search (keyword search + semantic search)     to enhance the precision and recall of information retrieval?</p> </li> <li> <p>Multi-Purpose Use of LLMs: Can we leverage Large Language Models     (LLMs) for tasks beyond text generation, such as question answering,     summarization, or knowledge graph construction, to augment the     capabilities of RAG systems and provide more comprehensive responses     to user queries?</p> </li> </ol> <p>Let's dive in a bit more deeply into aforementioned challenges and propose how to alleiviate each one.</p> <p>4.2 Optimal Chunk Size for Efficient Retrieval</p> <p>The chunk size in a RAG system is the size of the text passages that are extracted from the source text and used to generate the retrieval index. The chunk size has a significant impact on the system's efficiency and performance in several ways:</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#421-balance-between-context-and-efficiency","title":"4.2.1 Balance Between Context and Efficiency","text":"<p>The chunk size should strike a balance between providing sufficient context for generating coherent responses (i.e. Relevance and Granularity) and ensuring efficient retrieval and processing (i.e Performance).</p> <p>A smaller chunk size results in more granular chunks, which can improve the relevance of the retrieved chunks. However, it is important to note that the most relevant information may not be contained in the top retrieved chunks, especially if the similarity_top_k setting is low (e.g. k=2). A smaller chunk size also results in more chunks, which can increase the system's memory and processing requirements.</p> <p>A larger chunk size can improve the system's performance by reducing the number of chunks that need to be processed. However, it is important to note that a larger chunk size can also reduce the relevance of the retrieved chunks.</p> <p>The optimal chunk size for a RAG system depends on a number of factors, including the size and complexity of the source text, the desired retrieval performance, and the available system resources. However, it is important to experiment with different chunk sizes to find the one that works best for your specific system. But, how do we know what works and what doesn't?</p> <p>Question: How can we measure the performance of RAG?</p> <p>Answer: We have to define evaluation metrics and then use evaluation tools to measure how the RAG performs considering the metrics. There are several tools to evaluate the RAG including LlamaIndex Response Evaluation module to test, evaluate and choose the right chunk size. It contains a few components, particularly:</p> <ol> <li> <p>Faithfulness Evaluator: This tool assesses whether the response     includes fabricated information and determines if the response     aligns with any source nodes from the query engine.</p> </li> <li> <p>Relevancy Evaluator: This tool gauges whether the response     effectively addresses the query and evaluates if the combined     information from the response and source nodes corresponds to the     query.</p> </li> </ol> <p>The code below shows how to use Evaluation module and determine the optimal chunk size for retrieval. To read the full article, see this link.</p> <p>Figure 4.1 shows the data preparation for evaluation module.</p> <p>{width=\"4.856896325459317in\" height=\"3.318076334208224in\"}</p> <p>Figure 4.1: data preparation for response evaluation</p> <p>Figure 4.2 displays the criteria to set for evaluation.</p> <p>{width=\"4.856896325459317in\" height=\"2.302884951881015in\"}</p> <p>Figure 4.2: Define criteria for evaluation</p> <p>Figure 4.3 shows the evaluation function definition.</p> <p>And Figure 4.4 demonstrates testing the evaluation function with different chunk sizes.</p> <p>They test across different chunk sizes and conclude (in this experiment) that chunk_size = 1024 results in peaking of Average Faithfulness and Average Relevancy.</p> <p>Here are summary of the tips for choosing the optimal chunk size for a RAG system:</p> <ul> <li> <p>Consider the size and complexity of the source text. Larger and more     complex texts will require larger chunk sizes to ensure that all of     the relevant information is captured.</p> </li> <li> <p>Consider the desired retrieval performance. If you need to retrieve     the most relevant chunks possible, you may want to use a smaller     chunk size.</p> </li> </ul> <p>{width=\"4.856896325459317in\" height=\"4.536307961504812in\"}</p> <p>Figure 4.3: Define a function to perform evaluation</p> <p>{width=\"4.856896325459317in\" height=\"1.3892115048118985in\"}</p> <p>Figure 4.4: Run the evaluation function with different parameters</p> <p>However, if you need to retrieve chunks quickly, you may want to use a larger chunk size.</p> <ul> <li> <p>Consider the available system resources. If you are limited on     system resources, you may want to use a smaller chunk size. However,     if you have ample system resources, you may want to use a larger     chunk size. You can evaluate the optimal chunk size for your RAG     system by using a variety of metrics, such as:</p> </li> <li> <p>Relevance: The percentage of retrieved chunks that are relevant     to the query.</p> </li> <li> <p>Faithfulness: The percentage of retrieved chunks that are     faithful to the source text.</p> </li> <li> <p>Response time: The time it takes to retrieve chunks for a query.     Once you have evaluated the performance of your RAG system for     different chunk sizes, you can choose the chunk size that strikes     the best balance between relevance, faithfulness, and response time.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#422-additional-resources-for-rag-evaluation","title":"4.2.2 Additional Resources for RAG Evaluation","text":"<p>Evaluation of RAG applications remains an unsolved problem today and is an active research area. Here are just a few references for practical evaluation of</p> <p>RAG: 1. This blog from Databricks has some best practices to evaluate RAG applications. Figure 4.5 illustrates what their experiment setup looks like.</p> <p>{width=\"4.857096456692913in\" height=\"3.0304363517060366in\"}</p> <p>Figure 4.5: Databricks evaluation experiment setup. Image source</p> <ol> <li> <p>Zheng et al. (2023) propose a strong LLMs as judges to evaluate     these models on more open-ended questions.</p> </li> <li> <p>RAG Evaluation is another interesting blog that discuss how to use     Langchain for evaluating RAG applications.</p> </li> </ol> <p>4.3 Retrieval Chunks vs. Synthesis Chunks</p> <p>Another fundamental technique for enhancing retrieval in Retrieval Augmented Generation (RAG) systems is the decoupling of chunks used for retrieval from those used for synthesis (i.e. response generation). The main idea is optimal chunk representation for retrieval may not necessarily align with the requirements for effective synthesis. While a raw text chunk could contain essential details for the LLM to generate a comprehensive response, it might also contain filler words or information that could introduce biases into the embedding representation. Furthermore, it might lack the necessary global context, making it challenging to retrieve the chunk when a relevant query is received. To give an example, think about having a question answering system on emails. Emails often contain so much fluff (a big portion of the email is \"looking forward\", \"great to hear from you\", etc.) and so little information. Thus, retaining semantics in this context for better and more accurate question answering is very important.</p> <p>There are a few ways to implement this technique including:</p> <ol> <li> <p>Embed references to text chunks</p> </li> <li> <p>Expand sentence-level context window</p> </li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#431-embed-references-to-text-chunks","title":"4.3.1 Embed References to Text Chunks","text":"<p>The main idea here is that, we create an index in the vector database for storing document summaries. When a query comes in, we first fetch relevant document(s) at the high-level (i.e. summaries) before retrieving smaller text chunks directly, because it might retrieve irrelavant chunks. Then we can retrieve smaller chunks from the fetched document(s). In other words, we store the data in a hierarchical fashion: summaries of documents and chunks for each document. We can consider this approach as Dense Hierarchical Retrieval, in which a document-level retriever (i.e. summary index) first identifies the relevant documents, and then a passage-level retriever finds the relevant passages/chunks. Y. Liu et al. (2021) and Zhao et al. (2022) gives you a deep understanding of this approach. Figure 4.6 shows how this technique works.</p> <p>We can choose different strategies based on the type of documents we are dealing with. For example, if we have a list of web pages, we can consider each page as a document that we summarize, and also we split each document into a set of</p> <p>{width=\"3.980129046369204in\" height=\"4.010131233595801in\"}</p> <p>Figure 4.6: Document summary index</p> <p>smaller chunks as the second level of our data store strategy. When user asks a question, we first find the relevant page using the summary embeddings, and then we can retrieve the relevant chunks from that particular page.</p> <p>If we have a pdf document, we can consider each page of the pdf as a separate document, and then split each page into smaller chunks. If we have a list of pdf files, we can choose the entire content of each pdf to be a document and split the it into smaller chunks.</p> <p>Let's code it up for our pdf file.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#step-1-read-the-pdf-file","title":"Step 1: Read the PDF file","text":"<p>We read the pdf file, Figure 4.7, and create a list of pages as later we will view each page as a separate document.</p> <p>{width=\"4.8571467629046365in\" height=\"2.2747845581802273in\"}</p> <p>Figure 4.7: Read a list of documents from each page of the pdf</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#step-2-create-document-summary-index","title":"Step 2: Create Document Summary Index","text":"<p>In order to create an index, first we have to convert a list of texts into a list of Document that is compatible with LlamaIndex.</p> <p>Definition: A Document is a generic container around any data source, for instance, a PDF, an API output, or retrieved data from a database. It stores text along with some other properties such as metadata and relationships (Links to other Documents/Nodes)</p> <p>Figure 4.8 shows the code.</p> <p>We can use the summary index to get the summary of each page/document using the document id, for instanc, Figure 4.9 shows the output of a summary of a document.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#step-3-retrieve-and-generate-response-using-document-summary-index","title":"Step 3: Retrieve and Generate Response using Document Summary Index","text":"<p>In this step, when a query comes, we run a retrieval from the document summary index to find the relevant pages. Retrieved document has links to its corresponding chunks, that are used to generate the final response to the query.</p> <p>There are multiple ways to that in LlamaIndex:</p> <ul> <li> <p>High-level query execution</p> </li> <li> <p>LLM based retrieval</p> </li> <li> <p>Embedding based retrieval</p> </li> </ul> <p>The high-level approach is depicted in Figure 4.10</p> <p>LLM based retrieval: This approach is low-level so we can view and change the parameters. Figure 4.11 below displays the code snippet:</p> <p>{width=\"4.8571467629046365in\" height=\"5.196761811023622in\"}</p> <p>Figure 4.8: Build a document summary index</p> <p>{width=\"4.857145669291339in\" height=\"3.127027559055118in\"}</p> <p>Figure 4.9: Example of a document summary</p> <p>{width=\"4.8571467629046365in\" height=\"3.24877624671916in\"}</p> <p>Figure 4.10: High-level query execution approach (default approach)</p> <p>{width=\"4.857145669291339in\" height=\"4.709764873140857in\"}</p> <p>Figure 4.11: LLM based retrieval approach</p> <p>Embedding based retrieval: In this technique, we first define DocumentSummaryIndexEmbeddingRetriever retriever, and configure the response generator to use this retriever. We then, integrate these two components into a RetrieverQueryEngine and run that for the query. Figure 4.12 shows the code snippet for this approach.</p> <p>{width=\"4.857020997375328in\" height=\"2.9709055118110235in\"}</p> <p>Figure 4.12: Embedding based retrieval</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#432-expand-sentence-level-context-window","title":"4.3.2 Expand sentence-level context window","text":"<p>In this approach, we have split the text into sentence level chunks to be able to perform fine-grained retrieval. However, before passing the fetched sentences to LLM response generator, we include the sentences surrounding the retrieved sentence, to enlarge the context window for better accuaracy. Please be mindful of lost in the middle problem when splitting large textual content at a very finegrained level, such as sentence-level.</p> <p>Figure 4.13 illustrates this technique.</p> <p>{width=\"4.857282370953631in\" height=\"3.0153171478565177in\"}</p> <p>Figure 4.13: Expanding the sentence level context, so LLM has a bigger context to use to generate the response</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#implementation","title":"Implementation","text":"<p>Again, we rely on LlamaIndex to implement this technique. We use SentenceWindowNodeParser to split document into sentences and save each sentence in a node. Node contains a window property where we can adjust. During the retrieval step, each fetched sentence will be replaced with surrounding sentences depending on the window size via MetadataReplacementNodePostProcessor function.</p> <p>Figure 4.14 shows the basic setup such as importing necessary modules, reading the pdf file and initializing the LLM and embedding models.</p> <p>{width=\"4.856916010498687in\" height=\"4.380162948381452in\"}</p> <p>Figure 4.14: Basic setup for sentence window implementation Next, we have to define nodes that are going to be stored in the VectorIndex as well as sentence index. Then, we create a query engine and run the query. Figure 4.15 shows the steps.</p> <p>{width=\"4.857076771653543in\" height=\"3.664109798775153in\"}</p> <p>Figure 4.15: Build the sentence index, and run the query</p> <p>Figure 4.16 displays the response output.</p> <p>We can see the original sentence that is retrieved for each node (we show the first node below) and also the actual window of sentences in the Figure 4.17.</p> <p>{width=\"4.857071303587052in\" height=\"2.07992782152231in\"}</p> <p>Figure 4.16: Output of the window response</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#433-lost-in-the-middle-problem","title":"4.3.3 Lost in the Middle Problem","text":"<p>Retrieval process in a RAG based application is all about retrieving the right and most relevant documents for a given user's query. The way we find these documents is that retrieval method assigns a relevance score to each document based on their similarity to the query. Then, sorts them descendingly and returns them. Nevertheless, this approach might not work well when we are returning many documents such as top-k &gt;= 10. The reason is when we pass a very long context to LLM, it tend to ignore or overlook the documents in the middle. Consequently, putting the least relevant document to the bottom of the fetch documents is not the best strategy. A better way is to place the least relevant documents in the middle.</p> <p>N. F. Liu et al. (2023) in Lost in the Middle: How Language Models Use Long Contexts demonstrated interesting findings about LLMs behavior. They realized that performance of LLMs is typically at its peak when relevant information is located at the beginning or end of the input context. However, it notably deteriorates when models are required to access relevant information buried within the middle of lengthy contexts. Figure 4.18 demonstrates the results.</p> <p>{width=\"4.8568864829396325in\" height=\"3.9800349956255467in\"}</p> <p>Figure 4.17: Original sentence that was retrieved for each node, as well as the actual window of sentences</p> <p>{width=\"4.857240813648294in\" height=\"4.51416447944007in\"}</p> <p>Figure 4.18: Accuracy of the RAG based on the postions of the retrieved documents. Image source</p> <p>They also show that LLMs with longer context windows still face this problem and increasing the context window doesn't solve this issue. The following demonstrates this experiment.</p> <p>{width=\"4.857154418197725in\" height=\"1.5026388888888889in\"}</p> <p>Figure 4.19: Comparing LLM models with various context size and the impact of changing the position of relevant documents</p> <p>How can we alleviate this problem? The answer is to reorder the retrieved documents in such a way that most similar documents to the query are placed at the top, the less similar documents at the bottom, and the least similar documents in the middle.</p> <p>For implementation, we need a function to get the retrieved documents from the retriever and reorder them, i.e. place most relevant documents at the beginning and end. Figure 4.20 shows our code.</p> <p>We can instead utilize Langchain solution: LongContextReorder. It essentially implements a similar approach to Figure 4.20 function. You can read the documentation for more details.</p> <p>Figure 4.21 shows how to use Langchain solution to deal with this problem.</p> <p>We can also use Haystack to deal with this problem. Haystack is the open source framework for building custom NLP apps with large language models (LLMs) in an end-to-end fashion. It offers a few components that are building blocks for performing various tasks like document retrieval, and summarization. We can connect these components and create an end-to-end pipeline. The two very</p> <p>{width=\"4.857168635170604in\" height=\"1.9304790026246719in\"}</p> <p>Figure 4.20: Pseudocode of a function to solve lost in the middle problem</p> <p>{width=\"4.857057086614173in\" height=\"2.619251968503937in\"}</p> <p>Figure 4.21: Langchain approach for solving lost in the middle problem useful components that we can utilize are DiversityRanker and LostInTheMiddleRanker.</p> <p>\"DiversityRanker is designed to maximize the variety of given documents. It does so by selecting the most semantically similar document to the query, then selecting the least similar one, and continuing this process with the remaining documents until a diverse set is formed. It operates on the principle that a diverse set of documents can increase the LLM's ability to generate answers with more breadth and depth.\"</p> <p>\"LostInTheMiddleRanker sorts the documents based on the</p> <p>\"Lost in the Middle\" order. The ranker positions the most relevant documents at the beginning and at the end of the resulting list while placing the least relevant documents in the middle.\"</p> <p>Please check their documentation for more details.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#434-embedding-optimization","title":"4.3.4 Embedding Optimization","text":"<p>Optimizing embeddings can have a significant impact on the results of your use cases. There are various APIs and providers of embedding models, each catering to different objectives:</p> <ul> <li> <p>Some are best suited for coding tasks.</p> </li> <li> <p>Others are designed specifically for the English language.</p> </li> <li> <p>And there are also embedding models that excel in handling     multilingual datasets (e.g., Multilingual BERT/mBERT).</p> </li> </ul> <p>However, determining which embedding model is the best fit for your dataset requires an effective evaluation method.</p> <p>So which embedding models should we use?</p> <p>One approach is to rely on existing academic benchmarks. However, it's important to note that these benchmarks may not fully capture the real-world usage of retrieval systems in AI use cases. They are often synthetic benchmarks specifically designed for information retrieval problems.</p> <p>For example, there is a benchmark called MTEB (Massive Text Embedding Benchmark). MTEB's leaderboard showcases embedding models across 8 tasks, including multilingual tasks, and currently features 132 models. You can compare the performance, speed, or both for these models (refer to the graph below: Figure 4.22).</p> <p>{width=\"4.857319553805774in\" height=\"2.803472222222222in\"}</p> <p>Figure 4.22: Models by average English MTEB score (y) vs speed (x) vs embedding size (circle size). Image source</p> <p>For better results, you can still utilize open-source tools by applying them to your specific data and use case. Additionally, you can enhance relevance by incorporating human feedback through a simple relevance feedback endpoint.</p> <p>Constructing your own datasets is also important as you have a deep understanding of your production data, relevant metrics, and what truly matters to you. This allows you to tailor the training and evaluation process to your specific needs.</p> <p>In terms of evaluating the performance of embedding models, there are excellent evaluation tools available in the market. These tools can help you assess the effectiveness of different models and make informed decisions.</p> <p>It is worth noting that recent research and experiments have shown that embedding models with the same training objective and similar data tend to learn very similar representations, up to an affine linear transform. This means that it is possible to project one model's embedding space into another model's embedding space using a simple linear transform.</p> <p>By understanding and leveraging linear identifiability, you can explore ways to transfer knowledge between embedding models and potentially improve their performance in specific tasks.\"</p> <p>This is called linear identifiability, and it was discussed in the 2020 paper by Roeder and Kingma (2021) On Linear Identifiability of Learned Representations from Google Brain. The paper states,</p> <p>\"We demonstrate that as one increases the representational capacity of the model and dataset size, learned representations indeed tend towards solutions that are equal up to only a linear transformation.\"</p> <p>See an example below:</p> <p>Therefore, the selection of a particular embedding model may not be that important if you are able to discover and implement a suitable transformation from your own dataset.</p> <p>{width=\"4.857165354330709in\" height=\"2.475216535433071in\"}</p> <p>Figure 4.23: Linear Identifiability. Image source</p> <p>4.4 Rethinking Retrieval Methods for Heterogeneous</p> <p>Document Corpora</p> <p>Retrieval-augmented generation (RAG) applications, especially when dealing with a substantial volume of documents (e.g. having many pdf files), often face challenges related to performance, relevance, and latency.</p> <p>Example: Assume a user asks a question and the answer to user's question only involves two pdf files, we would rather first get those two relevant pdf documents and then find the actual answer from their chunks, instead of searching through thusands of text chunks. But how to do that?</p> <p>There are multiple ways to achieve that goal:</p> <ul> <li> <p>Have multi-level embeddings, i.e. embed document summaries, where     each document summary is related to its text chunks. This approach     is implemented in Section 4.3.1.</p> </li> <li> <p>Add metadata about each document and store that along with the     document in the vector database.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#441-how-metadata-can-help","title":"4.4.1 How metadata can help","text":"<p>Including metadata in the indexing process can be a powerful strategy to address these challenges and significantly enhance the overall system's efficiency. By narrowing down the search scope using metadata filters, the system can reduce the number of documents to be considered, resulting in faster retrieval times.</p> <p>Figure 4.24 shows how metadata filtering can help the retrieval process. When user asks a question, they can explicitly give metadata for instance by specifying filters such as dropdown list, etc., or we can use LLM to determine the metadata filters from the query and search the vector database using the filters. Vector database utilizes the filters to narrow down search to the documents that match with the filters, and then finds the most similar chunks from documents and returns the top-k chunks.</p> <p>Please note that although we can add metadata to the text chunks after they are stored in the vector database, we should do that in the preprocessing step while we are splitting and embedding the documents, because if the vector database index becomes very large (i.e. we already have a great deal of embeddings in vector database), updating it will be significantly time consuming.</p> <p>Use Langchain for Metadata Filtering</p> <p>Figure 4.25 shows how to define new metadata for text chunks, and how to use filters to perform retrieval.</p> <p>In this example, we load two pdf files, one file is about machine learning interview questions, and the other file is a research paper. We would like to add a topic or category of each file as metadata, so later we can restrict our search to</p> <p>{width=\"4.857317366579178in\" height=\"2.5449617235345583in\"}</p> <p>Figure 4.24: How metadata filtering can improve retrieval process</p> <p>the category. Therefore, we update the initial metadata field by adding a category property to each text chunk and then store them in the vector database.</p> <p>If we print out the metadata of documents, we can see the category property has been added, shown in Figure 4.26.</p> <p>Now, we define the index and use it to perform search and retrieval, which is shown in Figure 4.27.</p> <p>Use LlamaIndex for Metadata Filtering</p> <p>Figure 4.28 shows how to use LlamaIndex for metadata filtering.</p> <p>How to let LLM infer the metadata from user question</p> <p>In both of the previous techniques, we have to explicitly define the metadata/filters while doing the retrieval. However, the questions is: Can we ask</p> <p>{width=\"4.857047244094488in\" height=\"2.8064020122484687in\"}</p> <p>Figure 4.25: Read the files and update the metadata property</p> <p>{width=\"4.857147856517935in\" height=\"1.261596675415573in\"}</p> <p>Figure 4.26: Output example of metadata for text chunks</p> <p>{width=\"4.857123797025372in\" height=\"2.5413429571303587in\"}</p> <p>Figure 4.27: Insert text chunks into the vector database and perform retrieval LLM to infer the metadata from user query? The short answer is: Yes.</p> <p>Therefore, the general approach is we need to define a particular prompt for LLM, so it can use that to extract entities or metadata from the user query, map them to the existing metadata stored with text chunks in the vector database, and then perform the retrieval.</p> <p>That said, LlamaIndex provides us an implemented version of this approach.</p> <p>The following code is from the LlamaIndex documentation here. For this technique, we define the metadata (in this example category and country) along with each text chunk. Figure 4.29 shows the code snippet for this step.</p> <p>Figure 4.30 shows the retrieval process including how to define vector index, vector store, and VectorIndexAutoRetriever object.</p> <p>{width=\"4.857020997375328in\" height=\"6.18166447944007in\"}</p> <p>Figure 4.28: Metadata filtering in LlamaIndex for document retrieval</p> <p>{width=\"4.857147856517935in\" height=\"4.225661636045494in\"}</p> <p>Figure 4.29: Define text node and metadata for auto retrieval</p> <p>{width=\"4.857076771653543in\" height=\"5.630069991251093in\"}</p> <p>Figure 4.30: Define VectorIndexAutoRetriever retriever and VectorStoreInfo, which contains a structured description of the vector store collection and the metadata filters it supports.</p> <p>4.5 Hybrid Document Retrieval</p> <p>Hybrid document retrieval is an approach that combines traditional keywordbased search like BM25 with semantic (dense) search using embeddings, such as BERT or word2vec. Integrating this technique to Retrieval-Augmented Generation (RAG) applications can significantly enhance the effectiveness of document retrieval. It addresses scenarios where a basic keyword-based approach can outperform semantic search and demonstrates how combining these methods improves retrieval in RAG applications.</p> <p>In addition, often times a complete migration to a semantic-based search using RAG is challenging for most companies. They might already have a keywordbased search system and have been using it for quite a long time. Performing an overhaul to the company's information architecture, and migrating to a vector database is just infeasible.</p> <p>Scenarios Favoring Keyword-Based Search:</p> <ol> <li> <p>Highly Specific Queries: In cases where a user's query is highly     specific and focuses on precise terms or phrases, a keyword-based     approach can outperform semantic search. Keyword matching excels at     finding exact matches within documents.</p> </li> <li> <p>Niche Domains: In specialized domains with industry-specific     jargon or acronyms, keyword search can be more effective as it     directly matches terminology without requiring extensive semantic     understanding.</p> </li> <li> <p>Short Documents: When dealing with very short documents, such as     tweets or headlines, keyword-based search can be more efficient.     Semantic models often require longer text to derive meaningful     embeddings.</p> </li> <li> <p>Low Resource Usage: Keyword search typically requires fewer     computational resources compared to semantic search. This can be     advantageous when resource efficiency is a critical concern.</p> </li> </ol> <p>4.5HybridDocumentRetrieval</p> <p>Combining Keyword and Semantic Approaches for Improved Retrieval:</p> <p>To harness the strengths of both keyword-based and semantic (dense) retrievers effectively, a pragmatic approach is to integrate two retrievers into the pipeline and merge their outputs. This two-pronged strategy capitalizes on the unique advantages of each retriever, resulting in more comprehensive and accurate retrieval in Retrieval-Augmented Generation (RAG) applications.</p> <p>The process begins by employing both keyword-based as well as dense retrievers within the RAG pipeline. However, the challenge lies in merging the results obtained from these two retrievers, each of which returns ranked lists of results with relevance scores assigned to each document. Figure 4.31 illustrates the hybrid retrieval pipeline.</p> <p>{width=\"4.857107392825896in\" height=\"2.7311526684164478in\"}</p> <p>Figure 4.31: Hybrid retrieval pipeline</p> <p>Merging the results from keyword-based and semantic retrievers can be approached in several ways, depending on the nature of the RAG application:</p> <ol> <li> <p>Concatenation: This method involves simply appending all     documents from both retrievers (excluding duplicates) to create the     final list of results. Concatenation is suitable when you intend to     use all retrieved documents and the order of the results is not     crucial. This approach can be valuable in extractive     question-answering pipelines, where you aim to extract information     from multiple sources and are less concerned about ranking.</p> </li> <li> <p>Reciprocal Rank Fusion (RRF): RRF operates with a formula that     reranks documents from both retrievers, giving priority to those     that appear in both results lists. Its purpose is to elevate the     most relevant documents to the top of the list, thereby enhancing     the overall relevance of the results. RRF is particularly useful     when the order of the results is important or when you intend to     pass on only a subset of results to the subsequent processing     stages.</p> </li> <li> <p>Merging by Scoring: In this approach, documents are ranked based     on the scores assigned by the individual retrievers. This method is     suitable when you aim to prioritize results from one retriever over     the other. If the relevance scores assigned by the two retrievers     are comparable and you wish to emphasize results from a particular     retriever, this method can be employed. For instance, if you're     using dense retrievers from different sources that return documents     from various document stores, this method allows you to choose one     retriever's output over another.</p> </li> </ol> <p>Advantages of Hybrid Retrieval:</p> <ul> <li> <p>Enhanced Relevance: Hybrid retrieval leverages the strengths of     both keyword and semantic approaches, increasing the chances of     returning highly relevant documents.</p> </li> <li> <p>Coverage: It addresses scenarios where purely keyword or purely     semantic approaches might fail, providing a broader scope of     relevant documents.</p> </li> <li> <p>Resource Efficiency: By initially narrowing the search with     keywordbased filtering, the system conserves computational     resources, making the retrieval process more efficient.</p> </li> <li> <p>Adaptability: Hybrid retrieval allows for adaptability to     different user queries and document types, striking a balance     between precision and recall.</p> </li> </ul> <p>There are frameworks which support hybrid retrieval out of the box such as ElasticSearch, Haystack, Weaviate, and Cohere Rerank. Let's find out how to implement this approach using Haystack. The following code is from Haystack documentation, you can see all the implementation details here. The documents that are used for this example are abstracts of papers from PubMed. You can find and download the dataset here.</p> <p>pip install datasets&gt;=2.6.1 pip install farm-haystack[inference]</p> <p>Step 1: We load the dataset and initialize the document store (i.e. vector database). Figure 4.32 shows this step.</p> <p>Step 2: Define the retrievers, insert the documents and embeddings into the document store and choose the join document strategy. You can see this step in Figure 4.33.</p> <p>Step 3: Create an end-to-end pipeline in Haystack and perform the hybrid retrieval for a query. This step is depicted in Figure 4.34.</p> <p>4.6 Query Rewriting for Retrieval-Augmented Large</p> <p>Language Models</p> <p>Query rewriting is a sophisticated technique that plays a pivotal role in enhancing the performance of Retrieval-Augmented Large Language Models (RAG). The fundamental idea behind query rewriting is to optimize and fine-tune the</p> <p>{width=\"4.857020997375328in\" height=\"4.213779527559055in\"}</p> <p>Figure 4.32: Load documents and initialize document store</p> <p>{width=\"4.85691491688539in\" height=\"2.6891972878390202in\"}</p> <p>Figure 4.33: Define keyword and embedding based retrievers</p> <p>queries presented to the retrieval component of RAG systems, ultimately leading to more accurate and contextually relevant results.</p> <p>The core concept is to transform the initial user query into an improved form that effectively captures the user's intent and aligns with the document retrieval phase. This often involves various steps and considerations, including:</p> <ol> <li> <p>Expanding User Queries: Query rewriting can involve expanding     the initial user query by adding synonyms, related terms, or     concepts that might improve the retrieval of relevant documents.     This expansion can be based on linguistic analysis or external     knowledge sources.</p> </li> <li> <p>Rephrasing for Clarity: Queries are often rewritten to improve     their clarity and conciseness. Ambiguous or complex phrasings can be     simplified to make the user's intent more explicit.</p> </li> <li> <p>Contextual Adaptation: The rewriting process may take into     account the specific context of the documents available for     retrieval. It can adapt</p> </li> </ol> <p>{width=\"4.856963035870516in\" height=\"3.0972583114610672in\"}</p> <p>Figure 4.34: Create end-to-end pipeline and run the retrievers the query to the characteristics of the document corpus, which is particularly valuable in domain-specific applications.</p> <p>Query rewriting is closely tied to the document retrieval phase in RAG systems. It contributes to better retrieval rankings, which, in turn, leads to more informative and contextually relevant answers generated by the language model. The goal is to ensure that the retrieved documents align closely with the user's intent and cover a wide spectrum of relevant information.</p> <p>4.6.1 Leveraging Large Language Models (LLMs) for Query</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#rewriting-in-rags","title":"Rewriting in RAGs","text":"<p>Question: With the advent of Large Language Models (LLMs) that have revolutionized natural language understanding and generation tasks, can we use them for query rewriting? The answer is: Yes. They can help in two primary ways: query expansion and generating better prompts. Figure 4.36 shows how query rewriting works. We can use LLM to either expand (enhance) a query or generate multiple (sub)queries for better retreival process.</p> <p>{width=\"4.857413604549431in\" height=\"1.1704615048118985in\"}</p> <p>Figure 4.35: Query re-writing using LLMs. LLM can expand the query or create multiple sub-queries.</p> <p>4.6.1.1 Query Expansion with LLMs</p> <p>LLMs possess an extensive understanding of language and vast knowledge repositories, which makes them ideal for query expansion. Here's how LLMs can aid in query expansion:</p> <p>Synonym Generation: LLMs can identify synonyms and related terms for words in the user's query. By expanding the query with synonyms, it increases the chances of retrieving documents that may use different terminology but are contextually relevant.</p> <p>Example: User Query: \"Renewable energy sources\". LLM Query Expansion: \"Renewable energy sources\" -&gt; \"Green energy sources,\" \"Sustainable energy sources,\" \"Eco-friendly energy sources\".</p> <p>By suggesting synonyms for \"renewable,\" the LLM broadens the query's scope to retrieve documents that may use alternative terminology.</p> <p>Conceptual Expansion: LLMs can identify concepts and entities related to the query. They can suggest adding relevant concepts, entities, or phrases that can lead to more comprehensive results. For instance, if a user queries about \"climate change,\" the LLM might suggest adding \"global warming\" to ensure a broader document retrieval.</p> <p>Example: User Query: \"Mars exploration\". LLM Query Expansion: \"Mars exploration\" -&gt; \"Mars mission,\" \"Red planet research,\" \"Space exploration of Mars\".</p> <p>Multilingual Query Expansion: For multilingual queries, LLMs can assist in translating and expanding the query into multiple languages, broadening the search scope and retrieving documents in various languages.</p> <p>4.6.1.2 Generating Better Prompts with LLMs</p> <p>LLMs can assist in generating more effective prompts for retrieval, especially when a prompt-based retrieval mechanism is employed. Here's how LLMs contribute to prompt generation.</p> <p>Query Refinement: LLMs can refine a user query by making it more concise, unambiguous, and contextually relevant. The refined query can then serve as a prompt for the retrieval component, ensuring a more focused search.</p> <p>Example: User Query: \"How does photosynthesis work?\" LLM-Generated Prompt: \"Explain the process of photosynthesis.\"</p> <p>Multi-step Prompts: In complex queries, LLMs can generate multi-step prompts that guide the retrieval component through a series of sub-queries. This can help break down intricate requests into more manageable retrieval tasks.</p> <p>Example: User Query: \"Market trends for electric cars in 2021\". LLM-Generated Multi-Step Prompts:</p> <ul> <li> <p>\"Retrieve market trends for electric cars.\"</p> </li> <li> <p>\"Filter results to focus on trends in 2021.\"</p> </li> </ul> <p>The LLM generates sequential prompts to guide the retrieval component in finding documents related to market trends for electric cars in 2021.</p> <p>Context-Aware Prompts: LLMs can consider the context of the available document corpus and generate prompts that align with the characteristics of the documents. They can adapt prompts for specific domains or industries, ensuring that the retrieval phase retrieves contextually relevant documents.</p> <p>Example: User Query: \"Legal documents for the healthcare industry\". LLMGenerated Domain-Specific Prompt: \"Retrieve legal documents relevant to the healthcare industry.\"</p> <p>Understanding the context, the LLM generates a prompt tailored to the healthcare industry, ensuring documents retrieved are pertinent to that domain.</p> <p>Query rerwriting for RAGs is an active research area, and new approaches are suggested regularly. One recent research is Ma et al. (2023), where they propose a new framework for query generation. See here to learn more about their method.</p> <p>4.7 Query Routing in RAG</p> <p>Query routing in RAG, often facilitated by a router, is the process of automatically selecting and invoking the most suitable retrieval technique or tool for a given user query. It enables a dynamic, adaptive approach to choose how to retrieve information based on the specific requirements of each query.</p> <p>Rather than relying on a fixed retrieval method, query routing empowers the system to intelligently assess the user's query and select the appropriate retrieval mechanism. This approach is particularly powerful in scenarios where a diverse range of retrieval techniques or tools can be employed to answer different types of queries. The following shows the generate architecture of this approach.</p> <p>How Query Routing Works:</p> <ol> <li>User Query Input:</li> </ol> <p>A user enters a query into the RAG system. This query could encompass various types of information needs, such as fact-based lookup, summarization, translation, question answering, etc.</p> <ol> <li>Query Analysis:</li> </ol> <p>The router or the query routing component first performs an analysis of the user's query. This analysis involves understanding the nature of the query, its intent, and the type of information required.</p> <ol> <li>Detection of Retrieval Technique Requirement:</li> </ol> <p>Based on the analysis, the router detects the retrieval technique or tool that best matches the query's requirements. This detection is often driven by heuristics, pre-defined rules, machine learning models, or a combination of these methods.</p> <p>4.7QueryRoutinginRAG</p> <p>{width=\"3.1388495188101486in\" height=\"6.362533902012249in\"}</p> <p>Figure 4.36: Query router architecture</p> <ol> <li>Selection of Retrieval Technique:</li> </ol> <p>The router selects the most appropriate retrieval technique from a predefined set, which can include methods like fact-based lookup in a vector store, summarization, document retrieval, question answering, or translation, among others.</p> <ol> <li>Invoke the Retrieval Component:</li> </ol> <p>The router then calls the relevant retrieval component or \"tool\" that specializes in the chosen retrieval technique. This component may be a vector store interface, a summarization tool, a translation service, or any other retrieval method.</p> <ol> <li>Information Retrieval:</li> </ol> <p>The selected retrieval component performs the necessary information retrieval or processing based on the chosen technique. For example, if factbased lookup is required, it retrieves facts from a vector store. If summarization is needed, it generates a concise summary. If translation is the goal, it translates the content.</p> <ol> <li>Generation of Output:</li> </ol> <p>The retrieved or processed information is then used by the generation component in the RAG system to compose a response, which is presented to the user.</p> <p>There are multiple ways to implement query routing in Retrieval-Augmented Generation (RAG) systems:</p> <ol> <li> <p>Intent Classification Model:</p> <ul> <li>Use a Classifier: Employ a pre-trained classifier model that     can categorize user queries based on their intent. This     classifier can have predefined categories such as fact-based     lookup, summarization, translation, question answering, etc. The     selected category then dictates the retrieval method to be used.</li> </ul> </li> </ol> <p>4.7QueryRoutinginRAG</p> <ul> <li>Machine Learning: Train a custom intent classification model on     labeled query data to predict the query's intent. This model can be     fine-tuned on specific intent detection tasks relevant to the RAG     system.</li> </ul> <p>Figure 4.37 shows how to use a zero-shot classifer to categorize user queries.</p> <p>{width=\"4.857191601049869in\" height=\"3.4578444881889765in\"}</p> <p>Figure 4.37: Using a zero-shot classifier to categorize and route user queries</p> <ol> <li> <p>Prompt-Based Routing:</p> <ul> <li> <p>Leverage LLMs: Utilize Large Language Models (LLMs), such as     GPT-4 or similar models, to perform query classification. A     prompt can be designed to guide the LLM in classifying the query     based on its intent.</p> </li> <li> <p>Template Prompts: Create a set of template prompts that are     specifically designed to categorize queries. These templates can     include leading questions or cues to elicit the intent of the     query.</p> </li> </ul> </li> </ol> <p>The code in Figure 4.38 shows how to use a LLM text-davinci-002 for query routing.</p> <p>{width=\"4.8570352143482065in\" height=\"3.592600612423447in\"}</p> <p>Figure 4.38: Using a LLM to categorize and route user queries</p> <ol> <li> <p>Rule-Based Routing:</p> <ul> <li> <p>Rule-Based System: Develop a rule-based system that consists     of predefined rules or conditions to categorize queries. For     example, if a query starts with \"Translate,\" it is routed to a     translation retrieval method.</p> </li> <li> <p>Regular Expressions: Use regular expressions to match query     patterns and automatically route queries based on predefined     patterns, keywords, or structures.</p> </li> </ul> </li> </ol> <p>The choice of implementation depends on the complexity of the RAG system, the available resources, and the specific requirements of the application. Implementing a combination of these methods can provide a robust and adaptive query routing mechanism that enhances the overall performance of RAG systems.</p> <p>Let's take a look at an example where we use LlamaIndex framework for routing queries between a summarization route and fact-based route. LlamaIndex has a concept called RouterQueryEngine which accepts a set of query engines QueryEngineTool objects as input. A QueryEngineTool is essentially an index used for retrieving documents from vector database. First step is to load the documents and define the summary index and fact-based index, which is displayed in Figure 4.39.</p> <p>Then, we create QueryEngine objects for each index and add them to the RouterQueryEngine object. Figure 4.40 shows this step.</p> <p>By dynamically routing queries to the most suitable retrieval techniques, query routing enhances RAG by ensuring that the system adapts to the specific requirements of each user query, providing more accurate and context-aware responses. It overcomes the challenge of needing to know in advance which retrieval technique to apply and optimizes the retrieval process for different types of information needs.</p> <p>4.8 Leveraging User History to Enhance RAG Performance</p> <p>Retrieval-Augmented Generation (RAG) systems have gained prominence in natural language processing applications due to their ability to combine the</p> <p>{width=\"4.857200349956256in\" height=\"2.7889031058617673in\"}</p> <p>Figure 4.39: Query routing example in LlamaIndex. First we load documents and create different indecies.</p> <p>{width=\"4.857200349956256in\" height=\"4.565658355205599in\"}</p> <p>Figure 4.40: Define QueryEngine and RouterQueryEngine objects, and run the engine for user queries.</p> <p>strengths of information retrieval and language generation. However, RAG applications often involve significant computational and financial costs due to the necessity of embeddings, retrievals and even response generation. When dealing with recurrent user queries or similar questions, these costs can be optimized by leveraging the memory of previously asked questions.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#481-challenge","title":"4.8.1 Challenge","text":"<p>In many real-world applications, users tend to ask similar or nearly identical questions repeatedly. For instance, in a customer support chatbot, users may have common queries related to product information or troubleshooting procedures. While the RAG framework excels at generating relevant responses based on retrieval, it may not be efficient to re-embed and retrieve information for the same questions each time they are asked. This is where the concept of leveraging user history comes into play.</p>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#482-how-user-history-enhances-rag-performance","title":"4.8.2 How User History Enhances RAG Performance","text":"<p>Leveraging user history to enhance RAG performance involves building a memory or cache of previously asked questions and their corresponding responses. When a user query is received, instead of immediately triggering an embedding and retrieval process, the system can check the user history to see if it has encountered a similar or identical query before. This approach offers several advantages:</p> <ol> <li> <p>Faster Response Times (i.e. reduced latency): By comparing the     new query to the user history, the system can identify duplicate or     closely related questions. In such cases, it can bypass the     embedding and retrieval steps, significantly reducing response     times.</p> </li> <li> <p>Cost Reduction: Skipping the resource-intensive retrieval     process for recurrent queries leads to a notable reduction in     computational and financial costs, as the need for additional API     calls or data processing diminishes.</p> </li> <li> <p>Consistency and Accuracy: Responses to repeated questions can     remain consistent and accurate over time. The use of cached     responses from the user history ensures that users receive reliable     information without relying on new retrievals. Additionally, by     keeping track of user history, RAG can learn to better understand     the user's intent and context. This can lead to more accurate     answers to questions, even if the questions are not perfectly     phrased.</p> </li> </ol>"},{"location":"Spanda%20Bootcamp%20Day%204%20-%20Very%20Raw/#483-how-memoryuser-history-works","title":"4.8.3 How Memory/User History Works","text":"<p>A simple RAG memory can be implemented as follows: We need to maintain a log of user queries and their corresponding answers. Therefore, we can define the memory as a key-value store, where the keys are questions and the values are answers. When a user asks a question, RAG first checks its memory to see if it has already answered a similar question. If it has, then it simply retrieves the answer from its memory and returns it to the user. If it has not already answered a similar question, then it performs the embedding and retrieval process as usual.</p> <p>We need to have a mechanism that constantly updates the RAG's memory with new questions and answers. As a user asks more questions, RAG's memory grows and it becomes better able to answer future questions accurately and efficiently.</p> <p>Implementation: The snippet code in Figure 4.41 shows a very basic implementation of memory for RAG.</p> <p>While a simple in-memory dictionary, as shown in the previous example, can be effective for maintaining recent user history, it might not be sufficient for handling a large-scale conversation history or long-term memory. To address</p> <p>{width=\"4.857076771653543in\" height=\"4.521554024496938in\"}</p> <p>Figure 4.41: A basic key-value implementation of memory for RAG. these challenges, RAG systems can benefit from using a vector database for memory.</p> <p>The implementation of a vector database for memory in a RAG system involves several key steps:</p> <ol> <li> <p>Data Ingestion: Store historical question-answer pairs along     with their corresponding embeddings in the vector database. This     process typically involves a batch or incremental data ingestion     pipeline.</p> </li> <li> <p>Retrieval: When a user poses a question, retrieve the most     relevant historical answers by calculating the similarity between     the question's embedding and the embeddings in the vector database.</p> </li> <li> <p>Updating Memory: Periodically update the vector database with     new question-answer pairs and their embeddings to keep the memory up     to date.</p> </li> <li> <p>Caching: Implement a caching mechanism to enhance retrieval     speed by temporarily storing frequently accessed data in memory.</p> </li> <li> <p>Query Optimization: Use efficient indexing and search algorithms     to optimize the retrieval process, reducing query response times.</p> </li> </ol> <p>Incorporating a vector database for memory in a RAG system enhances its ability to provide contextually relevant and coherent responses by efficiently managing and retrieving historical information. This approach is particularly valuable in scenarios where extensive conversation histories or long-term memory are essential for the application's success.</p> <p>As we wrap up our exploration of advanced RAG systems in this Chapter, we are on the cusp of a new frontier. In Chapter 5 - \"Observability Tools for RAG,\" we will discuss various observability tools tailored for RAG systems. We will explore their integration with LlamaIndex, including Weights &amp; Biases, Phoenix, and HoneyHive. These tools will not only help us monitor and evaluate the performance of our RAG systems but also provide valuable insights for continuous improvement.</p> <ol> <li>Observability Tools for RAG</li> </ol> <p>An observability tool is a software or platform designed to help monitor, analyze, and gain insights into the performance, behavior, and health of a complex system, such as a machine learning model, a RAG system, or a software application. These tools provide visibility into various aspects of a system, allowing operators, administrators, and developers to understand how the system operates and to detect and troubleshoot issues.</p> <p>Key components and features of an observability tool typically include:</p> <ol> <li> <p>Data Collection: Observability tools collect data from various     sources within the system. This data can include metrics (e.g., CPU     usage, memory usage), logs, traces, events, and more. The broader     the range of data collected, the more comprehensive the     observability.</p> </li> <li> <p>Storage: Data collected by the tool is stored for analysis and     historical reference. The storage can be in the form of time-series     databases, log repositories, or other storage solutions designed to     handle large volumes of data.</p> </li> <li> <p>Analysis: Observability tools provide analytical capabilities to     process and make sense of the collected data. This includes querying     data, aggregating metrics, and identifying patterns or anomalies.</p> </li> <li> <p>Visualization: The tools offer visualization capabilities to     create graphs, charts, dashboards, and reports that make it easier     for users to interpret data. Visualizations help spot trends,     issues, and performance bottlenecks.</p> </li> <li> <p>Alerting: Many observability tools allow users to define     alerting rules. When certain conditions are met, the tool sends     notifications, enabling operators to respond to issues promptly.</p> </li> <li> <p>Tracing: For distributed systems, tracing is important.     Observability tools often provide tracing features to track requests     as they move through various services or components of a system.     This helps pinpoint performance bottlenecks and issues.</p> </li> <li> <p>User Interface: A user-friendly interface is essential for     interacting with the data and insights generated by the     observability tool. It should allow users to explore data, set up     alerts, and visualize information.</p> </li> <li> <p>Integration: Observability tools should integrate with various     components of the system, such as applications, databases,     containers, and cloud services, to capture relevant data.</p> </li> <li> <p>Scalability: The tool should be able to scale with the system it     monitors. It needs to handle growing data volumes and provide     insights without compromising performance.</p> </li> <li> <p>Customization: Users should be able to customize the tool to     meet the specific needs of their system. This includes defining     custom dashboards, alerts, and data collection methods.</p> </li> </ol> <p>There are several observability tools for RAG based systems. Frameworks like LlamaIndex also provides an easy way to integrate some of these tools with RAG application. This enable us to:</p> <ul> <li> <p>View LLM prompt inputs and outputs</p> </li> <li> <p>Make sure that all the components such as embedding models, LLMs and     vector databases are working as expected \u2022 View indexing and     querying traces</p> </li> </ul> <p>In order to integrate observability tools with LlamaIndex, we simply need to do the following, Figure 5.1:</p> <p>5.1Weights&amp;BiasesIntegrationwithLlamaIndex</p> <p>{width=\"4.857076771653543in\" height=\"1.1823140857392826in\"}</p> <p>Figure 5.1: General pattern for integrating observability tools into LlamaIndex</p> <ol> <li>Weights &amp; Biases Integration with LlamaIndex</li> </ol> <p>Weights &amp; Biases is a machine learning platform that empowers developers to enhance their models efficiently. It provides versatile tools for experiment tracking, dataset versioning, model evaluation, result visualization, regression identification, and seamless collaboration with peers.</p> <p>The code depicted in Figure 5.2 shows how to integrate W&amp;B with LlamaIndex. For complete example, please see here.</p> <p>We can even see the logs as shown in Figure 5.3.</p> <p>If we go the W&amp;B website and login, we can see all the details, Figure 5.4 displays our project including charts, artifacts, logs, and traces.</p> <ol> <li>Phoenix Integration with LlamaIndex</li> </ol> <p>Phoenix is an observability tool designed for LLM applications, offering insight into their inner workings. It provides a visual representation of query engine calls and highlights problematic execution spans based on factors like latency and token count, aiding in performance evaluation and optimization.</p> <p>Figure 5.5 shows the general usage pattern to use Phoenix.</p> <p>{width=\"4.857076771653543in\" height=\"3.9144192913385827in\"}</p> <p>Figure 5.2: W&amp;B integration with LlamaIndex</p> <p>{width=\"4.857156605424322in\" height=\"0.8855588363954505in\"}</p> <p>Figure 5.3: W&amp;B logs at different steps</p> <p>5.2PhoenixIntegrationwithLlamaIndex</p> <p>{width=\"4.8569378827646545in\" height=\"1.6475831146106736in\"}</p> <p>Figure 5.4: W&amp;B dashboard</p> <p>{width=\"4.857076771653543in\" height=\"2.093017279090114in\"}</p> <p>Figure 5.5: Phoenix integration with LlamaIndex RAG applications</p> <p>When we run queries, we can see the traces in real time in the Phoenix UI. Figure 5.6 illustrates the Phoenix UI for a RAG application.</p> <p>{width=\"4.856912729658792in\" height=\"3.359212598425197in\"}</p> <p>Figure 5.6: Phoenix UI that shows traces of queries in real time.</p> <p>A complete example of tracing a LlamaIndex RAG application using Phoenix is available at this link.</p> <p>5.3 HoneyHive Integration with LlamaIndex</p> <p>HoneyHive is a framework that can be used to test and evaluate, monitor and debug LLM applications. It can be seamlessly integrated as displayed in Figure 5.7 into LlamaIndex applications.</p> <p>The HoneyHive dashaboard looks like Figure 5.8 below:</p> <p>5.3HoneyHiveIntegrationwithLlamaIndex</p> <p>{width=\"4.857076771653543in\" height=\"1.4858814523184602in\"}</p> <p>Figure 5.7: HoneyHive integration with LlamaIndex</p> <p>{width=\"4.857031933508312in\" height=\"2.549229002624672in\"}</p> <p>Figure 5.8: HoneyHive dashboard</p> <p>For a complete guide, see this tutorial.</p> <p>Other observability tools we can use include Truera, databricks, and Elastic observability among many other tools that are available.</p> <ol> <li>Ending Note</li> </ol> <p>In concluding our journey through the pages of \"A Practical Approach to Retrieval Augmented Generation Systems,\" we hope you have gained valuable insights into the world of Retrieval-Augmented Generation. As the landscape of AI continues to evolve, RAG systems present an exciting intersection of retrieval and generation technologies, with immense potential across a multitude of industries and applications. With each chapter, we've delved deeper into the core principles, strategies, and techniques that underpin the development and implementation of RAG systems.</p> <p>Remember that the field of AI is dynamic and ever-changing. While this book has aimed to provide a comprehensive understanding of RAG, new developments and possibilities are always on the horizon. We encourage you to continue exploring, experimenting, and innovating in the realm of Retrieval-Augmented Generation. Your journey doesn't end here; it's just the beginning.</p> <p>Thank you for joining us on this transformative adventure into the heart of AI's future.</p> <p>6.1 Acknowledgements</p> <p>We would like to express our gratitude to the teams behind LlamaIndex,</p> <p>LangChain, and Haystack for their invaluable contributions to the field of Retrieval-Augmented Generation (RAG). Their comprehensive documentations and tutorials have been instrumental in our journey, allowing us to learn from their expertise and leverage the fascinating tools they have built.</p> <p>References</p> <p>Lewis, Patrick, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir</p> <p>Karpukhin, Naman Goyal, Heinrich K\u00fcttler, et al. 2020. \"Retrieval-</p> <p>Augmented Generation for Knowledge-Intensive Nlp Tasks.\" Advances in Neural Information Processing Systems 33: 9459--74.</p> <p>Liu, Nelson F, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. \"Lost in the Middle: How Language Models Use Long Contexts.\" arXiv Preprint arXiv:2307.03172.</p> <p>Liu, Ye, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, and</p> <p>Philip S Yu. 2021. \"Dense Hierarchical Retrieval for Open-Domain Question Answering.\" arXiv Preprint arXiv:2110.15439.</p> <p>Ma, Xinbei, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. \"Query Rewriting for Retrieval-Augmented Large Language Models.\" arXiv Preprint arXiv:2305.14283.</p> <p>Roeder, Luke Metz, Geoffrey, and Durk Kingma. 2021. \"On Linear Identifiability of Learned Representations.\" arXiv Preprint arXiv:2007.00810.</p> <p>Zhao, Wayne Xin, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. 2022. \"Dense Text Retrieval Based on Pretrained Language Models: A Survey.\" arXiv Preprint arXiv:2211.14876.</p> <p>Zheng, Lianmin, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, et al. 2023. \"Judging LLM-as-a-Judge with MTBench and Chatbot Arena.\" arXiv Preprint arXiv:2306.05685.</p> <p>121</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/","title":"Spanda Bootcamp Day 5   very raw","text":"<p>Spanda Bootcamp Day Five</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#introduction-to-neo4j-genai","title":"Introduction to Neo4j &amp; GenAI","text":"<p>[Today, we will cover knowledge graphs, Graph Databases (Neo4j) and LLMs.]{.mark}</p> <p>[You will learn about:]{.mark}</p> <ul> <li> <p>[Neo4j]{.mark}</p> </li> <li> <p>[Knowledge Graphs]{.mark}</p> </li> <li> <p>[LLMs]{.mark}</p> </li> <li> <p>[How to use Neo4j for Retrieval Augmented Generation (RAG)]{.mark}</p> </li> <li> <p>[Integrating Neo4j and LLMs using Python and Langchain]{.mark}</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#what-is-neo4jmark","title":"[What is Neo4j?]{.mark}","text":"<p>[Neo4j is a graph database and analytics system that allows us to store, manage, and query highly connected data.]{.mark}</p> <p>[Unlike traditional relational databases, which use tables and rows, Neo4j uses a graph-based model with nodes and relationships.]{.mark}</p> <p>{width=\"6.5in\" height=\"3.486111111111111in\"}</p> <p>[Making Neo4j particularly well-suited for representing and querying complex, interconnected data.]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#what-are-knowledge-graphsmark","title":"[What are Knowledge Graphs?]{.mark}","text":"<p>[Knowledge graphs are a specific implementation of a Graph Database, where information is captured and integrated from many different sources, representing the inherent knowledge of a particular domain.]{.mark}</p> <p>[They provide a structured way to represent entities, their attributes, and their relationships, allowing for a comprehensive and interconnected understanding of the information within that domain.]{.mark}</p> <p>[Knowledge graphs break down sources of information and integrate them, allowing you to see the relationships between the data.]{.mark}</p> <p>[You can tailor knowledge graphs for semantic search, data retrieval, and reasoning.]{.mark}</p> <p>[You may not be familiar with the term knowledge graph, but you have probably used one. Search engines typically use knowledge graphs to provide information about people, places, and things.]{.mark}</p> <p>[The following knowledge graph could represent Neo4j:]{.mark}</p> <p></p> <p>[A diagram of an abstract knowledge graph showing how sources contain chunks of data about topics which can be related to other topics]{.mark}</p> <p>[This integration from diverse sources gives knowledge graphs a more holistic view and facilitates complex queries, analytics, and insights.]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#knowledge-graphs-and-ontologiesmark","title":"[Knowledge Graphs and Ontologies]{.mark}","text":"<p>[For more on Knowledge Graphs, Ontologies, we recommend watching the Going Meta -- A Series on Graphs, Semantics and Knowledge series on YouTube.]{.mark}</p> <p>[Knowledge graphs can readily adapt and evolve as they grow, taking on new information and structure changes.]{.mark}</p> <p></p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#_1","title":"Spanda Bootcamp Day 5   very raw","text":""},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#neo4j-install-and-setupmark","title":"[Neo4j Install and setup:]{.mark}","text":""},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#_2","title":"Spanda Bootcamp Day 5   very raw","text":""},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#large-language-models-generative-aimark","title":"[Large Language Models &amp; Generative AI]{.mark}","text":"<p>[Large Language Models, referred to as LLMs, learn the underlying structure and distribution of the data and can then generate new samples that resemble the original data.]{.mark}</p> <p>[LLMs are trained on vast amounts of text data to understand and generate human-like text. LLMs can answer questions, create content, and assist with various linguistic tasks by leveraging patterns learned from the data.]{.mark}</p> <p>[Generative AI is a class of algorithms and models that can generate new content, such as images, text, or even music. New content is generated based on user prompting, existing patterns, and examples from existing data.]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#what-are-llmsmark","title":"[What are LLMs?]{.mark}","text":"<p>[LLMs or Large Language Models is a type of generative AI that has been trained on a massive scale of data (petabytes of data) and can produce novel responses to any type of question, that's why the name generative. These models are based on the Transformer architecture and require very big GPU-based data centers. FYI, it took around 100 million USD to train ChatGPT. These models are massive, they can't fit on any single server; their parameter counts go in the order of trillions.]{.mark}</p> <p>[One way to think about these models is to consider them as an idea-generation machine. They can generate or give an approximate answer to any textual query, even if it has not seen similar stuff in the past. Given the size of the model data, these models have somehow captured the essence of language, in some cases they even learned languages that were not even part of the training data.]{.mark}</p> <p>[Some of you may ask, how is this possible? This is called Emergent capabilities. It has been shown in research that these models might develop completely new capabilities and capacities as we increase their size.]{.mark}</p> <p>[Note: There are some doubts about LLMs\\' emergent capabilities as well.]{.mark}</p> <p>[If you want to know more details about Transformer architecture: click here]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#types-of-llmsmark","title":"[Types of LLMs]{.mark}","text":"<p>[Currently, we have three types of LLMs:]{.mark}</p> <p>[Closed Source: Models like ChatGPT or GPT-4 can only be accessed as a service through the API provided by OpenAI.]{.mark}</p> <p>[Open Source, closed architecture: Like Llama 2, the weight files of these models are open-sourced, meaning we can run them locally, and use them to train smaller models, but their architecture is still not public.]{.mark}</p> <p>[Fully open source: T5, BERT, and GPT-2 are a few of the fully open-sourced models, having both their weights and architecture open-sourced.]{.mark}</p> <p>[Compared to Fully open-sourced models, Closed or partial Open-source models are much bigger in size.]{.mark}</p> <p>[But, what exactly are these models, how do they look like, and what is their structure?]{.mark}</p> <p>[If we talk about models like Llama 2, 70B, it is a model with 70 billion parameters. There are two files that we need to use this model. A weight file or parameter file (140 GB of basic matrices) and a 500-line C code that run these weights or parameter files.]{.mark}</p> <p>[The reason why the parameter file is 140 GB is because the precision of weight matrices is Float16, which means 2 bytes, meaning 140 GB for a 70 billion parameters model.]{.mark}</p> <p>[These two files are completely self-contained packages, we don't need anything else, just compile the C code that points towards this parameter file and we are good to go to generate text on any kind of question locally (without internet or anything).]{.mark}</p> <p>[Running LLMs on your Laptop:]{.mark}</p> <p>[LM Studio is a free, desktop software tool that makes installing and using open-source LLM models extremely easy.]{.mark}</p> <p>[Here's how to use it:]{.mark}</p> <p>[1. Go to \"lmstudio.ai\":]{.mark}</p> <p>[2. Download and install the version for your operating system:]{.mark}</p> <p></p> <p>[LM Studio webpage]{.mark}</p> <p>[3. Open LM Studio using the newly created desktop icon:]{.mark}</p> <p>[4. Select an LLM to install. (1) You can do this by either selecting one of the community suggested models listed in the main window, or (2) by using the search bar for any model available at HuggingFace (just look up a keyword and all associated models will be listed). Note that there are currently 371,692 models listed at HuggingFace.co:]{.mark}</p> <p></p> <p>[selecting LLMs]{.mark}</p> <p>[5. Whether you elect to download from the community suggested models, or search for one on your own, you can see the size of the install/download file. So be sure you are okay with the size of the download.]{.mark}</p> <p></p> <p>[specific model information]{.mark}</p> <p>[You will note that at the top of the left half of the screen over the release date column, is \"compatibility guess\". LM Studio has checked your system and is presenting those models which it feels you will be able to run on your computer. To see All Models, click on \"compatibility guess\" (#1). Clicking on a model on the left, will present the available versions on the right and display those models which should work given your computer's specs (#2).]{.mark}</p> <p></p> <p>[Compatibility and Should Work indicators]{.mark}</p> <p>[Note that depending on the capabilities/speed of your computer, larger models will be more accurate but slower. You will also find that most of these models are quantized.]{.mark}</p> <p>[A little explanation about quantization. Quantization refers to using lower precision numbers like 8-bit integers rather than 32-bit floating point values to represent the weights and activations in the model. This reduces memory usage and speeds up inference on your computer's hardware. Quantization can reduce model accuracy slightly compared to a full precision version, but provides up to 4x memory savings and faster inference. Think of it like how MP-3's are compressed music files or .jpgs are compressed image files. Although these are of less quality, you often won't see a significant difference. In the case of LLM's, the \"Q\" number you see in the listing of the LLM, represents the amount of quantization. Lower is more and higher is less quantization.]{.mark}</p> <p>[Also, in the model listing, you will see references to GGML and GGUF. Don't worry about it. These are two quantization strategies; \"Mixed Logits\" vs \"Uniformly Quantized Fully Connected\". GGML provides a more flexible mixed-precision quantization framework while GGUF is specifically optimized for uniformly quantizing all layers of Transformer models. GGML may enable higher compression rates but GGUF offers simpler deployment.]{.mark}</p> <p>[6. Once the model has finished its download, (1) select the model from the drop-down menu at the top of the window; (2) select the chat bubble in the left side column; (3) open up the following sections on the right, \"Context Overflow Policy\" and \"Chat Appearance\".]{.mark}</p> <p></p> <p>[ready the model]{.mark}</p> <p>[7. Make sure \"Maintain a rolling window and truncate past messages\" is selected under \"Content Overflow Policy\" and \"Plaintext\" is selected under \"Chat Appearance\".]{.mark}</p> <p></p> <p>[8. Now close those two areas and open up \"Model Configuration\" and then open \"Prompt Format\" and scroll down to \"Pre-prompt / System prompt\" and select the \"&gt;\" symbol to open that. Here you can enter the system \"role\". Meaning, you can set up how you want the bot to act and what \"skills\" or other specific qualities should be provided in its answers. You can modify what is there to suit your needs. If you have a ChatGPT Plus account, this is the same as \"Custom instructions\".]{.mark}</p> <p></p> <p></p> <p>[adding system role / custom instructions]{.mark}</p> <p>[9. Continue to scroll down in this column until you come to \"Hardware Settings\". Open this area if you wish to offload some processing to your GPU. The default is to allow your computer's CPU to do all the work, but if you have a GPU installed, you will see it listed here. If you find the processing of your queries is annoyingly slow, offloading to your GPU will greatly assist with this. Play around with how many layers you want it to handle (start with 10--20). This really depends on the model and your GPU. Leaving it all to be handled by the CPU is fine but the model might run a bit slow (again... depending on the model and its size). You also have the option to increase the number of CPU threads the LLM uses. The default is 4 but you can increase the number, or just leave it where it is if you don't feel comfortable experimenting and don't know how many threads your CPU has to play with.]{.mark}</p> <p></p> <p>[optional hardware settings]{.mark}</p> <p>[10. After these changes, you are now ready to use your local LLM. Simply enter your query in the \"USER\" field and the LLM will respond as \"AI\".]{.mark}</p> <p></p> <p>[chat dialogue]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#model-trainingmark","title":"[Model training]{.mark}","text":"<p>[Model training is not as straightforward as Model Inference, for inference we can use a local machine with a decent GPU, but training LLMs is massively expensive.]{.mark}</p> <p>[Llama 2, which has around 10 TB of training data, was gathered by crawling all over the internet. It needed 6000 GPUs running for 12 days costing 2 Million USD to train it.]{.mark}</p> <p>[Basically, it compressed this huge chunk of internet (10 GB) into a 140 GB of parameter file. You can think of it as zipping the internet into a single file but with one big difference, it is a lossy compression, unlike lossless compression of zipping.]{.mark}</p> <p>{width=\"6.5in\" height=\"3.6666666666666665in\"}</p> <p>[LLM compressing the internet to model parameters ([Img Src]{.underline})]{.mark}</p> <p>[Just so you know, the current SOTA in LLMs is maybe 10 or even 100 times bigger than this.]{.mark}</p> <p>[But why are we calling training of LLMs as compression of the internet?]{.mark}</p> <p>[The simplest way to understand LLMs is that they are the next word prediction machine, and it can be mathematically shown that predicting the next word is very similar to compressing that data.]{.mark}</p> <p>[Compression is not the only way to think about LLMs, we can also think about it as finding the conditional probability, predicting the next token (token can be thought of similar to a word) given a sequence of words, like in the below example, finding the probability of predicting store, given I went to the. LLMs can be thought of as a model that learns all these conditional probabilities.]{.mark}</p> <p>[However, this view of conditional probability is very simplistic, and a lot of researchers don't agree with that. For instance, Illya Sutskever (creator of ChatGPT) thinks that in order to predict the next words, these models have built internal world models like humans do to understand concepts.]{.mark}</p> <p></p> <p>[Next word prediction ([Img Src]{.underline})]{.mark}</p> <p>[But how? He believes that since the internet is a representation of human experience, and these models are being trained on massive amounts of data, somehow these models have built a detailed understanding of abstract concepts. That's why they have shown the ability to score better than humans in a lot of cases and even surpass the [Turing Test]{.underline}.]{.mark}</p> <p>[Illya's position is highly contested by other Scientists like VP of META Yann LeCun. As of now, there is no consensus on how and why LLMs are giving superb performance in some cases showing almost sentient behavior and at times failing miserably to do even basic stuff.]{.mark}</p> <p>[Here's a great article highlighting this: [click here]{.underline}]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#network-dreamsmark","title":"[Network dreams]{.mark}","text":"<p>[Since the LLMs are trained on raw internet text, they come across different types of textual data. For instance, in the below diagram, we can see the three different types of output dreamed by the network. If you zoom in on the details of this, you will realize it is creating fake ISBN numbers and that type of stuff. It has seen so much of that type of data, so it thinks that whenever it says ISBN, it needs to be followed by a random number.]{.mark}</p> <p>{width=\"6.5in\" height=\"2.4583333333333335in\"}</p> <p>[Network dreams different types of internet documents ([Img Src]{.underline})]{.mark}</p> <p>[Another important thing to remember here is that these models are called [autoregressive]{.underline} types of models. These models do not produce the entire texts in one go, but they predict one word at a time. Whatever word is predicted, that word is also taken as input to predict the next words and this process continues; it is summarized beautifully in the below diagram.]{.mark}</p> <p>{width=\"6.5in\" height=\"3.2916666666666665in\"}</p> <p>[Auto-Regressive nature of LLMs ([Img Src]{.underline})]{.mark}</p> <p>[Some of you may wonder, then how does it know when to stop producing the next word? These models are fine-tuned to predict special tokens like \\&lt;BREAK&gt;. The moment this token is produced, the model stops producing any text.]{.mark}</p> <p>[Karpathy on Hallucinations:]{.mark}</p> <p>{width=\"5.625in\" height=\"8.333333333333334in\"}</p> <p>[Karpahty on Hallucinations ([Img Src]{.underline})]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#how-does-it-workmark","title":"[How does it work?]{.mark}","text":"<p>[As mentioned above, LLMs use Transformer architecture to do their magic.]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#a-detailed-look-at-self-attention-and-transformersmark","title":"[A detailed look at Self-Attention and Transformers]{.mark}","text":""},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#a-deep-intuition-behind-transformer-architecturemark","title":"[A deep intuition behind Transformer architecture.]{.mark}","text":"<p>[medium.com]{.mark}</p> <p>[We know precisely how transformer architecture works, we know all the mathematical operations, yet]{.mark}</p> <p>{width=\"5.541666666666667in\" height=\"1.4583333333333333in\"}</p> <p>[Why are we saying that despite knowing how these models work, why are we not sure about what they learned? The answer to this lies in something called the [reversal curse [3]]{.underline}.]{.mark}</p> <p>[For instance, if we ask these models, who is Elon Musk's mother? It answers correctly, but when we ask \"who is Maye Musk's (Elon's mother) son?\", it fails.]{.mark}</p> <p>[And that's why we say we don't know what it learned.]{.mark}</p> <p>[To understand in more detail this behavior of LLMs, there is a completely new field emerging right now called Mechanistic Interpretability.]{.mark}</p> <p>[Here's a series on [Mechanistic Interpretability [4]]{.underline}. Do check it out if you want to know the best research area in the overall AI field.]{.mark}</p> <p>[Mechanistic interpretability in the context of DL involves delving into the inner workings of these models to understand how individual components contribute to overall behavior. To put it technically, we want to elucidate the function of each neuron, layer, and pathway within the network with respect to how they process inputs and affect the final output.]{.mark}</p> <p>[Simply put, we somehow need to reverse-engineer the weights of the trained neural networks and convert them into a large binary file or Python code that, later on, we can use to know the limits of what the given model can't do. It's easier said than done; doing this is extremely hard; even small models of a few layers forget about models like GPT.]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#training-an-assistantmark","title":"[Training an Assistant]{.mark}","text":"<p>[What we talked till now is called pre-training, giving our models to just predict document-like answers, but that's not what we want. We want an assistant-like model.]{.mark}</p> <p>[Training LLMs have several components, the below diagram perfectly summarizes the entire LLM training pipeline.]{.mark}</p> <p>{width=\"6.5in\" height=\"3.6666666666666665in\"}</p> <p>[LLM training pipeline ([Img Src]{.underline})]{.mark}</p> <p>[So the use of the pre-training is to give a rough understanding of the language to the world. It learned grammatical rules and other rules a language might follow, but to make the network produce factually correct and coherent answers, we do something called, Supervised Fine-tuning.]{.mark}</p> <p>[Supervised Fine-tuning is a very time-consuming process, where we get actual people to write a set of questions and answers. We expose the model to this human-written content, and by doing so, the model learns to start behaving like an assistant.]{.mark}</p> <p>[Note: The pre-training stage has much more data but of low quality, compared to low volume and high-quality text of Supervised Fine-tuning. This step makes the model behave more like an assistant rather than producing entire documents. In short, it changes model behavior from producing internet documents to question-answer pairs.]{.mark}</p> <p>[To learn more about Supervised Fine-tuning: [click here]{.underline}]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#reinforced-learning-human-feedback-rlhfmark","title":"[Reinforced Learning Human Feedback (RLHF)]{.mark}","text":"<p>[As mentioned above getting the data for the Supervised Fine-tuning stage is very costly and time-consuming. For instance, it is much easier for humans to identify a good paragraph than write a good paragraph.]{.mark}</p> <p>[So this third stage is an optional stage, where we use the LLM itself to generate multiple answers to the same question and then select the best answers, mixed with some more edits with human data labelers, we fine-tune the model even further to get it more accurate. The below image summarizes the full process of RLHF.]{.mark}</p> <p>{width=\"6.5in\" height=\"3.6666666666666665in\"}</p> <p>[Instruct GPT based on RLHF ([Img Src]{.underline})]{.mark}</p> <p>[Read the full article from OpenAI on RLHF-based Instruct GPT: [click here]{.underline}]{.mark}</p> <p>[To learn more about Supervised Fine-tuning: [click here]{.underline}]{.mark}</p> <p>[There is a brand new research called]{.mark} [[RLAIF [7]]{.underline}]{.mark} [where we completely replace humans and use AI itself to self-critique its own response.]{.mark}</p> <p>[Note: Despite all this training and fine-tuning LLMs hallucinate quite a lot.]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#current-sota-llmsmark","title":"[Current SOTA LLMs]{.mark}","text":"<p>[Different LLMs can produce completely different results, thus evaluating them becomes quite tricky. There are some standard tests, yet, it is impossible to tell when an LLM is going to hallucinate.]{.mark}</p> <p>[Look at the below screenshot, I asked the same question to both the LLMs, and their answers were wildly different. I asked both the LLMs to explain to me something an LLM can't explain.]{.mark}</p> <p>[This question in itself is contradictory, in the case of Model B, it started talking about consciousness and other abstract ideas, and Model A gave a simple two-line response.]{.mark}</p> <p>[Do you see the problem? None of them are wrong, yet they are very very different.]{.mark}</p> <p>{width=\"6.5in\" height=\"2.3333333333333335in\"}</p> <p>[Comparing LLMs responses (Img Src: Belongs to author)]{.mark}</p> <p>[Given below is the ranking of different LLMs on this benchmark [[10]]{.underline}.]{.mark}</p> <p>{width=\"6.5in\" height=\"2.2222222222222223in\"}</p> <p>[Different LLMs leader board ranking ([Img Src]{.underline})]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#llm-scaling-lawsmark","title":"[LLM scaling laws]{.mark}","text":"<p>[It has been shown by many researchers that LLMs accuracy is remarkably smooth and well-behaved function. It means that we might need not to invent a new algorithm to reach AGI, we can just keep scaling these networks and keep getting better and more intelligent models.]{.mark}</p> <p>[This theory has worked well till now, but that doesn't mean that it can't hit the wall. It is very much possible that after a certain point, accuracy will not be increased no matter how much we scale these systems.]{.mark}</p> <p>[A few scientists believe that we can't achieve AGI by just using LLMs and Backpropagation. All the emergent intelligent behavior might not be a direct result of more computing. But as of now, we can neither confirm nor deny any of the positions. Scaling laws are working as of present.]{.mark}</p> <p></p> <p>[Scaling laws for LLMs ([Img Src]{.underline})]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#ai-assistant-using-toolsmark","title":"[AI assistant using tools]{.mark}","text":"<p>[The newer version of ChatGPT called GPT-4 can not only produce text, but it can also use different tools. For instance, this new version has been given the capability to search the internet and use that information to give more up-to-date and better answers.]{.mark}</p> <p>[Earlier versions of GPT could only answer things till the last date of their training data, usually a year behind the present.]{.mark}</p> <p>[How does GPT know when to use a browser?]{.mark}</p> <p>[Let Karpathy answer this himself:]{.mark}</p> <p>{width=\"6.236111111111111in\" height=\"2.5555555555555554in\"}</p> <p>[How GPT decides to use the internet ([Img Src]{.underline})]{.mark}</p> <p>[GPT 4-V can now even create Images, and understand sound and images directly. This new breed of LLMs is becoming more and more Multi-Modal (ability to take different types of Inputs).]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#thinking-systemsmark","title":"[Thinking Systems]{.mark}","text":"<p>[There is this idea that our brain works in two modes, one where it just retrieves information and the other where it has to think consciously. If someone asks you 2+2, we don't actually think, we just retrieve this information, but if someone asks 1089x12, we have to consciously calculate it.]{.mark}</p> <p>[Another way to think about this System 1 and System 2 type of intelligence is blitz chess, where you play instinctively, compared to regular chess, where you analyze everything in detail.]{.mark}</p> <p>[So we believe that current LLMs are type 1 intelligence, that's why every word is taking almost the same time to be predicted.]{.mark}</p> <p>[So, the idea is, can we make these LLMs self-reflect and move towards a type 2 intelligent system? Luckily, there has been some progress in this area as well, like the Chain of Thought, Tree of Thoughts, Graph of Thought, and the most recent one called Everything of Thoughts [11].]{.mark}</p> <p>[Here's an interesting blog on the same topic: [click here]{.underline}]{.mark}</p> <p>[Note: Because everything is so open-ended in language space, it is very tough to train LLMs in a reward function setting like Alpha Go, where it self-improved itself just by creating multiple agents.]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#custom-llmsmark","title":"[Custom LLMs]{.mark}","text":"<p>[Recently GPT released something called a GPT store, where we can give custom instructions to slightly modify the behavior of LLMs, where we can give our LLMs a type of personality. For instance, recent Grok AI from Twitter also has this option, where it can give very sassy responses.]{.mark}</p> <p>[Personally, I have tested out the GPT store, not the GROK AI, but honestly speaking I'm not that impressed by this personality thing, it's a hit-and-miss kind of a thing.]{.mark}</p> <p></p> <p>[Giving custom instruction to GPT (Img Src; Belong to author)]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#llm-os-similaritiesmark","title":"[LLM-OS similarities]{.mark}","text":"<p>[LLMs started as a generative AI model that just produces the next word given a set of tokens. But in the past few months, their capabilities have been extended to so many things, that they can hardly be called Language based models only.]{.mark}</p> <p>[LLMs have evolved to do all these things:]{.mark}</p> <p>{width=\"6.055555555555555in\" height=\"1.8472222222222223in\"}</p> <p>[LLM current functionalities ([Img Src]{.underline})]{.mark}</p> <p>[They are getting closer and closer to an Operating system. We can think of context window as RAM memory. We can think of retrieval Augmented generation as Hard Disk memory, where it pulls up relevant information from a Database and uses that to generate new text. We now even have an app store like Google App Store.]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#jailbreaksmark","title":"[Jailbreaks]{.mark}","text":"<p>[Just like any other piece of software, LLMs have not yet reached a place where they are very secure.]{.mark}</p> <p>[ChatGPT can be fooled with different types of attacks. By shifting the tone of the question in the below answer the user was able to ask GPT about how to make Napalms, now the answer might not be correct but that doesn't mean it can't be used to produce bio weapons and other types of dangerous stuff.]{.mark}</p> <p>[Avoiding this type of situation is the biggest challenge in AI safety.]{.mark}</p> <p></p> <p>[Jailbreaking ChatGPT ([Img Src]{.underline})]{.mark}</p> <p>[Another example of Jailbreak is Claude being fooled by Base 64 encoding of the same input text.]{.mark}</p> <p>{width=\"5.527777777777778in\" height=\"2.9444444444444446in\"}</p> <p>[Claude was fooled by the base64 version of the same text ([Img Src]{.underline})]{.mark}</p> <p>[Other type of attack is Prompt Injection, in the below diagram, GPT accessed a website and that website was injected with this malicious prompt to show a fraud link to the user.]{.mark}</p> <p>{width=\"5.847222222222222in\" height=\"5.236111111111111in\"}</p> <p>[GPT guiding user towards a fraud link ([Img Src]{.underline})]{.mark}</p> <p>[This type of attack is already a big problem, but the bigger problem is attackers were able to extract private data. For instance a lot of companies banned using ChatGPT because the system released their internal data. which got stored in GPT servers when those people asked their questions to GPT.]{.mark}</p> <p>[Not only that, a recent researcher even extracted training data from GPT.]{.mark}</p> <p>[To learn more about this latest jailbreak: [click here]{.underline}]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#conclusionmark","title":"[Conclusion]{.mark}","text":"<p>[The field of LLMs is still evolving, there are a lot of cool, exciting, and a bit dangerous things happening in the field. LLM security is a big risk, and my personal belief is that these models should not be released without proper safeguarding (that's just my personal belief). Next time, we will try to get a more detailed look at what GPTs can and can't do.]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#instructing-an-llmmark","title":"[Instructing an LLM]{.mark}","text":"<p>[The response generated by an LLM is a probabilistic continuation of the instructions it receives. The LLM provides the most likely response based on the patterns it has learned from its training data.]{.mark}</p> <p>[In simple terms, if presented with the prompt \\\"Continue this sequence - A B C\\\", an LLM could respond \\\"D E F\\\".]{.mark}</p> <p>[To get an LLM to perform a task, you provide a prompt, a piece of text that should specify your requirements and provide clear instructions on how to respond.]{.mark}</p> <p>[A user asks an LLM the question \\'What is an LLM? Give the response using simple language avoiding jargon.\\']{.mark}</p> <p>{width=\"5.151042213473316in\" height=\"2.7291666666666665in\"}</p> <p>[Precision in the task description, potentially combined with examples or context, ensures that the model understands the intent and produces relevant and accurate outputs.]{.mark}</p> <p>[An example prompt may be a simple question.]{.mark}</p> <p>[What is the capital of Japan?]{.mark}</p> <p>[Or, it could be more descriptive. For example:]{.mark}</p> <p>[Tell me about the capital of Japan.]{.mark}</p> <p>[Produce a brief list of talking points exploring its culture and history.]{.mark}</p> <p>[The content should be targeted at tourists.]{.mark}</p> <p>[Your readers may have English as a second language, so use simple terms and avoid colloquialisms.]{.mark}</p> <p>[Avoid Jargon at all costs.]{.mark}</p> <p>[Return the results as a list of JSON strings containing content formatted in Markdown.]{.mark}</p> <p>[The LLM will interpret these instructions and return a response based on the patterns it has learned from its training data.]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#potential-problemsmark","title":"[Potential Problems]{.mark}","text":"<p>[While LLMs provide a lot of potential, you should also be cautious.]{.mark}</p> <p>[At their core, LLMs are trained to predict the following word(s) in a sequence.]{.mark}</p> <p>[The words are based on the patterns and relationships from other text in the training data. The sources for this training data are often the internet, books, and other publicly available text. This data could be of questionable quality and maybe be incorrect. Training happens at a point in time, it may not reflect the current state of the world and would not include any private information.]{.mark}</p> <p>[LLMs are fine-tuned to be as helpful as possible, even if that means occasionally generating misleading or baseless content, a phenomenon known as hallucination.]{.mark}</p> <p>[For example, when asked to \\\"Describe the moon.\\\" and LLM may respond with \\\"The moon is made of cheese.\\\". While this is a common saying, it is not true.]{.mark}</p> <p>{width=\"3.125in\" height=\"1.3020833333333333in\"}</p> <p>[While LLMs can represent the essence of words and phrases, they don't possess a genuine understanding or ethical judgment of the content.]{.mark}</p> <p>[These factors can lead to outputs that might be biased, devoid of context, or lack logical coherence.]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#fixing-hallucinationsmark","title":"[Fixing Hallucinations]{.mark}","text":"<p>[Providing additional contextual data helps to ground the LLM's responses and make them more accurate.]{.mark}</p> <p>[A knowledge graph is a mechanism for providing additional data to an LLM. Data within the knowledge graph can guide the LLM to provide more relevant, accurate, and reliable responses.]{.mark}</p> <p>[While the LLM uses its language skills to interpret and respond to the contextual data, it will not disregard the original training data.]{.mark}</p> <p>[You can think of the original training data as the base knowledge and linguistic capabilities, while the contextual information guides in specific situations.]{.mark}</p> <p>[The combination of both approaches enables the LLM to generate more meaningful responses.]{.mark}</p> <p>[Throughout this course, you will explore how to leverage the capabilities of Neo4j and Generative AI to build intelligent, context-aware systems.]{.mark}</p> <p>[You will apply the information and skills learned in the course to build an engine that provides recommendations and information about movies and people.]{.mark}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#_3","title":"Spanda Bootcamp Day 5   very raw","text":"<p>There is no intelligence without knowledge. Despite the incredible power of Large Language Models (LLM), they still significantly struggle with a lack of domain knowledge, which can cause them to generate stale, incomplete, or inaccurate responses. This shortfall presents one of the biggest challenges for companies seeking to adopt domain-specific LLMs, as it directly impacts the effectiveness and reliability of these models in practical business scenarios.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#rag","title":"RAG","text":"<p>To overcome this challenge, [Retrieval-Augmented Generation (RAG)]{.underline} has emerged as the most popular solution to provide LLMs with domain knowledge. With RAG, we retrieve relevant information from an external knowledge base and provide it to the LLM via the [context window of the prompt]{.underline}. For example, if we want to ask a question about our employer's vacation policy, we can use RAG to retrieve relevant HR policy documents from a database and provide that information as context to the LLM.</p> <p>{width=\"6.5in\" height=\"1.5277777777777777in\"}</p> <p>This RAG workflow is what I like to call: conceptually simple but practically difficult.</p> <p>The practical challenges of implementing RAG often manifest in the nuances of data processing, selection, and integration. For a system to effectively retrieve and utilize the right information, it must navigate vast amounts of data sources with precision, understanding the context and relevance of each piece of data.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#vector-database","title":"Vector Database","text":"<p>While RAG significantly enhances the performance of LLMs, it relies heavily on the sophisticated management and retrieval of data. This is where vector databases come into play.</p> <p>A [vector database]{.underline} indexes and stores [vector embeddings]{.underline} for fast information retrieval via vector similarity search. The embeddings are generated by AI models like LLMs and carry rich semantic information that can be queried with a vector database. Typically, this process involves converting a user's question into an embedding, then using vector search to match it against [document text chunk]{.underline} embeddings. The most relevant text chunks are retrieved based on high vector similarity scores, calculated using metrics such as [cosine similarity]{.underline}.</p> <p>Taking our earlier example, the question \"Has the vacation policy changed?\" can be transformed into a query embedding. We then run vector similarity search against HR policy document embeddings to pinpoint documents that are relevant to the vacation policy.</p> <p>{width=\"6.5in\" height=\"2.9583333333333335in\"}</p> <p>Although RAG with a vector database stands as the established baseline for providing LLMs with domain knowledge, it still faces critical challenges that affect its overall effectiveness. The most common challenges in practice include:</p> <ol> <li> <p>Text Chunking Strategy: Determining the best way to chunk     documents presents a significant hurdle. The size of each chunk must     strike a delicate balance between providing sufficient detail and     maintaining broader context. Larger chunks may offer more context     but risk diluting specific, relevant information, whereas smaller     chunks focus on details but might overlook essential contextual     nuances.</p> </li> <li> <p>Top k Parameter: RAG with a vector database requires a [top     k]{.underline} parameter     to select how many text chunks are retrieved and fed into the LLM.     Selecting an appropriate value for top k is complex; it is usually a     fixed number, but its effectiveness varies depending on the text     chunking strategy. If chunks are too small or lack adequate context,     the LLM may not retrieve all necessary information for accurate     response generation.</p> </li> <li> <p>Redundancy and Repetition: If text chunks contain similar     information, we risk the issue of redundancy and repetition in the     top k chunks retrieved. This redundancy not only hampers the     diversity of the information provided to the LLM, but can also lead     to skewed or unbalanced responses. Effective mechanisms to detect     and mitigate such overlap are crucial in ensuring that the LLM     receives a comprehensive and varied set of data for generating     accurate and contextually rich answers.</p> </li> <li> <p>Query Understanding: The effectiveness of vector search is     highly dependent on the semantic similarity between user queries and     documents. Ambiguous or poorly formulated questions can lead to     subpar retrieval results. This occurs because both questions and     documents are represented as vectors in high-dimensional space, and     a vague question may result in a vector that doesn't closely align     with relevant document vectors. This [query-document     mismatch]{.underline}     phenomenon poses a significant challenge in ensuring accurate     information retrieval.</p> </li> </ol> <p>In addition to these common challenges, there is another nuanced aspect and profound issue that often goes overlooked: vector similarity does not satisfy the transitive property.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#vector-similarity-is-not-transitive","title":"Vector Similarity Is Not Transitive","text":"<p>In vector space:</p> <ul> <li> <p>If vector A is similar to vector B</p> </li> <li> <p>And vector B is similar to vector C</p> </li> <li> <p>That does not mean vector A is similar to vector C.</p> </li> </ul> <p>The lack of transitivity has significant implications in how we interpret and respond to queries in AI systems. When a user's query, represented as vector A, seeks information that aligns with vector C, the direct similarity might not be immediately apparent due to the intermediary vector B. In essence, A's connection to C is indirect, mediated through its similarity to B.</p> <p>This means that direct connections between query vectors and relevant document vectors may be missed, potentially leading to gaps or inaccuracies in the information retrieved. Understanding and addressing this gap is an opportunity for the development of more effective and nuanced AI-driven search and retrieval systems.</p> <p>To illustrate this concept, consider a customer support scenario for a mobile network provider:</p> <p>{width=\"6.5in\" height=\"2.0694444444444446in\"}</p> <p>To demonstrate the non-transitive nature of vector similarity, we use the [gte-large]{.underline} embedding model to generate vector embeddings for question A, document B, and document C. We then compute the cosine similarity for each pair of vectors. The following table displays these scores, offering a quantitative perspective on the relationships between vectors A, B, and C.</p> <p>{width=\"6.5in\" height=\"1.2777777777777777in\"}</p> <ul> <li> <p>Direct Similarity of A to B: Customer asks a question A     about frequent dropped calls at their home. This query aligns     closely with document B, which mentions how network congestion     can cause such problems.</p> </li> <li> <p>Direct Similarity of B to C: Document B, discussing network     congestion, is closely related to document C. While document     B outlines the issue, document C describes the telecom     company's long-term solution, like the rollout of a 5G network.</p> </li> <li> <p>Indirect Similarity of A to C: Although document C, focusing     on the 5G network rollout, may not seem to address the customer's     immediate issue of dropped calls (question A), it is indirectly     relevant. This document outlines long-term solutions that could     ultimately resolve such connectivity issues. Understanding this     indirect connection is key, as information about future network     improvements provides valuable context and hope for resolving     ongoing problems, despite not offering an immediate fix.</p> </li> </ul> <p>This example underlines the complexity of accurately mapping user queries to the most relevant information. In developing AI systems, particularly for customer support, it's crucial to consider not just direct similarity but also the broader context and the ultimate utility of the information to the user. Advanced algorithms and approaches that account for these subtleties are key to enhancing the performance and reliability of these systems.</p> <p>Therein lies the question: how can we equip AI systems to effectively discover and draw these nuanced connections, enabling them to connect disparate dots of knowledge and provide comprehensive, contextually rich answers?</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#introducing-a-knowledge-vector-graph-connecting-the-dots-in-vector-space","title":"Introducing a Knowledge Vector Graph: Connecting the Dots in Vector Space","text":"<p>We need a new RAG architecture that combines vector databases with graph databases to orchestrate a solution that gets the best of both worlds: vector search and graph traversal. A Knowledge Vector Graph.</p> <p></p> <p>Imagine storing all pieces of information from your business documents as nodes on a graph with edges connecting nodes to indicate their relationship. Now when a user asks a question, we can run a two-stage \"vector graph\" search operation:</p> <ol> <li> <p>Node Vector Similarity Search: Search for relevant nodes within     the graph based on vector similarity.</p> </li> <li> <p>Graph Traversal: Start from the initially retrieved nodes and     traverse the graph, taking multiple hops along the interconnected     paths of knowledge to explore and retrieve additional, contextually     linked nodes.</p> </li> </ol> <p>At a high level, this is our solution to provide AI with complete domain knowledge, helping AI connect disparate dots of knowledge to provide the most useful answer.</p> <p>Example 1: Using an LLM and a Vector Database</p> <p></p> <p>we will use LangChain to fetch podcast captions from YouTube, embed and store them in Weaviate, and then use a local LLM to build a RAG application.</p> <p>The code is available on [GitHub]{.underline}.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#weaviate-cloud-services","title":"Weaviate cloud services","text":"<p>To follow the examples in this blog post, you first need to [register with WCS]{.underline}. Once you are registered, you can create a new Weaviate Cluster by clicking the \"Create cluster\" button. For this tutorial, we will be using the free trial plan, which will provide you with a sandbox for 14 days.</p> <p>For the next steps, you will need the following two pieces of information to access your cluster:</p> <ul> <li> <p>The cluster URL</p> </li> <li> <p>Weaviate API key (under \"Enabled --- Authentication\")</p> </li> </ul> <p>import weaviate</p> <p>WEAVIATE_URL = \\\"WEAVIATE_CLUSTER_URL\\\"</p> <p>WEAVIATE_API_KEY = \\\"WEAVIATE_API_KEY\\\"</p> <p>client = weaviate.Client(</p> <p>url=WEAVIATE_URL, auth_client_secret=weaviate.AuthApiKey(WEAVIATE_API_KEY)</p> <p>)</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#local-embedding-and-llm-models","title":"Local embedding and LLM models","text":"<p>I am most familiar with the LangChain LLM framework, so we will be using it to ingest documents as well as retrieve them. We will be using sentence_transformers/all-mpnet-base-v2 embedding model and zephyr-7b-alpha llm. Both of these models are open source and available on HuggingFace. The implementation code for these two models in LangChain was kindly borrowed from the following repository:</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#github-aigeek0x0zephyr-7b-alpha-langchain-chatbot-chat-with-pdf-using-zephyr-7b-alpha","title":"GitHub - aigeek0x0/zephyr-7b-alpha-langchain-chatbot: Chat with PDF using Zephyr 7B Alpha...","text":""},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#chat-with-pdf-using-zephyr-7b-alpha-langchain-chromadb-and-gradio-with-free-google-colab-github","title":"Chat with PDF using Zephyr 7B Alpha, Langchain, ChromaDB, and Gradio with Free Google Colab - GitHub ...","text":"<p>github.com</p> <p>If you are using Google Collab environment, make sure to use GPU runtime.</p> <p>We will begin by defining the embedding model, which can be easily retrieved from HuggingFace using the following code:</p> <p># specify embedding model (using huggingface sentence transformer)</p> <p>embedding_model_name = \\\"sentence-transformers/all-mpnet-base-v2\\\"</p> <p>model_kwargs = {\\\"device\\\": \\\"cuda\\\"}</p> <p>embeddings = HuggingFaceEmbeddings(</p> <p>model_name=embedding_model_name,</p> <p>model_kwargs=model_kwargs</p> <p>)</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#ingest-hubermanlabs-podcasts-into-weaviate","title":"Ingest HubermanLabs podcasts into Weaviate","text":"<p>I have learned that each channel on YouTube has an RSS feed, that can be used to fetch links to the latest 10 videos. As the RSS feed returns a XML, we need to employ a simple Python script to extract the links.</p> <p>import requests</p> <p>import xml.etree.ElementTree as ET</p> <p>URL = \\\"https://www.youtube.com/feeds/videos.xml?channel_id=UC2D2CMWXMOVWx7giW1n3LIg\\\"</p> <p>response = requests.get(URL)</p> <p>xml_data = response.content</p> <p># Parse the XML data</p> <p>root = ET.fromstring(xml_data)</p> <p># Define the namespace</p> <p>namespaces = {</p> <p>\\\"atom\\\": \\\"http://www.w3.org/2005/Atom\\\",</p> <p>\\\"media\\\": \\\"http://search.yahoo.com/mrss/\\\",</p> <p>}</p> <p># Extract YouTube links</p> <p>youtube_links = [</p> <p>link.get(\\\"href\\\")</p> <p>for link in root.findall(\\\".//atom:link[@rel=\\'alternate\\']\\\", namespaces)</p> <p>][1:]</p> <p>Now that we have the links to the videos at hand, we can use the YoutubeLoader from LangChain to retrieve the captions. Next, as with most RAG ingestions pipelines, we have to chunk the text into smaller pieces before ingestion. We can use the text splitter functionality that is built into LangChain.</p> <p>from langchain.document_loaders import YoutubeLoader</p> <p>all_docs = []</p> <p>for link in youtube_links:</p> <p># Retrieve captions</p> <p>loader = YoutubeLoader.from_youtube_url(link)</p> <p>docs = loader.load()</p> <p>all_docs.extend(docs)</p> <p># Split documents</p> <p>text_splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=0)</p> <p>split_docs = text_splitter.split_documents(all_docs)</p> <p># Ingest the documents into Weaviate</p> <p>vector_db = Weaviate.from_documents(</p> <p>split_docs, embeddings, client=client, by_text=False</p> <p>)</p> <p>You can test the vector retriever using the following code:</p> <p>print(</p> <p>vector_db.similarity_search(</p> <p>\\\"Which are tools to bolster your mental health?\\\", k=3)</p> <p>)</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#setting-up-a-local-llm","title":"Setting up a local LLM","text":"<p>This part of the code was completely [copied from the example provided by the AI Geek]{.underline}. It loads the zephyr-7b-alpha-sharded model and its tokenizer from HuggingFace and loads it as a LangChain LLM module.</p> <p># specify model huggingface mode name</p> <p>model_name = \\\"anakin87/zephyr-7b-alpha-sharded\\\"</p> <p># function for loading 4-bit quantized model</p> <p>def load_quantized_model(model_name: str):</p> <p>\\\"\\\"\\\"</p> <p>:param model_name: Name or path of the model to be loaded.</p> <p>:return: Loaded quantized model.</p> <p>\\\"\\\"\\\"</p> <p>bnb_config = BitsAndBytesConfig(</p> <p>load_in_4bit=True,</p> <p>bnb_4bit_use_double_quant=True,</p> <p>bnb_4bit_quant_type=\\\"nf4\\\",</p> <p>bnb_4bit_compute_dtype=torch.bfloat16,</p> <p>)</p> <p>model = AutoModelForCausalLM.from_pretrained(</p> <p>model_name,</p> <p>load_in_4bit=True,</p> <p>torch_dtype=torch.bfloat16,</p> <p>quantization_config=bnb_config,</p> <p>)</p> <p>return model</p> <p># function for initializing tokenizer</p> <p>def initialize_tokenizer(model_name: str):</p> <p>\\\"\\\"\\\"</p> <p>Initialize the tokenizer with the specified model_name.</p> <p>:param model_name: Name or path of the model for tokenizer initialization.</p> <p>:return: Initialized tokenizer.</p> <p>\\\"\\\"\\\"</p> <p>tokenizer = AutoTokenizer.from_pretrained(model_name, return_token_type_ids=False)</p> <p>tokenizer.bos_token_id = 1 # Set beginning of sentence token id</p> <p>return tokenizer</p> <p># initialize tokenizer</p> <p>tokenizer = initialize_tokenizer(model_name)</p> <p># load model</p> <p>model = load_quantized_model(model_name)</p> <p># specify stop token ids</p> <p>stop_token_ids = [0]</p> <p># build huggingface pipeline for using zephyr-7b-alpha</p> <p>pipeline = pipeline(</p> <p>\\\"text-generation\\\",</p> <p>model=model,</p> <p>tokenizer=tokenizer,</p> <p>use_cache=True,</p> <p>device_map=\\\"auto\\\",</p> <p>max_length=2048,</p> <p>do_sample=True,</p> <p>top_k=5,</p> <p>num_return_sequences=1,</p> <p>eos_token_id=tokenizer.eos_token_id,</p> <p>pad_token_id=tokenizer.eos_token_id,</p> <p>)</p> <p># specify the llm</p> <p>llm = HuggingFacePipeline(pipeline=pipeline)</p> <p>I haven't played around yet, but you could probably reuse this code to load other LLMs from HuggingFace.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#building-a-conversation-chain","title":"Building a conversation chain","text":"<p>Now that we have our vector retrieval and th LLM ready, we can implement a retrieval-augmented chatbot in only a couple lines of code.</p> <p>qa_chain = RetrievalQA.from_chain_type(</p> <p>llm=llm, chain_type=\\\"stuff\\\", retriever=vector_db.as_retriever()</p> <p>)</p> <p>Let's now test how well it works:</p> <p>response = qa_chain.run(</p> <p>\\\"How does one increase their mental health?\\\")</p> <p>print(response)</p> <p></p> <p>Let's try another one:</p> <p>response = qa_chain.run(\\\"How to increase your willpower?\\\")</p> <p>print(response)</p> <p>{width=\"6.5in\" height=\"1.8055555555555556in\"}</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#summary","title":"Summary","text":"<p>Only a couple of months ago, most of us didn't realize that we will be able to run LLMs on our laptop or free-tier Google Collab so soon. Many RAG applications deal with private and confidential data, where it can't be shared with third-party LLM providers. In those cases, using a local embedding and LLM models as described here is the ideal solution.</p> <p>https://github.com/tomasonjo/blogs/blob/master/weaviate/HubermanWeaviate.ipynb</p> <p>Example 2:</p> <p>Using a Knowledge Graph to implement a DevOps RAG application</p> <p>RAG applications are all the rage at the moment.</p> <p>Everyone is building their company documentation chatbot or similar.</p> <p>Mostly, they all have in common that their source of knowledge is unstructured text, which gets chunked and embedded in one way or another.</p> <p>However, not all information arrives as unstructured text.</p> <p>Say, for example, you wanted to create a chatbot that could answer questions about your microservice architecture, ongoing tasks, and more.</p> <p>Tasks are mostly defined as unstructured text, so there wouldn't be anything different from the usual RAG workflow there.</p> <p>However, how could you prepare information about your microservices architecture so the chatbot can retrieve up-to-date information?</p> <p>One option would be to create daily snapshots of the architecture and transform them into text that the LLM would understand.</p> <p>However, what if there is a better approach? Meet knowledge graphs, which can store both structured and unstructured information in a single database.</p> <p>{width=\"6.5in\" height=\"3.6666666666666665in\"}[Knowledge graph schema representing microservice architecture and their tasks.]{.mark}</p> <p>Nodes and relationships are used to describe data in a knowledge graph.</p> <p>Typically, nodes are used to represent entities or concepts like people, organizations, and locations.</p> <p>In the microservice graph example, nodes describe people, teams, microservices, and tasks.</p> <p>On the other hand, relationships are used to define connections between these entities, like dependencies between microservices or task owners.</p> <p>Both nodes and relationships can have property values stored as key-value pairs.</p> <p>{width=\"6.5in\" height=\"3.4583333333333335in\"}[Node properties of a Microservice and Task nodes. Image by author.]{.mark}</p> <p>The microservice nodes have two node properties describing their name and technology. On the other hand, task nodes are more complex.</p> <p>They all have the name, status, description, as well as embedding properties. By storing text embedding values as node properties, you can perform a vector similarity search of task descriptions identical to if you had the tasks stored in a vector database.</p> <p>Therefore, knowledge graphs allow you to store and retrieve both structured and unstructured information to power your RAG applications.</p> <p>Here we will go through a scenario of implementing a knowledge graph based RAG application with LangChain to support your DevOps team. The code is available on GitHub.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#neo4j-environment-setup","title":"Neo4j Environment Setup","text":"<p>You need to set up a Neo4j 5.11 or greater to follow along with the examples in this blog post. The easiest way is to start a free instance on Neo4j Aura, which offers cloud instances of Neo4j database. Alternatively, you can also set up a local instance of the Neo4j database by downloading the Neo4j Desktop application and creating a local database instance.from langchain.graphs import Neo4jGraph.</p> <p>from langchain.graphs import Neo4jGraph</p> <p>url = \\\"neo4j+s://databases.neo4j.io\\\"</p> <p>username =\\\"neo4j\\\"</p> <p>password = \\\"\\\"</p> <p>graph = Neo4jGraph(</p> <p>url=url,</p> <p>username=username,</p> <p>password=password</p> <p>)</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#dataset","title":"Dataset","text":"<p>Knowledge graphs are excellent at connecting information from multiple data sources. You could fetch information from cloud services, task management tools, and more when developing a DevOps RAG application.</p> <p>{width=\"6.5in\" height=\"6.152777777777778in\"}[Combining multiple data sources into a knowledge graph. Image by author.]{.mark}</p> <p>import requests</p> <p>url = \\\"https://gist.githubusercontent.com/tomasonjo/08dc8ba0e19d592c4c3cde40dd6abcc3/raw/da8882249af3e819a80debf3160ebbb3513ee962/microservices.json\\\"</p> <p>import_query = requests.get(url).json()[\\'query\\']</p> <p>graph.query(</p> <p>import_query</p> <p>)</p> <p>If you inspect the graph in Neo4j Browser, you should get a similar visualization.</p> <p>{width=\"6.5in\" height=\"4.791666666666667in\"}[Subset of the DevOps graph. Image by author.]{.mark}</p> <p>Blue nodes describe microservices. These microservices may have dependencies on one another, implying that the functioning or the outcome of one might be reliant on another's operation. On the other hand, the brown nodes represent tasks that are directly linked to these microservices. Besides showing how things are set up and their linked tasks, our graph also shows which teams are in charge of what.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#neo4j-vector-index","title":"Neo4j Vector index","text":"<p>We will begin by implementing a vector index search for finding relevant tasks by their name and description. If you are unfamiliar with vector similarity search, let me give you a quick refresher. The key idea is to calculate the text embedding values for each task based on their description and name. Then, at query time, find the most similar tasks to the user input using a similarity metric like a cosine distance.</p> <p>{width=\"6.5in\" height=\"3.1944444444444446in\"}[Vector similarity search in a RAG application. Image by author.]{.mark}</p> <p>The retrieved information from the vector index can then be used as context to the LLM so it can generate accurate and up-to-date answers.</p> <p>The tasks are already in our knowledge graph. However, we need to calculate the embedding values and create the vector index. This can be achieved with the from_existing_graph method.</p> <p>import os</p> <p>from langchain.vectorstores.neo4j_vector import Neo4jVector</p> <p>from langchain.embeddings.openai import OpenAIEmbeddings</p> <p>os.environ[\\'OPENAI_API_KEY\\'] = \\\"OPENAI_API_KEY\\\"</p> <p>vector_index = Neo4jVector.from_existing_graph(</p> <p>OpenAIEmbeddings(),</p> <p>url=url,</p> <p>username=username,</p> <p>password=password,</p> <p>index_name=\\'tasks\\',</p> <p>node_label=\\\"Task\\\",</p> <p>text_node_properties=[\\'name\\', \\'description\\', \\'status\\'],</p> <p>embedding_node_property=\\'embedding\\',</p> <p>)</p> <p>In this example, we used the following graph-specific parameters for the from_existing_graph method.</p> <ul> <li> <p>index_name: name of the vector index</p> <p>node_label: node label of relevant nodes</p> <p>text_node_properties: properties to be used to calculate embeddings and retrieve from the vector index</p> <p>embedding_node_property: which property to store the embedding values to</p> </li> </ul> <p>Now that the vector index has been initiated, we can use it as any other vector index in LangChain.</p> <p>response = vector_index.similarity_search(</p> <p>\\\"How will RecommendationService be updated?\\\"</p> <p>)</p> <p>print(response[0].page_content)</p> <p># name: BugFix</p> <p># description: Add a new feature to RecommendationService to provide ...</p> <p># status: In Progress</p> <p>You can observe that we construct a response of a map or dictionary-like string with defined properties in the text_node_properties parameter.</p> <p>Now we can easily create a chatbot response by wrapping the vector index into a RetrievalQA module.</p> <p>from langchain.chains import RetrievalQA</p> <p>from langchain.chat_models import ChatOpenAI</p> <p>vector_qa = RetrievalQA.from_chain_type(</p> <p>llm=ChatOpenAI(),</p> <p>chain_type=\\\"stuff\\\",</p> <p>retriever=vector_index.as_retriever()</p> <p>)</p> <p>vector_qa.run(</p> <p>\\\"How will recommendation service be updated?\\\"</p> <p>)</p> <p># The RecommendationService is currently being updated to include a new feature</p> <p># that will provide more personalized and accurate product recommendations to</p> <p># users. This update involves leveraging user behavior and preference data to</p> <p># enhance the recommendation algorithm. The status of this update is currently</p> <p># in progress.</p> <p>One limitation of vector indexes, in general, is that they don't provide the ability to aggregate information like you would with a structured query language like Cypher. Take, for example, the following example:</p> <p>vector_qa.run(</p> <p>\\\"How many open tickets there are?\\\"</p> <p>)</p> <p># There are 4 open tickets.</p> <p>The response seems valid, and the LLM uses assertive language, making you believe the result is correct. However, the problem is that the response directly correlates to the number of retrieved documents from the vector index, which is four by default. What actually happens is that the vector index retrieves four open tickets, and the LLM unquestioningly believes that those are all the open tickets. However, the truth is different, and we can validate it using a Cypher statement.</p> <p>graph.query(</p> <p>\\\"MATCH (t:Task {status:\\'Open\\'}) RETURN count(*)\\\"</p> <p>)</p> <p># [{\\'count(*)\\': 5}]</p> <p>There are five open tasks in our toy graph. While vector similarity search is excellent for sifting through relevant information in unstructured text, it lacks the capability to analyze and aggregate structured information. Using Neo4j, this problem can be easily solved by employing Cypher, which is a structured query language for graph databases.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#graph-cypher-search","title":"Graph Cypher search","text":"<p>Cypher is a structured query language designed to interact with graph databases and provides a visual way of matching patterns and relationships. It relies on the following ascii-art type of syntax:</p> <p>(:Person {name:\\\"Tomaz\\\"})-[:LIVES_IN]-&gt;(:Country {name:\\\"Slovenia\\\"})</p> <p>This patterns describes a node with a label Person and the name property Tomaz that has a LIVES_IN relationship to the Country node of Slovenia.</p> <p>The neat thing about LangChain is that it provides a GraphCypherQAChain, which generates the Cypher queries for you, so you don't have to learn Cypher syntax in order to retrieve information from a graph database like Neo4j.</p> <p>The following code will refresh the graph schema and instantiate the Cypher chain.</p> <p>from langchain.chains import GraphCypherQAChain</p> <p>graph.refresh_schema()</p> <p>cypher_chain = GraphCypherQAChain.from_llm(</p> <p>cypher_llm = ChatOpenAI(temperature=0, model_name=\\'gpt-4\\'),</p> <p>qa_llm = ChatOpenAI(temperature=0), graph=graph, verbose=True,</p> <p>)</p> <p>Generating valid Cypher statements is a complex task. Therefore, it is recommended to use state-of-the-art LLMs like gpt-4 to generate Cypher statements, while generating answers using the database context can be left to gpt-3.5-turbo.</p> <p>Now, you can ask the same question about how many tickets are open.</p> <p>cypher_chain.run(</p> <p>\\\"How many open tickets there are?\\\"</p> <p>)</p> <p>Result is the following</p> <p>{width=\"4.263888888888889in\" height=\"1.8194444444444444in\"}</p> <p>You can also ask the chain to aggregate the data using various grouping keys, like the following example.</p> <p>cypher_chain.run(</p> <p>\\\"Which team has the most open tasks?\\\"</p> <p>)</p> <p>Result is the following</p> <p>{width=\"4.597222222222222in\" height=\"2.3472222222222223in\"}</p> <p>You might say these aggregations are not graph-based operations, and you will be correct. We can, of course, perform more graph-based operations like traversing the dependency graph of microservices.</p> <p>cypher_chain.run(</p> <p>\\\"Which services depend on Database directly?\\\"</p> <p>)</p> <p>Result is the following</p> <p>{width=\"6.5in\" height=\"1.5555555555555556in\"}</p> <p>\\ Of course, you can also ask the chain to produce variable-length path traversals by asking questions like:</p> <p>cypher_chain.run(</p> <p>\\\"Which services depend on Database indirectly?\\\"</p> <p>)</p> <p>{width=\"6.5in\" height=\"1.6666666666666667in\"}</p> <p>Some of the mentioned services are the same as in the directly dependent question. The reason is the structure of the dependency graph and not the invalid Cypher statement.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#knowledge-graph-agent","title":"Knowledge graph agent","text":"<p>Since we have implemented separate tools for the structured and unstructured parts of the knowledge graph, we can add an agent that can use these two tools to explore the knowledge graph.</p> <p>from langchain.agents import initialize_agent, Tool</p> <p>from langchain.agents import AgentType</p> <p>tools = [</p> <p>Tool(</p> <p>name=\\\"Tasks\\\",</p> <p>func=vector_qa.run,</p> <p>description=\\\"\\\"\\\"Useful when you need to answer questions about descriptions of tasks.</p> <p>Not useful for counting the number of tasks.</p> <p>Use full question as input.</p> <p>\\\"\\\"\\\",</p> <p>),</p> <p>Tool(</p> <p>name=\\\"Graph\\\",</p> <p>func=cypher_chain.run,</p> <p>description=\\\"\\\"\\\"Useful when you need to answer questions about microservices,</p> <p>their dependencies or assigned people. Also useful for any sort of</p> <p>aggregation like counting the number of tasks, etc.</p> <p>Use full question as input.</p> <p>\\\"\\\"\\\",</p> <p>),</p> <p>]</p> <p>mrkl = initialize_agent(</p> <p>tools,</p> <p>ChatOpenAI(temperature=0, model_name=\\'gpt-4\\'),</p> <p>agent=AgentType.OPENAI_FUNCTIONS, verbose=True</p> <p>)</p> <p>Let's try out how well does the agent works.</p> <p>response = mrkl.run(\\\"Which team is assigned to maintain PaymentService?\\\")</p> <p>print(response)</p> <p>Result is the following</p> <p>{width=\"6.5in\" height=\"2.6805555555555554in\"}</p> <p>Let's now try to invoke the Tasks tool.</p> <p>response = mrkl.run(\\\"Which tasks have optimization in their description?\\\")</p> <p>print(response)</p> <p>Result is the following</p> <p>{width=\"6.5in\" height=\"2.4722222222222223in\"}</p> <p>One thing is certain. I have to work on my agent prompt engineering skills. There is definitely room for improvement in tools description. Additionally, you can also customize the agent prompt.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#conclusion","title":"Conclusion","text":"<p>Knowledge graphs are an excellent fit when you require structured and unstructured data to power your RAG applications. With the approach shown in this blog post, you can avoid polyglot architectures, where you must maintain and sync multiple types of databases. Learn more about graph-based search in LangChain here.</p> <p>The code is available on GitHub.</p> <p>Finally, a note on DPO</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#_4","title":"Spanda Bootcamp Day 5   very raw","text":"<p>The current gold standard to build LLM models is as follows:</p> <p>{width=\"6.5in\" height=\"2.611111111111111in\"}</p> <p>Source: Chip Huyen</p> <p>You first assemble billions of documents with trillions of words and, in a self-supervised manner, you ask the model to predict the next token (word or sub-word) to a given sequence.</p> <p>Self-supervised training is a training method where the supervisory signal, the element that tells the model if it is correct or wrong, is given by the training data without requiring human labeling.</p> <p>In the case of ChatGPT, it is given by masking --- hiding --- the next word, forcing a prediction, and comparing the word the model chose with the actual masked word. Next, you want to teach the model to behave in a certain way, like being able to answer questions when prompted. That takes us to step 2, where we assemble a dataset where the model performs the same exercise, but on a curated, dialogue-based form, becoming what we define as assistants.</p> <p>[But we not only want the model to behave in a certain way, we also want it to maximize the quality and safety of its responses.]{.mark}</p> <p>[Reinforcement Learning]{.mark} [from Human Feedback, or RLHF, steps 3 and 4, does exactly that, by teaching the model to not only answer as expected but to give the best and safest possible answer aligned with human preferences.]{.mark}</p> <p>[However, there's a problem here.]{.mark}</p> <p>[Money goes brrrr]{.mark}</p> <p>[RLHF involves three steps:]{.mark}</p> <p>[Building a preference dataset. By prompting the model in step 2, they build a dataset of {prompt, response 1, response 2}, where a group of humans decides which response is better, 1 or 2.]{.mark}</p> <p>[Training a reward model. Step 3 in the previous diagram, you build a model that learns to predict, for every new response, how good is it. As you may imagine, we use the preference dataset to train this model, by training it to assign a higher score to the preferred response out of every pair.]{.mark}</p> <p>[Maximize reward. Finally, in step 4 of the diagram, we train the model to maximize the reward. In other words, we train our model on a policy that learns to obtain the highest rewards from the reward model. As this model represents human preferences, we are implicitly aligning our model to those human preferences.]{.mark}</p> <p>[In layman's terms, you are training a model to find the ideal policy that learns to take the best action, in this case word prediction, for every given text.]{.mark}</p> <p>[Think of a policy as a decision-making framework, the model learns to take the best action (word prediction) to a given state (prompt).]{.mark}</p> <p>[And that gives us ChatGPT. And Claude. And Gemini. But with RLHF, costs get real.]{.mark}</p> <p>[Even though the first two steps before RLHF already consume millions of US dollars, RLHF is just prohibitive for the majority of the world's researchers, because:]{.mark}</p> <p>[It involves building a highly curated dataset that requires extensive model prompting and also the collaboration of expensive human experts.]{.mark}</p> <p>[It requires training an entirely new model, the reward model. This model is often as big and as good as the model we are aligning, doubling the compute costs.]{.mark}</p> <p>[And it also involves running the soon-to-be-aligned model and reward model in an iterative Reinforcement Learning training cycle.]{.mark}</p> <p>[Put simply, unless you go by the name of Microsoft, Anthropic, Google, and a few others, RLFH is way out of your league.]{.mark}</p> <p>[But DPO certainly isn't.]{.mark}</p> <p>[Keep it Simple, Stupid]{.mark}</p> <p>[Direct Preference Optimization (DPO) is a mathematical breakthrough where the trained model is aligned to human preferences without requiring a Reinforcement Learning loop.]{.mark}</p> <p>[In other words, you optimize against an implicit reward without explicitly materializing that reward.]{.mark}</p> <p>[Without materializing a reward model.]{.mark}</p> <p>[But before answering what the hell that means, we need to review how models learn.]{.mark}</p> <p>[It's just trial and error!]{.mark}</p> <p>[Basically, all neural networks, be that ChatGPT or Stable Diffusion, are trained using backpropagation.]{.mark}</p> <p>[In succinct terms, it's no more than glorified trial and error.]{.mark}</p> <p>[You define a computable loss that tells the model how wrong its prediction is, and you apply undergraduate-level calculus to optimize the parameters of the model to slowly minimize that loss using partial derivatives.]{.mark}</p> <p>[How do we actually optimize the model?]{.mark}</p> <p>[In very simple terms, for every prediction, you compute the loss and calculate the gradient of each parameter to that loss.]{.mark}</p> <p>[A positive gradient for a parameter means increasing the value of the parameter increases the loss, thus signaling us to reduce its value, and viceversa.]{.mark}</p> <p>[But what does this mean in the context of ChatGPT and other LLMs?]{.mark}</p> <p>[If we think about ChatGPT and its next token prediction task, as in training we know what the predicted word should be, our loss function is computed as the probability the model gives to the correct word out of all the words in its vocabulary.]{.mark}</p> <p>[If you're having trouble following, that's probably because I must clarify that when we say that ChatGPT 'predicts the next word in the sequence' what it's really doing is, out of the thousands of words in its vocabulary, it is assigning a likelihood percentage to each word, signaling how confident it is about that word being the most appropriate to go next.]{.mark}</p> <p>[In most cases, models simply choose the word with the highest assigned probability, which goes by the name of 'greedy decoding'.]{.mark}</p> <p>[For instance, the loss is quite big if we know the next word should be \"Cafe\" and the model has only assigned a 10% probability to that word.]{.mark}</p> <p></p> <p>[Source: \\@akshar_pachaar (X.com)]{.mark}</p> <p>[Consequently, the model slowly learns to assign the highest probability possible to the correct word, thereby learning to efficiently model language.]{.mark}</p> <p>[Therefore, looking at the earlier four-step diagram, in steps 1 and 2 the model learns just how we just explained, and in the case of RLHF, the loss function essentially teaches the model to maximize a reward.]{.mark}</p> <p>[Specifically, the loss function in RLHF looks something like this:]{.mark}</p> <p>{width=\"6.5in\" height=\"0.5138888888888888in\"}</p> <p>[where the first term r(x,y) computes the reward given by the reward model.]{.mark}</p> <p>[And what about the subtracting term?]{.mark}</p> <p>[The RLHF loss function also includes a regularization term to avoid the model drifting too much from the original model.]{.mark}</p> <p>[So, what makes DPO different from RLHF?]{.mark}</p> <p>[Algebra comes to our aide]{.mark}</p> <p>[The key intuition is that, unlike RLHF, DPO does not need a new model --- the reward model --- to compute the alignment process.]{.mark}</p> <p>[In other words, the Language Model you are training is secretly its own reward model.]{.mark}</p> <p>[Now, what on Earth do we mean by that.]{.mark}</p> <p>{width=\"6.5in\" height=\"1.3611111111111112in\"}</p> <p>[Source: Stanford]{.mark}</p> <p>[Using clever algebra and based on the Bradley-Terry preference model --- a probability framework that essentially predicts the best option among a comparison of two possible options --- they implicitly define the reward and train the LLM directly without requiring an explicit reward model.]{.mark}</p> <p>[Although the DPO paper gives the complete mathematical procedure, the key intuition is that the process goes from:]{.mark}</p> <p>[Training an LLM \u00bb define a preference dataset \u00bb training a reward model \u00bb training the LLM to find the optimal policy that maximizes the reward, to:]{.mark}</p> <p>[Training an LLM \u00bb define a preference dataset \u00bb training the LLM to find the optimal policy.]{.mark}</p> <p>[In other words, no reward model. But how do we compute then the reward in DPO?]{.mark}</p> <p>[Fascinatingly, the reward is implicitly defined as part of the optimal policy.]{.mark}</p> <p>[What DPO proves is that, when working with a human preference dataset, we don't need to first create a reward model that predicts what a human would choose and then use that model to optimize our goal model.]{.mark}</p> <p>[In fact, we can do both steps in one by directly finding the optimal policy that aligns with our model without calculating the actual reward. This works because the optimal policy is a function of the reward, meaning that by finding that policy we are implicitly maximizing the reward.]{.mark}</p> <p>[Bottom line, you can think of DPO as a cool algebra trick that skips calculating the reward explicitly by directly finding the policy that implicitly maximizes the reward.]{.mark}</p> <p>[This gives us the following loss function:]{.mark}</p> <p>{width=\"6.5in\" height=\"0.6527777777777778in\"}</p> <p>[Where yw and yl stand for the winning and losing response in a given comparison.]{.mark}</p> <p>[The intuition is that the higher the probability the policy gives to the preferred response, and the lower the assigned probability to the losing response, the smaller the loss.]{.mark}</p> <p>[Compared to the loss function used in RLHF we saw earlier, instead of having our loss function as a function of a reward calculated by another model, our loss function is a function of the optimal policy that maximizes the reward without us having to actually compute that reward.]{.mark}</p> <p>[Brilliant!]{.mark}</p> <p>[2024, the Year of Efficiency]{.mark}</p> <p>[We aren't even halfway through January and we have already seen the disruption of one of the most painful and expensive, yet essential steps in the creation of our best models.]{.mark}</p> <p>[Unequivocally, DPO levels the playing field, allowing universities and small-time research labs to build models that can be aligned with orders of magnitude lower costs.]{.mark}</p> <p>[What you need to know about Self-Hosting Large Language Models (LLMs)]{.mark}</p> <p>[Companies in regulated industries need to have the ability to self-host open-source LLM models to regain control of their own privacy.]{.mark}</p> <p>[Since its arrival in November 2022, ChatGPT has revolutionized the way we all work by leveraging generative artificial intelligence (AI) to streamline tasks, produce content, and provide swift and error-free recommendations. By harnessing the power of this groundbreaking technology, companies and individuals can amplify efficiency and precision while reducing reliance on human intervention.\\ \\ At the core of ChatGPT and other AI algorithms lie Large Language Models (LLMs), renowned for their remarkable capacity to generate human-like written content. One prominent application of LLMs is in the realm of website chatbots utilized by companies.]{.mark}</p> <p>[By feeding customer and product data into LLMs and continually refining the training, these chatbots can deliver instantaneous responses, personalized recommendations, and unfettered access to information. Furthermore, their round-the-clock availability empowers websites to provide continuous customer support and engagement, unencumbered by constraints of staff availability.\\ \\ While LLMs are undeniably beneficial for organizations, enabling them to operate more efficiently, there is also a significant concern regarding the utilization of cloud-based services like OpenAI and ChatGPT for LLMs. With sensitive data being entrusted to these cloud-based platforms, companies can potentially lose control over their data security.]{.mark}</p> <p>[Simply put, they relinquish ownership]{.mark} of their data. In these privacy-conscious times, companies in regulated industries are expected to adhere to the highest standards when it comes to handling customer data and other sensitive information.\\ \\ In heavily regulated industries like healthcare and finance, companies need to have the ability to self-host some open-source LLM models to regain control of their own privacy. Here is what you need to know about self-hosting LLMs and how you can easily do so with Plural.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#before-you-decide-to-self-host","title":"Before you decide to self-host","text":"<p>In the past year, the discussion surrounding LLMs has evolved, transitioning from \\\"Should we utilize LLMs?\\\" to \\\"Should we opt for a self-hosted solution or rely on a proprietary off-the-shelf alternative?\\\"</p> <p>Like many engineering questions, the answer to this one is not straightforward. While we are strong proponents of self-hosting infrastructure -- we even self-host our AI chatbot for compliance reasons -- we also rely on our Plural platform, leveraging the expertise of our team, to ensure our solution is top-notch.</p> <p>We often urge our customers to answer these questions below before self-hosting LLMs.</p> <ul> <li> <p>Where would you want to host LLMs?</p> </li> <li> <p>Do you have a client-server architecture in mind? Or, something with     edge devices, such as on your phone?</p> </li> </ul> <p>It also depends on your use case:</p> <ul> <li> <p>What will the LLMs be used for in your organization?</p> </li> <li> <p>Do you work in a regulated industry and need to own your proprietary     data?</p> </li> <li> <p>Does it need to be in your product in a short period?</p> </li> <li> <p>Do you have engineering resources and expertise available to build a     solution from scratch?</p> </li> </ul> <p>If you require compliance as a crucial feature for your LLM and have the necessary engineering expertise to self-host, you\\'ll find an abundance of tools and frameworks available. By combining these various components, you can build your solution from the ground up, tailored to your specific needs.</p> <p>If your aim is to quickly implement an off-the-shelf model for a RAG-LLM application, which only requires proprietary context, consider using a solution at a higher abstraction level such as OpenLLM, TGI, or vLLM.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#why-self-host-llms","title":"Why Self-Host LLMs?","text":"<p>Although there are various advantages to self-hosting LLMs, three key benefits stand out prominently.</p> <ol> <li>Greater security, privacy, and compliance: It is ultimately the     main reason why companies often opt to self-host LLMs. If you were     to look at [OpenAI's Terms of     Use]{.underline},     it even mentions that \"We may use Content from Services other than     our API (\"Non-API Content\") to help develop and improve our     Services.</li> </ol> <p>{width=\"6.5in\" height=\"1.9583333333333333in\"}OpenAI Terms of Use neglect a users privacy.</p> <p>Anything you or your [employees upload into ChatGPT will be included in future training data]{.underline}. And, despite its attempt to anonymize the data, it ultimately contributes knowledge of the model. Unsurprisingly, there is even a conversation happening in the space as to whether or not [ChatGPT\\'s use of data is even legal]{.underline}, but that's a topic for a different day. What we do know is that many privacy-conscious companies have already begun to [prohibit employees from using ChatGPT]{.underline}.\\ \\ 2. Customization: By self-hosting LLMs, you can scale alongside your use case. Organizations that rely heavily on LLMs might reach a point where it becomes economical to self-host. A telltale sign of this occurring is when you begin to hit rate limits with public API endpoints and the performance of these models is ultimately affected. Ideally, you can build it all yourself, train a model, and create a model server for your chosen ML framework/model runtime (e.g. tf, [PyTorch,]{.underline} Jax.), but most likely you would leverage a distributed ML framework like [Ray]{.underline}.</p> <p>3. Avoid Vendor-Lock-In: When between open-source and proprietary solutions, a crucial question to address is your comfort with cloud vendor lock-in. Major machine learning services provide their own managed ML services, allowing you to host an LLM model server. However, migrating between them can be challenging, and depending on your specific use case, it may result in higher long-term expenses compared to open-source alternatives.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#popular-solutions-to-host-llms","title":"Popular Solutions to host LLMs","text":""},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#openllm-via-yatai","title":"OpenLLM via Yatai","text":"<p>GitHub - bentoml/OpenLLM: Operating LLMs in production</p> <p>Operating LLMs in production. Contribute to bentoml/OpenLLM development by creating an account on GitHub.</p> <p>{width=\"5.333333333333333in\" height=\"5.333333333333333in\"}GitHubbentoml</p> <p></p> <p>OpenLLM via Yatai</p> <p>OpenLLM is specifically tailored for AI application developers who are tirelessly building production-ready applications using LLMs. It brings forth an extensive array of tools and functionalities to seamlessly fine-tune, serve, deploy, and monitor these models, streamlining the end-to-end deployment workflow for LLMs.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#features-that-stand-out","title":"Features that stand out","text":"<ul> <li> <p>Serve LLMs over a RESTful API or gRPC with a single command. You can     interact with the model using a Web UI, CLI, Python/JavaScript     client, or any HTTP client of your choice.</p> </li> <li> <p>First-class support for LangChain, BentoML, and Hugging Face Agents</p> </li> <li> <p>E.g., tie a remote self-hosted OpenLLM into your langchain app</p> </li> <li> <p>Token streaming support</p> </li> <li> <p>Embedding endpoint support</p> </li> <li> <p>Quantization support</p> </li> <li> <p>You can fuse model-compatible existing pre-trained [QLoRAa/LoRA     adapters]{.underline}     with the chosen LLM with the addition of a flag to the serve     command, still experimental though:\\     [https://github.com/bentoml/OpenLLM#\ufe0f-fine-tuning-support-experimental]{.underline}</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#why-run-yatai-on-plural","title":"Why run Yatai on Plural","text":"<p>GitHub - bentoml/Yatai: Model Deployment at Scale on Kubernetes \ud83e\udd84\ufe0f</p> <p>Model Deployment at Scale on Kubernetes \ud83e\udd84\ufe0f. Contribute to bentoml/Yatai development by creating an account on GitHub.</p> <p>{width=\"5.333333333333333in\" height=\"5.333333333333333in\"}GitHubbentoml</p> <p></p> <p>Yatai on Plural.sh</p> <p>If you check out the official GitHub repo of OpenLLM you'll see that the integration with BentoML makes it easy to run multiple LLMs in parallel across multiple GPUs/Nodes, or chain LLMs with other types of AI/ML models, and deploy the entire pipeline on BentoCloud https://l.bentoml.com/bento-cloud. However, you can achieve the same on a [Plural-deployed Kubernetes via Yatai]{.underline} , which is essentially an open-source BentoCloud which should come at a much lower price point.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#ray-serve-via-ray-cluster","title":"Ray Serve Via Ray Cluster","text":"<p>Ray Serve: Scalable and Programmable Serving --- Ray 2.7.0</p> <p>{width=\"0.6944444444444444in\" height=\"0.6944444444444444in\"}Ray 2.7.0</p> <p>{width=\"0.6944444444444444in\" height=\"0.6944444444444444in\"}</p> <p>Ray Serve via Ray Cluster</p> <p>Ray Serve is a scalable model-serving library for building online inference APIs. Serve is framework-agnostic, so you can use a single toolkit to serve everything from deep learning models built with frameworks like PyTorch, TensorFlow, and Keras, to Scikit-Learn models, to arbitrary Python business logic. It has several features and performance optimizations for serving Large Language Models such as response streaming, dynamic request batching, multi-node/multi-GPU serving, etc.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#features-that-stand-out_1","title":"Features that stand out","text":"<ul> <li> <p>It's a huge well-documented ML Platform. In our opinion, it is the     best-documented platform with loads of examples to work off of.     However, you need to know what you're doing when working with it,     and it takes some time to get adapted.</p> </li> <li> <p>Not focused on LLMs, but there are many examples of how to OS LLMS     from Hugging Face,</p> </li> <li> <p>Integrates nicely with Prometheus for cluster metrics and comes with     a useful dashboard for you to monitor both servings and if you're     doing anything else on your ray cluster like data processing or     model training, that can be monitored nicely.</p> </li> <li> <p>It's [what OpenAI uses to train and host their     models]{.underline},     so it's fair to say it is probably the most robust solution ready to     handle production-ready use cases.</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#why-run-ray-on-plural","title":"Why run Ray on Plural","text":"<p>Plural offers a fully functional Ray cluster on a Plural-deployed Kubernetes cluster where you can do anything you can do with Ray, from data-parallel data-crunching over distributed model training to serving off-the-shelf OS LLMs</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#hugginfaces-tgi","title":"Hugginface's TGI","text":"<p>GitHub - huggingface/text-generation-inference: Large Language Model Text Generation Inference</p> <p>Large Language Model Text Generation Inference. Contribute to huggingface/text-generation-inference development by creating an account on GitHub.</p> <p>{width=\"5.333333333333333in\" height=\"5.333333333333333in\"}GitHubhuggingface</p> <p></p> <p>Hugginface TGI</p> <p>A Rust, Python, and gRPC server for text generation inference. Used in production at [HuggingFace]{.underline} to power Hugging Chat, the Inference API, and Inference Endpoint.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#features-that-stand-out_2","title":"Features that stand out","text":"<ul> <li> <p>Everything you need is containerized, so if you just want to run     off-the-shelf HF models, this is probably one of the quickest ways     to do it.</p> </li> <li> <p>They have no intent at the time of this writing to provide     [official Kubernetes     support]{.underline},     citing</p> </li> </ul>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#why-run-hugging-face-llm-on-plural","title":"Why run Hugging Face LLM on Plural","text":"<p>When you run an HF LLM model inference server via Text Generation Inference (TGI) on a Plural-deployed Kubernetes cluster you benefit from all the goodness of our built-in telemetry, monitoring, and integration with other marketplace apps to orchestrate it and host your data and vector stores. Here is a great example we recommend following along for deploying TGI on Kubernetes.</p> <p>GitHub - louis030195/text-generation-inference-helm</p> <p>Contribute to louis030195/text-generation-inference-helm development by creating an account on GitHub.</p> <p>{width=\"5.333333333333333in\" height=\"5.333333333333333in\"}GitHublouis030195</p> <p></p> <p>Example of deploying TGI on Kubernetes</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#building-a-llm-stack-to-self-host","title":"Building a LLM stack to self-host","text":"<p>When building an LLM stack, the first hurdle you\\'ll encounter is finding the ideal stack that caters to your specific requirements. Given the multitude of available options, the decision-making process can be overwhelming. Once you\\'ve narrowed down your choices, creating and deploying a small application on a local host becomes a relatively straightforward task.</p> <p>However, scaling said application presents an entirely separate challenge, which requires a certain level of expertise and time. For that, you'll want to leverage some of the OS cloud-native platforms/tools we outlined above. It might make sense to use [Ray]{.underline} in some cases as it gives you an end-to-end platform to process data, train, tune, and serve your ML applications beyond LLMs.</p> <p>[OpenLLM]{.underline} is more geared towards simplicity and operates at a higher abstraction level than Ray. If your end goal is to host a [RAG]{.underline} LLM-app using langchain and/or llama-index, OpenLLM in conjunction with Yatai probably can get you there quickest. Keep in mind if you do end up going that route you'll likely compromise on flexibility as opposed to Ray.\\ \\ For a typical RAG LLM app, you want to set up a data stack alongside the model serving component where you orchestrate periodic or event-driven updates to your data as well as all the related data-mangling, creating embeddings, fine-tuning the models, etc.</p> <p>The [Plural marketplace]{.underline} offers various data stack apps that can perfectly suit your needs. Additionally, our marketplace provides document-store/retrieval optimized databases, such as [Elastic]{.underline} or Weaviate, which can be used as vector databases. Furthermore, during operations, monitoring and telemetry play a crucial role. For instance, a Grafana dashboard for your self-hosted LLM app could prove to be immensely valuable.</p> <p>If you choose to go a different route you can elect to use a proprietary managed service or SaaS solution (which doesn't come without overhead either, as it would require additional domain-specific knowledge as well.) Operating and maintaining those platforms on Kubernetes is the main overhead you'll have.</p>"},{"location":"Spanda%20Bootcamp%20Day%205%20-%20very%20raw/#plural-to-self-host-llms","title":"Plural to self-host LLMs","text":"<p>If you were to choose a solution like Plural you can focus on building your applications and not worry about the day-2 operations that come with maintaining those applications. If you are still debating between ML tooling, it could be beneficial to spin up an example architecture using Plural.</p> <p>Our platform can bridge the gap between the \"localhost\" and \"hello-world\" examples in these frameworks to scalable production-ready apps because you don't lose time on figuring out how to self-host model-hosting platforms like [Ray]{.underline} and [Yatai]{.underline}.</p> <p>[Plural]{.underline} is a solution that aims to provide a balance between self-hosting infrastructure applications within your own cloud account, seamless upgrades, and scaling.</p> <p>To learn more about how Plural works and how we are helping organizations deploy secure and scalable machine learning infrastructure on Kubernetes, reach out to our team to [schedule a demo]{.underline}.</p> <p>If you would like to test out Plural, sign up for a [free open-source account]{.underline} and get started today.</p>"}]}